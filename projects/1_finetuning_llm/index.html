<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Fine-tuning LLM for Intents | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="Custom Fine-Tuned Large Language Models for specific domain intent recognition."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/projects/1_finetuning_llm/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Fine-tuning LLM for Intents</h1> <p class="post-description">Custom Fine-Tuned Large Language Models for specific domain intent recognition.</p> </header> <article> <h1 id="fine-tuning-llm-for-intent-recognition">Fine-tuning LLM for Intent Recognition</h1> <h2 id="executive-summary">Executive Summary</h2> <p>The widespread adoption of Large Language Models (LLMs) has revolutionized natural language processing, yet generic models often struggle with highly specialized domain language and specific intent recognition. This project aimed to bridge that gap by fine-tuning open-source LLMs (specifically LLaMA 3 and Mistral 7B) on a proprietary dataset of customer interactions. The result was a specialized model capable of identifying over 150 distinct user intents with an accuracy exceeding 94%, significantly outperforming generic zero-shot baselines and reducing latency by 40% compared to using larger, api-based models.</p> <h2 id="problem-statement">Problem Statement</h2> <p>In a customer support environment handling thousands of queries daily, accurately routing requests is critical for efficiency. Traditional intent classifiers (using BERT or simpler ML models) often failed to capture nuance or handle complex, multi-intent queries. Generic LLMs were too slow and costly for real-time classification, and often “hallucinated” categories not in our taxonomy. We needed a solution that combined the reasoning capabilities of an LLM with the strict adherence to a defined intent schema required for enterprise routing.</p> <h2 id="methodology">Methodology</h2> <h3 id="1-data-curation-and-synthesis">1. Data Curation and Synthesis</h3> <ul> <li> <strong>Historical Data</strong>: Aggregated 500,000+ historical chat logs, manually labeled by support agents.</li> <li> <strong>Synthetic Generation</strong>: Used GPT-4 to generate synthetic variations of rare intents to balance the dataset, employing “chain-of-thought” prompting to ensure diversity in phrasing while maintaining semantic integrity.</li> <li> <strong>Cleaning</strong>: Implemented rigorous deduplication and PII redaction pipelines using scrubadub and custom regex filters.</li> </ul> <h3 id="2-model-selection-and-architecture">2. Model Selection and Architecture</h3> <ul> <li> <strong>Base Models</strong>: Evaluated LLaMA 3 (8B) and Mistral (7B) for their balance of performance and inference speed.</li> <li> <strong>LoRA (Low-Rank Adaptation)</strong>: Utilized LoRA to fine-tune only a small subset of parameters (approx. 1-2%), significantly reducing computational requirements while maintaining the base model’s general linguistic capabilities.</li> <li> <strong>Config</strong>: <ul> <li>Rank (r): 64</li> <li>Alpha: 16</li> <li>Dropout: 0.1</li> <li>Quantization: 4-bit (QLoRA) for training on single A100 GPUs.</li> </ul> </li> </ul> <h3 id="3-training-process">3. Training Process</h3> <ul> <li> <strong>Instruction Tuning</strong>: Formatted data into <code class="language-plaintext highlighter-rouge">(System, User, Assistant)</code> tuples, where the system prompt defined the taxonomy and the assistant output was the JSON-structured intent.</li> <li> <strong>Hyperparameters</strong>: <ul> <li>Learning rate: 2e-4</li> <li>Batch size: 128 (with accumulation)</li> <li>Epochs: 3 (early stopping based on validation loss)</li> </ul> </li> <li> <strong>Framework</strong>: Utilized the <code class="language-plaintext highlighter-rouge">Axolotl</code> library and <code class="language-plaintext highlighter-rouge">Unsloth</code> for optimized training speeds (2x faster than standard HuggingFace Trainer).</li> </ul> <h2 id="implementation-details">Implementation Details</h2> <p>The fine-tuning pipeline was containerized using Docker and orchestrated via Kubernetes (EKS).</p> <ul> <li> <strong>Inference Server</strong>: Deployed the fine-tuned model using <code class="language-plaintext highlighter-rouge">vLLM</code> (Versatile Large Language Model) serving engine, which utilizes PagedAttention to maximize throughput.</li> <li> <strong>Quantization</strong>: The final model was served in 4-bit quantization using AWQ (Activation-aware Weight Quantization) to fit within 24GB VRAM consumer-grade GPUs for cost efficiency.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Sample Inference Code using vLLM
</span><span class="kn">from</span> <span class="n">vllm</span> <span class="kn">import</span> <span class="n">LLM</span><span class="p">,</span> <span class="n">SamplingParams</span>

<span class="n">llm</span> <span class="o">=</span> <span class="nc">LLM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">models/llama-3-8b-intent-finetuned-v1</span><span class="sh">"</span><span class="p">,</span> <span class="n">quantization</span><span class="o">=</span><span class="sh">"</span><span class="s">awq</span><span class="sh">"</span><span class="p">)</span>
<span class="n">sampling_params</span> <span class="o">=</span> <span class="nc">SamplingParams</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>

<span class="n">outputs</span> <span class="o">=</span> <span class="n">llm</span><span class="p">.</span><span class="nf">generate</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="n">sampling_params</span><span class="p">)</span>
</code></pre></div></div> <h2 id="results-and-impact">Results and Impact</h2> <ul> <li> <strong>Accuracy</strong>: Achieved 94.2% F1-score on the held-out test set, a 12% improvement over the previous BERT-based baseline.</li> <li> <strong>Latency</strong>: Average latency per request was ~150ms on an A10G GPU, making it suitable for real-time chat applications.</li> <li> <strong>Cost</strong>: Reduced operational costs by 70% compared to using GPT-3.5-Turbo API calls for the same volume of requests.</li> </ul> <h2 id="future-work">Future Work</h2> <p>Future iterations will focus on:</p> <ul> <li> <strong>Direct Preference Optimization (DPO)</strong>: Further aligning the model’s outputs with human preference data to reduce hallucinations.</li> <li> <strong>Continuous Learning</strong>: Implementing a feedback loop where corrected predictions from human agents are automatically added to the training dataset for weekly model updates.</li> </ul> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>