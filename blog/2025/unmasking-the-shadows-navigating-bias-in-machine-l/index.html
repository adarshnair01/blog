<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Unmasking the Shadows: Navigating Bias in Machine Learning's Mirror | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/unmasking-the-shadows-navigating-bias-in-machine-l/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Unmasking the Shadows: Navigating Bias in Machine Learning's Mirror</h1> <p class="post-meta"> Created on May 20, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/ai-ethics"> <i class="fa-solid fa-hashtag fa-sm"></i> AI Ethics</a>   <a href="/blog/blog/tag/data-bias"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Bias</a>   <a href="/blog/blog/tag/fairness"> <i class="fa-solid fa-hashtag fa-sm"></i> Fairness</a>   <a href="/blog/blog/tag/responsible-ai"> <i class="fa-solid fa-hashtag fa-sm"></i> Responsible AI</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>As a data scientist, I’ve spent countless hours wrestling with data, building models, and chasing that elusive ‘perfect’ prediction. There’s a certain thrill in watching an algorithm learn, generalize, and make decisions. But over time, a deeper, more profound realization has taken root: our powerful AI systems, far from being objective truth-tellers, often serve as mirrors, reflecting not just the data we feed them, but also the biases, assumptions, and imperfections of the human world that created that data.</p> <p>This isn’t a problem of malicious intent; it’s far more insidious. It’s about <strong>bias in machine learning</strong>, a silent yet powerful force that can subtly skew results, perpetuate discrimination, and erode trust in the very technology we hope will improve our lives. If we, as builders of this future, don’t understand these biases, we risk coding inequality into the fabric of our digital world.</p> <h3 id="what-exactly-is-bias-in-machine-learning">What Exactly is Bias in Machine Learning?</h3> <p>When I first encountered the term “bias,” my mind went to its statistical definition: a systematic deviation from the true value. For example, a measurement tool consistently giving readings that are 2 units too high has a positive bias. In machine learning, this statistical concept often intertwines with a more human, societal interpretation: prejudice or unfairness against a person or group, often in a way considered unfair.</p> <p>So, when we talk about bias in ML, we’re referring to a systematic error that causes a model to produce prejudiced or unfair outcomes, often disadvantaging certain demographic groups (like based on race, gender, age, socioeconomic status, etc.). It’s not about an algorithm ‘deciding’ to be unfair; it’s about the patterns it learned from the data, or the way it was designed, leading to disproportionately negative impacts on specific populations.</p> <p>It struck me that this isn’t some rare edge case; it’s an inherent challenge because ML models learn from historical data, which itself can be a repository of historical human biases.</p> <h3 id="the-roots-of-the-problem-where-does-bias-creep-in">The Roots of the Problem: Where Does Bias Creep In?</h3> <p>Understanding where bias originates is the first crucial step towards addressing it. I’ve categorized the main sources into three interconnected areas:</p> <h4 id="1-data-bias-the-echoes-of-our-past">1. Data Bias: The Echoes of Our Past</h4> <p>This is, by far, the most common and pervasive source. Our models are only as good as the data we feed them, and if that data is flawed, biased, or incomplete, the model will faithfully learn and perpetuate those flaws.</p> <ul> <li> <strong>Historical Bias:</strong> Perhaps the most challenging. This occurs when historical data reflects past societal prejudices. Imagine using historical arrest data to predict future crime hotspots. If certain neighborhoods were historically over-policed due to racial bias, the model will learn this historical pattern and recommend continued over-policing of those same neighborhoods, perpetuating the cycle.</li> <li> <strong>Selection Bias:</strong> This happens when the data collected doesn’t accurately represent the real-world population or scenario it’s meant to model. <ul> <li> <em>Sampling Bias:</em> If you’re building a facial recognition system but your training data consists predominantly of light-skinned individuals, the system will naturally perform poorly on individuals with darker skin tones.</li> <li> <em>Self-Selection Bias:</em> Think about online surveys. People who choose to participate might share certain characteristics that differentiate them from the general population.</li> </ul> </li> <li> <strong>Measurement Bias:</strong> Inconsistent or inaccurate data collection can introduce bias. For example, if sensors used to collect medical data are less accurate for certain body types or skin complexions, the model trained on that data will inherit those measurement inaccuracies, leading to biased diagnoses.</li> <li> <strong>Annotation Bias:</strong> Often, human annotators label data (e.g., categorizing images, transcribing audio). If these annotators hold unconscious biases, they can embed them directly into the training labels. For instance, annotators might label job candidates with certain names or backgrounds as ‘less qualified’ based on stereotypes, even if their resumes are identical.</li> <li> <strong>Reporting Bias:</strong> This occurs when certain outcomes or information are more likely to be reported or recorded than others. For example, negative reviews might be more common for certain products than positive ones, skewing sentiment analysis.</li> </ul> <h4 id="2-algorithmic-bias-the-developers-footprint">2. Algorithmic Bias: The Developer’s Footprint</h4> <p>While data bias is often about what the model learns, algorithmic bias is about how the model is designed and optimized.</p> <ul> <li> <strong>Feature Selection Bias:</strong> The features we choose to include (or exclude) in our model can introduce bias. If we accidentally include a feature that is a proxy for a sensitive attribute (like zip code acting as a proxy for race or socioeconomic status), the model might inadvertently use that proxy to discriminate.</li> <li> <strong>Algorithm Design &amp; Optimization Bias:</strong> The choice of algorithm itself, the loss function, and the optimization process can amplify existing biases. For instance, an algorithm optimized purely for overall accuracy might ignore the fact that it performs extremely poorly for a minority group, deeming the error rate for that group an acceptable trade-off for higher overall performance. A common loss function, mean squared error (MSE), treats all errors equally, which might not be desirable when fairness across groups is a concern.</li> </ul> <h4 id="3-interaction-bias-the-feedback-loop">3. Interaction Bias: The Feedback Loop</h4> <p>This type of bias emerges when users interact with the system, inadvertently creating feedback loops that reinforce existing biases.</p> <ul> <li>Imagine a search engine that, due to historical patterns, initially shows more male candidates for “engineer” searches. Users click on these results more often, reinforcing the algorithm’s belief that male engineers are more relevant, thereby further entrenching the bias. It’s a self-fulfilling prophecy.</li> </ul> <h3 id="stories-from-the-real-world-when-bias-harms">Stories from the Real World: When Bias Harms</h3> <p>The impact of these biases isn’t theoretical; it manifests in tangible, often harmful ways:</p> <ul> <li> <strong>Facial Recognition Systems:</strong> Studies have repeatedly shown that many commercial facial recognition systems perform significantly worse on women and people of color, particularly darker-skinned individuals. This can lead to wrongful arrests, misidentification, and disproportionate surveillance.</li> <li> <strong>Hiring Algorithms:</strong> Amazon famously scrapped an AI recruiting tool after it was found to discriminate against women. The model penalized resumes that included the word “women’s” (as in “women’s chess club”) because it had been trained on historical hiring data dominated by male applicants.</li> <li> <strong>Credit Scoring:</strong> AI systems used for credit decisions can inadvertently penalize individuals from certain socioeconomic backgrounds or neighborhoods, even without explicitly using race or gender, by relying on proxy features that correlate with these sensitive attributes.</li> <li> <strong>Healthcare Diagnostics:</strong> Algorithms designed to diagnose diseases like skin cancer have been found to be less accurate for darker skin tones due to insufficient representation in training datasets, leading to potential misdiagnoses and health disparities.</li> </ul> <p>These examples underscore why understanding and mitigating bias isn’t just a technical exercise; it’s a social responsibility.</p> <h3 id="why-it-matters-deeply-the-ripple-effect">Why It Matters Deeply: The Ripple Effect</h3> <p>The consequences of biased ML systems are far-reaching:</p> <ul> <li> <strong>Ethical Implications:</strong> Bias leads to unfairness, discrimination, and the reinforcement of harmful stereotypes, violating principles of justice and equality.</li> <li> <strong>Societal Impact:</strong> It can perpetuate systemic inequalities, limit access to opportunities (jobs, loans, housing), and erode public trust in AI, leading to a rejection of potentially beneficial technologies.</li> <li> <strong>Business Impact:</strong> Biased models can lead to financial losses due to poor decision-making, reputational damage, and legal challenges. Regulatory bodies globally are increasingly scrutinizing AI fairness.</li> </ul> <h3 id="fighting-the-shadows-detecting-and-mitigating-bias">Fighting the Shadows: Detecting and Mitigating Bias</h3> <p>As a data scientist, I believe we have a critical role to play in building a more equitable AI future. This involves a multi-pronged approach, tackling bias at every stage of the ML lifecycle.</p> <h4 id="1-before-training-data-pre-processing-is-key">1. Before Training: Data Pre-processing is Key</h4> <p>This is where the battle truly begins. Proactive data management is our most potent weapon.</p> <ul> <li> <strong>Data Auditing and Exploration:</strong> This means meticulously examining our datasets. Are all demographic groups adequately represented? Are there imbalances in features or labels? Visualizations and statistical analysis (e.g., comparing distributions of features across different sensitive groups) are invaluable here. We look for signs of underrepresentation or skewed distributions.</li> <li> <table> <tbody> <tr> <td> <strong>Fairness Metrics in Data:</strong> We can use metrics to quantify data imbalance or bias. For example, if we have a binary classification task and a sensitive attribute $S$ (like gender), we can check the base rates of the positive class for different groups: $P(Y=1</td> <td>S=female)$ vs. $P(Y=1</td> <td>S=male)$.</td> </tr> </tbody> </table> </li> <li> <strong>Data Augmentation and Re-sampling:</strong> If certain groups are underrepresented, we can augment their data or strategically re-sample the dataset to achieve better balance. This might involve oversampling minority classes or synthesizing new data points.</li> <li> <strong>Feature Engineering with Care:</strong> Be mindful of features that could act as proxies for sensitive attributes. Consider removing them if necessary, or transforming them to reduce their discriminatory potential.</li> </ul> <h4 id="2-during-training-building-fairer-models">2. During Training: Building Fairer Models</h4> <p>Even with perfectly balanced data (which is rarely the case), algorithmic design can still introduce or amplify bias.</p> <ul> <li> <strong>Fairness-Aware Algorithms:</strong> Some algorithms are specifically designed with fairness constraints. These might involve adding a regularization term to the standard loss function. For example, a common approach is to add a penalty for unfairness: $L_{total} = L_{standard} + \lambda L_{fairness}$ where $L_{standard}$ is the usual loss (e.g., cross-entropy), $L_{fairness}$ is a term that quantifies bias, and $\lambda$ is a hyperparameter balancing accuracy and fairness.</li> <li> <strong>Fairness-Aware Loss Functions:</strong> Instead of just optimizing for overall accuracy, we can design loss functions that ensure more equitable performance across different groups.</li> <li> <strong>Group-Wise Performance Evaluation:</strong> During training, we don’t just look at overall metrics. We break down metrics like accuracy, precision, recall, and F1-score by sensitive groups (e.g., male vs. female, different racial groups) to identify disparities.</li> <li> <strong>Addressing Trade-offs:</strong> Sometimes, there’s a trade-off between maximizing overall accuracy and achieving perfect fairness. We need to consciously decide where to draw the line, understanding the societal implications of that choice.</li> </ul> <h4 id="3-after-training-monitoring-and-explainability">3. After Training: Monitoring and Explainability</h4> <p>The job isn’t done once the model is deployed.</p> <ul> <li> <strong>Model Explainability (XAI):</strong> Tools like SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic Explanations) help us understand <em>why</em> a model made a specific decision. This can uncover unexpected biases the model might have learned. For example, SHAP values can show if a sensitive feature or its proxy is disproportionately influencing predictions for certain individuals.</li> <li> <strong>Continuous Monitoring:</strong> Bias can emerge or shift over time as data distributions change in the real world. Regularly monitoring model performance across different demographic groups in production is essential to catch emergent biases.</li> <li> <strong>Human-in-the-Loop:</strong> For high-stakes decisions, human oversight remains crucial. AI can assist, but the ultimate decision-making power should sometimes rest with a human who can apply context, empathy, and ethical reasoning.</li> <li> <strong>Retraining and Feedback Loops:</strong> Establish mechanisms to collect feedback, identify biased outcomes, and retrain models with updated, debiased data or improved algorithms.</li> </ul> <h3 id="the-unfolding-challenge-defining-fairness">The Unfolding Challenge: Defining Fairness</h3> <p>One of the biggest challenges I’ve encountered is that “fairness” isn’t a single, universally agreed-upon definition. What might be considered fair in one context could be unfair in another.</p> <ul> <li> <table> <tbody> <tr> <td> <strong>Demographic Parity:</strong> Requires that the proportion of positive outcomes ($\hat{Y}=1$) is equal across different groups ($S=s_1, S=s_2$). Mathematically, $P(\hat{Y}=1</td> <td>S=s_1) = P(\hat{Y}=1</td> <td>S=s_2)$. This means, for example, the same percentage of males and females get a loan.</td> </tr> </tbody> </table> </li> <li> <strong>Equalized Odds:</strong> Requires that true positive rates and false positive rates are equal across groups.</li> <li> <strong>Predictive Parity:</strong> Requires that the precision (positive predictive value) is equal across groups.</li> </ul> <p>Often, you can’t satisfy all these definitions of fairness simultaneously, leading to difficult ethical and technical trade-offs. This highlights the need for interdisciplinary collaboration – bringing together data scientists, ethicists, sociologists, and policymakers to define what fairness means in specific contexts.</p> <h3 id="conclusion-our-shared-responsibility">Conclusion: Our Shared Responsibility</h3> <p>The journey to building truly fair and unbiased machine learning systems is complex and ongoing. It requires more than just technical prowess; it demands a deep understanding of societal dynamics, ethical considerations, and a commitment to continuous learning and improvement.</p> <p>As data scientists and machine learning engineers, we hold immense power in shaping the future. With that power comes a profound responsibility. We must move beyond simply optimizing for accuracy and efficiency. We must actively seek out and dismantle biases in our data and algorithms. We must ask ourselves not just “Can we build it?” but “Should we build it?” and “Is it fair to everyone?”</p> <p>By embracing these challenges, fostering transparency, and collaborating across disciplines, we can steer machine learning away from reflecting our imperfections and towards building a future that is more equitable, inclusive, and truly intelligent for all. The mirror of AI can, and must, reflect a better version of ourselves.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>