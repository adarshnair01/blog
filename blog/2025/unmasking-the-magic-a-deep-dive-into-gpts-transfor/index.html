<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Unmasking the Magic: A Deep Dive into GPT's Transformer Architecture | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/unmasking-the-magic-a-deep-dive-into-gpts-transfor/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Unmasking the Magic: A Deep Dive into GPT's Transformer Architecture</h1> <p class="post-meta"> Created on December 04, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> NLP</a>   <a href="/blog/blog/tag/transformers"> <i class="fa-solid fa-hashtag fa-sm"></i> Transformers</a>   <a href="/blog/blog/tag/gpt"> <i class="fa-solid fa-hashtag fa-sm"></i> GPT</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hello fellow data enthusiasts and aspiring AI builders!</p> <p>If you’ve spent any time online recently, you’ve undoubtedly encountered the incredible capabilities of GPT-powered models. From writing essays to debugging code, generating creative stories, or simply answering complex questions, these models often feel like magic. But as any good scientist knows, magic is just science we don’t understand yet. Today, we’re going to demystify some of that “magic” by taking a deep dive into the core architecture that makes GPT tick: the Transformer.</p> <p>Even if you’re new to the world of deep learning, I’ll walk you through the concepts step-by-step. Think of this as our personal journal entry into understanding one of the most impactful breakthroughs in AI history.</p> <h3 id="the-genesis-why-transformers">The Genesis: Why Transformers?</h3> <p>Before GPT, recurrent neural networks (RNNs) and their more sophisticated cousins, Long Short-Term Memory networks (LSTMs), were the go-to for sequence data like text. They processed words one by one, maintaining a “memory” of previous words. This worked, but had a few major drawbacks:</p> <ol> <li> <strong>Slow Training:</strong> The sequential nature meant they couldn’t process words in parallel.</li> <li> <strong>Long-Range Dependencies:</strong> Remembering information from many steps ago was hard, leading to what’s called the “vanishing gradient problem.”</li> </ol> <p>Enter the <strong>Transformer</strong> in 2017, with its groundbreaking paper “Attention Is All You Need.” It completely revolutionized how we handle sequences. The core idea? Instead of sequential processing, let the model look at <em>all</em> words in a sentence simultaneously and decide which ones are most relevant to each other – this is “attention.”</p> <p>GPT, standing for <strong>Generative Pre-trained Transformer</strong>, takes this Transformer architecture and adapts it specifically for generation tasks. Unlike the original Transformer, which has both an encoder (for understanding input) and a decoder (for generating output), GPT models are <strong>decoder-only</strong> architectures. This means they are inherently designed for generating sequences, one token at a time, based on everything they’ve seen before.</p> <h3 id="deconstructing-the-gpt-decoder-block">Deconstructing the GPT Decoder Block</h3> <p>At its heart, GPT is a stack of identical “decoder blocks.” Each block processes the input and passes its refined representation to the next. Let’s break down what’s inside a single decoder block.</p> <h4 id="1-input-embeddings-and-positional-encoding-giving-words-meaning-and-order">1. Input Embeddings and Positional Encoding: Giving Words Meaning and Order</h4> <p>Before any computation can happen, our words need to be converted into a numerical format that the model can understand.</p> <ul> <li> <p><strong>Token Embeddings:</strong> Each word (or sub-word, called a “token”) in our vocabulary gets mapped to a high-dimensional vector. Words with similar meanings will have similar vectors. These embeddings aren’t pre-defined; the model <em>learns</em> them during training. It’s like teaching the model the nuances of language from scratch!</p> </li> <li> <p><strong>Positional Encoding:</strong> Transformers process all words at once. This means they lose the crucial information about word order. “Dog bites man” is very different from “Man bites dog.” To reintroduce this order, we add a “positional encoding” vector to each token’s embedding. This encoding provides information about the token’s absolute or relative position in the sequence. A common method uses sine and cosine functions:</p> <p>$PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{model}})$ $PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d_{model}})$</p> <p>Here, $pos$ is the token’s position, $i$ is the dimension of the embedding vector, and $d_{model}$ is the dimensionality of the embedding. This clever approach allows the model to infer relative positions between tokens.</p> </li> </ul> <p>So, the actual input to our first decoder block is the sum of the token embedding and its positional encoding.</p> <h4 id="2-the-star-of-the-show-masked-multi-head-self-attention">2. The Star of the Show: Masked Multi-Head Self-Attention</h4> <p>This is where the magic truly happens. Self-attention allows each word in a sequence to “look” at all other words and determine their relevance. For example, in “The animal didn’t cross the street because <em>it</em> was too tired,” self-attention helps the model understand that “it” refers to “the animal.”</p> <p>The core idea involves three learned linear projections (fancy matrix multiplications) of our input embeddings:</p> <ul> <li> <strong>Query (Q):</strong> What am I looking for? (The current word’s perspective)</li> <li> <strong>Key (K):</strong> What do I have? (Other words’ perspectives)</li> <li> <strong>Value (V):</strong> What information do I carry? (The actual content of other words)</li> </ul> <p>The attention score is calculated by taking the dot product of the Query with all Keys, scaling it, and then applying a softmax function to get attention weights. These weights tell us how much each other word’s Value should contribute to the current word’s new representation.</p> <p>$\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$</p> <p>The division by $\sqrt{d_k}$ (the square root of the dimension of the keys) is a scaling factor to prevent the dot products from becoming too large, which could push the softmax into regions with very small gradients.</p> <p><strong>Masked Attention (Crucial for GPT):</strong> Since GPT is generative and predicts the <em>next</em> word, it’s vital that it doesn’t “cheat” by looking at future words during training. This is where <strong>masked self-attention</strong> comes in. We apply a “mask” (typically by setting future word scores to negative infinity before the softmax) to prevent any token from attending to subsequent tokens in the sequence. It’s like putting blinders on the model, forcing it to predict purely based on past context.</p> <p><strong>Multi-Head Attention:</strong> Instead of just one set of Q, K, V projections, we have multiple independent “attention heads.” Each head learns to focus on different aspects of the relationships between words. For instance, one head might focus on grammatical dependencies, while another might capture semantic relationships. The outputs from all heads are then concatenated and linearly transformed to produce the final output for the multi-head attention layer. This enriches the model’s ability to capture diverse patterns.</p> <h4 id="3-feed-forward-networks-ffn-adding-non-linearity">3. Feed-Forward Networks (FFN): Adding Non-Linearity</h4> <p>After the attention mechanism, the output of the multi-head attention layer passes through a simple, point-wise feed-forward network. This is essentially two linear transformations with a non-linear activation function (like ReLU or GELU) in between. It processes each position independently and identically. Its job is to introduce non-linearity and allow the model to learn more complex patterns in the data.</p> <h4 id="4-add--normalize-stability-and-deeper-learning">4. Add &amp; Normalize: Stability and Deeper Learning</h4> <p>Two important techniques are applied repeatedly within each decoder block:</p> <ul> <li> <p><strong>Residual Connections (Add):</strong> The output of each sub-layer (attention, FFN) is added to its input. This is represented as $x + \text{Sublayer}(x)$. This “skip connection” helps mitigate the vanishing gradient problem in deep networks, allowing information to flow more directly through the layers and making it easier to train very deep models.</p> </li> <li> <p><strong>Layer Normalization (Normalize):</strong> After the residual connection, Layer Normalization is applied. Unlike Batch Normalization which normalizes across the batch for each feature, Layer Normalization normalizes the features <em>within each individual sample</em>. This helps stabilize training by keeping the activations within a reasonable range and speeds up convergence.</p> </li> </ul> <h3 id="putting-it-all-together-the-gpt-stack">Putting It All Together: The GPT Stack</h3> <p>A GPT model is constructed by stacking many of these identical decoder blocks on top of each other. The output of one block becomes the input to the next. The initial input (embeddings + positional encodings) goes into the first block, and the final output of the last block is then fed into a final linear layer and a softmax function. This output layer’s job is to predict the probability distribution over the entire vocabulary for the <em>next token</em>.</p> <h3 id="how-gpt-learns-pre-training-and-prediction">How GPT Learns: Pre-training and Prediction</h3> <p>The sheer scale of GPT’s pre-training is astounding. Models like GPT-3 were trained on hundreds of billions of tokens from diverse internet texts. The primary pre-training objective is elegantly simple: <strong>next token prediction</strong>.</p> <p>Given a sequence of words, the model is trained to predict the next word. It does this by calculating the probability of each word in its vocabulary being the next one in the sequence. During inference, it samples a word based on these probabilities, adds it to the sequence, and repeats the process, generating text word by word.</p> <h3 id="why-gpt-is-a-game-changer">Why GPT is a Game-Changer</h3> <ol> <li> <strong>Massive Scale:</strong> The number of parameters (ranging from millions to hundreds of billions) allows GPT models to learn incredibly complex patterns and store vast amounts of “knowledge.”</li> <li> <strong>Attention Mechanism:</strong> The Transformer’s attention mechanism efficiently captures long-range dependencies, allowing GPT to maintain context over very long texts.</li> <li> <strong>Generative Pre-training:</strong> The unsupervised nature of pre-training on massive text corpora allows the model to learn grammar, facts, reasoning abilities, and even some world knowledge without explicit labels.</li> <li> <strong>Emergent Abilities:</strong> As models scale, they often exhibit “emergent abilities” – capabilities not explicitly trained for, like common-sense reasoning or in-context learning, simply arising from the extensive pre-training.</li> </ol> <h3 id="wrapping-up-our-journey">Wrapping Up Our Journey</h3> <p>Phew! We’ve covered a lot of ground today. From the foundational shift from RNNs to Transformers, through the intricacies of positional encoding, the magic of masked multi-head self-attention, and the stabilizing forces of residual connections and layer normalization, you now have a solid understanding of the architectural genius behind GPT.</p> <p>It’s a testament to human ingenuity that such a seemingly simple set of building blocks, scaled to an unprecedented degree, can yield such astonishing intelligence. The journey to truly understand and harness AI is just beginning, and understanding architectures like GPT’s Transformer is a crucial step.</p> <p>I hope this “journal entry” has illuminated some of the core concepts for you. Keep exploring, keep asking “how does that work?”, and keep building! The world of AI is yours to discover.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>