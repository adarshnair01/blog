<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Unmasking Bias: How Our Machines Learn Our Flaws (And What We Can Do About It) | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/unmasking-bias-how-our-machines-learn-our-flaws-an/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Unmasking Bias: How Our Machines Learn Our Flaws (And What We Can Do About It)</h1> <p class="post-meta"> Created on August 12, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/ai-ethics"> <i class="fa-solid fa-hashtag fa-sm"></i> AI Ethics</a>   <a href="/blog/blog/tag/data-bias"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Bias</a>   <a href="/blog/blog/tag/algorithmic-fairness"> <i class="fa-solid fa-hashtag fa-sm"></i> Algorithmic Fairness</a>   <a href="/blog/blog/tag/responsible-ai"> <i class="fa-solid fa-hashtag fa-sm"></i> Responsible AI</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hello fellow data adventurers!</p> <p>As I’ve journeyed through the intricate landscapes of data science and machine learning, one concept has repeatedly emerged as both profoundly important and deeply complex: <strong>bias</strong>. It’s not just a technical glitch; it’s a mirror reflecting our own human societies, flaws and all, back into the artificial intelligence we build.</p> <p>Imagine teaching a child about the world. If you only show them pictures of doctors who are men, or nurses who are women, what will their mental model of these professions become? They’ll develop a biased understanding, not because they’re inherently prejudiced, but because their training data was skewed. Our machine learning models are no different. They learn from the data we feed them, and if that data is biased, the models will inevitably become biased too.</p> <h3 id="what-exactly-is-bias-in-machine-learning">What Exactly <em>Is</em> Bias in Machine Learning?</h3> <p>In its simplest form, <strong>bias in machine learning</strong> refers to systematic errors in a computer system’s output that lead to unfair or discriminatory outcomes. It’s a deviation from the truth or fairness.</p> <p>Statistically speaking, an estimator $\hat{\theta}$ for a true parameter $\theta$ is biased if its expected value is not equal to the true parameter: $E[\hat{\theta}] \neq \theta$. In plain language, this means our model’s predictions, on average, consistently miss the mark in a particular direction.</p> <p>But in the context of AI ethics, bias means much more than just statistical inaccuracy. It refers to the systematic and unfair discrimination against certain individuals or groups in machine learning models, often based on sensitive attributes like gender, race, age, or socioeconomic status.</p> <h3 id="the-invisible-seeds-where-does-bias-come-from">The Invisible Seeds: Where Does Bias Come From?</h3> <p>Bias isn’t something we intentionally code into our algorithms (at least, not usually!). It seeps in, often unnoticed, from various stages of the machine learning pipeline. Let’s explore some of its most common origins:</p> <h4 id="1-data-collection--selection-bias-the-echo-chamber-effect">1. Data Collection &amp; Selection Bias: The Echo Chamber Effect</h4> <p>This is arguably the most common and potent source of bias. <strong>Selection bias</strong> occurs when the data used to train a model is not representative of the real-world population or phenomenon the model is intended to predict.</p> <ul> <li> <strong>Example</strong>: Training a facial recognition system primarily on images of people with lighter skin tones. When deployed, this system will inevitably perform poorly on individuals with darker skin tones, potentially leading to misidentification or denial of access.</li> <li> <strong>Another Example</strong>: A job application screening AI trained on historical hiring data where, historically, certain demographics were underrepresented or discriminated against. The AI will learn these historical patterns and continue to favor candidates from the historically preferred demographics, even if they aren’t objectively more qualified.</li> </ul> <p>This kind of bias is insidious because it often reflects existing societal inequalities. Our datasets are snapshots of our world, and if our world is biased, so will be the data we collect from it.</p> <h4 id="2-reporting-bias-what-gets-recorded-and-what-doesnt">2. Reporting Bias: What Gets Recorded (and What Doesn’t)</h4> <p><strong>Reporting bias</strong> arises when the frequency of certain events, properties, or outcomes is over- or under-reported in the available data. This isn’t about <em>who</em> is in the data, but <em>what</em> information is captured about them.</p> <ul> <li> <strong>Example</strong>: In medical diagnostics, if research papers or medical records disproportionately document symptoms for certain diseases in men versus women (or vice-versa), an AI trained on this data might struggle to accurately diagnose women (or men) presenting with those same symptoms.</li> </ul> <h4 id="3-automation-bias-the-trap-of-trust">3. Automation Bias: The Trap of Trust</h4> <p><strong>Automation bias</strong> is a cognitive bias where humans are more likely to trust and follow recommendations from an automated system, even when they have contradictory information or their own judgment suggests otherwise. While not directly a bias <em>within</em> the algorithm, it amplifies the impact of any algorithmic bias. If we blindly trust an AI’s biased decision, we are perpetuating that bias.</p> <h4 id="4-measurement-bias-the-flawed-ruler">4. Measurement Bias: The Flawed Ruler</h4> <p>This type of bias occurs when there are inconsistencies or errors in how features are measured across different groups.</p> <ul> <li> <strong>Example</strong>: If a smart fitness tracker consistently underestimates the calorie burn of certain types of exercise common among a particular demographic (e.g., dancing vs. running), or if sensors perform differently based on skin pigmentation, then models trained on this data will carry measurement bias.</li> </ul> <h4 id="5-algorithmic-bias-or-inductive-bias-design-choices-matter">5. Algorithmic Bias (or Inductive Bias): Design Choices Matter</h4> <p>Sometimes, the bias isn’t just in the data but in the algorithm’s design or the choices made during its development and training. This is often called <strong>inductive bias</strong>, referring to the assumptions a learning algorithm makes to generalize from training data to unseen data.</p> <ul> <li> <strong>Example</strong>: A search engine algorithm that, due to its design or optimization goals, implicitly prioritizes certain types of content or perspectives, leading to skewed search results for particular queries. Or, regularization techniques ($L_1$, $L_2$) can be seen as an inductive bias towards simpler models. If not applied carefully, they can impact fairness.</li> </ul> <h4 id="6-pre-existing-societal-bias-the-deepest-roots">6. Pre-existing (Societal) Bias: The Deepest Roots</h4> <p>This is the most pervasive and challenging form of bias, as it’s fundamentally a reflection of human prejudices, stereotypes, and inequalities present in society. These societal biases are then encoded into datasets through historical decisions, actions, and systemic discrimination.</p> <ul> <li> <strong>Example</strong>: Geographic “redlining” practices from the past, which denied services to residents of specific, often minority, neighborhoods. If an AI for loan applications learns from a dataset reflecting these historical lending patterns, it might inadvertently perpetuate discrimination against applicants from those same neighborhoods, even without explicitly using race as a feature. The zip code becomes a proxy.</li> </ul> <h3 id="real-world-scars-when-bias-hits-hard">Real-World Scars: When Bias Hits Hard</h3> <p>The consequences of biased ML systems are not theoretical; they are impacting real lives:</p> <ul> <li> <strong>Criminal Justice</strong>: Predictive policing algorithms have been shown to disproportionately target minority neighborhoods, and recidivism prediction tools have been found to assign higher risk scores to Black defendants than white defendants who committed similar crimes.</li> <li> <strong>Hiring</strong>: AI-powered resume screeners, like Amazon’s infamous failed attempt, can learn to discriminate against female candidates by penalizing keywords common in women’s resumes (e.g., “women’s chess club captain”).</li> <li> <strong>Healthcare</strong>: Algorithms used to predict health risks or allocate medical resources have been found to systematically underestimate the health needs of Black patients, leading to less access to critical care programs.</li> <li> <strong>Financial Services</strong>: Loan approval or credit scoring algorithms can perpetuate historical biases, making it harder for certain demographics to access credit.</li> </ul> <p>These aren’t just minor inconveniences; they can mean the difference between freedom and incarceration, getting a job or being overlooked, receiving life-saving medical care or being denied.</p> <h3 id="fighting-back-strategies-to-detect-and-mitigate-bias">Fighting Back: Strategies to Detect and Mitigate Bias</h3> <p>Addressing bias isn’t a one-time fix; it’s an ongoing, multi-faceted process that requires vigilance at every stage of the ML lifecycle.</p> <h4 id="1-data-centric-approaches-the-foundation-of-fairness">1. Data-Centric Approaches: The Foundation of Fairness</h4> <ul> <li> <strong>Careful Data Collection &amp; Auditing</strong>: Proactively design data collection strategies to ensure diverse and representative samples. Regularly audit datasets for imbalances, missing values, and potential proxies for sensitive attributes.</li> <li> <strong>Data Augmentation &amp; Re-sampling</strong>: For underrepresented groups, techniques like data augmentation (generating synthetic examples) or re-sampling (oversampling minority classes, undersampling majority classes) can help balance the dataset.</li> <li> <strong>Fairness Metrics</strong>: Quantify bias using statistical fairness metrics. Instead of just focusing on accuracy, we might look at: <ul> <li> <table> <tbody> <tr> <td> <strong>Demographic Parity</strong>: This aims for the model to make positive predictions at roughly the same rate across different groups. Mathematically, for a protected attribute $A$ (e.g., gender, race) and a positive prediction $\hat{Y}=1$, we want $P(\hat{Y}=1</td> <td>A=a_1) \approx P(\hat{Y}=1</td> <td>A=a_2)$ for different groups $a_1$ and $a_2$.</td> </tr> </tbody> </table> </li> <li> <strong>Equalized Odds</strong>: This ensures that true positive rates and false positive rates are equal across different groups. This is crucial in high-stakes applications where misclassification has significant consequences.</li> <li> <table> <tbody> <tr> <td> <strong>Equal Opportunity</strong>: A simpler variant of Equalized Odds, focused on ensuring equal true positive rates for different groups: $P(\hat{Y}=1</td> <td>A=a_1, Y=1) \approx P(\hat{Y}=1</td> <td>A=a_2, Y=1)$. This means the model is equally good at identifying positive cases (e.g., correctly approving a loan applicant) across groups.</td> </tr> </tbody> </table> </li> </ul> </li> <li> <strong>Adversarial Debiasing</strong>: Train a primary model to perform its task (e.g., classification) and simultaneously train an “adversary” model to predict the protected attribute from the primary model’s representations. The primary model is then encouraged to make predictions without revealing information about the protected attribute to the adversary, thus making it “fairer.”</li> </ul> <h4 id="2-model-centric-approaches-building-fairer-algorithms">2. Model-Centric Approaches: Building Fairer Algorithms</h4> <ul> <li> <strong>Regularization &amp; Constraints</strong>: During training, add regularization terms to the loss function that penalize disparate impact or unfairness, alongside accuracy.</li> <li> <strong>Pre-processing, In-processing, Post-processing</strong>: <ul> <li> <strong>Pre-processing</strong>: Modify the input data before training (e.g., re-weighting samples, relabeling).</li> <li> <strong>In-processing</strong>: Incorporate fairness constraints directly into the model training algorithm (e.g., adversarial debiasing).</li> <li> <strong>Post-processing</strong>: Adjust the model’s predictions after training (e.g., threshold adjustment for different groups).</li> </ul> </li> </ul> <h4 id="3-human-centric-approaches-the-ethical-imperative">3. Human-Centric Approaches: The Ethical Imperative</h4> <ul> <li> <strong>Diverse Teams</strong>: Encourage diverse perspectives in AI development teams. Different backgrounds lead to different questions, assumptions, and blind spots being identified.</li> <li> <strong>Transparency &amp; Explainability (XAI)</strong>: Develop models that can explain their decisions. Understanding <em>why</em> a model made a particular prediction can help uncover underlying biases.</li> <li> <strong>Continuous Auditing &amp; Monitoring</strong>: Deploying a model isn’t the end. Real-world data can change, and new biases can emerge. Regular monitoring and auditing of model performance across different demographic groups are essential.</li> <li> <strong>Ethical AI Guidelines &amp; Regulations</strong>: Establish clear ethical guidelines and, where necessary, regulatory frameworks to govern the development and deployment of AI.</li> </ul> <h3 id="the-ethical-horizon-beyond-the-code">The Ethical Horizon: Beyond the Code</h3> <p>Addressing bias in ML isn’t just a technical challenge; it’s a profound ethical and societal one. It forces us to confront uncomfortable truths about our own world. As data scientists and machine learning engineers, we hold immense power. The models we build can amplify existing inequalities or, conversely, become tools for positive social change.</p> <p>The goal isn’t necessarily to create “perfectly unbiased” AI, which might be an impossible ideal given that our world isn’t perfectly unbiased. Instead, the goal is to build <strong>responsible AI</strong>: systems that are aware of their potential biases, strive to mitigate them, and are transparent about their limitations. It’s about ensuring fairness, accountability, and ultimately, building a future where technology uplifts everyone, not just a privileged few.</p> <p>So, as you dive deeper into the fascinating world of machine learning, remember that the numbers and algorithms are only part of the story. The human impact, the ethical considerations, and the constant pursuit of fairness are just as, if not more, important. Let’s commit to building AI that reflects the best of humanity, not its prejudices.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>