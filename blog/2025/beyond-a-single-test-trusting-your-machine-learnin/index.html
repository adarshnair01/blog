<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Beyond a Single Test: Trusting Your Machine Learning Models with Cross-Validation | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/beyond-a-single-test-trusting-your-machine-learnin/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Beyond a Single Test: Trusting Your Machine Learning Models with Cross-Validation</h1> <p class="post-meta"> Created on April 04, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/model-evaluation"> <i class="fa-solid fa-hashtag fa-sm"></i> Model Evaluation</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/cross-validation"> <i class="fa-solid fa-hashtag fa-sm"></i> Cross-Validation</a>   <a href="/blog/blog/tag/python"> <i class="fa-solid fa-hashtag fa-sm"></i> Python</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>It’s a fantastic feeling, isn’t it? You’ve spent hours, maybe days, meticulously cleaning your data, selecting features, and finally, training a machine learning model. You run it on a small batch of data it hasn’t seen before, and <em>bam</em>! High accuracy, low error — you’re practically glowing. You feel like you’ve just built the next big thing, a predictive masterpiece ready to conquer the world.</p> <p>But then a tiny, nagging voice in the back of your mind whispers: “Is it <em>really</em> that good? Or am I just… lucky?”</p> <p>That little voice, my friends, is the sound of good data science intuition. It’s asking a crucial question about the <em>reliability</em> and <em>generalizability</em> of your model. And today, we’re going to dive deep into a technique that silences that voice with robust, data-driven confidence: <strong>Cross-Validation</strong>.</p> <h3 id="the-peril-of-a-single-test-why-one-and-done-isnt-enough">The Peril of a Single Test: Why “One-and-Done” Isn’t Enough</h3> <p>Before we jump into the magic of cross-validation, let’s briefly revisit the standard practice of evaluating machine learning models. Typically, we split our precious dataset into two parts:</p> <ol> <li> <strong>Training Set</strong>: The larger portion (e.g., 70-80%) that your model learns from. It’s like the textbook your model studies.</li> <li> <strong>Test Set</strong>: The smaller, untouched portion (e.g., 20-30%) that your model is evaluated on <em>after</em> training. This is like the final exam, checking if it truly understood the material.</li> </ol> <p>This train-test split is good, it’s essential, and it prevents a common pitfall called <strong>overfitting</strong>. Overfitting is when your model learns the training data <em>too well</em>, memorizing noise and specific patterns that aren’t representative of the real world. When an overfit model encounters new, unseen data (like our test set), its performance dramatically drops. It’s like acing a specific practice test but failing the real exam because the questions were phrased slightly differently.</p> <p>However, a single train-test split has its own subtle weakness: <strong>variance</strong>. What if the particular split you made just happened to be “easy” for your model? What if the test set, by pure chance, contained examples that your model found simple to predict? Or, conversely, what if it contained unusually difficult examples, leading you to believe your model is worse than it truly is?</p> <p>A single train-test split gives you <em>one estimate</em> of your model’s performance. It’s like asking only one person for a restaurant review. While their opinion is valid, it might not be representative of everyone’s experience. To truly trust a restaurant, you’d want many reviews, right?</p> <p>This is where cross-validation steps in – it’s like getting <em>many</em> reviews for your model, systematically and fairly.</p> <h3 id="enter-cross-validation-the-ultimate-model-audition">Enter Cross-Validation: The Ultimate Model Audition</h3> <p>At its core, cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample. Its primary goal is to assess how well your model will generalize to an independent dataset (i.e., real-world, unseen data). It helps us get a more stable and reliable estimate of our model’s performance by training and testing it multiple times on different subsets of the data.</p> <p>The most common and widely used form is <strong>K-Fold Cross-Validation</strong>. Let’s break down how it works step-by-step:</p> <ol> <li> <p><strong>Shuffle and Divide</strong>: First, you randomly shuffle your entire dataset to ensure no hidden order biases the splits. Then, you divide it into ‘K’ equally sized (or as equal as possible) “folds” or “partitions.”</p> <p>Imagine your dataset as a deck of cards. You shuffle it, then deal it out into K piles.</p> </li> <li> <strong>Iterate and Evaluate</strong>: Now, the magic happens. You’ll run K separate training and evaluation rounds (also called “folds” or “iterations”): <ul> <li>In the first round, you’ll pick the first fold as your <strong>test set</strong> and combine the remaining K-1 folds into your <strong>training set</strong>. You train your model on this training set and then evaluate its performance on the held-out test set. Record the performance metric (e.g., accuracy, mean squared error, F1-score).</li> <li>In the second round, you pick the <em>second</em> fold as your test set and use the other K-1 folds for training. Train, evaluate, and record.</li> <li>You continue this process K times, ensuring that <strong>each fold gets to be the test set exactly once</strong>.</li> </ul> <p>Think of it this way: if you have 5 folds (K=5):</p> <ul> <li>Round 1: Test on Fold 1, Train on Folds 2, 3, 4, 5</li> <li>Round 2: Test on Fold 2, Train on Folds 1, 3, 4, 5</li> <li>Round 3: Test on Fold 3, Train on Folds 1, 2, 4, 5</li> <li>Round 4: Test on Fold 4, Train on Folds 1, 2, 3, 5</li> <li>Round 5: Test on Fold 5, Train on Folds 1, 2, 3, 4</li> </ul> </li> <li> <p><strong>Aggregate Results</strong>: After K rounds, you’ll have K different performance scores (one from each round). To get a robust estimate of your model’s overall performance, you calculate the <strong>mean</strong> and <strong>standard deviation</strong> of these scores.</p> <p>The mean performance $\mu$ is calculated as: \(\mu = \frac{1}{K} \sum\_{i=1}^{K} P_i\) where $P_i$ is the performance metric obtained in the $i$-th fold.</p> <p>The standard deviation $\sigma$ is calculated as: \(\sigma = \sqrt{\frac{1}{K-1} \sum\_{i=1}^{K} (P_i - \mu)^2}\)</p> <p>The mean gives you a much more reliable estimate of your model’s expected performance on unseen data. The standard deviation tells you how much the performance varies across different splits. A low standard deviation means your model’s performance is consistent, which is a great sign!</p> </li> </ol> <h3 id="why-is-k-fold-cross-validation-so-awesome">Why is K-Fold Cross-Validation So Awesome?</h3> <ol> <li> <strong>Reduced Variance</strong>: It significantly reduces the variance of your performance estimate compared to a single train-test split. You’re not relying on one arbitrary split; you’re averaging across many.</li> <li> <strong>Efficient Data Usage</strong>: Every single data point eventually gets to be part of the test set <em>and</em> part of the training set. This maximizes the use of your limited data, which is especially crucial for smaller datasets.</li> <li> <strong>Robust Generalization Estimate</strong>: It provides a much better and more reliable estimate of how well your model will perform on truly unseen, real-world data. It checks for robustness against different subsets of your data.</li> <li> <strong>Hyperparameter Tuning Aid</strong>: While we’re not diving deep into it today, cross-validation is indispensable when you’re tuning hyperparameters (settings for your model that aren’t learned from data). You can use cross-validation to find the hyperparameter values that consistently yield the best average performance across all folds.</li> </ol> <h3 id="choosing-your-k-how-many-folds">Choosing Your K: How Many Folds?</h3> <p>The choice of ‘K’ is a balance between bias and variance, and computational cost:</p> <ul> <li> <strong>Common Choices</strong>: K=5 or K=10 are the most common values. <ul> <li>K=10 is generally preferred as it provides a good trade-off. It leads to 90% of data for training and 10% for testing in each fold, resulting in a low-bias estimate of performance.</li> </ul> </li> <li> <strong>Small K (e.g., K=2, 3)</strong>: Each test set would be larger, leading to a higher bias in the performance estimate (because the training sets are smaller). However, it’s faster.</li> <li> <strong>Large K (e.g., K=N, where N is the number of data points - known as Leave-One-Out Cross-Validation or LOOCV)</strong>: This means each data point is its own test set. While it provides a very low-bias estimate, it’s extremely computationally expensive (you train N models!) and often has high variance in the performance estimates because the training sets are nearly identical.</li> </ul> <h3 id="beyond-basic-k-fold-other-flavors-of-cross-validation">Beyond Basic K-Fold: Other Flavors of Cross-Validation</h3> <p>K-Fold is a fantastic general-purpose technique, but data comes in many forms, and sometimes we need specialized cross-validation strategies:</p> <ol> <li> <strong>Stratified K-Fold Cross-Validation</strong>: <ul> <li> <strong>When to use</strong>: Crucial for datasets with imbalanced classes (e.g., predicting a rare disease, fraud detection).</li> <li> <strong>How it works</strong>: It ensures that each fold has approximately the same percentage of samples of each target class as the complete set. If your dataset has 90% class A and 10% class B, stratified K-fold will try to maintain that 9:1 ratio in <em>every</em> fold. This prevents a fold from ending up with only one class, which would make training or testing impossible or highly biased.</li> </ul> </li> <li> <strong>Time Series Cross-Validation (Walk-Forward or Rolling Origin)</strong>: <ul> <li> <strong>When to use</strong>: Absolutely essential for time-series data, where the order of observations matters, and future data cannot be used to predict the past.</li> <li> <strong>How it works</strong>: Instead of random splits, you maintain the temporal order. You train on an initial segment of the data and predict the <em>next</em> segment. Then, you incrementally expand the training window by including the predicted segment and predict the next one. This mimics real-world scenarios where you predict the future based on past and present data.</li> </ul> </li> <li> <strong>Group K-Fold Cross-Validation</strong>: <ul> <li> <strong>When to use</strong>: When you have groups of data points that are related and should not be split across training and testing sets. For example, if you have multiple measurements from the same patient, you’d want all measurements from one patient to either be in the training set or the test set, but not both.</li> <li> <strong>How it works</strong>: It ensures that all samples from a specific group (e.g., all data points from ‘Patient A’) appear in only one fold.</li> </ul> </li> <li> <strong>Leave-One-Out Cross-Validation (LOOCV)</strong>: <ul> <li>As mentioned, this is K-Fold where K equals the number of samples (N). Each data point is individually used as the test set, and the remaining N-1 points are used for training. High computational cost, often high variance, but nearly unbiased.</li> </ul> </li> </ol> <h3 id="putting-it-all-together-a-mental-sandbox">Putting it All Together: A Mental Sandbox</h3> <p>Imagine you’re building a model to predict student performance in a challenging course. You have data from previous semesters: grades, study hours, attendance, etc.</p> <p>If you just do a simple train-test split, you might get an 85% accuracy. But what if that particular test set happened to include students who were all high-achievers? Your model might look great, but it could perform terribly on a test set of average students.</p> <p>With <strong>K-Fold Cross-Validation (K=5)</strong>:</p> <ol> <li>You divide your student data into 5 groups.</li> <li>In Round 1, you train on 4 groups and test on the 1st group. Maybe you get 82% accuracy.</li> <li>In Round 2, you train on the other 4 groups (including the 1st group now!) and test on the 2nd group. Maybe you get 79% accuracy.</li> <li>…and so on, for all 5 rounds.</li> <li>You then average those 5 accuracy scores (e.g., 82%, 79%, 85%, 80%, 83%). Let’s say the average is 81.8% with a standard deviation of 2.2%.</li> </ol> <p>Now, you have a much more trustworthy estimate: your model can predict student performance with about 81.8% accuracy, and that performance tends to vary by about 2.2% depending on the specific group of students. This is a far more robust and reliable statement about your model’s true capability than a single 85% score.</p> <h3 id="your-journey-to-building-trustworthy-models">Your Journey to Building Trustworthy Models</h3> <p>Cross-validation isn’t just another technique to memorize; it’s a fundamental principle for building robust, reliable, and trustworthy machine learning models. It’s a reality check that ensures your model isn’t just lucky on one particular subset of data but genuinely understands the underlying patterns.</p> <p>As you continue your data science journey, embrace cross-validation as your faithful companion. It will save you from deploying models that only work well in your carefully curated lab environment and empower you to build solutions that truly shine in the messy, unpredictable real world. So go forth, cross-validate, and build with confidence!</p> <p>Happy modeling!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>