<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Why Your Model Needs a Reality Check: Demystifying Cross-Validation for Robust Predictions | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/why-your-model-needs-a-reality-check-demystifying/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Why Your Model Needs a Reality Check: Demystifying Cross-Validation for Robust Predictions</h1> <p class="post-meta"> Created on March 26, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/cross-validation"> <i class="fa-solid fa-hashtag fa-sm"></i> Cross-Validation</a>   <a href="/blog/blog/tag/model-evaluation"> <i class="fa-solid fa-hashtag fa-sm"></i> Model Evaluation</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/overfitting"> <i class="fa-solid fa-hashtag fa-sm"></i> Overfitting</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>Welcome back to my corner of the internet where we unravel the fascinating world of machine learning. Today, I want to talk about something absolutely fundamental, a concept that took my understanding of building reliable models from “guessing” to “knowing”: <strong>Cross-Validation</strong>.</p> <p>Think back to your school days. Remember studying for a big exam? There were two types of students: those who <em>memorized</em> every single practice question and answer, and those who <em>understood</em> the underlying concepts, even if they hadn’t seen that exact question before. The memorizers often did great on <em>those specific</em> practice tests, but struggled when the actual exam threw a slightly different curveball. The concept-learners, however, were prepared for anything.</p> <p>In machine learning, our models can be like those memorizing students. They can become really good at predicting the data they’ve <em>already seen</em> during training – we call this <strong>overfitting</strong>. But what we really want is a model that <em>understands</em> the underlying patterns and can make accurate predictions on <em>new, unseen data</em>. This is where Cross-Validation comes in, acting as our ultimate reality check.</p> <h3 id="the-problem-with-a-simple-train-test-split-a-single-practice-test">The Problem with a Simple Train-Test Split: A Single Practice Test</h3> <p>When you first learn about model evaluation, you’re usually introduced to the <strong>train-test split</strong>. It’s simple: you take your entire dataset, split it into two parts – one for training your model (the training set) and one for evaluating its performance (the test set).</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[  Training Data (e.g., 80%)  ] --- Model Learns Patterns
                                  |
                                  V
                           [  Trained Model  ]
                                  |
                                  V
[  Test Data (e.g., 20%)  ] --- Model Makes Predictions --- [  Performance Metrics  ]
</code></pre></div></div> <p>This is a huge improvement over just evaluating on the training data itself (which would be like a student grading their own homework!). It gives us an estimate of how well our model might generalize.</p> <p>But here’s the catch: <em>what if that particular 20% test set was “easy”?</em> What if, by sheer luck, it contained examples that the model just happened to get right, but it would fail miserably on a different 20% slice of the data? Or, what if the split was “hard,” making your model look worse than it actually is?</p> <p>A single train-test split can give you a very optimistic, or sometimes overly pessimistic, view of your model’s true performance. It’s like relying on just one practice test score to determine if you’re ready for the final exam. It’s an okay start, but it’s not robust. We need something more thorough, something that truly tests our model’s understanding, not just its memory of one specific “practice test.”</p> <h3 id="enter-cross-validation-the-savvy-solution">Enter Cross-Validation: The Savvy Solution</h3> <p>This is where <strong>Cross-Validation</strong> shines. Instead of just one split, we perform <em>multiple</em> splits of our data, training and testing the model on different subsets each time. Then, we average the results to get a much more reliable and robust estimate of our model’s performance.</p> <p>It’s like having a battery of practice tests, each covering a slightly different mix of questions. By seeing how you perform across all of them, we get a much clearer picture of your overall understanding.</p> <p>The most common and widely used form of cross-validation is <strong>K-Fold Cross-Validation</strong>. Let’s break it down.</p> <h3 id="how-k-fold-cross-validation-works-the-workhorse">How K-Fold Cross-Validation Works (The Workhorse)</h3> <p>Imagine you have your entire dataset. Here’s how K-Fold Cross-Validation works, step-by-step:</p> <ol> <li> <p><strong>Choose a ‘k’:</strong> You decide on a number, <code class="language-plaintext highlighter-rouge">k</code>, which represents how many “folds” or segments you want to divide your data into. Common choices for <code class="language-plaintext highlighter-rouge">k</code> are 5 or 10. For this example, let’s pick $k=5$.</p> </li> <li> <p><strong>Divide the Data:</strong> Your entire dataset is randomly shuffled and then split into <code class="language-plaintext highlighter-rouge">k</code> equally sized segments (or “folds”). So, if $k=5$, you’ll have 5 folds, each containing approximately 20% of your data.</p> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>[ Fold 1 ] [ Fold 2 ] [ Fold 3 ] [ Fold 4 ] [ Fold 5 ]
</code></pre></div> </div> </li> <li> <strong>The Iteration Loop:</strong> Now, we run <code class="language-plaintext highlighter-rouge">k</code> iterations (in our case, 5 iterations). In each iteration: <ul> <li> <strong>One fold becomes the test set:</strong> One of your <code class="language-plaintext highlighter-rouge">k</code> folds is reserved exclusively for testing. It will not be seen by the model during training in that iteration.</li> <li> <strong>The remaining $k-1$ folds become the training set:</strong> The other $k-1$ folds are combined to form the training data. Your model learns from this combined data.</li> <li> <strong>Train and Evaluate:</strong> The model is trained on the training set and then evaluated on the designated test set. We record its performance (e.g., accuracy, precision, F1-score).</li> </ul> <p>Let’s visualize this with our $k=5$ example:</p> <ul> <li> <strong>Iteration 1:</strong> <ul> <li>Training Set: [Fold 2] [Fold 3] [Fold 4] [Fold 5]</li> <li>Test Set: [Fold 1]</li> <li>Record Performance 1 (P1)</li> </ul> </li> <li> <strong>Iteration 2:</strong> <ul> <li>Training Set: [Fold 1] [Fold 3] [Fold 4] [Fold 5]</li> <li>Test Set: [Fold 2]</li> <li>Record Performance 2 (P2)</li> </ul> </li> <li> <strong>Iteration 3:</strong> <ul> <li>Training Set: [Fold 1] [Fold 2] [Fold 4] [Fold 5]</li> <li>Test Set: [Fold 3]</li> <li>Record Performance 3 (P3)</li> </ul> </li> <li> <strong>Iteration 4:</strong> <ul> <li>Training Set: [Fold 1] [Fold 2] [Fold 3] [Fold 5]</li> <li>Test Set: [Fold 4]</li> <li>Record Performance 4 (P4)</li> </ul> </li> <li> <strong>Iteration 5:</strong> <ul> <li>Training Set: [Fold 1] [Fold 2] [Fold 3] [Fold 4]</li> <li>Test Set: [Fold 5]</li> <li>Record Performance 5 (P5)</li> </ul> </li> </ul> </li> <li> <p><strong>Aggregate the Results:</strong> Once all <code class="language-plaintext highlighter-rouge">k</code> iterations are complete, you’ll have <code class="language-plaintext highlighter-rouge">k</code> different performance scores (P1, P2, …, Pk). The final, robust estimate of your model’s performance is simply the average of these <code class="language-plaintext highlighter-rouge">k</code> scores.</p> <p>Mathematically, if $\text{Metric}(M_i, D_i^{\text{test}})$ is the performance of the model trained in iteration $i$ ($M_i$) on its corresponding test fold ($D_i^{\text{test}}$), then the overall cross-validated performance is:</p> \[\text{Cross-Validated Performance} = \frac{1}{k} \sum\_{i=1}^{k} \text{Metric}(M_i, D_i^{\text{test}})\] <p>You can also look at the standard deviation of these scores to understand the variance in your model’s performance across different data subsets. A low standard deviation means your model is consistently performing well, regardless of the data split – a sign of a robust model!</p> </li> </ol> <h3 id="why-k-fold-is-better-reliability-and-data-efficiency">Why K-Fold is Better: Reliability and Data Efficiency</h3> <p>K-Fold Cross-Validation offers several key advantages over a simple train-test split:</p> <ol> <li> <strong>Robustness:</strong> By averaging results over multiple splits, the performance estimate is much less sensitive to the particular way the data was divided. It gives you a more reliable picture of how your model will perform on <em>unseen</em> data in the real world.</li> <li> <strong>Better Data Utilization:</strong> Every single data point in your dataset gets to be in the test set <em>exactly once</em>, and in the training set $k-1$ times. This means we’re making the most of our potentially limited data, rather than holding back a large chunk just for one test.</li> <li> <strong>Detects Overfitting:</strong> If your model performs exceptionally well on one fold but poorly on others, it’s a strong indicator that it might be overfitting to specific patterns in the training data of the good-performing fold. Cross-validation helps highlight this inconsistency.</li> <li> <strong>Variance Estimation:</strong> As mentioned, you don’t just get an average score; you also get a range of scores (and a standard deviation). This tells you how consistent your model’s performance is, which is crucial for understanding its stability.</li> </ol> <h3 id="other-flavors-of-cross-validation-beyond-k-fold">Other Flavors of Cross-Validation: Beyond K-Fold</h3> <p>While K-Fold is the everyday superhero, there are specialized types of cross-validation for different scenarios:</p> <ul> <li> <strong>Leave-One-Out Cross-Validation (LOOCV):</strong> This is an extreme case of K-Fold where $k$ is equal to the total number of data points, $N$. You train on $N-1$ samples and test on the single remaining sample, repeating this $N$ times. It’s very thorough but computationally expensive, often impractical for large datasets.</li> <li> <strong>Stratified K-Fold Cross-Validation:</strong> Essential for datasets with imbalanced classes (e.g., predicting a rare disease). Stratified K-Fold ensures that each fold maintains the same proportion of target classes as the original dataset. This prevents a fold from, say, having all instances of the minority class in its test set, leading to misleading performance metrics.</li> <li> <strong>Time Series Cross-Validation (Walk-Forward Validation):</strong> When your data has a temporal order (like stock prices), random shuffling and regular K-Fold can “leak” future information into the past. Time series cross-validation simulates the real-world scenario by always training on past data and testing on future data, moving forward in time.</li> <li> <strong>Group K-Fold Cross-Validation:</strong> If your data has natural groupings (e.g., multiple samples from the same patient), you want to ensure that all samples from a particular group are either entirely in the training set or entirely in the test set. Group K-Fold prevents data leakage where information from a group in the training set could influence the test set.</li> </ul> <h3 id="choosing-the-right-k-a-balancing-act">Choosing the Right ‘k’: A Balancing Act</h3> <p>The choice of <code class="language-plaintext highlighter-rouge">k</code> involves a trade-off:</p> <ul> <li> <strong>Small <code class="language-plaintext highlighter-rouge">k</code> (e.g., $k=3$):</strong> <ul> <li> <strong>Pros:</strong> Faster to compute. Each training set is large (e.g., 2/3 of data), leading to a less biased estimate of the model’s performance (model gets more data to learn from).</li> <li> <strong>Cons:</strong> Each test set is small (e.g., 1/3 of data), making the performance estimate more variable (less stable) and less reliable.</li> </ul> </li> <li> <strong>Large <code class="language-plaintext highlighter-rouge">k</code> (e.g., $k=10$, or even $k=N$ for LOOCV):</strong> <ul> <li> <strong>Pros:</strong> Each training set is smaller (e.g., 9/10 of data), but each test set is also smaller. The estimate of performance has lower variance (more stable) because you’re averaging more test folds. It makes better use of the data for testing.</li> <li> <strong>Cons:</strong> Slower to compute (more iterations). Each training set is smaller, which might introduce a <em>bias</em> in the performance estimate (the model is trained on less data than it eventually would be).</li> </ul> </li> </ul> <p>Generally, $k=5$ or $k=10$ are good defaults that strike a balance between computational cost and a robust performance estimate.</p> <h3 id="putting-it-into-practice-its-easier-than-you-think">Putting It Into Practice (It’s Easier Than You Think!)</h3> <p>Modern machine learning libraries make implementing cross-validation incredibly straightforward. In Python, with <code class="language-plaintext highlighter-rouge">scikit-learn</code>, you can achieve K-Fold cross-validation with just a few lines of code. Functions like <code class="language-plaintext highlighter-rouge">KFold</code> for generating the splits or <code class="language-plaintext highlighter-rouge">cross_val_score</code> for an all-in-one solution are your best friends.</p> <p>For instance, if you have a <code class="language-plaintext highlighter-rouge">model</code> and your <code class="language-plaintext highlighter-rouge">X</code> (features) and <code class="language-plaintext highlighter-rouge">y</code> (target) data, you could get cross-validated scores like this (conceptual, not actual code block):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="kn">from</span> <span class="n">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">LogisticRegression</span><span class="p">()</span>
<span class="n">scores</span> <span class="o">=</span> <span class="nf">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Cross-validation scores: </span><span class="si">{</span><span class="n">scores</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Mean accuracy: </span><span class="si">{</span><span class="n">scores</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Standard deviation of accuracy: </span><span class="si">{</span><span class="n">scores</span><span class="p">.</span><span class="nf">std</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>This simple snippet will perform 5-fold cross-validation and print out the accuracy for each fold, followed by the mean accuracy and its standard deviation. Powerful stuff, right?</p> <h3 id="the-takeaway-build-trustworthy-models">The Takeaway: Build Trustworthy Models</h3> <p>Cross-Validation is not just another statistical trick; it’s a fundamental principle for building trustworthy and reliable machine learning models. It helps us move beyond potentially misleading evaluations from single train-test splits and gives us a more honest assessment of how our models will truly perform in the real world.</p> <p>If you’re serious about data science and machine learning, mastering cross-validation isn’t optional – it’s essential. It empowers you to build models that not only <em>perform</em> well but also <em>generalize</em> well, giving you confidence in your predictions.</p> <p>So next time you’re evaluating a model, remember to give it a proper reality check with cross-validation. Your future self (and anyone relying on your model’s predictions) will thank you for it!</p> <p>Happy modeling!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>