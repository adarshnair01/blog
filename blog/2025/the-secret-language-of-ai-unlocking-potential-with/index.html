<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Secret Language of AI: Unlocking Potential with Prompt Engineering | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/the-secret-language-of-ai-unlocking-potential-with/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Secret Language of AI: Unlocking Potential with Prompt Engineering</h1> <p class="post-meta"> Created on February 18, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/prompt-engineering"> <i class="fa-solid fa-hashtag fa-sm"></i> Prompt Engineering</a>   <a href="/blog/blog/tag/large-language-models"> <i class="fa-solid fa-hashtag fa-sm"></i> Large Language Models</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   <a href="/blog/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> NLP</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey there, fellow explorers of the digital frontier!</p> <p>I remember the first time I truly felt the raw power of a Large Language Model (LLM). It wasn’t just generating text; it was <em>creating</em>, <em>reasoning</em>, and <em>solving</em>. But, like many of you, my initial interactions were a mixed bag. Sometimes I’d get brilliant insights, other times a string of generic fluff. It was like talking to a genius who occasionally misunderstood the entire premise of my question. Frustrating, right?</p> <p>That’s where my journey into Prompt Engineering began – a journey that transformed those hit-or-miss interactions into a deliberate art form, allowing me to consistently coax incredible results from these digital brains. If you’ve ever tried to get an AI to do exactly what you want, you’ve already dipped your toes into this exciting field. This post is my personal journal, a walkthrough of what I’ve learned, and an invitation for you to dive deeper.</p> <h3 id="what-is-prompt-engineering">What <em>Is</em> Prompt Engineering?</h3> <p>Imagine you’re trying to communicate with an incredibly intelligent, but extremely literal, alien. This alien knows everything, has access to all information, and can process it at light speed. But it only understands precisely what you tell it. If you say “tell me about dogs,” it might give you a dictionary definition. If you say “Write a heartwarming short story from the perspective of a golden retriever puppy discovering snow for the first time, ending with a cozy nap,” you’re much more likely to get something magical.</p> <p>That, in essence, is Prompt Engineering. It’s the discipline of designing and refining inputs (prompts) to effectively guide a Large Language Model (LLM) towards generating desired, high-quality outputs. It’s about translating your intent into the language the AI understands best, maximizing its capabilities, and minimizing unexpected or irrelevant responses. For me, it quickly became a fascinating blend of psychology, linguistics, and systematic experimentation – a true “AI whisperer” skill.</p> <h3 id="a-peek-behind-the-curtain-how-llms-think-sort-of">A Peek Behind the Curtain: How LLMs “Think” (Sort Of)</h3> <p>Before we dive into techniques, let’s briefly demystify LLMs. At their core, these models are incredibly sophisticated pattern-matching machines. They’ve been trained on truly colossal amounts of text data from the internet – books, articles, code, conversations, you name it. Their primary job is to predict the next word (or more accurately, the next “token” – a word or sub-word unit) in a sequence, given all the preceding tokens.</p> <p>When you send a prompt, say, “The capital of France is…”, the LLM processes it by breaking it into tokens. Then, based on its vast training, it calculates the probability of what token should come next. The most probable next token might be “Paris”. This process repeats, token by token, building up the response. We can represent this fundamental process as:</p> <table> <tbody> <tr> <td>$P(token_{n+1}</td> <td>\text{context})$</td> </tr> </tbody> </table> <p>Where $P$ is the probability, $token_{n+1}$ is the next token, and $\text{context}$ is the sequence of all previous tokens (including your prompt).</p> <p>This means your prompt isn’t just a question; it’s the <em>initial context</em> that sets the entire stage for the LLM’s predictive dance. A slight change in context can dramatically shift the probabilities of subsequent tokens, leading to a completely different output. This insight was game-changing for me – it made it clear why “how you ask” is everything.</p> <h3 id="my-prompt-engineering-playbook-core-strategies">My Prompt Engineering Playbook: Core Strategies</h3> <p>Through countless hours of experimentation, I’ve gathered a set of strategies that consistently yield better results. Think of this as my personal toolkit.</p> <h4 id="1-clarity-and-specificity-the-gps-for-your-ai">1. Clarity and Specificity: The GPS for Your AI</h4> <p>This might sound obvious, but it’s astonishing how often we rely on implicit assumptions that an AI simply doesn’t share. Vague prompts lead to vague, often unhelpful, answers.</p> <p><strong>The Principle:</strong> Be unambiguous. Define your goal, constraints, and desired output format explicitly.</p> <p><strong>My Experience:</strong> I learned this the hard way. Early on, I’d say things like “Write about climate change.” The LLM would then produce a generic essay. When I started prompting with “You are a climate scientist explaining the Greenhouse Effect to a group of high school students. Focus on analogies, provide 3 actionable steps for individuals, and format your response with bullet points for the steps,” the difference was night and day. It’s like giving your friend directions: “Go to the store” vs. “Please drive to the grocery store on Elm Street, pick up milk, eggs, and bread, and text me when you’re leaving.”</p> <p><strong>Example:</strong></p> <ul> <li> <strong>Bad Prompt:</strong> “Tell me about the universe.”</li> <li> <strong>Good Prompt:</strong> “Explain the Big Bang theory to a 10-year-old. Use simple language and include an analogy to help them understand the concept of expansion. Keep the explanation under 200 words.”</li> </ul> <h4 id="2-role-playing-donning-a-digital-persona">2. Role-Playing: Donning a Digital Persona</h4> <p>Giving the AI a specific persona or role can dramatically influence the tone, style, and depth of its responses.</p> <p><strong>The Principle:</strong> Instruct the LLM to adopt a specific identity or expertise before it generates its response.</p> <p><strong>My Experience:</strong> This technique is incredibly powerful for tailoring content to specific audiences or needs. I once needed to explain a complex software architecture to both a technical team and a non-technical stakeholder. Instead of writing two separate explanations myself, I used role-playing. For the technical team, I prompted: “You are an expert software architect. Explain the microservices architecture, focusing on its benefits for scalability and maintainability, using technical jargon appropriate for fellow developers.” For the stakeholder, I used: “You are a business consultant. Explain the benefits of moving to a microservices architecture to a CEO, focusing on how it impacts business agility, cost-efficiency, and future innovation, avoiding technical jargon.” The results were perfectly tailored.</p> <p><strong>Example:</strong></p> <ul> <li> <strong>Prompt:</strong> “You are a seasoned travel blogger specializing in budget European travel. Recommend a 7-day itinerary for exploring Rome on less than €50 a day, including tips for cheap eats and free attractions.”</li> </ul> <h4 id="3-few-shot-learning-learning-by-example">3. Few-Shot Learning: Learning by Example</h4> <p>LLMs are excellent at “in-context learning.” This means they can learn from examples provided directly within the prompt, without needing to be retrained.</p> <p><strong>The Principle:</strong> Provide 1-3 examples of input-output pairs that demonstrate the desired behavior, then present your actual query.</p> <p><strong>My Experience:</strong> This technique is my go-to for tasks requiring a specific output format, tone, or when the task is nuanced. For instance, extracting specific information from unstructured text, or generating text in a very particular stylistic cadence. I used this to train an LLM to rephrase technical documentation into concise, user-friendly FAQs.</p> <p><strong>Example:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>This is a list of programming languages and their primary use cases:

Language: Python
Use: Web development, data analysis, AI, scripting.

Language: Java
Use: Enterprise applications, Android app development.

Language: JavaScript
Use: Frontend web development, backend (Node.js).

Language: C++
Use: Game development, operating systems, high-performance computing.

Language: Go
Use: Cloud services, networking, distributed systems.

Language: Ruby
Use: Web development (Ruby on Rails).

Language: Swift
Use: iOS and macOS app development.

Language: PHP
Use: Web development (especially backend).

Language: C#
Use: Windows desktop apps, game development (Unity), web (ASP.NET).

Language: TypeScript
Use: Scalable JavaScript applications, frontend/backend.

---
Summarize the primary use cases for the following languages in the format 'Language: Use':

Language: Rust
Use: Systems programming, web assembly, performance-critical applications.

Language: Kotlin
Use: Android app development, backend web development.
</code></pre></div></div> <p>By giving it examples, the LLM understood the <code class="language-plaintext highlighter-rouge">Language: Use</code> format and applied it to the new languages.</p> <h4 id="4-chain-of-thought-cot-prompting-thinking-step-by-step">4. Chain-of-Thought (CoT) Prompting: Thinking Step-by-Step</h4> <p>For complex reasoning tasks, simply asking for the answer often fails. LLMs benefit immensely from being encouraged to “think aloud” or break down the problem into smaller steps.</p> <p><strong>The Principle:</strong> Guide the LLM to perform intermediate reasoning steps before arriving at a final answer. Often, simply adding “Let’s think step by step” is enough.</p> <p><strong>My Experience:</strong> This was a revelation for solving multi-step math problems or logical puzzles. Instead of just giving an incorrect final answer, the LLM would show its work, and often, errors could be identified in the intermediate steps. It’s like asking a student to show their calculations rather than just writing down the final number.</p> <p>We can formalize the CoT process as: $Q \rightarrow S_1 \rightarrow S_2 \rightarrow \dots \rightarrow S_k \rightarrow A$ Where $Q$ is the initial query, $S_i$ are the intermediate reasoning steps, and $A$ is the final answer.</p> <p><strong>Example:</strong></p> <ul> <li> <strong>Prompt:</strong> “There are 15 apples in a basket. You take 3, and your friend takes 2 more. Then, you put 5 back. How many apples are in the basket now? Let’s think step by step.”</li> </ul> <p>The LLM would then typically output something like: “1. Initially, there are 15 apples. 2. You take 3: $15 - 3 = 12$ apples. 3. Your friend takes 2 more: $12 - 2 = 10$ apples. 4. You put 5 back: $10 + 5 = 15$ apples. Answer: There are 15 apples in the basket now.”</p> <p>This process significantly improves accuracy on complex tasks.</p> <h4 id="5-iterative-refinement-the-prompt-engineers-loop">5. Iterative Refinement: The Prompt Engineer’s Loop</h4> <p>Prompt Engineering is rarely a one-shot deal. It’s an iterative process of trial and error, learning from each interaction.</p> <p><strong>The Principle:</strong> Submit a prompt, evaluate the response, identify shortcomings, and refine the prompt based on what you learned. Repeat.</p> <p><strong>My Experience:</strong> This is the core of practical prompt engineering. I’ve spent hours refining prompts, adding constraints, removing ambiguities, testing different role-plays, and trying different CoT variations until I get the desired output. It’s very much like debugging code – you run it, see the error (or undesirable output), and then tweak your input until it works.</p> <p><strong>The Loop:</strong> $\text{Prompt}_0 \xrightarrow{\text{Model}} \text{Response}_0 \xrightarrow{\text{Evaluate &amp; Refine}} \text{Prompt}_1 \xrightarrow{\text{Model}} \text{Response}_1 \dots$</p> <h3 id="beyond-the-basics-a-glimpse-at-parameters-and-advanced-techniques">Beyond the Basics: A Glimpse at Parameters and Advanced Techniques</h3> <p>While mastering the prompt text is key, understanding model parameters also helps. Two common ones are:</p> <ul> <li> <strong>Temperature:</strong> Controls the randomness of the output. A higher temperature (e.g., 0.7-1.0) means the model takes more risks, leading to creative, diverse, and sometimes surprising results. A lower temperature (e.g., 0.1-0.3) makes the model more deterministic and focused, ideal for factual recall or precise tasks.</li> <li> <strong>Top-P:</strong> Another way to control diversity, by considering only tokens whose cumulative probability exceeds a certain threshold.</li> </ul> <p>There are also advanced techniques like <strong>Retrieval Augmented Generation (RAG)</strong>, where you provide the LLM with external knowledge (e.g., from a database or document) before it generates a response, reducing hallucinations and grounding its answers in specific facts. But that’s a topic for another deep dive!</p> <h3 id="the-art-and-science-of-it-all">The Art and Science of It All</h3> <p>Prompt Engineering sits at a fascinating intersection. It’s an <strong>art</strong> because it requires creativity, intuition, and an understanding of language nuances. It’s a <strong>science</strong> because it demands systematic experimentation, evaluation, and iteration. Documenting your effective prompts and the outputs they produce is crucial for building your own “prompt library.”</p> <h3 id="challenges-and-the-road-ahead">Challenges and the Road Ahead</h3> <p>Even with expert prompting, LLMs have limitations:</p> <ul> <li> <strong>Hallucinations:</strong> They can generate plausible-sounding but factually incorrect information. Prompt engineering, especially with RAG, helps mitigate this.</li> <li> <strong>Bias:</strong> Inherited from their training data, LLMs can perpetuate stereotypes. Careful prompt design can sometimes steer around this, but it remains a significant challenge.</li> <li> <strong>Context Window Limitations:</strong> LLMs can only process a finite amount of text at once. Very long prompts or documents might get truncated.</li> </ul> <p>Despite these, the field is evolving at a breakneck pace. New models, techniques, and frameworks are emerging constantly. Becoming proficient in Prompt Engineering isn’t just a cool party trick; it’s rapidly becoming an essential skill for anyone interacting with or building applications on top of AI.</p> <h3 id="my-invitation-to-you">My Invitation to You</h3> <p>If my journey has inspired you even a little, I encourage you to start experimenting! Open up ChatGPT, Gemini, or any LLM playground, and just start prompting. Try different roles, few-shot examples, and most importantly, use “Let’s think step by step.”</p> <p>The future of interacting with AI is not about passively receiving answers; it’s about actively shaping the conversation, wielding the power of language to unlock incredible capabilities. Prompt Engineering is your key to that future. It’s challenging, rewarding, and undeniably one of the most exciting skills I’ve ever had the pleasure of developing.</p> <p>Happy prompting!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>