<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> From Slow Code to Superfast Arrays: My Journey into NumPy Optimization | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/from-slow-code-to-superfast-arrays-my-journey-into/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">From Slow Code to Superfast Arrays: My Journey into NumPy Optimization</h1> <p class="post-meta"> Created on April 16, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/numpy"> <i class="fa-solid fa-hashtag fa-sm"></i> NumPy</a>   <a href="/blog/blog/tag/optimization"> <i class="fa-solid fa-hashtag fa-sm"></i> Optimization</a>   <a href="/blog/blog/tag/python"> <i class="fa-solid fa-hashtag fa-sm"></i> Python</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/performance"> <i class="fa-solid fa-hashtag fa-sm"></i> Performance</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>My data science journey began much like many others: with Python, pandas, and a healthy dose of enthusiasm. I was building models, crunching numbers, and feeling pretty good about myself. That is, until I hit my first truly massive dataset. Suddenly, simple operations that used to take milliseconds were stretching into seconds, even minutes. My code felt like it was wading through treacle.</p> <p>I remember staring at my screen, watching a <code class="language-plaintext highlighter-rouge">for</code> loop painstakingly process millions of data points. “There <em>has</em> to be a better way,” I muttered to myself. And that’s when I truly discovered the magic of NumPy. It wasn’t just a library for numerical operations; it was a superpower. But like any superpower, you need to know how to wield it effectively.</p> <p>This blog post is a reflection of my journey, a guide to understanding <em>why</em> NumPy is fast and <em>how</em> to write optimized code that leverages its full potential. Whether you’re a budding data scientist or a high school student curious about making your code fly, come along! We’re about to turn slow Python loops into blazing-fast array computations.</p> <h2 id="the-bottleneck-why-python-loops-are-slow">The Bottleneck: Why Python Loops Are Slow</h2> <p>Before we dive into optimization, let’s understand the problem. Python is a wonderfully versatile language, but it’s an interpreted language. When you write a <code class="language-plaintext highlighter-rouge">for</code> loop in Python to perform an operation on each element of a list, like adding a constant:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">10_000_000</span><span class="p">))</span>
<span class="n">result</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
    <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Python loop took </span><span class="si">{</span><span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s"> seconds</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><em>(Self-correction: I need to import <code class="language-plaintext highlighter-rouge">time</code> for this example)</em></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">time</span>

<span class="n">data</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">10_000_000</span><span class="p">))</span>
<span class="n">result</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
    <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Python loop took </span><span class="si">{</span><span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s"> seconds</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>Each iteration of that loop involves:</p> <ol> <li> <strong>Type Checking:</strong> Python variables are dynamically typed. The interpreter has to check the type of <code class="language-plaintext highlighter-rouge">x</code> and <code class="language-plaintext highlighter-rouge">5</code> in each iteration to ensure the addition is valid.</li> <li> <strong>Object Creation/Access:</strong> Every number in a Python list is an object, not just a raw value. Accessing and creating these objects adds overhead.</li> <li> <strong>Interpreter Overhead:</strong> The Python interpreter itself adds a layer of abstraction that, while flexible, isn’t designed for raw computational speed at the micro-level.</li> </ol> <p>This overhead quickly adds up when you’re dealing with millions of elements.</p> <h2 id="enter-numpy-the-vectorization-king">Enter NumPy: The Vectorization King</h2> <p>This is where NumPy sweeps in like a superhero. NumPy’s core strength lies in <strong>vectorization</strong>. Instead of operating on individual elements one by one using Python loops, NumPy allows you to perform operations on entire arrays at once.</p> <p>Think of it this way: Imagine you have a stack of 1,000 envelopes to stamp.</p> <ul> <li> <strong>Python loop:</strong> You pick up one envelope, find the stamp, apply the stamp, put the envelope down. Repeat 1,000 times.</li> <li> <strong>NumPy vectorization:</strong> You grab a rolling stamper, align all 1,000 envelopes, and roll the stamper over them in one fluid motion.</li> </ul> <p>Let’s see that same addition operation with NumPy:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">time</span>

<span class="n">data_np</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">10_000_000</span><span class="p">)</span>
<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="n">result_np</span> <span class="o">=</span> <span class="n">data_np</span> <span class="o">+</span> <span class="mi">5</span>
<span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">NumPy vectorized operation took </span><span class="si">{</span><span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s"> seconds</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>You’ll immediately notice the difference. The NumPy version is <em>orders of magnitude</em> faster!</p> <h3 id="why-is-vectorization-so-fast-the-under-the-hood-magic">Why is Vectorization So Fast? The Under-the-Hood Magic</h3> <p>This isn’t magic for magic’s sake; there’s solid engineering behind it:</p> <ol> <li> <strong>C and Fortran Backend:</strong> The heavy lifting in NumPy isn’t done in Python. The core operations are implemented in highly optimized, pre-compiled C and Fortran code. When you call <code class="language-plaintext highlighter-rouge">data_np + 5</code>, NumPy hands off the entire array and the operation to these fast, low-level routines.</li> <li> <strong>Contiguous Memory Allocation:</strong> Unlike Python lists, where elements can be scattered in memory, NumPy arrays store elements of the same data type <strong>contiguously</strong> in a single block of memory. <ul> <li> <strong>CPU Cache Efficiency:</strong> Imagine your CPU as having a small, super-fast scratchpad called a cache. When it needs data, it often fetches not just one piece, but a whole block around it (a “cache line”). If your data is contiguous, the CPU can load many elements into its cache at once, ready for processing. If data is scattered, it has to make many separate fetches, slowing things down.</li> </ul> </li> <li> <strong>SIMD (Single Instruction, Multiple Data):</strong> Modern CPUs have special instructions that can perform the <em>same operation</em> on <em>multiple pieces of data</em> simultaneously. These are called SIMD instructions. Because NumPy data is stored contiguously and uniformly, these C/Fortran routines can leverage SIMD to process several array elements in parallel with a single CPU instruction.</li> </ol> <p>This combination of factors makes vectorized operations incredibly efficient. When you’re dealing with matrices, for example, operations like matrix multiplication:</p> <p>$C_{ij} = \sum_k A_{ik} B_{kj}$</p> <p>which would involve triple nested loops in pure Python, become a single, highly optimized call to <code class="language-plaintext highlighter-rouge">np.dot()</code> or <code class="language-plaintext highlighter-rouge">@</code> in NumPy.</p> <h2 id="mastering-broadcasting-the-art-of-dimension-mismatch">Mastering Broadcasting: The Art of Dimension Mismatch</h2> <p>Broadcasting is another beautiful NumPy feature that allows operations between arrays of different shapes or sizes. It’s like NumPy intelligently stretching or repeating smaller arrays to match the shape of larger ones, without actually creating extra copies in memory.</p> <p>The core rules for broadcasting are simple:</p> <ol> <li> <strong>Rule 1: Equal Dimensions:</strong> If the arrays have different numbers of dimensions, the shape of the smaller array is padded with ones on its left side.</li> <li> <strong>Rule 2: Compatible Dimensions:</strong> Two dimensions are compatible when they are equal, or one of them is 1.</li> </ol> <p>If these rules are met, the smaller array is “broadcast” across the larger one. Let’s see an example:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Adding a scalar to an array
</span><span class="n">arr</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">scalar</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">arr</span> <span class="o">+</span> <span class="n">scalar</span> <span class="c1"># The scalar 10 is broadcast to [10, 10, 10]
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Scalar addition: </span><span class="si">{</span><span class="n">result</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># Output: [11 12 13]
</span>
<span class="c1"># Adding a 1D array to a 2D array
</span><span class="n">matrix</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                   <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span>
                   <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]])</span>
<span class="n">row_vector</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">])</span>
<span class="n">result_matrix</span> <span class="o">=</span> <span class="n">matrix</span> <span class="o">+</span> <span class="n">row_vector</span> <span class="c1"># row_vector is broadcast across each row
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">Matrix + Row Vector:</span><span class="se">\n</span><span class="si">{</span><span class="n">result_matrix</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="c1"># Output:
# [[11 22 33]
#  [14 25 36]
#  [17 28 39]]
</span></code></pre></div></div> <p>In the matrix example, <code class="language-plaintext highlighter-rouge">row_vector</code> (shape <code class="language-plaintext highlighter-rouge">(3,)</code>) is effectively stretched to <code class="language-plaintext highlighter-rouge">(1, 3)</code> and then “repeated” three times to match the <code class="language-plaintext highlighter-rouge">(3, 3)</code> shape of the <code class="language-plaintext highlighter-rouge">matrix</code>. This saves a huge amount of memory and computation compared to manually tiling the <code class="language-plaintext highlighter-rouge">row_vector</code> into a <code class="language-plaintext highlighter-rouge">(3,3)</code> matrix before addition.</p> <p>Broadcasting allows for extremely concise and efficient code, especially in linear algebra operations.</p> <h2 id="choose-your-dtype-wisely-memory-and-speed">Choose Your <code class="language-plaintext highlighter-rouge">dtype</code> Wisely: Memory and Speed</h2> <p>NumPy arrays are homogeneous, meaning all elements must be of the same data type (<code class="language-plaintext highlighter-rouge">dtype</code>). By default, NumPy often uses <code class="language-plaintext highlighter-rouge">int64</code> for integers and <code class="language-plaintext highlighter-rouge">float64</code> for floating-point numbers. These are 64-bit types, meaning each number takes up 8 bytes of memory.</p> <p>While <code class="language-plaintext highlighter-rouge">float64</code> offers high precision, do you <em>always</em> need it? If you’re working with pixel data (0-255), an <code class="language-plaintext highlighter-rouge">uint8</code> (unsigned 8-bit integer, 1 byte) is perfectly sufficient. If you’re counting items that won’t exceed 32,767, an <code class="language-plaintext highlighter-rouge">int16</code> (signed 16-bit integer, 2 bytes) works just fine.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Default dtype
</span><span class="n">arr_float_default</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">1_000_000</span><span class="p">)</span> <span class="c1"># float64 by default
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Default float array size: </span><span class="si">{</span><span class="n">arr_float_default</span><span class="p">.</span><span class="n">nbytes</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> MB</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Specify a smaller dtype
</span><span class="n">arr_float_32</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">1_000_000</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Float32 array size: </span><span class="si">{</span><span class="n">arr_float_32</span><span class="p">.</span><span class="n">nbytes</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> MB</span><span class="sh">"</span><span class="p">)</span>

<span class="n">arr_int_default</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">1_000_000</span><span class="p">)</span> <span class="c1"># int64 by default
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Default int array size: </span><span class="si">{</span><span class="n">arr_int_default</span><span class="p">.</span><span class="n">nbytes</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> MB</span><span class="sh">"</span><span class="p">)</span>

<span class="n">arr_int_16</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">1_000_000</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">int16</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Int16 array size: </span><span class="si">{</span><span class="n">arr_int_16</span><span class="p">.</span><span class="n">nbytes</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> MB</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>You’ll see a significant difference in memory usage. Smaller data types not only conserve memory (which can prevent out-of-memory errors with huge datasets) but can also lead to faster computations. Less data to move from RAM to CPU cache means faster processing.</p> <h2 id="universal-functions-ufuncs-pre-compiled-powerhouses">Universal Functions (Ufuncs): Pre-compiled Powerhouses</h2> <p>NumPy provides a suite of “universal functions” (ufuncs) that operate element-wise on arrays. These include <code class="language-plaintext highlighter-rouge">np.add</code>, <code class="language-plaintext highlighter-rouge">np.subtract</code>, <code class="language-plaintext highlighter-rouge">np.multiply</code>, <code class="language-plaintext highlighter-rouge">np.divide</code>, <code class="language-plaintext highlighter-rouge">np.sqrt</code>, <code class="language-plaintext highlighter-rouge">np.sin</code>, <code class="language-plaintext highlighter-rouge">np.cos</code>, <code class="language-plaintext highlighter-rouge">np.exp</code>, <code class="language-plaintext highlighter-rouge">np.log</code>, and many more.</p> <p>The crucial point is that these ufuncs are <em>also</em> implemented in highly optimized C code. When you write <code class="language-plaintext highlighter-rouge">arr + 5</code> or <code class="language-plaintext highlighter-rouge">np.sqrt(arr)</code>, you’re implicitly using these ufuncs. Directly using them (e.g., <code class="language-plaintext highlighter-rouge">np.add(arr, 5)</code>) can sometimes be slightly faster for complex operations, though the arithmetic operators usually call them under the hood.</p> <p>The key takeaway here is: <strong>always prefer NumPy’s built-in functions over writing your own Python loops for element-wise operations.</strong></p> <h2 id="avoid-unnecessary-copies-views-vs-copies">Avoid Unnecessary Copies: Views vs. Copies</h2> <p>NumPy has a concept of “views” and “copies” of arrays, and understanding this distinction can be critical for performance and memory management.</p> <ul> <li> <strong>Copy:</strong> A copy creates a completely new array in memory. Changes to the copy do not affect the original array. This takes time and memory.</li> <li> <strong>View:</strong> A view is essentially a different way of looking at the <em>same data</em> in memory. It’s like having two pointers to the same object. Changes to the view <em>will</em> affect the original array, and vice-versa. Views are very fast to create because no data is duplicated.</li> </ul> <p>Slicing an array (e.g., <code class="language-plaintext highlighter-rouge">arr[1:5]</code>) typically returns a view, not a copy. Operations that reorder or reshape arrays, like <code class="language-plaintext highlighter-rouge">.reshape()</code> or <code class="language-plaintext highlighter-rouge">.T</code> (transpose), also often return views. Functions like <code class="language-plaintext highlighter-rouge">np.copy()</code> explicitly create a copy.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">original_array</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Original array: </span><span class="si">{</span><span class="n">original_array</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Slicing creates a view
</span><span class="n">view_array</span> <span class="o">=</span> <span class="n">original_array</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">5</span><span class="p">]</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">View array: </span><span class="si">{</span><span class="n">view_array</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="n">view_array</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">99</span> <span class="c1"># Modifying the view
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Original array after modifying view: </span><span class="si">{</span><span class="n">original_array</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># Original is affected!
</span>
<span class="c1"># Explicitly creating a copy
</span><span class="n">copy_array</span> <span class="o">=</span> <span class="n">original_array</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">5</span><span class="p">].</span><span class="nf">copy</span><span class="p">()</span>
<span class="n">copy_array</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">101</span> <span class="c1"># Modifying the copy
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Original array after modifying copy: </span><span class="si">{</span><span class="n">original_array</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># Original is NOT affected
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Copy array after modification: </span><span class="si">{</span><span class="n">copy_array</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>For performance, be mindful of when operations might implicitly create copies (e.g., combining arrays in certain ways) and consider if a view would suffice. Also, using in-place operations like <code class="language-plaintext highlighter-rouge">arr += 5</code> is generally more memory-efficient than <code class="language-plaintext highlighter-rouge">arr = arr + 5</code>, as the former modifies the array directly without creating a new temporary array.</p> <h2 id="leveraging-npdot-and-nplinalg-for-linear-algebra">Leveraging <code class="language-plaintext highlighter-rouge">np.dot()</code> and <code class="language-plaintext highlighter-rouge">np.linalg</code> for Linear Algebra</h2> <p>For anyone diving into machine learning, linear algebra is fundamental. Matrix multiplications, inversions, and decompositions are common. NumPy provides highly optimized functions for these operations, often relying on specialized libraries like BLAS (Basic Linear Algebra Subprograms) and LAPACK (Linear Algebra PACKage) under the hood.</p> <p>Always use <code class="language-plaintext highlighter-rouge">np.dot()</code> for dot products or the <code class="language-plaintext highlighter-rouge">@</code> operator for matrix multiplication instead of trying to implement it with loops:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">matrix_a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">matrix_b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="n">product_matrix</span> <span class="o">=</span> <span class="n">matrix_a</span> <span class="o">@</span> <span class="n">matrix_b</span> <span class="c1"># Or np.dot(matrix_a, matrix_b)
</span><span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Matrix multiplication with @ took </span><span class="si">{</span><span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s"> seconds</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>A pure Python equivalent for this would take <em>hours</em>, if not longer. This truly highlights the power of relying on NumPy’s optimized functions.</p> <h2 id="when-not-to-optimize-the-perils-of-premature-optimization">When Not to Optimize: The Perils of Premature Optimization</h2> <p>While optimization is crucial, it’s equally important to know <em>when</em> and <em>what</em> to optimize. A famous quote by Donald Knuth states: “Premature optimization is the root of all evil (or at least most of it) in programming.”</p> <ol> <li> <strong>Readability First:</strong> Write clear, understandable code first.</li> <li> <strong>Profile Your Code:</strong> Don’t guess where the bottlenecks are. Use profiling tools to identify the parts of your code that consume the most time. In Jupyter notebooks, <code class="language-plaintext highlighter-rouge">%timeit</code> is a quick way to measure the execution time of a single line or cell. For more detailed analysis, consider <code class="language-plaintext highlighter-rouge">cProfile</code>.</li> <li> <strong>Optimize Bottlenecks:</strong> Once you’ve identified the slow parts, <em>then</em> apply optimization techniques. Often, a small percentage of your code accounts for a large percentage of its execution time.</li> </ol> <h2 id="my-final-thoughts">My Final Thoughts</h2> <p>My journey into NumPy optimization was a revelation. It transformed my perspective on writing efficient code and empowered me to tackle much larger, more complex datasets with confidence. The transition from agonizingly slow loops to lightning-fast array operations felt like upgrading from a bicycle to a rocket ship!</p> <p>Here are the key takeaways I want you to remember:</p> <ul> <li> <strong>Embrace Vectorization:</strong> Always prioritize operations on entire arrays over Python <code class="language-plaintext highlighter-rouge">for</code> loops.</li> <li> <strong>Leverage Broadcasting:</strong> Use it to perform operations on arrays of different shapes efficiently.</li> <li> <strong>Mind Your <code class="language-plaintext highlighter-rouge">dtype</code>:</strong> Choose the smallest data type that meets your precision needs to save memory and boost speed.</li> <li> <strong>Utilize Ufuncs and Built-in Functions:</strong> <code class="language-plaintext highlighter-rouge">np.add</code>, <code class="language-plaintext highlighter-rouge">np.sqrt</code>, <code class="language-plaintext highlighter-rouge">np.sum</code>, <code class="language-plaintext highlighter-rouge">np.dot</code> — these are your friends. They are pre-optimized in C/Fortran.</li> <li> <strong>Understand Views vs. Copies:</strong> Be aware of memory usage and potential side effects.</li> <li> <strong>Profile, Don’t Guess:</strong> Only optimize after identifying actual performance bottlenecks.</li> </ul> <p>NumPy is an indispensable tool for data scientists, machine learning engineers, and anyone working with numerical data in Python. By understanding and applying these optimization techniques, you’re not just making your code faster; you’re developing a deeper intuition for how computers process data efficiently. Go forth and write some superfast Python!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>