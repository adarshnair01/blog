<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Unlocking the Language of Machines: My Journey Through Natural Language Processing | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/unlocking-the-language-of-machines-my-journey-thro/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Unlocking the Language of Machines: My Journey Through Natural Language Processing</h1> <p class="post-meta"> Created on December 22, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> NLP</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/text-analysis"> <i class="fa-solid fa-hashtag fa-sm"></i> Text Analysis</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>As a budding Data Scientist and Machine Learning Engineer, few fields have captivated my imagination quite like Natural Language Processing (NLP). It’s the frontier where human communication meets computational power, allowing us to bridge the gap between our nuanced, often messy, language and the binary precision of machines. If you’ve ever spoken to a virtual assistant, used Google Translate, or even just received a spam email filtered out of your inbox, you’ve witnessed NLP in action.</p> <p>But what exactly <em>is</em> NLP? At its core, it’s about enabling computers to understand, interpret, and generate human language in a valuable way. Sounds simple, right? Try telling a joke to a computer, or explaining sarcasm. Our language is teeming with ambiguity, context, cultural references, and subtle nuances that even humans sometimes struggle with. For a machine, it’s an incredibly complex puzzle.</p> <p>My journey into NLP felt a lot like learning a new language myself – the language spoken by data, interpreted by algorithms. From my early days tinkering with basic text analysis to wrestling with the intricacies of deep learning models, I’ve seen this field transform at an incredible pace. Let’s dive into some of the key concepts and milestones that have shaped this fascinating domain.</p> <h3 id="the-foundation-from-words-to-numbers">The Foundation: From Words to Numbers</h3> <p>The first big hurdle in NLP is simple: computers only understand numbers. How do you turn a sentence like “The quick brown fox jumps over the lazy dog” into something a machine can process? This is where the initial stages of text processing come in.</p> <ol> <li> <strong>Tokenization:</strong> Imagine breaking down a sentence into its smallest meaningful units, typically words or subwords. So, “Hello, world!” becomes [<code class="language-plaintext highlighter-rouge">Hello</code>, <code class="language-plaintext highlighter-rouge">,</code>, <code class="language-plaintext highlighter-rouge">world</code>, <code class="language-plaintext highlighter-rouge">!</code>]. This is our starting point.</li> <li> <strong>Stop Word Removal:</strong> Words like “a,” “the,” “is,” “and” are extremely common but often carry little unique meaning for analysis. Removing these “stop words” can help focus our attention on more important terms.</li> <li> <strong>Stemming and Lemmatization:</strong> English, like many languages, has many forms of the same word (e.g., “run,” “running,” “ran”). <ul> <li> <strong>Stemming</strong> is a crude way of chopping off suffixes to get to a root form (e.g., “running” -&gt; “run”). It’s fast but can sometimes create non-words.</li> <li> <strong>Lemmatization</strong> is more sophisticated, using vocabulary and morphological analysis to return the dictionary form (lemma) of a word (e.g., “better” -&gt; “good”).</li> </ul> </li> </ol> <p>Once we have our cleaned list of tokens, the real transformation begins: turning them into numerical representations.</p> <h4 id="vectorization-the-art-of-quantifying-language">Vectorization: The Art of Quantifying Language</h4> <p>The most fundamental way to represent text numerically is through <strong>vectorization</strong>. Early methods were relatively simple but incredibly powerful:</p> <ul> <li> <strong>Bag-of-Words (BoW):</strong> This approach treats a document as a “bag” of words, disregarding grammar and even word order, but keeping track of the frequency of each word. <ul> <li>Consider two simple sentences: <ul> <li>Document 1: “I love cats. I love dogs.”</li> <li>Document 2: “I hate cats. I love birds.”</li> </ul> </li> <li>Our vocabulary would be: [<code class="language-plaintext highlighter-rouge">I</code>, <code class="language-plaintext highlighter-rouge">love</code>, <code class="language-plaintext highlighter-rouge">cats</code>, <code class="language-plaintext highlighter-rouge">dogs</code>, <code class="language-plaintext highlighter-rouge">hate</code>, <code class="language-plaintext highlighter-rouge">birds</code>].</li> <li>The BoW vectors might look like: <ul> <li>Doc 1: [<code class="language-plaintext highlighter-rouge">2</code>, <code class="language-plaintext highlighter-rouge">2</code>, <code class="language-plaintext highlighter-rouge">1</code>, <code class="language-plaintext highlighter-rouge">1</code>, <code class="language-plaintext highlighter-rouge">0</code>, <code class="language-plaintext highlighter-rouge">0</code>] (I appears 2 times, love 2 times, etc.)</li> <li>Doc 2: [<code class="language-plaintext highlighter-rouge">2</code>, <code class="language-plaintext highlighter-rouge">1</code>, <code class="language-plaintext highlighter-rouge">1</code>, <code class="language-plaintext highlighter-rouge">0</code>, <code class="language-plaintext highlighter-rouge">1</code>, <code class="language-plaintext highlighter-rouge">1</code>]</li> </ul> </li> </ul> <p>While intuitive, BoW treats every word equally. A common word like “the” might appear frequently but doesn’t necessarily tell us much about a document’s unique content.</p> </li> <li> <p><strong>TF-IDF (Term Frequency-Inverse Document Frequency):</strong> This technique addresses the limitations of BoW by weighting words based on how often they appear in a document (Term Frequency) <em>and</em> how rare they are across all documents (Inverse Document Frequency). This gives more importance to unique, distinguishing words.</p> <p>The math behind TF-IDF is straightforward:</p> <p>$TF(t, d) = \frac{\text{number of times term t appears in document d}}{\text{total number of terms in document d}}$</p> <p>$IDF(t, D) = \log \frac{\text{total number of documents D}}{\text{number of documents with term t in it}}$</p> <p>And finally, the $TF-IDF$ score:</p> <p>$TF-IDF(t, d, D) = TF(t, d) \times IDF(t, D)$</p> <p>A high TF-IDF score means a term is frequent in a specific document but rare in the corpus, making it a good indicator of that document’s topic. This was a game-changer for information retrieval and topic modeling.</p> </li> </ul> <p>While powerful, BoW and TF-IDF still had a critical drawback: they treated each word as an independent entity, losing all information about semantic relationships or context. “King” and “Queen” were just different words, not related concepts. “Apple” the company and “apple” the fruit were indistinguishable. This is where the deep learning revolution stepped in.</p> <h3 id="the-deep-learning-revolution-understanding-meaning-and-context">The Deep Learning Revolution: Understanding Meaning and Context</h3> <p>For me, the real “aha!” moment in NLP came with the advent of deep learning techniques. Suddenly, we weren’t just counting words; we were trying to capture their <em>meaning</em>.</p> <h4 id="word-embeddings-words-as-coordinates-in-a-semantic-space">Word Embeddings: Words as Coordinates in a Semantic Space</h4> <p>Imagine assigning each word a unique set of coordinates in a multi-dimensional space. The magic of <strong>Word Embeddings</strong> (like Word2Vec, GloVe, FastText) is that words with similar meanings are located closer to each other in this space.</p> <p>For instance, the vector for “King” might be very close to “Queen,” and the vector difference between “King” and “Man” could be surprisingly similar to the difference between “Queen” and “Woman.” This means we can do fascinating arithmetic with words:</p> <p><code class="language-plaintext highlighter-rouge">vector("King") - vector("Man") + vector("Woman") ≈ vector("Queen")</code></p> <p>This was mind-blowing! It meant that computers could now grasp analogies and relationships between words, a huge leap from simple frequency counts. These embeddings are typically learned by neural networks, which predict a word based on its context (or vice-versa).</p> <p>A word embedding might look like a list of numbers, e.g., for “cat”: <code class="language-plaintext highlighter-rouge">[0.2, -0.5, 0.8, 0.1, ..., -0.3]</code> (often hundreds of dimensions long). These dense vectors capture rich semantic and syntactic information.</p> <h4 id="recurrent-neural-networks-rnns-and-lstms-remembering-the-sequence">Recurrent Neural Networks (RNNs) and LSTMs: Remembering the Sequence</h4> <p>Human language is sequential. The meaning of a word often depends on the words that came before it. Traditional neural networks, which treat inputs independently, struggled with this. This led to the development of <strong>Recurrent Neural Networks (RNNs)</strong>.</p> <p>RNNs have a “memory” – a hidden state that carries information from previous steps in the sequence. This allows them to process words one by one while keeping track of the context built up so far. Think of it like reading a sentence word by word and remembering what you’ve read to understand the next word.</p> <p>However, basic RNNs often struggled with “long-term dependencies” – remembering information from many steps ago. This is where <strong>Long Short-Term Memory (LSTM)</strong> networks came to the rescue. LSTMs are a special type of RNN with internal “gates” that can selectively remember or forget information, effectively solving the vanishing gradient problem that plagued simple RNNs. LSTMs became the backbone of many early successes in machine translation and language modeling.</p> <h4 id="transformers-the-attention-revolution">Transformers: The Attention Revolution</h4> <p>While LSTMs were powerful, they had a limitation: they processed sequences one element at a time, making them slow and difficult to parallelize. Then came the <strong>Transformer architecture</strong> in 2017, and it completely changed the game.</p> <p>The core idea behind Transformers is <strong>self-attention</strong>. Instead of processing words sequentially, Transformers process all words in a sentence simultaneously. The “attention mechanism” allows the model to weigh the importance of different words in the input sequence when processing each word. For example, when translating “The animal didn’t cross the street because <em>it</em> was too tired,” the Transformer can quickly realize that “<em>it</em>” refers to “the animal,” regardless of how far apart they are in the sentence.</p> <p>This parallel processing and ability to capture long-range dependencies efficiently made Transformers incredibly powerful. It paved the way for models like:</p> <ul> <li> <strong>BERT (Bidirectional Encoder Representations from Transformers):</strong> Trained by Google, BERT could understand context from both left-to-right and right-to-left (bidirectional). It revolutionized how we approach transfer learning in NLP: pre-train a massive model on a huge amount of text data, then fine-tune it for specific tasks with much smaller datasets.</li> <li> <strong>GPT (Generative Pre-trained Transformer) series:</strong> Developed by OpenAI, these models (like ChatGPT) are experts at generating human-like text. They are “generative” because they predict the next word in a sequence, building coherent and contextually relevant text piece by piece.</li> </ul> <p>The scale of these Transformer models is immense, often involving billions of parameters and trained on vast portions of the internet’s text. They have truly unlocked unprecedented capabilities in language understanding and generation.</p> <h3 id="practical-applications-nlp-in-the-real-world">Practical Applications: NLP in the Real World</h3> <p>The theories and models we’ve discussed power a myriad of applications that touch our daily lives:</p> <ul> <li> <strong>Sentiment Analysis:</strong> Determining the emotional tone of text (positive, negative, neutral). Crucial for understanding customer feedback or social media trends.</li> <li> <strong>Named Entity Recognition (NER):</strong> Identifying and classifying specific entities in text, such as names of people, organizations, locations, dates, etc. Essential for information extraction.</li> <li> <strong>Machine Translation:</strong> The seamless translation we experience with tools like Google Translate, breaking down language barriers.</li> <li> <strong>Text Summarization:</strong> Condensing long documents into shorter, coherent summaries, saving countless hours of reading.</li> <li> <strong>Chatbots and Virtual Assistants:</strong> Powering conversational AI like Siri, Alexa, and customer service chatbots, making human-computer interaction more natural.</li> <li> <strong>Spam Detection:</strong> Filtering unwanted emails by analyzing their content for suspicious patterns.</li> </ul> <h3 id="the-future-and-ethical-considerations">The Future and Ethical Considerations</h3> <p>NLP is still evolving at breakneck speed. We’re seeing models that are increasingly multimodal (understanding text, images, and audio together), more robust in handling different languages and dialects, and capable of even more sophisticated reasoning.</p> <p>However, this power comes with significant responsibilities. As an MLE practitioner, I find it crucial to consider the ethical implications:</p> <ul> <li> <strong>Bias:</strong> Models are trained on data, and if that data reflects societal biases (gender, race, etc.), the models will perpetuate and even amplify them. Ensuring fair and unbiased data and model output is paramount.</li> <li> <strong>Misinformation and Deepfakes:</strong> Generative models can create highly convincing but fake text, audio, and video, posing challenges for discerning truth.</li> <li> <strong>Privacy:</strong> Large language models often collect and process vast amounts of personal data, raising concerns about privacy and data security.</li> <li> <strong>Job Displacement:</strong> As NLP models become more capable, they will undoubtedly impact various industries and job roles.</li> </ul> <p>The journey through NLP is a constant learning curve, a fascinating blend of linguistics, statistics, computer science, and creative problem-solving. Every new paper, every new model, opens up a world of possibilities and challenges.</p> <h3 id="wrapping-up">Wrapping Up</h3> <p>From the simple elegance of TF-IDF to the complex, attention-driven architectures of Transformers, Natural Language Processing has transformed our ability to interact with machines. It’s a field brimming with innovation, impactful applications, and critical ethical considerations.</p> <p>For anyone looking to delve into Data Science or Machine Learning, NLP offers a rewarding and endlessly interesting path. It’s not just about understanding algorithms; it’s about understanding language, communication, and ultimately, a little more about what it means to be human in an increasingly interconnected, data-driven world. So, dive in, experiment, and prepare to be amazed by the power of words in the hands of machines.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>