<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Taming the Beast: How Regularization Keeps Our AI Models Honest | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/taming-the-beast-how-regularization-keeps-our-ai-m/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Taming the Beast: How Regularization Keeps Our AI Models Honest</h1> <p class="post-meta"> Created on November 16, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/regularization"> <i class="fa-solid fa-hashtag fa-sm"></i> Regularization</a>   <a href="/blog/blog/tag/overfitting"> <i class="fa-solid fa-hashtag fa-sm"></i> Overfitting</a>   <a href="/blog/blog/tag/bias-variance"> <i class="fa-solid fa-hashtag fa-sm"></i> Bias-Variance</a>   <a href="/blog/blog/tag/model-training"> <i class="fa-solid fa-hashtag fa-sm"></i> Model Training</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>As a budding data scientist, I’ve had my share of “aha!” moments, and perhaps just as many “oh no!” moments. One of the biggest revelations came early on when I realized that building a model wasn’t just about getting the <em>lowest error</em> on my training data. In fact, sometimes, that was exactly the path to disaster. It was like a diligent student memorizing every single question from the practice exam, only to completely bomb the real test because the questions were slightly different.</p> <p>This, my friends, is the tale of <strong>overfitting</strong>, and how a brilliant set of techniques called <strong>regularization</strong> helps us prevent our models from becoming overly specialized, ensuring they learn the underlying patterns rather than just memorizing noise.</p> <h3 id="the-overfitting-monster-when-models-get-too-smart-for-their-own-good">The Overfitting Monster: When Models Get Too Smart for Their Own Good</h3> <p>Imagine you’re trying to draw a line that separates red dots from blue dots on a graph.</p> <p><strong>(Figure 1: Simple Scatter Plot with Linear Separator)</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        . Red
    .
  .           . Blue
      .
   .       .
</code></pre></div></div> <p>A simple straight line might do a decent job. It’s easy to understand, and it generalizes well if new dots appear.</p> <p>Now, imagine your data is a bit messy, with some red dots mixed in with blue, and vice versa. An overzealous model might try to draw a ridiculously complex, squiggly line that perfectly encompasses <em>every single red dot</em> and <em>every single blue dot</em> in the training set.</p> <p><strong>(Figure 2: Complex, Overfit Separator)</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  . Red          . Blue
    \  /
  .  \/  .
    /\
   /  \
.           .
</code></pre></div></div> <p>It looks perfect on the training data! “Wow,” you might think, “my model has 100% accuracy!” But then, you show it new, unseen data, and it completely falls apart. That squiggly line was so specific to the training data’s noise that it can’t make sense of anything slightly different. It memorized the answers instead of understanding the rules.</p> <p>This is overfitting in a nutshell: a model that performs exceptionally well on the data it was trained on but poorly on new, unseen data. It essentially “learns the noise” rather than the signal.</p> <h3 id="our-hero-arrives-the-philosophy-of-regularization">Our Hero Arrives: The Philosophy of Regularization</h3> <p>So, how do we rein in this overly enthusiastic model? We introduce a “penalty” for complexity. Regularization techniques essentially add a cost to the model’s objective function (what it’s trying to minimize, like error) based on the magnitude of its coefficients (the weights it assigns to different features).</p> <p>Think of it like this: your model is trying to minimize its error. Regularization says, “Okay, minimize your error, <em>but also</em>, try to keep your feature weights (the importance you assign to different inputs) as small as possible.” This pushes the model towards simpler solutions, discouraging it from creating those wild, squiggly lines.</p> <p>The core idea is to find a balance: a model that fits the data well <em>enough</em> without becoming overly sensitive to every single data point. We’re looking for a sweet spot in the <strong>Bias-Variance Trade-off</strong>. Overfitting implies high variance (model changes wildly with small changes in data) and low bias (model fits training data very well). Regularization gently increases bias (makes the model a little less perfect on training data) to significantly reduce variance (makes it much more stable and reliable on new data).</p> <h3 id="the-mathematical-architects-l1-l2-and-elastic-net">The Mathematical Architects: L1, L2, and Elastic Net</h3> <p>Let’s dive into the two most common types of regularization: L1 (Lasso) and L2 (Ridge).</p> <h4 id="1-l2-regularization-ridge-regression">1. L2 Regularization (Ridge Regression)</h4> <p>Also known as <strong>Ridge Regression</strong>, L2 regularization adds a penalty term proportional to the <em>square</em> of the magnitude of the coefficients.</p> <p>The original cost function (let’s say Mean Squared Error for linear regression) looks something like this:</p> <p>$ J(w) = \frac{1}{2m} \sum_{i=1}^{m} (h_w(x^{(i)}) - y^{(i)})^2 $</p> <p>With L2 regularization, it becomes:</p> <p>$ J_{L2}(w) = \frac{1}{2m} \sum_{i=1}^{m} (h_w(x^{(i)}) - y^{(i)})^2 + \lambda \sum_{j=1}^{p} w_j^2 $</p> <p>Let’s break that down:</p> <ul> <li>The first part is our standard error function – what we want to minimize.</li> <li>$ \lambda $ (lambda) is the <strong>regularization parameter</strong>. This is a hyperparameter we tune. It controls the strength of the penalty. A larger $ \lambda $ means a stronger penalty.</li> <li>$ \sum_{j=1}^{p} w_j^2 $ is the sum of the squares of all the model’s coefficients (weights).</li> </ul> <p><strong>Intuition:</strong> L2 regularization tries to keep all feature weights small. It “shrinks” them towards zero, but it rarely makes them exactly zero. Imagine you have a team of contributors (features). Ridge regularization says, “Everyone contribute, but don’t let anyone get <em>too</em> dominant. Share the responsibility.” This means all features will typically have <em>some</em> impact on the model, even if very small.</p> <h4 id="2-l1-regularization-lasso-regression">2. L1 Regularization (Lasso Regression)</h4> <p>Also known as <strong>Lasso Regression</strong> (Least Absolute Shrinkage and Selection Operator), L1 regularization adds a penalty term proportional to the <em>absolute value</em> of the magnitude of the coefficients.</p> <p>The cost function with L1 regularization looks like this:</p> <table> <tbody> <tr> <td>$ J_{L1}(w) = \frac{1}{2m} \sum_{i=1}^{m} (h_w(x^{(i)}) - y^{(i)})^2 + \lambda \sum_{j=1}^{p}</td> <td>w_j</td> <td>$</td> </tr> </tbody> </table> <p>Again, let’s unpack:</p> <ul> <li>The first part is the standard error function.</li> <li>$ \lambda $ is our regularization parameter, just like in L2.</li> <li> <table> <tbody> <tr> <td>$ \sum_{j=1}^{p}</td> <td>w_j</td> <td>$ is the sum of the absolute values of the coefficients.</td> </tr> </tbody> </table> </li> </ul> <p><strong>Intuition:</strong> L1 regularization also shrinks coefficients towards zero, but unlike L2, it has a tendency to drive some coefficients <em>exactly</em> to zero. This means it effectively performs <strong>feature selection</strong>. It says, “Okay, some of you (features) are not that important. I’m going to kick you off the team entirely.”</p> <p><strong>Why does L1 do this and L2 doesn’t?</strong> It’s a bit of a geometric nuance. When minimizing the cost function, the “L1 penalty diamond” has sharp corners along the axes, making it more likely for the optimal solution to land on an axis, thus setting a coefficient to zero. The “L2 penalty circle” is smooth, so coefficients are shrunk but rarely hit exactly zero.</p> <h4 id="3-elastic-net-regularization">3. Elastic Net Regularization</h4> <p>What if we want the best of both worlds? Enter <strong>Elastic Net regularization</strong>. It combines both L1 and L2 penalties:</p> <table> <tbody> <tr> <td>$ J_{EN}(w) = \frac{1}{2m} \sum_{i=1}^{m} (h_w(x^{(i)}) - y^{(i)})^2 + \lambda_1 \sum_{j=1}^{p}</td> <td>w_j</td> <td>+ \lambda_2 \sum_{j=1}^{p} w_j^2 $</td> </tr> </tbody> </table> <p>Here, we have two regularization parameters, $ \lambda_1 $ for the L1 penalty and $ \lambda_2 $ for the L2 penalty, giving us even more control. Elastic Net is particularly useful when you have many highly correlated features, as L1 tends to pick only one of them, while Elastic Net can select groups of correlated features.</p> <h3 id="the-power-of-lambda-lambda-tuning-the-penalty">The Power of Lambda ($\lambda$): Tuning the Penalty</h3> <p>The regularization parameter $ \lambda $ is crucial.</p> <ul> <li> <strong>If $ \lambda $ is 0:</strong> There’s no penalty, and our model is free to overfit.</li> <li> <strong>If $ \lambda $ is very small:</strong> The penalty is weak, and the model might still overfit.</li> <li> <strong>If $ \lambda $ is very large:</strong> The penalty is too strong. The model’s coefficients will be forced to be extremely small (or zero), leading to a very simple model that might <strong>underfit</strong> (it’s too simple to capture the underlying patterns).</li> <li> <strong>The sweet spot:</strong> We need to find an optimal $ \lambda $ that balances complexity and performance.</li> </ul> <p>How do we find this sweet spot? Through techniques like <strong>cross-validation</strong>. We train our model with different values of $ \lambda $, evaluate its performance on a validation set (data it hasn’t seen during training), and pick the $ \lambda $ that yields the best generalization performance.</p> <h3 id="beyond-l1l2-other-forms-of-regularization">Beyond L1/L2: Other Forms of Regularization</h3> <p>While L1 and L2 are fundamental, regularization is a broad concept. Other techniques that serve a similar purpose include:</p> <ul> <li> <strong>Dropout (for Neural Networks):</strong> Randomly “turns off” a fraction of neurons during training, preventing individual neurons from co-adapting too much.</li> <li> <strong>Early Stopping:</strong> Monitoring the model’s performance on a validation set during training and stopping when validation error starts to increase, even if training error is still decreasing.</li> <li> <strong>Data Augmentation:</strong> Creating more training data by applying transformations (e.g., rotating images, adding noise to text) to existing data. This makes the model more robust to variations.</li> <li> <strong>Batch Normalization:</strong> Standardizes the inputs to layers within a neural network, which can have a regularizing effect by smoothing the loss landscape.</li> </ul> <h3 id="when-to-use-regularization">When to Use Regularization?</h3> <p>Regularization is almost always a good idea, especially when:</p> <ul> <li>You have a complex model (e.g., many features, deep neural networks).</li> <li>Your dataset is small relative to the number of features.</li> <li>Your data is noisy.</li> <li>You suspect overfitting.</li> </ul> <p>In practice, it’s rare to train a complex machine learning model without some form of regularization. It’s a standard tool in the data scientist’s arsenal.</p> <h3 id="conclusion-the-art-of-balance">Conclusion: The Art of Balance</h3> <p>My journey into machine learning quickly taught me that the goal isn’t just to build <em>a</em> model, but to build a <em>robust, generalizable</em> model. Regularization is that crucial set of techniques that empowers us to do just that. It’s the wise mentor telling our overly eager models to simplify, to generalize, to truly understand rather than just memorize.</p> <p>Understanding L1, L2, and the concept of a penalty term not only enhances your model’s performance but also deepens your understanding of the subtle dance between bias and variance, and the constant quest for balance in the exciting world of data science. So, next time you’re training a model, remember to give it a little nudge towards humility with regularization – your test data will thank you!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>