<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Decoding the Giants: My Journey into Large Language Models | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/decoding-the-giants-my-journey-into-large-language/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Decoding the Giants: My Journey into Large Language Models</h1> <p class="post-meta"> Created on October 11, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/natural-language-processing"> <i class="fa-solid fa-hashtag fa-sm"></i> Natural Language Processing</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/transformers"> <i class="fa-solid fa-hashtag fa-sm"></i> Transformers</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>If you’re anything like me, you’ve probably been equal parts amazed and bewildered by the rapid advancements in Artificial Intelligence, especially the rise of those incredibly articulate “chatbots” we now interact with daily. From writing essays to debugging code, these Large Language Models (LLMs) seem to do it all. But what <em>are</em> they, really? Are they truly intelligent? Or just incredibly sophisticated parrots?</p> <p>As someone diving deep into Data Science and Machine Learning Engineering, I’ve found myself increasingly drawn to the magic behind these models. This post is a journey through my understanding of LLMs, designed to be accessible enough for curious high school students, yet deep enough for my fellow technical enthusiasts. Think of it as peeking under the hood of a futuristic car – we’ll understand the engine without needing to design every nut and bolt ourselves.</p> <h3 id="so-what-exactly-is-a-large-language-model">So, What Exactly <em>Is</em> a Large Language Model?</h3> <p>Forget the sci-fi movie robots for a moment. At their core, LLMs are incredibly complex statistical models designed to understand and generate human-like text. The “Large” in LLM refers to a few things:</p> <ol> <li> <strong>Large Amount of Data:</strong> They are trained on truly colossal datasets – think a significant chunk of the internet’s text (books, articles, websites, code).</li> <li> <strong>Large Number of Parameters:</strong> These models have billions, sometimes even trillions, of internal variables (parameters) that they adjust during training. It’s like having a brain with an unimaginable number of connections.</li> <li> <strong>Large Computational Resources:</strong> Training these models requires immense computing power, often utilizing thousands of specialized chips (GPUs or TPUs) running for months.</li> </ol> <p>Essentially, an LLM learns the patterns, grammar, facts, and even some nuances of human language by sifting through this mountain of text. Its primary goal? To predict the next word (or piece of a word) in a sequence.</p> <h3 id="the-ultimate-autocomplete-next-token-prediction">The Ultimate Autocomplete: Next Token Prediction</h3> <p>Imagine your phone’s autocomplete feature, but on steroids, with an IQ in the thousands, and access to all human knowledge. That’s essentially what an LLM does. Given a sequence of words, its fundamental task is to calculate the probability distribution of what the next word should be.</p> <p>Let’s say we have the sentence: “The cat sat on the…” An LLM doesn’t <em>understand</em> in the human sense, but it has learned from its training data that words like “mat,” “rug,” “floor,” or “sofa” are highly probable to follow. It assigns probabilities to countless possibilities:</p> <table> <tbody> <tr> <td>$P(next_word</td> <td>\text{“The cat sat on the”}) = { \text{mat}: 0.4, \text{rug}: 0.2, \text{floor}: 0.1, \text{moon}: 0.0001, … }$</td> </tr> </tbody> </table> <p>When you ask an LLM a question, it doesn’t “think” of an answer. It simply starts generating text, one token at a time, based on what it predicts is the most probable next token given the prompt and the text it has generated so far. This iterative process creates coherent, contextually relevant, and often astonishingly creative responses.</p> <h3 id="peeking-under-the-hood-the-transformer-architecture">Peeking Under the Hood: The Transformer Architecture</h3> <p>Before LLMs dominated the scene, models like Recurrent Neural Networks (RNNs) and Long Short-Term Memory networks (LSTMs) were the go-to for sequence data. However, they struggled with very long sequences and couldn’t process parts of the input simultaneously. Enter the Transformer!</p> <p>Introduced in 2017 by Google in the groundbreaking paper “Attention Is All You Need,” the Transformer architecture is the backbone of almost all modern LLMs. It introduced a revolutionary mechanism called <strong>Self-Attention</strong>.</p> <p>Let’s break down the key ideas:</p> <ol> <li> <p><strong>Tokenization:</strong> First, raw text needs to be converted into numerical representations the model can understand. This is done by a “tokenizer.” Instead of just words, LLMs often use sub-word units (tokens) like “run”, “##ning”, or “un##”. This helps handle unknown words and allows the model to work with a smaller vocabulary. Techniques like Byte-Pair Encoding (BPE) or WordPiece are commonly used.</p> </li> <li> <p><strong>Embeddings:</strong> Each token is then converted into a high-dimensional vector called an “embedding.” Think of these embeddings as coordinates in a vast semantic space where words with similar meanings (e.g., “king” and “queen”) are closer together.</p> </li> <li> <p><strong>Positional Encoding:</strong> Unlike RNNs, Transformers don’t inherently process words sequentially. To give the model information about the order of words in a sentence, we add “positional encodings” to the embeddings. This helps the model understand that “dog bites man” is different from “man bites dog.”</p> </li> <li> <p><strong>The Self-Attention Mechanism:</strong> This is the real star of the show. Imagine you’re reading a sentence: “The animal didn’t cross the street because <em>it</em> was too tired.” When you read “it,” you instinctively know “it” refers to “the animal.” Self-attention allows the model to do something similar.</p> <p>For each word in the input sequence, self-attention calculates how much “attention” it should pay to every other word in the sequence. It does this by creating three vectors for each word:</p> <ul> <li> <strong>Query (Q):</strong> What am I looking for? (e.g., “what’s the subject of this sentence?”)</li> <li> <strong>Key (K):</strong> What do I have to offer? (e.g., “I’m ‘animal’, a noun.”)</li> <li> <strong>Value (V):</strong> What information do I carry? (e.g., “I’m tired.”)</li> </ul> <p>The attention score between two words is computed by taking the dot product of their Query and Key vectors. A higher dot product means more relevance. These scores are then normalized (using a softmax function) and multiplied by the Value vectors to create a weighted sum.</p> <p>Mathematically, for a single attention head: $Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$ where $d_k$ is the dimension of the key vectors, used for scaling.</p> <p>This mechanism allows the model to dynamically weigh the importance of different words when processing a particular word, effectively capturing long-range dependencies in the text.</p> </li> <li> <p><strong>Multi-Head Attention:</strong> Instead of just one set of Q, K, V, Transformers use multiple “attention heads” in parallel. Each head learns to focus on different aspects of the relationships between words (e.g., one head might look for subjects, another for objects, another for sentiment). Their outputs are then concatenated and linearly transformed.</p> </li> <li> <p><strong>Feed-Forward Networks &amp; Residual Connections:</strong> After the attention layers, each token’s representation passes through a simple feed-forward neural network. Crucially, “residual connections” (or skip connections) and layer normalization are used throughout the architecture. These help stabilize training and prevent information loss by allowing gradients to flow more easily through the deep network.</p> </li> </ol> <p>Modern LLMs like GPT-3/4 or Llama are typically <em>decoder-only</em> Transformers. This means they are primarily designed for generating text, taking an input sequence and iteratively predicting the next token until a stop condition is met.</p> <h3 id="the-training-regimen-pre-training-and-fine-tuning">The Training Regimen: Pre-training and Fine-tuning</h3> <p>The journey of an LLM involves two main phases:</p> <ol> <li> <p><strong>Pre-training:</strong> This is the massive, unsupervised learning phase. The model is fed vast amounts of raw text and trained on a simple, yet powerful, objective: predict the next token. By doing this repeatedly across trillions of tokens, the model develops a deep statistical understanding of language, grammar, facts, and even reasoning patterns. It’s like a student who has read every book in the world and can now finish any sentence you start.</p> </li> <li> <p><strong>Fine-tuning (Optional but Crucial):</strong> While pre-training gives the model general language abilities, it might not be great at following specific instructions or behaving safely. This is where fine-tuning comes in.</p> <ul> <li> <strong>Instruction Tuning:</strong> The model is further trained on datasets of prompt-response pairs, where humans have demonstrated how to follow instructions (e.g., “Summarize this article,” “Write a poem about X”).</li> <li> <strong>Reinforcement Learning from Human Feedback (RLHF):</strong> This is a powerful technique where human evaluators rank the quality of different model responses. These rankings are then used to train a “reward model,” which in turn guides the LLM to generate responses that are preferred by humans. This makes models more helpful, harmless, and honest.</li> </ul> </li> </ol> <h3 id="why-large-matters-scaling-laws-and-emergent-abilities">Why “Large” Matters: Scaling Laws and Emergent Abilities</h3> <p>The surprising thing about LLMs is not just that they work, but that they get <em>dramatically</em> better as they scale up in size (parameters) and training data. This phenomenon is described by <strong>scaling laws</strong>, which suggest a predictable relationship between model size, data, computation, and performance.</p> <p>What’s truly fascinating are <strong>emergent abilities</strong>. These are capabilities that aren’t present in smaller models but suddenly “emerge” in larger ones, often without explicit training. Examples include:</p> <ul> <li> <strong>In-context learning:</strong> The ability to learn a new task from just a few examples given in the prompt, without updating its weights.</li> <li> <strong>Chain-of-thought prompting:</strong> Breaking down complex problems into intermediate steps to improve reasoning.</li> <li><strong>Complex code generation or mathematical problem-solving.</strong></li> </ul> <p>These emergent abilities are what make LLMs feel so incredibly intelligent and versatile.</p> <h3 id="challenges-and-the-road-ahead">Challenges and the Road Ahead</h3> <p>Despite their incredible power, LLMs are not perfect and come with significant challenges:</p> <ul> <li> <strong>Hallucinations:</strong> They can confidently generate factually incorrect information. Remember, they are predicting probabilities, not accessing a knowledge base.</li> <li> <strong>Bias:</strong> They can perpetuate and even amplify biases present in their training data.</li> <li> <strong>Computational Cost:</strong> Training and running these models is extremely expensive, both financially and environmentally.</li> <li> <strong>Lack of True Understanding:</strong> While they mimic understanding, they don’t possess consciousness, common sense, or real-world experience. They are statistical models, not sentient beings.</li> <li> <strong>Safety and Ethics:</strong> Ensuring these models are used responsibly, preventing misuse, and addressing job displacement are ongoing ethical considerations.</li> </ul> <h3 id="my-takeaway-and-the-future">My Takeaway and the Future</h3> <p>My journey into understanding LLMs has been one of constant awe and a healthy dose of critical thinking. They are undeniably powerful tools that are reshaping industries and our daily lives. From aiding scientific discovery to revolutionizing creative workflows, their potential is immense.</p> <p>However, it’s crucial to remember their limitations. As data scientists and MLEs, our role isn’t just to build these systems, but to understand their inner workings, anticipate their flaws, and guide their ethical development. The field is moving at lightning speed, with new architectures and training techniques emerging constantly.</p> <p>If you’re interested in diving deeper, I encourage you to explore resources on Transformers, experiment with open-source LLMs, and perhaps even try fine-tuning a smaller model yourself. The future of AI is being written right now, and understanding these giants is a fantastic first step to being a part of it.</p> <p>Happy learning!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>