<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Beyond Accuracy: Charting the True Course of Your Classification Models with ROC and AUC | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/beyond-accuracy-charting-the-true-course-of-your-c/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Beyond Accuracy: Charting the True Course of Your Classification Models with ROC and AUC</h1> <p class="post-meta"> Created on October 29, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/classification"> <i class="fa-solid fa-hashtag fa-sm"></i> Classification</a>   <a href="/blog/blog/tag/model-evaluation"> <i class="fa-solid fa-hashtag fa-sm"></i> Model Evaluation</a>   <a href="/blog/blog/tag/roc-curve"> <i class="fa-solid fa-hashtag fa-sm"></i> ROC Curve</a>   <a href="/blog/blog/tag/auc"> <i class="fa-solid fa-hashtag fa-sm"></i> AUC</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>It’s [Your Name Here], and today, I want to share something that fundamentally changed how I evaluate classification models. When I first started diving into machine learning, accuracy was my go-to metric. My model correctly predicted 90% of the cases? Fantastic! 95%? Even better! But soon, I hit a wall, realizing that accuracy, while seemingly straightforward, can be a deceptive friend.</p> <p>Imagine you’re building a model to detect a rare but critical disease that affects only 1% of the population. A model that simply predicts “no disease” for everyone would achieve a 99% accuracy! Sounds great, right? But it’s utterly useless for diagnosing the actual disease. This eye-opening realization made me question: <strong>How do we truly understand if our model is good at distinguishing between classes, regardless of their prevalence?</strong></p> <p>That’s where the mighty duo of <strong>ROC (Receiver Operating Characteristic) Curve</strong> and <strong>AUC (Area Under the Curve)</strong> steps in. They’re not just fancy terms; they’re essential tools in every data scientist’s arsenal, helping us peer deeper into our model’s performance.</p> <p>Let’s unpack this, piece by piece, as if we’re discovering them together.</p> <hr> <h3 id="the-bedrock-the-confusion-matrix">The Bedrock: The Confusion Matrix</h3> <p>Before we leap into ROC and AUC, we need to get cozy with their foundational elements: the <strong>Confusion Matrix</strong>. Think of it as a scorecard for your classification model, breaking down how well it did across different types of predictions.</p> <p>Let’s consider a binary classification problem – say, predicting if an email is “spam” (Positive) or “not spam” (Negative).</p> <table> <thead> <tr> <th style="text-align: left"> </th> <th style="text-align: left">Predicted Positive</th> <th style="text-align: left">Predicted Negative</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Actual Positive</td> <td style="text-align: left">True Positive (TP)</td> <td style="text-align: left">False Negative (FN)</td> </tr> <tr> <td style="text-align: left">Actual Negative</td> <td style="text-align: left">False Positive (FP)</td> <td style="text-align: left">True Negative (TN)</td> </tr> </tbody> </table> <p>Here’s what each cell means:</p> <ul> <li> <strong>True Positive (TP):</strong> The model correctly predicted spam when it was actually spam. (Yay!)</li> <li> <strong>True Negative (TN):</strong> The model correctly predicted not-spam when it was actually not-spam. (Another yay!)</li> <li> <strong>False Positive (FP):</strong> The model incorrectly predicted spam when it was actually not-spam. (Uh oh, important email in spam!)</li> <li> <strong>False Negative (FN):</strong> The model incorrectly predicted not-spam when it was actually spam. (Oops, spam in my inbox!)</li> </ul> <p>These four values are the ingredients for nearly every classification metric, including the ones we’re focusing on today.</p> <hr> <h3 id="the-core-metrics-tpr-and-fpr">The Core Metrics: TPR and FPR</h3> <p>From the confusion matrix, we can derive several crucial rates that form the very essence of the ROC curve. The two most important for our discussion are:</p> <ol> <li> <strong>True Positive Rate (TPR)</strong>: Also known as <strong>Sensitivity</strong> or <strong>Recall</strong>. <ul> <li>This tells us, “Out of all the actual positive cases, how many did our model correctly identify?”</li> <li>It’s the proportion of actual positive instances that are correctly predicted as positive.</li> <li>The formula is: $TPR = \frac{TP}{TP + FN}$</li> </ul> <p>In our spam example, a high TPR means our model is great at catching spam emails. We want to maximize this!</p> </li> <li> <strong>False Positive Rate (FPR)</strong>: <ul> <li>This tells us, “Out of all the actual negative cases, how many did our model <em>incorrectly</em> identify as positive?”</li> <li>It’s the proportion of actual negative instances that are wrongly predicted as positive.</li> <li>The formula is: $FPR = \frac{FP}{FP + TN}$</li> </ul> <p>In the spam example, a high FPR means our model is wrongly flagging legitimate emails as spam. This is bad; nobody wants their important emails in the junk folder! We generally want to minimize this.</p> </li> </ol> <p>Notice the tension here? Often, increasing TPR (catching more spam) might lead to an increase in FPR (more legitimate emails wrongly classified as spam), and vice-versa. It’s a balancing act!</p> <hr> <h3 id="the-visual-storyteller-the-roc-curve">The Visual Storyteller: The ROC Curve</h3> <p>Now for the magic! How do we visualize this trade-off between TPR and FPR across <em>all possible scenarios</em> for our model? Enter the ROC curve.</p> <p>Most classification models don’t just spit out “spam” or “not spam.” Instead, they output a <strong>probability</strong> (or a score) that an email is spam (e.g., “This email has an 80% chance of being spam”). To make a final decision, we apply a <strong>classification threshold</strong>. If the probability is above the threshold, we classify it as positive (spam); otherwise, negative (not spam).</p> <ul> <li>If our threshold is very low (e.g., &gt;0.1 probability = spam), we’ll catch almost all spam (high TPR), but we’ll also misclassify many legitimate emails as spam (high FPR).</li> <li>If our threshold is very high (e.g., &gt;0.9 probability = spam), we’ll have very few legitimate emails ending up in spam (low FPR), but we might miss a lot of actual spam (low TPR).</li> </ul> <p>The ROC curve is created by plotting the TPR against the FPR at <em>every possible classification threshold</em>.</p> <p><strong>What does it look like and what does it mean?</strong></p> <ul> <li>The x-axis represents the <strong>False Positive Rate (FPR)</strong>.</li> <li>The y-axis represents the <strong>True Positive Rate (TPR)</strong>.</li> </ul> <p>Let’s visualize the journey of an ROC curve:</p> <ol> <li> <strong>Starting Point (0,0):</strong> This point represents a very strict threshold (e.g., classifying nothing as positive). Here, both TPR and FPR are 0 because we’re not making any positive predictions.</li> <li> <strong>End Point (1,1):</strong> This point represents a very lenient threshold (e.g., classifying everything as positive). Here, both TPR and FPR are 1 because we’re predicting every instance as positive.</li> <li> <strong>The Random Classifier (Diagonal Line):</strong> A model that makes predictions randomly will generate an ROC curve that roughly follows the diagonal line from (0,0) to (1,1). This means its TPR is roughly equal to its FPR – it’s no better than guessing.</li> <li> <strong>The Perfect Classifier:</strong> A dream model would have an ROC curve that shoots straight up from (0,0) to (0,1) and then straight across to (1,1). This means it achieves a TPR of 1 (catches all positives) with an FPR of 0 (no false alarms), for some threshold. Pure perfection!</li> </ol> <p><strong>Interpreting ROC Curve Shapes:</strong></p> <ul> <li> <strong>A good model’s ROC curve will bow up towards the top-left corner.</strong> This indicates that the model achieves a high TPR while keeping FPR low. The closer the curve is to the top-left corner, the better the model’s discriminative ability.</li> <li> <strong>The further away the curve is from the diagonal line, the better.</strong> The area <em>above</em> the diagonal line represents real discriminative power.</li> </ul> <p>ROC curves give us a comprehensive, visual understanding of our model’s performance across all possible decision thresholds, making it invaluable for comparing models.</p> <hr> <h3 id="the-quantitative-summary-auc-area-under-the-curve">The Quantitative Summary: AUC (Area Under the Curve)</h3> <p>While the ROC curve provides a fantastic visual, sometimes we need a single number to summarize its performance, especially when comparing multiple models. That’s where <strong>AUC (Area Under the ROC Curve)</strong> comes in.</p> <p>As its name suggests, the AUC is simply the <strong>area underneath the entire ROC curve</strong>.</p> <p><strong>Key characteristics of AUC:</strong></p> <ul> <li> <strong>Range:</strong> AUC values typically range from 0 to 1 ($0 \le AUC \le 1$).</li> <li> <strong>Interpretation:</strong> <ul> <li> <strong>AUC = 0.5:</strong> This means your model is performing no better than a random guess (like flipping a coin). Its ROC curve would lie along the diagonal line.</li> <li> <strong>AUC = 1.0:</strong> This represents a perfect classifier, one that can distinguish between positive and negative classes perfectly. Its ROC curve would hit the top-left corner.</li> <li> <strong>AUC &lt; 0.5:</strong> This is rare, but it means your model is performing <em>worse</em> than random. Interestingly, you could simply invert its predictions, and it would perform better than random!</li> </ul> </li> </ul> <p><strong>The Deeper Meaning of AUC:</strong></p> <p>Beyond just being an area, AUC has a beautiful probabilistic interpretation:</p> <blockquote> <p><strong>AUC represents the probability that a randomly chosen positive instance will be ranked higher than a randomly chosen negative instance by the classifier.</strong></p> </blockquote> <p>Think about that for a moment. If your AUC is 0.8, it means there’s an 80% chance that if you pick one random spam email and one random legitimate email, your model will assign a higher spam probability to the actual spam email. This is incredibly powerful and intuitive for understanding a model’s ability to discriminate.</p> <hr> <h3 id="why-roc-and-auc-are-your-best-friends-often-better-than-accuracy">Why ROC and AUC are Your Best Friends (Often Better Than Accuracy)</h3> <p>Now, let’s circle back to why these metrics are so crucial, especially for you, budding data scientists:</p> <ol> <li> <strong>Threshold-Independent Evaluation:</strong> <ul> <li>Accuracy depends entirely on the chosen classification threshold. Change the threshold, and your accuracy changes.</li> <li>ROC and AUC evaluate the model’s performance across <em>all possible thresholds</em>. This gives you a holistic view of the model’s inherent ability to separate classes, irrespective of where you eventually set your decision boundary. You can assess if your model <em>can</em> perform well, even if you need to fine-tune the threshold for specific business needs later.</li> </ul> </li> <li> <strong>Insensitivity to Class Imbalance:</strong> <ul> <li>Remember our disease detection example with 99% negative cases? Accuracy was 99% for a useless model.</li> <li>ROC and AUC are <em>not</em> fooled by imbalanced datasets. They measure the model’s ability to distinguish between classes. A model that predicts “no disease” for everyone would have an AUC of 0.5 (random guess), correctly reflecting its uselessness, despite its high accuracy. This is perhaps their most celebrated strength.</li> </ul> </li> <li> <strong>Comprehensive Comparison:</strong> <ul> <li>When comparing multiple models, simply looking at accuracy can be misleading. A model with slightly lower accuracy might have a much better ROC curve, indicating superior discriminative power, especially in areas of the curve that are important for your specific problem (e.g., needing a very low FPR).</li> <li>AUC provides a single, robust number to compare models, making it easy to identify which model is generally better at distinguishing positive from negative classes.</li> </ul> </li> </ol> <hr> <h3 id="a-quick-peek-under-the-hood-conceptual">A Quick Peek Under the Hood (Conceptual)</h3> <p>In practice, generating an ROC curve and calculating AUC is straightforward with libraries like <code class="language-plaintext highlighter-rouge">scikit-learn</code> in Python.</p> <p>You typically train your classification model, then predict probabilities for your test set.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_curve</span><span class="p">,</span> <span class="n">roc_auc_score</span>
<span class="c1"># ... train your_model ...
# Get predicted probabilities for the positive class
</span><span class="n">y_prob</span> <span class="o">=</span> <span class="n">your_model</span><span class="p">.</span><span class="nf">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>
<span class="c1"># Calculate FPR, TPR, and thresholds
</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> <span class="nf">roc_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_prob</span><span class="p">)</span>
<span class="c1"># Calculate AUC
</span><span class="n">auc_score</span> <span class="o">=</span> <span class="nf">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_prob</span><span class="p">)</span>

<span class="c1"># Plotting fpr vs tpr gives you the ROC curve!
</span></code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">roc_curve</code> function automatically calculates the TPR and FPR at various thresholds extracted from your model’s probability predictions. You then plot these points to visualize the curve. The <code class="language-plaintext highlighter-rouge">roc_auc_score</code> function gives you the numerical summary.</p> <hr> <h3 id="a-word-of-caution-when-auc-isnt-the-only-answer">A Word of Caution: When AUC Isn’t the Only Answer</h3> <p>While incredibly powerful, ROC and AUC aren’t always the <em>absolute</em> final word.</p> <ul> <li> <strong>Extreme Class Imbalance:</strong> In cases of <em>extremely</em> skewed class distributions (e.g., 1 positive case in 1,000,000), the Precision-Recall (PR) curve might offer a more informative view, especially when the cost of False Positives is very high. PR curves focus on the positive class performance more directly.</li> <li> <strong>Cost Sensitivity:</strong> ROC/AUC tell you <em>how well</em> your model distinguishes classes, but they don’t inherently tell you the <em>optimal threshold</em> for your specific problem, which depends on the relative costs of False Positives vs. False Negatives in your domain. You might still need to select a threshold on the ROC curve based on your business objectives.</li> </ul> <hr> <h3 id="wrapping-up-see-beyond-the-surface">Wrapping Up: See Beyond the Surface!</h3> <p>I hope this journey into ROC curves and AUC scores has illuminated their power and importance. My aim was to show you that relying solely on accuracy can sometimes lead you astray, especially in the nuanced world of machine learning where data is rarely perfectly balanced or ideal.</p> <p>By understanding the confusion matrix, TPR, FPR, and then visualizing their trade-offs with the ROC curve and summarizing it with AUC, you gain a far more robust and insightful perspective on your model’s true discriminative capabilities. This understanding empowers you to build not just “accurate” models, but truly effective and reliable ones.</p> <p>So, next time you’re evaluating a classification model, push beyond that initial accuracy score. Ask yourself: “What does the ROC curve look like? What’s the AUC telling me?” You’ll be amazed at the deeper story they tell.</p> <p>Happy modeling! [Your Name Here]</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>