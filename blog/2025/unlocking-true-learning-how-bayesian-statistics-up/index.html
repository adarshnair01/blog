<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Unlocking True Learning: How Bayesian Statistics Updates Our Worldview (and Our Data Models) | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/unlocking-true-learning-how-bayesian-statistics-up/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Unlocking True Learning: How Bayesian Statistics Updates Our Worldview (and Our Data Models)</h1> <p class="post-meta"> Created on February 12, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/bayesian-statistics"> <i class="fa-solid fa-hashtag fa-sm"></i> Bayesian Statistics</a>   <a href="/blog/blog/tag/probability"> <i class="fa-solid fa-hashtag fa-sm"></i> Probability</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/statistical-inference"> <i class="fa-solid fa-hashtag fa-sm"></i> Statistical Inference</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>As a budding data scientist, I’ve spent a lot of time diving into different ways we make sense of data. We learn about averages, standard deviations, hypothesis testing, and the dreaded p-value. These tools are incredibly powerful, and they form the bedrock of much of what we do. But sometimes, when I’m looking at a p-value or constructing a confidence interval, I can’t help but feel like something’s missing. It’s like I’m asking a specific question, and the statistics are giving me an answer to a slightly different, more indirect one.</p> <p>“What’s the probability that <em>my hypothesis is true</em>, given the data I’ve observed?” – this is the question that often swirls in my mind. Yet, traditional (frequentist) hypothesis testing, with its focus on p-values, essentially asks: “What’s the probability of observing this data (or more extreme data), <em>if my null hypothesis were true</em>?” It’s a subtle but profoundly important distinction. It leaves me wanting to incorporate my prior understanding, my intuition, or any existing knowledge I have <em>before</em> seeing the data. And that’s exactly where Bayesian statistics steps in, offering a profoundly intuitive and powerful alternative framework for understanding and learning from the world.</p> <h3 id="the-problem-with-fixed-truth">The Problem with “Fixed Truth”</h3> <p>Let’s be honest, in the real world, we rarely start with a blank slate. If you’re trying to decide if a new drug works, you don’t ignore all previous research or biological understanding. If you’re predicting stock prices, you don’t forget everything you know about the company or the market. But frequentist methods often treat hypotheses as fixed, unchangeable truths (or falsehoods) that we try to “reject” or “fail to reject.” We get point estimates and p-values that tell us about the data under a specific assumption, but not directly about the probability of our assumptions being true. This can lead to a feeling of disconnect, especially when dealing with smaller datasets or when strong prior information exists.</p> <p>What if we could start with a degree of belief, then systematically update that belief as new evidence rolls in? What if statistics could formalize the way humans naturally learn? This is the core idea of Bayesian thinking.</p> <h3 id="enter-thomas-bayes-and-the-art-of-updating-beliefs">Enter Thomas Bayes and the Art of Updating Beliefs</h3> <p>At the heart of Bayesian statistics lies a beautiful, elegant formula discovered by an 18th-century Presbyterian minister and mathematician, Thomas Bayes. It’s known as <strong>Bayes’ Theorem</strong>, and it’s the mathematical engine for updating our beliefs.</p> <p>Bayes’ Theorem states:</p> \[P(H|E) = \frac{P(E|H) \cdot P(H)}{P(E)}\] <p>Let’s break down this seemingly simple equation because each piece tells a crucial part of our story:</p> <ul> <li> <table> <tbody> <tr> <td>**$P(H</td> <td>E)$ (The Posterior Probability)<strong>: This is what we <em>really</em> want to know! It’s the probability of our **Hypothesis (H)</strong> being true, <em>given the Evidence (E)</em> we’ve just observed. This is our updated belief.</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>**$P(E</td> <td>H)$ (The Likelihood)<strong>: This tells us how likely we were to observe the **Evidence (E)</strong> if our <strong>Hypothesis (H)</strong> were actually true. It’s the engine that links the data to our hypothesis.</td> </tr> </tbody> </table> </li> <li> <strong>$P(H)$ (The Prior Probability)</strong>: This is our initial belief or knowledge about the <strong>Hypothesis (H)</strong> <em>before</em> we’ve seen any new evidence. This is where we inject our existing understanding, intuition, or historical data. It’s our starting point.</li> <li> <strong>$P(E)$ (The Evidence or Marginal Likelihood)</strong>: This is the overall probability of observing the <strong>Evidence (E)</strong>, regardless of whether our hypothesis is true or not. In many practical applications, we don’t need to calculate this term directly because it acts as a normalizing constant to ensure our posterior probabilities sum to 1. For now, think of it as “the probability of seeing the data.”</li> </ul> <p>In essence, Bayes’ Theorem says: <strong>Our updated belief about a hypothesis (Posterior) is proportional to our initial belief (Prior) multiplied by how well the evidence supports that hypothesis (Likelihood).</strong></p> <h3 id="a-fair-coin-an-intuitive-walkthrough">A Fair Coin? An Intuitive Walkthrough</h3> <p>Let’s make this concrete with a simple, relatable example.</p> <p>Imagine you find a coin on the street. You pick it up, and you want to know if it’s a fair coin or if it’s somehow biased towards heads.</p> <p>We can define two competing hypotheses:</p> <ul> <li> <table> <tbody> <tr> <td>$H_F$: The coin is <strong>Fair</strong>. ($P(\text{Heads}</td> <td>H_F) = 0.5$)</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>$H_B$: The coin is <strong>Biased</strong>. (Let’s say it lands heads 90% of the time: $P(\text{Heads}</td> <td>H_B) = 0.9$)</td> </tr> </tbody> </table> </li> </ul> <p><strong>Step 1: Define Your Priors ($P(H)$)</strong></p> <p>Before you even flip the coin, what do you believe? Most coins are fair, so you might lean towards $H_F$. But for simplicity, let’s say you’re equally open to either possibility. This is your <em>prior belief</em>:</p> <ul> <li>$P(H_F) = 0.5$ (50% chance it’s fair)</li> <li>$P(H_B) = 0.5$ (50% chance it’s biased)</li> </ul> <p><strong>Step 2: Gather Evidence (E)</strong></p> <p>You flip the coin once. It lands on <strong>Heads</strong>. This is your evidence, $E = \text{Heads}$.</p> <table> <tbody> <tr> <td>**Step 3: Calculate the Likelihoods ($P(E</td> <td>H)$)**</td> </tr> </tbody> </table> <p>Now, how likely is it to get a Head <em>under each hypothesis</em>?</p> <ul> <li> <table> <tbody> <tr> <td>If the coin is fair ($H_F$): $P(\text{Heads}</td> <td>H_F) = 0.5$</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>If the coin is biased ($H_B$): $P(\text{Heads}</td> <td>H_B) = 0.9$</td> </tr> </tbody> </table> </li> </ul> <p><strong>Step 4: Calculate the Evidence Probability ($P(E)$)</strong></p> <p>This is the overall probability of observing a Head, considering both hypotheses and their priors. $P(E) = P(E|H_F) \cdot P(H_F) + P(E|H_B) \cdot P(H_B)$ $P(E) = (0.5 \cdot 0.5) + (0.9 \cdot 0.5) = 0.25 + 0.45 = 0.7$</p> <p>So, there’s a 70% chance of getting a Head in this scenario (before knowing which hypothesis is true).</p> <table> <tbody> <tr> <td>**Step 5: Calculate the Posteriors ($P(H</td> <td>E)$)**</td> </tr> </tbody> </table> <p>Now we can update our beliefs using Bayes’ Theorem:</p> <p>For the coin being Fair ($H_F$): $P(H_F|\text{Heads}) = \frac{P(\text{Heads}|H_F) \cdot P(H_F)}{P(\text{Heads})} = \frac{0.5 \cdot 0.5}{0.7} = \frac{0.25}{0.7} \approx 0.357$</p> <p>For the coin being Biased ($H_B$): $P(H_B|\text{Heads}) = \frac{P(\text{Heads}|H_B) \cdot P(H_B)}{P(\text{Heads})} = \frac{0.9 \cdot 0.5}{0.7} = \frac{0.45}{0.7} \approx 0.643$</p> <p><strong>What happened?</strong></p> <p>Our initial belief was a 50/50 chance for either coin. After seeing just one Head, our belief in the coin being fair dropped to about 35.7%, while our belief in it being biased jumped to about 64.3%. We’ve <em>updated our beliefs</em>!</p> <p>If you were to flip the coin again and get another Head, you would use these new posterior probabilities (0.357 and 0.643) as your <em>new priors</em> and repeat the process. This is the beauty of sequential learning in Bayesian statistics – your knowledge accumulates.</p> <h3 id="the-power-of-priors-not-just-a-guess">The Power of Priors: Not Just a Guess</h3> <p>One of the most common critiques of Bayesian statistics is the “subjectivity” of priors. “How do you choose $P(H)$?” people ask. But let’s clarify:</p> <ol> <li> <strong>Priors are not random guesses:</strong> They can be based on historical data, expert opinion, previous studies, or even physical laws. They are a formal way to incorporate existing knowledge.</li> <li> <strong>Priors can be “uninformative”</strong>: If you truly have no strong prior belief, you can use a “flat” or “uninformative” prior (e.g., $P(H_F)=P(H_B)$ like our example, or a uniform distribution over a range of possible values). This essentially lets the data speak for itself.</li> <li> <table> <tbody> <tr> <td> <strong>Priors get “washed out” with enough data</strong>: As you gather more and more evidence, the likelihood term ($P(E</td> <td>H)$) starts to dominate, and the influence of your initial prior diminishes. With a ton of data, different reasonable priors will often lead to very similar posterior results.</td> </tr> </tbody> </table> </li> </ol> <p>The prior isn’t a weakness; it’s a strength! It makes our assumptions explicit and allows our models to learn more efficiently, especially when data is scarce.</p> <h3 id="why-bayesian-thinking-rocks-for-data-scientists-and-mles">Why Bayesian Thinking Rocks for Data Scientists and MLEs</h3> <ol> <li> <table> <tbody> <tr> <td> <strong>Direct Answers to the Right Questions</strong>: We get $P(\text{hypothesis}</td> <td>\text{data})$, which is often what we intuitively want to know. No more trying to interpret what “failing to reject the null” means for our specific research question.</td> </tr> </tbody> </table> </li> <li> <strong>Full Uncertainty Quantification</strong>: Instead of just a point estimate (like a mean), Bayesian methods give us an entire <strong>posterior distribution</strong>. This distribution tells us not just the most probable value for a parameter, but also how certain we are about it, including credible intervals (the Bayesian equivalent of confidence intervals, but much more intuitive: “There’s a 95% probability the true value lies within this range”).</li> <li> <strong>Sequential Learning is Natural</strong>: As seen with the coin flip, our models can continuously update as new data arrives without having to restart from scratch. This is invaluable in real-time systems, A/B testing (allowing us to stop experiments early if there’s clear evidence), and online learning.</li> <li> <strong>Incorporating Domain Knowledge</strong>: Priors allow experts to contribute their knowledge directly into the statistical model, leading to more robust and accurate inferences, especially in fields where data might be sparse or expensive to collect (e.g., medical research, climate modeling).</li> <li> <strong>Robustness with Small Data</strong>: When you don’t have mountains of data, strong priors can help prevent overfitting and provide more stable estimates than frequentist methods might.</li> <li> <strong>Bayesian Methods in Machine Learning</strong>: <ul> <li> <strong>A/B Testing</strong>: Deciding which website variant is better can be done more efficiently and ethically.</li> <li> <strong>Recommender Systems</strong>: Bayesian methods can model user preferences and item characteristics to provide personalized recommendations.</li> <li> <strong>Bayesian Optimization</strong>: Efficiently finding the best hyperparameters for complex ML models.</li> <li> <strong>Uncertainty in Deep Learning</strong>: Bayesian Neural Networks provide not just predictions but also a measure of their confidence in those predictions, which is crucial for high-stakes applications like autonomous driving or medical diagnosis.</li> <li> <strong>Gaussian Processes</strong>: Powerful non-parametric models used for regression, classification, and optimization, inherently Bayesian.</li> </ul> </li> </ol> <h3 id="the-road-ahead-challenges--computations">The Road Ahead: Challenges &amp; Computations</h3> <p>While conceptually elegant, Bayesian statistics can be computationally more intensive, especially for complex models. Calculating that $P(E)$ term (the marginal likelihood) often involves complex integrals that don’t have analytical solutions.</p> <p>This is where advanced computational techniques like <strong>Markov Chain Monte Carlo (MCMC)</strong> methods come into play. MCMC algorithms (like Metropolis-Hastings or Gibbs sampling) essentially “sample” from the posterior distribution when direct calculation isn’t feasible. They’re powerful workhorses that allow us to apply Bayesian methods to almost any problem, though they require careful setup and convergence diagnostics.</p> <p>Choosing appropriate priors can also be a challenge, requiring thought and often sensitivity analyses to see how much your choice influences the results. However, this explicit choice is also a strength – it forces us to acknowledge and formalize our assumptions.</p> <h3 id="conclusion-embracing-a-more-intuitive-path">Conclusion: Embracing a More Intuitive Path</h3> <p>Bayesian statistics isn’t just a set of equations; it’s a philosophy, a way of thinking about how we learn. It mirrors our natural human process of starting with a belief, encountering new information, and updating our understanding. For data scientists and machine learning engineers, it provides a powerful, flexible, and intuitive framework for building models that truly learn, quantify uncertainty, and leverage all available information – both from data and from domain knowledge.</p> <p>It offers a compelling answer to the question: “What’s the probability that <em>this is true</em>?” By embracing Bayesian methods, we move beyond just rejecting null hypotheses and step into a world of continuously evolving, nuanced beliefs about the true state of the world. So, next time you’re grappling with data, consider letting Bayes guide your learning journey. You might find it provides not just answers, but a deeper understanding.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>