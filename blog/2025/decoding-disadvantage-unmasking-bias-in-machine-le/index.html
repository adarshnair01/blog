<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Decoding Disadvantage: Unmasking Bias in Machine Learning | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/2025/decoding-disadvantage-unmasking-bias-in-machine-le/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Decoding Disadvantage: Unmasking Bias in Machine Learning</h1> <p class="post-meta"> Created on February 21, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/ai-ethics"> <i class="fa-solid fa-hashtag fa-sm"></i> AI Ethics</a>   <a href="/blog/blog/tag/bias"> <i class="fa-solid fa-hashtag fa-sm"></i> Bias</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/fairness"> <i class="fa-solid fa-hashtag fa-sm"></i> Fairness</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>As a budding data scientist and ML engineer, I’m constantly fascinated by the power of artificial intelligence to transform our world. From predicting stock market trends to powering self-driving cars, algorithms are becoming interwoven with the fabric of our daily lives. But beneath the shiny surface of innovation lies a profound challenge, one that demands our urgent attention: <strong>bias in machine learning</strong>.</p> <p>I remember my first deep dive into a real-world dataset. The numbers told a compelling story, seemingly objective and unbiased. Yet, as I began to build models and observe their outputs, I started noticing subtle patterns – patterns that sometimes felt… off. It wasn’t the algorithms themselves being intentionally malicious; it was something more insidious, a reflection of the world’s imperfections mirrored in the very data these systems learned from.</p> <p>This isn’t just an abstract academic problem; it has very real, very human consequences. This post is an exploration into what bias in ML truly means, where it hides, how it impacts us, and what we, as responsible builders and users of AI, can do to combat it.</p> <h3 id="what-is-bias-in-machine-learning-its-more-than-just-bad">What is Bias in Machine Learning? It’s More Than Just “Bad”</h3> <p>When we talk about “bias” in everyday language, we usually mean prejudice or unfair favoritism. In the context of machine learning, it’s a bit more nuanced.</p> <p>First, there’s a statistical definition of bias: the difference between the expected value of a statistical estimator and the true underlying parameter it’s trying to estimate. Think of it like a dart player who consistently aims slightly to the left of the bullseye; their throws are “biased” to the left. This kind of bias isn’t inherently good or bad; it’s just a statistical property.</p> <p>However, the “bias” we’re primarily concerned with in ethical AI discussions is <strong>societal or ethical bias</strong>. This refers to systematic and unfair discrimination against certain individuals or groups, often based on attributes like race, gender, age, socioeconomic status, or religion. When an ML model exhibits this kind of bias, it makes predictions or decisions that disproportionately disadvantage or harm specific groups.</p> <p>Imagine a machine learning model as a student. If that student is only given textbooks written by a very narrow group of authors, with a limited worldview, their understanding of the world will inevitably be skewed. Our AI models are those students, and their textbooks are the data we feed them.</p> <h3 id="where-does-bias-come-from-the-roots-of-the-problem">Where Does Bias Come From? The Roots of the Problem</h3> <p>Understanding the sources of bias is the first step towards mitigating it. Bias isn’t a bug in the code; it’s often a feature of the data or the design process itself.</p> <h4 id="1-data-bias-the-mirror-reflects-our-flaws">1. Data Bias: The Mirror Reflects Our Flaws</h4> <p>This is arguably the most pervasive source of bias. Machine learning models learn from data, and if that data is incomplete, unrepresentative, or reflects existing societal prejudices, the model will simply learn and perpetuate those biases.</p> <ul> <li> <p><strong>Historical Bias:</strong> This is perhaps the most difficult to address. Our past data often reflects historical and systemic inequalities. For example, if historical loan application data shows that a particular demographic group received fewer loans (due to past discrimination, not necessarily creditworthiness), an ML model trained on this data might learn to unfairly deny loans to new applicants from that same group. The model isn’t “racist”; it’s simply learning the patterns of a biased history.</p> </li> <li> <strong>Selection Bias:</strong> Occurs when the data used to train a model is not truly representative of the real-world population it’s meant to serve. <ul> <li> <em>Example:</em> Early facial recognition systems were notoriously less accurate for women and people of color because their training datasets were overwhelmingly composed of white men. This imbalance meant the model had less “experience” recognizing other demographics.</li> </ul> </li> <li> <strong>Reporting Bias:</strong> When certain outcomes or attributes are over- or under-represented in the data because of how information is collected or reported. <ul> <li> <em>Example:</em> If online review platforms are predominantly used by younger, tech-savvy demographics, a recommendation system trained on this data might not accurately reflect the preferences of older users.</li> </ul> </li> <li> <strong>Confirmation Bias (in data labeling):</strong> Human annotators, when labeling data, might inadvertently reinforce their own existing stereotypes or beliefs. <ul> <li> <em>Example:</em> When labeling images for “professionalism,” annotators might subconsciously label images of men in suits as “professional” more readily than images of women in similar attire or people from non-traditional backgrounds.</li> </ul> </li> <li> <strong>Measurement Bias:</strong> Inaccuracies or inconsistencies in how data is collected or measured across different groups. <ul> <li> <em>Example:</em> Using body mass index (BMI) as a health predictor can be biased because BMI doesn’t account for differences in body composition across various ethnicities and body types.</li> </ul> </li> </ul> <h4 id="2-algorithm-bias-design-choices-matter">2. Algorithm Bias: Design Choices Matter</h4> <p>While data bias is paramount, the algorithms themselves can sometimes introduce or amplify existing biases, often unintentionally.</p> <ul> <li> <strong>Flawed Objective Functions:</strong> The goal an algorithm is optimized for might inadvertently lead to biased outcomes. If a model is optimized purely for predictive accuracy without considering fairness, it might achieve high overall accuracy by sacrificing performance on smaller, underrepresented groups.</li> <li> <strong>Proxy Features:</strong> Sometimes, seemingly neutral features can act as proxies for sensitive attributes. For example, using zip code as a feature might seem neutral, but if zip codes are highly correlated with race or socioeconomic status, the model can inadvertently learn biases based on these protected attributes.</li> <li> <strong>Hyperparameter Tuning:</strong> The choices made during model training (e.g., regularization, learning rate) can sometimes inadvertently exacerbate bias, especially if the evaluation metrics don’t account for fairness across subgroups.</li> </ul> <h4 id="3-human-bias-in-development-and-deployment-the-architects-blind-spots">3. Human Bias (in Development and Deployment): The Architects’ Blind Spots</h4> <p>Ultimately, AI systems are designed, built, and deployed by humans. Our own biases, conscious or unconscious, can seep into every stage of the development pipeline.</p> <ul> <li> <strong>Lack of Diversity in Teams:</strong> Homogeneous development teams may overlook potential biases because they lack diverse perspectives that could identify problematic assumptions or data collection practices.</li> <li> <strong>Unexamined Assumptions:</strong> Developers might make assumptions about their user base or the problem domain that don’t hold true for all groups.</li> <li> <strong>Deployment Context:</strong> How and where an AI system is deployed can also introduce bias if the context isn’t thoroughly understood or if the system is used for purposes it wasn’t designed for.</li> </ul> <h3 id="the-real-world-impact-when-bias-bites-back">The Real-World Impact: When Bias Bites Back</h3> <p>The consequences of biased ML models are far-reaching and can perpetuate or even amplify existing societal inequalities.</p> <ul> <li> <strong>Facial Recognition and Law Enforcement:</strong> Studies have shown that facial recognition systems have significantly higher error rates for women and people with darker skin tones. This can lead to wrongful arrests, misidentification, and a chilling effect on civil liberties, particularly for already marginalized communities.</li> <li> <strong>Recruitment and Hiring:</strong> Amazon famously scrapped an AI recruiting tool after discovering it discriminated against women. The tool had learned from historical hiring data, which predominantly favored men, and penalized resumes containing words like “women’s chess club.”</li> <li> <strong>Loan Applications and Credit Scoring:</strong> Algorithms used to assess creditworthiness can disproportionately deny loans or offer worse terms to certain demographic groups if the training data reflects historical lending discrimination or uses proxies for sensitive attributes.</li> <li> <strong>Criminal Justice:</strong> Predictive policing tools, which attempt to forecast where and when crimes are likely to occur, have been criticized for directing law enforcement resources disproportionately to minority neighborhoods, leading to over-policing and a feedback loop of biased data. Similarly, recidivism risk assessment tools have been shown to falsely flag Black defendants as future criminals at nearly twice the rate of white defendants.</li> <li> <strong>Healthcare:</strong> AI models for disease diagnosis or treatment recommendations could lead to misdiagnosis or suboptimal care for certain groups if the data used to train them doesn’t adequately represent those populations or reflects historical disparities in healthcare access and quality.</li> </ul> <p>These examples highlight a critical point: AI doesn’t just reflect bias; it can <em>amplify</em> it, scaling prejudiced decisions faster and wider than human decision-makers ever could.</p> <h3 id="unpacking-the-math-a-glimpse-into-fairness-metrics">Unpacking the Math: A Glimpse into Fairness Metrics</h3> <p>Addressing bias isn’t simple because “fairness” itself is a complex concept. There isn’t a single mathematical definition of fairness that applies to all situations. What one person considers fair, another might not.</p> <p>Let’s consider a binary classification task, where a model predicts a positive outcome ($\hat{Y}=1$, e.g., “gets a loan”) or a negative outcome ($\hat{Y}=0$, e.g., “denied a loan”). We also have a sensitive attribute $A$ (e.g., $A=0$ for Group A, $A=1$ for Group B).</p> <p>Here are a couple of common fairness notions:</p> <ol> <li> <p><strong>Demographic Parity (or Statistical Parity):</strong> This metric requires that the proportion of individuals receiving the positive outcome is the same across different groups, regardless of their sensitive attribute. \(P(\hat{Y}=1 | A=0) = P(\hat{Y}=1 | A=1)\) In simpler terms: The percentage of people from Group A who get the loan should be the same as the percentage of people from Group B who get the loan. This ensures equal opportunity in terms of outcomes. However, it might not be ideal if the underlying true positive rates are different between groups (e.g., if Group A is genuinely less creditworthy than Group B due to non-discriminatory reasons, forcing equal outcomes might lead to giving loans to undeserving people).</p> </li> <li> <p><strong>Equalized Odds:</strong> This is a stronger notion of fairness. It requires that the true positive rate (TPR) and the false positive rate (FPR) are equal across different groups for a specific outcome. \(P(\hat{Y}=1 | Y=y, A=0) = P(\hat{Y}=1 | Y=y, A=1) \quad \text{for } y \in \{0, 1\}\) This means:</p> <ul> <li> <table> <tbody> <tr> <td>$P(\hat{Y}=1</td> <td>Y=1, A=0) = P(\hat{Y}=1</td> <td>Y=1, A=1)$ (Equal True Positive Rate): Among those who <em>should</em> get a loan (true positive), the model correctly identifies them at the same rate for both groups.</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>$P(\hat{Y}=1</td> <td>Y=0, A=0) = P(\hat{Y}=1</td> <td>Y=0, A=1)$ (Equal False Positive Rate): Among those who <em>should not</em> get a loan (true negative), the model incorrectly gives them a loan at the same rate for both groups.</td> </tr> </tbody> </table> </li> </ul> </li> </ol> <p>It’s important to note that satisfying all fairness metrics simultaneously is often impossible (a concept known as the “impossibility theorems of fairness”). This forces us to make ethical choices about which type of fairness is most important for a given application.</p> <h3 id="fighting-back-strategies-for-mitigating-bias">Fighting Back: Strategies for Mitigating Bias</h3> <p>Combating bias in ML requires a multi-pronged approach, encompassing data, algorithms, and human processes.</p> <h4 id="1-before-training-pre-processing-cleaning-the-mirror">1. Before Training (Pre-processing): Cleaning the Mirror</h4> <ul> <li> <strong>Data Auditing and Exploration:</strong> Thoroughly inspect datasets for imbalances, missing values, and problematic features. This involves visualizing distributions across different demographic groups and identifying areas of underrepresentation or overrepresentation.</li> <li> <strong>Data Augmentation and Re-weighting:</strong> If a sensitive group is underrepresented, techniques like oversampling (duplicating examples from the minority group) or synthesizing new data can help balance the dataset. Re-weighting involves assigning different weights to samples from different groups during training.</li> <li> <strong>Fairness-aware Feature Engineering:</strong> Carefully examine features for potential proxies of sensitive attributes and either remove them or transform them to reduce their discriminatory potential.</li> </ul> <h4 id="2-during-training-in-processing-building-fairer-models">2. During Training (In-processing): Building Fairer Models</h4> <ul> <li> <strong>Fairness Constraints in Objective Functions:</strong> Modify the model’s objective function to include a fairness regularization term. This encourages the model to optimize for predictive performance <em>while also</em> minimizing disparities across groups.</li> <li> <strong>Adversarial Debiasing:</strong> A technique where two neural networks are trained simultaneously: one is the main classifier, and the other (the “adversary”) tries to predict the sensitive attribute from the classifier’s output. The classifier is then trained to be accurate <em>and</em> to “fool” the adversary, making its predictions independent of the sensitive attribute.</li> <li> <strong>Algorithm Selection:</strong> Some algorithms are inherently more susceptible to bias than others. Choosing robust models or using ensemble methods can sometimes help.</li> </ul> <h4 id="3-after-training-post-processing-calibrating-the-outcomes">3. After Training (Post-processing): Calibrating the Outcomes</h4> <ul> <li> <strong>Threshold Adjustment:</strong> Even if a model’s internal scores are biased, we can sometimes adjust the decision threshold (the cutoff point for classifying a positive outcome) differently for different groups to achieve a desired fairness metric. For example, lowering the threshold for a disadvantaged group to increase their positive outcome rate.</li> <li> <strong>Recalibration:</strong> Aligning the predicted probabilities with the true probabilities across different groups to ensure that a prediction of, say, 70% confidence means the same thing for everyone.</li> <li> <strong>Model Monitoring:</strong> Continuously monitor deployed models for performance disparities across different groups over time. Real-world data can drift, and new biases can emerge.</li> </ul> <h4 id="beyond-technical-solutions-a-holistic-approach">Beyond Technical Solutions: A Holistic Approach</h4> <p>Technical solutions are vital, but they are only part of the puzzle.</p> <ul> <li> <strong>Diverse and Inclusive Teams:</strong> Teams with diverse backgrounds, experiences, and perspectives are better equipped to identify potential biases, question assumptions, and design more equitable systems.</li> <li> <strong>Ethical Guidelines and Regulations:</strong> Developing clear ethical guidelines and, where appropriate, regulatory frameworks can provide a roadmap for responsible AI development and deployment.</li> <li> <strong>Transparency and Explainability (XAI):</strong> Understanding <em>why</em> a model makes a particular decision is crucial for identifying and debugging bias. Techniques like LIME (Local Interpretable Model-agnostic Explanations) or SHAP (SHapley Additive exPlanations) help illuminate model behavior.</li> <li> <strong>Human Oversight:</strong> Even with advanced AI, human oversight and intervention remain critical, especially in high-stakes decision-making contexts.</li> </ul> <h3 id="conclusion">Conclusion</h3> <p>The journey to building truly fair and equitable machine learning systems is complex and ongoing. It’s a journey that demands not only technical prowess but also deep ethical consideration, critical thinking, and a commitment to social justice.</p> <p>As we continue to push the boundaries of AI, we must remember that our algorithms are not neutral observers; they are active participants in shaping our world. They reflect our past, embody our present, and profoundly influence our future. It’s our collective responsibility, as data scientists, engineers, policymakers, and citizens, to ensure that the future we build with AI is one that is fair, just, and beneficial for <em>everyone</em>. Let’s strive not just for intelligent machines, but for intelligent, <em>ethical</em> machines.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>