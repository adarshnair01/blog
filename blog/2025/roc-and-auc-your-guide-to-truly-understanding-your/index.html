<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> ROC and AUC: Your Guide to Truly Understanding Your Classification Model | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/roc-and-auc-your-guide-to-truly-understanding-your/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">ROC and AUC: Your Guide to Truly Understanding Your Classification Model</h1> <p class="post-meta"> Created on April 26, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/model-evaluation"> <i class="fa-solid fa-hashtag fa-sm"></i> Model Evaluation</a>   <a href="/blog/blog/tag/roc-curve"> <i class="fa-solid fa-hashtag fa-sm"></i> ROC Curve</a>   <a href="/blog/blog/tag/auc"> <i class="fa-solid fa-hashtag fa-sm"></i> AUC</a>   <a href="/blog/blog/tag/classification"> <i class="fa-solid fa-hashtag fa-sm"></i> Classification</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey there, fellow data enthusiast!</p> <p>If you’ve spent any time building classification models, you’ve probably celebrated a high accuracy score. “My model is 95% accurate!” you might exclaim, and that’s fantastic! But what if I told you that accuracy, while intuitive, doesn’t always paint the full picture? Sometimes, it can even be downright misleading.</p> <p>I remember my early days, proudly showing off a model with 99% accuracy. My mentor, with a knowing smile, asked, “What about the other 1%? Is it important?” That question stuck with me. It turns out, in many real-world scenarios – like detecting a rare disease, identifying fraud, or predicting critical equipment failure – the “other 1%” is precisely what we care about most. A model that’s 99% accurate might simply be predicting “no fraud” or “no disease” almost all the time, even if it misses the few critical cases. That’s where we need more sophisticated tools to truly understand our model’s performance.</p> <p>Today, we’re going to dive deep into two such tools: the <strong>Receiver Operating Characteristic (ROC) curve</strong> and the <strong>Area Under the Curve (AUC)</strong>. These aren’t just fancy metrics; they’re essential for anyone serious about building robust and reliable classification models.</p> <hr> <h3 id="the-heart-of-classification-scores-and-thresholds">The Heart of Classification: Scores and Thresholds</h3> <p>At its core, a binary classification model (one that predicts one of two outcomes, like “yes” or “no,” “positive” or “negative”) doesn’t just spit out a hard “yes” or “no.” Instead, it usually outputs a <strong>probability score</strong> (a value between 0 and 1) or a <strong>confidence score</strong>. For example, a model might say, “There’s an 85% chance this email is spam,” or “This patient has a 0.2 probability of having the disease.”</p> <p>To turn these probabilities into concrete predictions, we use a <strong>threshold</strong>.</p> <ul> <li>If the score is <strong>above</strong> the threshold, we classify it as <strong>Positive</strong>.</li> <li>If the score is <strong>below</strong> the threshold, we classify it as <strong>Negative</strong>.</li> </ul> <p>Conventionally, we often start with a threshold of 0.5. But here’s the kicker: that 0.5 isn’t set in stone. Changing this threshold can dramatically alter our model’s behavior and the types of errors it makes. And understanding this trade-off is key to ROC curves.</p> <hr> <h3 id="understanding-the-trade-offs-the-confusion-matrixs-insights">Understanding the Trade-offs: The Confusion Matrix’s Insights</h3> <p>Before we plot anything, let’s quickly review the fundamental building blocks of classification evaluation, derived from what’s known as the <strong>Confusion Matrix</strong>. Imagine we’re building a model to detect a rare disease.</p> <table> <thead> <tr> <th style="text-align: left"> </th> <th style="text-align: left"><strong>Actual Positive</strong></th> <th style="text-align: left"><strong>Actual Negative</strong></th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Predicted Positive</strong></td> <td style="text-align: left">True Positive (TP)</td> <td style="text-align: left">False Positive (FP)</td> </tr> <tr> <td style="text-align: left"><strong>Predicted Negative</strong></td> <td style="text-align: left">False Negative (FN)</td> <td style="text-align: left">True Negative (TN)</td> </tr> </tbody> </table> <ul> <li> <strong>True Positive (TP):</strong> We predicted positive, and it was actually positive (e.g., correctly identified a sick person).</li> <li> <strong>True Negative (TN):</strong> We predicted negative, and it was actually negative (e.g., correctly identified a healthy person).</li> <li> <strong>False Positive (FP):</strong> We predicted positive, but it was actually negative (e.g., said a healthy person was sick – a “false alarm”). This is also known as a <strong>Type I Error</strong>.</li> <li> <strong>False Negative (FN):</strong> We predicted negative, but it was actually positive (e.g., said a sick person was healthy – a “missed detection”). This is also known as a <strong>Type II Error</strong>.</li> </ul> <p>From these, we derive two crucial rates for understanding our model’s performance at a <em>specific threshold</em>:</p> <ol> <li> <p><strong>True Positive Rate (TPR)</strong> or <strong>Sensitivity</strong> or <strong>Recall</strong>: This tells us what proportion of <em>all actual positive cases</em> our model correctly identified as positive. It’s about how many of the truly sick people we caught. \(TPR = \frac{TP}{TP + FN}\)</p> </li> <li> <p><strong>False Positive Rate (FPR)</strong>: This tells us what proportion of <em>all actual negative cases</em> our model incorrectly identified as positive. It’s about how many healthy people we wrongly flagged as sick. \(FPR = \frac{FP}{FP + TN}\) You might also hear of <strong>Specificity</strong>, which is $1 - FPR$. Specificity measures the proportion of actual negative cases correctly identified as negative.</p> </li> </ol> <hr> <h3 id="the-roc-curve-visualizing-the-trade-off-across-all-thresholds">The ROC Curve: Visualizing the Trade-off Across All Thresholds</h3> <p>Okay, now for the exciting part! What if we didn’t just pick <em>one</em> threshold, but instead evaluated our model at <em>every possible threshold</em>? That’s precisely what the ROC curve does.</p> <p>The <strong>ROC curve</strong> plots the <strong>True Positive Rate (TPR)</strong> on the y-axis against the <strong>False Positive Rate (FPR)</strong> on the x-axis, as we vary the classification threshold from 1 down to 0.</p> <ul> <li> <strong>How it’s built conceptually:</strong> <ol> <li>Start with a threshold of 1. At this threshold, only the most confident predictions will be positive. We’ll likely have a very low TPR (missing many positives) and a very low FPR (almost no false alarms). This corresponds to the bottom-left corner of the graph (0,0).</li> <li>Gradually decrease the threshold. As the threshold drops, more predictions will be classified as positive. Both TPR and FPR will increase.</li> <li>Continue decreasing until the threshold is 0. At this point, everything is classified as positive. We’ll catch all actual positives (TPR = 1) but also wrongly classify all actual negatives (FPR = 1). This corresponds to the top-right corner of the graph (1,1).</li> <li>Connecting these points for every threshold gives us the ROC curve.</li> </ol> </li> </ul> <h4 id="interpreting-the-roc-curve">Interpreting the ROC Curve</h4> <ul> <li> <strong>The Ideal Curve:</strong> The perfect classifier would have a curve that shoots straight up to the top-left corner (0,1) and then goes straight across to (1,1). This means it achieves a 100% TPR with a 0% FPR – it catches all positives without any false alarms. In reality, this is rarely achievable.</li> <li> <strong>The Diagonal Line ($y=x$):</strong> A model that predicts randomly would follow the diagonal line from (0,0) to (1,1). This means its TPR is roughly equal to its FPR – it’s no better than guessing. Any useful model should have a curve that bows out above this diagonal.</li> <li> <strong>Below the Diagonal:</strong> If your ROC curve dips below the diagonal, your model is performing worse than random guessing. This usually means something is fundamentally wrong, or your model is <em>inversely</em> correlated with the true outcome (meaning you could just flip its predictions to get a better-than-random model!).</li> <li> <strong>Curve Shape:</strong> The more the curve hugs the top-left corner, the better the model’s ability to discriminate between positive and negative classes across various thresholds.</li> </ul> <p>The beauty of the ROC curve is that it visualizes the trade-off. Do you want to catch almost all sick people (high TPR), even if it means many healthy people get false alarms (high FPR)? Or do you want to minimize false alarms (low FPR), even if it means missing some sick people (lower TPR)? The ROC curve lets you pick the operating point (threshold) that best suits your specific problem’s costs and benefits.</p> <hr> <h3 id="auc-the-single-number-summary-of-model-performance">AUC: The Single Number Summary of Model Performance</h3> <p>While the ROC curve gives us a visual representation, sometimes we need a single number to quickly compare different models or summarize a model’s overall performance. That’s where <strong>AUC</strong>, the <strong>Area Under the (ROC) Curve</strong>, comes in.</p> <ul> <li> <p><strong>What is AUC?</strong> As its name suggests, AUC is simply the area underneath the ROC curve. It quantifies the overall ability of a classifier to distinguish between positive and negative classes.</p> </li> <li> <p><strong>Interpreting AUC:</strong></p> <ul> <li> <strong>AUC = 1.0:</strong> A perfect classifier. It achieves 100% TPR with 0% FPR across all thresholds.</li> <li> <strong>AUC = 0.5:</strong> A random classifier. It performs no better than chance (like flipping a coin).</li> <li> <strong>AUC &lt; 0.5:</strong> Worse than random. As mentioned, if you have an AUC below 0.5, your model is performing worse than a random guess. You might want to check for data issues or simply flip your predictions.</li> <li> <strong>General Interpretation:</strong> The higher the AUC, the better the model is at distinguishing between positive and negative classes. An AUC of 0.7-0.8 is generally considered good, while 0.8-0.9 is very good, and above 0.9 is excellent.</li> </ul> </li> </ul> <h4 id="why-is-auc-so-powerful">Why is AUC so powerful?</h4> <ol> <li> <strong>Threshold-Independent:</strong> Unlike accuracy, precision, or recall, AUC evaluates the model’s performance across <em>all possible classification thresholds</em>. This means you get a complete picture of its discriminative power, regardless of where you decide to set your operating point.</li> <li> <strong>Robust to Class Imbalance:</strong> This is a huge one! If you have a dataset where 99% of samples are negative and only 1% are positive (a common scenario in fraud detection or disease prediction), a model that always predicts “negative” would achieve 99% accuracy. This sounds great, but it’s useless because it misses all the positive cases. AUC, however, won’t be fooled. It assesses how well the model ranks positives higher than negatives <em>overall</em>, making it a much more reliable metric for imbalanced datasets.</li> <li> <strong>Probabilistic Interpretation:</strong> AUC can be interpreted as the probability that a randomly chosen positive example will be ranked higher (assigned a higher score) than a randomly chosen negative example by the classifier. This intuitive probabilistic interpretation makes it a very appealing metric.</li> </ol> <p>Let’s imagine you have two models for detecting our rare disease. Model A has an accuracy of 98%, and Model B has an accuracy of 97%. Model A seems better, right? But if Model A just predicts “no disease” for almost everyone (getting 98% accuracy by correctly identifying healthy people), and Model B, while slightly less accurate, is actually better at catching the <em>few</em> sick people, AUC would likely reveal Model B as the superior choice for this critical task.</p> <hr> <h3 id="a-quick-peek-at-implementation-conceptual">A Quick Peek at Implementation (Conceptual)</h3> <p>In most data science libraries, calculating ROC and AUC is quite straightforward once you have your model’s predicted probabilities.</p> <p>Imagine you have a trained classifier <code class="language-plaintext highlighter-rouge">my_classifier</code> and some test data <code class="language-plaintext highlighter-rouge">X_test</code>, <code class="language-plaintext highlighter-rouge">y_test</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Conceptual Python code with sklearn
</span><span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_curve</span><span class="p">,</span> <span class="n">auc</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># 1. Get the predicted probabilities for the positive class
# Let's say your model outputs probabilities for both classes [P(class 0), P(class 1)]
</span><span class="n">y_probabilities</span> <span class="o">=</span> <span class="n">my_classifier</span><span class="p">.</span><span class="nf">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="c1"># We want P(class 1)
</span>
<span class="c1"># 2. Calculate the False Positive Rate (FPR), True Positive Rate (TPR),
#    and thresholds for different operating points
</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> <span class="nf">roc_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probabilities</span><span class="p">)</span>

<span class="c1"># 3. Calculate the Area Under the ROC Curve (AUC)
</span><span class="n">roc_auc</span> <span class="o">=</span> <span class="nf">auc</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">)</span>

<span class="c1"># 4. Plot the ROC curve (optional, but highly recommended!)
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">darkorange</span><span class="sh">'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">ROC curve (area = %0.2f)</span><span class="sh">'</span> <span class="o">%</span> <span class="n">roc_auc</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">navy</span><span class="sh">'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">)</span> <span class="c1"># Random classifier line
</span><span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">False Positive Rate</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">True Positive Rate</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Receiver Operating Characteristic</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="sh">"</span><span class="s">lower right</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">The AUC score for the model is: </span><span class="si">{</span><span class="n">roc_auc</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>This snippet shows how simple it is to get these metrics, but the real challenge (and fun!) comes from understanding what those numbers and curves <em>mean</em> for your specific problem.</p> <hr> <h3 id="when-to-look-beyond-rocauc">When to Look Beyond ROC/AUC</h3> <p>While ROC and AUC are incredibly powerful, no metric is a silver bullet. For highly imbalanced datasets, especially when the positive class is extremely rare and false positives are very costly, you might also want to look at the <strong>Precision-Recall (PR) Curve</strong>. The PR curve focuses on the performance of the positive class and can sometimes provide a more insightful view in such extreme imbalance scenarios. But that’s a topic for another deep dive!</p> <hr> <h3 id="wrapping-it-up">Wrapping It Up</h3> <p>So, there you have it! ROC curves and AUC scores are fundamental tools in a data scientist’s toolkit, moving us beyond the simplicity (and sometimes deception) of mere accuracy. They provide a comprehensive, threshold-independent view of your model’s ability to discriminate between classes, making them indispensable for model evaluation, comparison, and selection, especially when dealing with imbalanced data.</p> <p>The next time you’re evaluating a classification model, don’t just stop at accuracy. Plot that ROC curve, calculate that AUC, and truly understand the power and limitations of your model across all its potential operating points. Your more robust and insightful models will thank you for it!</p> <p>Keep learning, keep building, and keep pushing the boundaries of what your data can tell you!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>