<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Your Digital Concierge: A Deep Dive into Recommender Systems | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/your-digital-concierge-a-deep-dive-into-recommende/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Your Digital Concierge: A Deep Dive into Recommender Systems</h1> <p class="post-meta"> Created on August 22, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/recommender-systems"> <i class="fa-solid fa-hashtag fa-sm"></i> Recommender Systems</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/collaborative-filtering"> <i class="fa-solid fa-hashtag fa-sm"></i> Collaborative Filtering</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>As a data science enthusiast, I’ve always been captivated by systems that feel almost magical in their ability to understand and predict human preferences. And few things feel more magical in our digital lives than recommender systems. You know the drill: you finish a movie on Netflix, and instantly, a new list of “Because you watched…” appears. You browse on Amazon, and suddenly, items you <em>didn’t even know you wanted</em> pop up. This isn’t just coincidence; it’s sophisticated engineering and data science at play.</p> <p>Today, I want to pull back the curtain and explore how these digital concierges work. We’ll start simple, build up our understanding, and even peek at some of the advanced techniques powering the recommendations we rely on every single day. So, grab a coffee, and let’s dive into the fascinating world of Recommender Systems!</p> <h3 id="the-problem-information-overload--the-quest-for-discovery">The Problem: Information Overload &amp; The Quest for Discovery</h3> <p>Think about it: the internet is an ocean of content. Millions of movies, billions of products, countless songs. Without a guide, navigating this vastness would be overwhelming. This is where recommender systems step in. They solve two critical problems:</p> <ol> <li> <strong>Information Overload:</strong> They filter out the noise and present you with items most likely to be relevant or interesting.</li> <li> <strong>Discovery:</strong> They help you find new things you might love, broadening your horizons beyond what you’d explicitly search for.</li> </ol> <p>For businesses, this translates directly into increased user engagement, higher sales, and improved customer satisfaction. It’s a win-win!</p> <h3 id="the-two-pillars-collaborative-filtering-and-content-based-filtering">The Two Pillars: Collaborative Filtering and Content-Based Filtering</h3> <p>At their heart, most recommender systems build upon two fundamental strategies. Let’s break them down.</p> <h4 id="1-collaborative-filtering-tell-me-what-people-like-you-like">1. Collaborative Filtering: “Tell me what people like you like!”</h4> <p>This is perhaps the most intuitive approach. Collaborative Filtering (CF) operates on the principle that if two users share similar tastes in the past, they will likely share similar tastes in the future. It’s like asking your friend, “Hey, we both love sci-fi, what’s a good book you’ve read lately?”</p> <p>CF can be further divided:</p> <ul> <li> <p><strong>User-Based Collaborative Filtering:</strong> Imagine you’re “User A.” The system first finds other users (“User B,” “User C”) who have similar taste profiles to you (i.e., they rated items similarly to how you did). Once it finds these “neighbors,” it recommends items to you that your neighbors liked but you haven’t seen yet.</p> <p>The core idea here is finding “similarity.” How do we quantify how similar two users’ tastes are? We can represent each user’s ratings as a vector in a multi-dimensional space. Then, we use similarity metrics like <strong>Cosine Similarity</strong>.</p> <p>Let $A$ and $B$ be two users. Their ratings for various items can be represented as vectors. Cosine similarity measures the cosine of the angle between these two vectors. A smaller angle (cosine close to 1) means higher similarity.</p> \[similarity(A, B) = \frac{A \cdot B}{||A|| \cdot ||B||} = \frac{\sum_{i=1}^{n} A_i B_i}{\sqrt{\sum_{i=1}^{n} A_i^2} \sqrt{\sum_{i=1}^{n} B_i^2}}\] <p>Where $A_i$ and $B_i$ are the ratings of user A and B for item $i$, respectively. The numerator is the dot product, and the denominator accounts for the magnitude of the vectors.</p> </li> <li> <p><strong>Item-Based Collaborative Filtering:</strong> Instead of finding similar users, this approach finds similar <em>items</em>. If you liked “Star Wars,” the system looks for other movies that are frequently liked by people who also liked “Star Wars.” Then, it recommends those similar movies to you. This is often more scalable than user-based CF because item similarity tends to be more stable over time than user tastes, and the number of items is often less than the number of users. The same cosine similarity formula can be applied, but now $A$ and $B$ represent item rating vectors across users.</p> </li> </ul> <p><strong>Challenges with Collaborative Filtering:</strong></p> <ul> <li> <strong>Cold Start Problem:</strong> What happens when a new user joins? We don’t have enough data to find similar users or items for them. What about a brand new item? No one has rated it yet! This is a major hurdle.</li> <li> <strong>Sparsity:</strong> Most users only interact with a tiny fraction of all available items. This means our user-item interaction matrix is mostly empty, making similarity calculations difficult.</li> <li> <strong>Scalability:</strong> As the number of users and items grows into millions, calculating all pairwise similarities becomes computationally expensive.</li> </ul> <h4 id="2-content-based-filtering-tell-me-more-of-what-you-like">2. Content-Based Filtering: “Tell me more of what you like!”</h4> <p>Content-Based Filtering (CBF) takes a different route. Instead of relying on other users, it focuses solely on <em>your</em> past preferences and the characteristics (content) of the items.</p> <p>Imagine you loved a movie because it was a “sci-fi action film starring Dwayne ‘The Rock’ Johnson.” A content-based system would build a profile of your preferences (e.g., you like sci-fi, action, and The Rock) and then recommend other movies that share these attributes.</p> <p>To do this, items need rich descriptive features (genres, actors, directors, keywords, text descriptions). Your user profile is then constructed based on the features of items you’ve interacted with (liked, bought, rated highly).</p> <p>For example, if you’ve liked several sci-fi films and action films, your profile might show a strong preference for “sci-fi” and “action” genres. When a new item comes along, its features are compared to your profile’s features, and if there’s a good match, it’s recommended.</p> <p><strong>Mathematical Intuition:</strong> Item features can be represented as vectors (e.g., using techniques like TF-IDF for text descriptions or one-hot encoding for genres). Your user profile can be an aggregate vector of all items you’ve liked. Then, similarity (again, often Cosine Similarity) is calculated between your profile vector and the new item’s feature vector.</p> <p><strong>Challenges with Content-Based Filtering:</strong></p> <ul> <li> <strong>Limited Discovery (Filter Bubble):</strong> Because it only recommends items similar to what you already like, CBF struggles to introduce you to diverse items or new genres. You might get stuck in a “filter bubble.”</li> <li> <strong>Feature Engineering:</strong> It relies heavily on having rich, well-structured metadata about items. If items lack good descriptions, CBF falls short.</li> </ul> <h3 id="hybrid-approaches-the-best-of-both-worlds">Hybrid Approaches: The Best of Both Worlds</h3> <p>Given the strengths and weaknesses of CF and CBF, many production-level recommender systems employ <strong>Hybrid Approaches</strong>. These systems combine elements of both techniques to achieve more robust and accurate recommendations.</p> <p>Common hybrid strategies include:</p> <ul> <li> <strong>Weighted Hybrid:</strong> Combining the scores from CF and CBF models with a weighted sum.</li> <li> <strong>Switching Hybrid:</strong> Choosing between CF and CBF based on the situation (e.g., using CBF for cold-start users, then switching to CF).</li> <li> <strong>Feature Augmentation:</strong> Using content features to enrich the user-item interaction matrix before applying collaborative filtering.</li> </ul> <h3 id="beyond-the-basics-modern-trends-and-advanced-techniques">Beyond the Basics: Modern Trends and Advanced Techniques</h3> <p>The field of recommender systems is constantly evolving. Here are some cutting-edge approaches:</p> <h4 id="1-matrix-factorization-mf-unveiling-latent-factors">1. Matrix Factorization (MF): Unveiling Latent Factors</h4> <p>Remember the sparsity and scalability issues of traditional CF? Matrix Factorization techniques, like <strong>Singular Value Decomposition (SVD)</strong> or <strong>Alternating Least Squares (ALS)</strong>, came to the rescue.</p> <p>The core idea is to decompose the sparse user-item interaction matrix into two lower-dimensional matrices: a user-feature matrix and an item-feature matrix. These “features” are not explicit like genres or actors; they are <em>latent factors</em> – hidden characteristics or preferences that explain the observed ratings.</p> <p>Imagine our sparse rating matrix $R$ (where $R_{ui}$ is user $u$’s rating for item $i$). We want to approximate this matrix by multiplying a user matrix $P$ and an item matrix $Q$:</p> \[R \approx P Q^T\] <p>Here, $P$ would be an $M \times K$ matrix (M users, K latent factors) and $Q$ would be an $N \times K$ matrix (N items, K latent factors). $K$ is typically a much smaller number (e.g., 50-200) than the number of users or items.</p> <p>Each row in $P$ represents a user’s “strength” for each latent factor, and each row in $Q$ represents an item’s “affinity” for each latent factor. By multiplying these matrices, we can predict ratings for items a user hasn’t seen, effectively filling in the blanks in our sparse $R$ matrix.</p> <p>This approach is highly effective for reducing dimensionality, handling sparsity, and producing good recommendations.</p> <h4 id="2-deep-learning-for-recommenders-power-of-neural-networks">2. Deep Learning for Recommenders: Power of Neural Networks</h4> <p>Deep learning has revolutionized many areas of AI, and recommender systems are no exception. Neural networks can learn complex, non-linear relationships in data that traditional methods might miss.</p> <ul> <li> <strong>Embeddings:</strong> A key deep learning concept is learning dense <em>embeddings</em> for users and items. An embedding is a low-dimensional vector that captures semantic information about a user or item. For example, similar items will have embedding vectors close to each other in the embedding space. These embeddings can be learned from interaction data, content features, or even auxiliary data.</li> <li> <strong>Neural Collaborative Filtering (NCF):</strong> This approach replaces the simple dot product in traditional matrix factorization with a neural network to learn the interaction function between user and item embeddings.</li> <li> <strong>Recurrent Neural Networks (RNNs) / Transformers:</strong> For sequential recommendation tasks (e.g., “what to watch next in a series,” or “what to buy next in a shopping session”), RNNs and Transformer architectures can model the temporal dependencies in user behavior, leading to highly relevant, context-aware suggestions.</li> <li> <strong>Reinforcement Learning (RL):</strong> Some advanced systems even frame recommendations as a reinforcement learning problem, where the system “learns” to make recommendations that maximize long-term user satisfaction and engagement.</li> </ul> <h4 id="3-session-based-recommenders-real-time-dynamics">3. Session-Based Recommenders: Real-time Dynamics</h4> <p>Unlike traditional systems that build long-term user profiles, session-based recommenders focus on short-term, real-time user interactions within a single session (e.g., a browsing session on an e-commerce site). They’re highly dynamic and crucial for applications where user intent changes rapidly.</p> <h3 id="the-ever-present-challenges">The Ever-Present Challenges</h3> <p>Even with advanced techniques, recommender systems still grapple with tough challenges:</p> <ul> <li> <strong>Explainability:</strong> “Why was this recommended?” Users often want to understand the rationale behind a suggestion, especially for high-stakes decisions (e.g., financial products, healthcare).</li> <li> <strong>Fairness and Bias:</strong> If historical data contains biases (e.g., certain demographics are underrepresented in recommendations), the system can perpetuate or even amplify these biases. Ensuring fairness and preventing discrimination is a critical ethical consideration.</li> <li> <strong>Serendipity and Diversity:</strong> Over-personalization can lead to a “filter bubble,” where users are only exposed to what they already like. Recommending diverse, unexpected, yet relevant items (serendipity) is a hard but valuable goal.</li> <li> <strong>Privacy:</strong> Balancing personalized recommendations with user data privacy is an ongoing challenge, especially with evolving regulations like GDPR.</li> </ul> <h3 id="building-your-own-simple-recommender-conceptually">Building Your Own Simple Recommender (Conceptually)</h3> <p>Want to get your hands dirty? Let’s sketch out how you might build a <em>very</em> basic item-based collaborative filtering system:</p> <ol> <li> <strong>Gather Data:</strong> You need user-item interaction data, typically ratings (e.g., 1-5 stars) or implicit feedback (clicks, purchases).</li> <li> <strong>Create a User-Item Matrix:</strong> Represent this data as a matrix where rows are users, columns are items, and cells contain ratings. Fill missing values (items not rated by a user) with 0 or a neutral value.</li> <li> <strong>Calculate Item Similarity:</strong> For every pair of items, calculate their similarity using a metric like Cosine Similarity based on how users have rated them. Store these similarities.</li> <li> <strong>Generate Recommendations:</strong> <ul> <li>For a target user, identify the items they have rated highly.</li> <li>Find the items most similar to these highly-rated items (using your pre-calculated similarities).</li> <li>Exclude items the user has already seen or rated.</li> <li>Rank the remaining similar items by their similarity scores and recommend the top N.</li> </ul> </li> </ol> <p>This is a simplified view, but it captures the essence! Libraries like <code class="language-plaintext highlighter-rouge">Surprise</code> in Python make implementing more sophisticated CF algorithms (like SVD) much more accessible.</p> <h3 id="conclusion-your-digital-concierge-smarter-than-ever">Conclusion: Your Digital Concierge, Smarter Than Ever</h3> <p>Recommender systems are more than just fancy algorithms; they are crucial components of our digital ecosystem, shaping how we discover, consume, and interact with the world around us. From simple similarity matching to complex deep learning architectures, they represent a vibrant and challenging field within data science and machine learning.</p> <p>As data scientists, our journey involves not just understanding <em>how</em> these systems work, but also continually improving them – making them more accurate, scalable, fair, and transparent. The future promises even more intelligent, contextual, and ethically sound recommendation experiences.</p> <p>So, the next time Netflix suggests your next binge-watch, take a moment to appreciate the incredible data science magic happening behind the scenes. And who knows, maybe you’ll be the one building the next generation of these remarkable systems!</p> <p>Thanks for joining me on this deep dive! Keep learning, keep exploring.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>