<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Data Whisperer's Guide: Mastering the Art of Data Cleaning Strategies | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/the-data-whisperers-guide-mastering-the-art-of-dat/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Data Whisperer's Guide: Mastering the Art of Data Cleaning Strategies</h1> <p class="post-meta"> Created on March 04, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/data-cleaning"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Cleaning</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/data-preprocessing"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Preprocessing</a>   <a href="/blog/blog/tag/python"> <i class="fa-solid fa-hashtag fa-sm"></i> Python</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>If you’re anything like me when I first dove into the exciting world of data science and machine learning, you probably imagined spending most of your time building cool models, tweaking hyper-parameters, and marveling at incredible predictions. While that’s definitely a part of the job, I quickly learned a powerful, humbling truth: the glamorous modeling work often accounts for a surprisingly small fraction of a data scientist’s time.</p> <p>The real heavy lifting, the unsung hero of every successful project, is <strong>data cleaning</strong>. Some folks even say it can consume 70-80% of a data scientist’s effort. Why? Because real-world data is almost never perfect. It’s collected by humans, stored across various systems, transmitted imperfectly, and sometimes, well, it just gets messy.</p> <p>Think of it like being a chef. You can have the most advanced oven and the most intricate recipe, but if your ingredients are expired, bruised, or just plain wrong, your dish won’t turn out well. In data science, clean data is the gourmet ingredient that allows your models to truly shine.</p> <p>Today, I want to share some of the essential data cleaning strategies I’ve picked up along my journey. This isn’t just about fixing errors; it’s about understanding your data, making informed decisions, and ultimately building more robust, reliable, and insightful models.</p> <h3 id="what-does-dirty-data-even-look-like-the-many-faces-of-mess">What Does “Dirty Data” Even Look Like? The Many Faces of Mess</h3> <p>Before we dive into cleaning, let’s understand the kinds of “dirt” we’re up against. In my experience, dirty data usually falls into a few common categories:</p> <ol> <li> <strong>Missing Values (NaNs):</strong> Imagine a survey where some people skipped questions. That’s a missing value. Represented as <code class="language-plaintext highlighter-rouge">NaN</code> (Not a Number) in Pandas, these are incredibly common.</li> <li> <strong>Inconsistent Data:</strong> This is where things get tricky. Think of a city column with “NY”, “New York”, and “new york”. Or “Male”, “male”, and “M”. Same meaning, different representation.</li> <li> <strong>Outliers:</strong> Data points that are extremely far from others. If most people in a dataset are 5-6 feet tall, but one entry says “70 feet”, that’s an outlier (and likely a data entry error!).</li> <li> <strong>Duplicates:</strong> Identical rows or entries. If you’re counting unique customers, having the same customer listed twice will skew your numbers.</li> <li> <strong>Incorrect Data Types:</strong> A column that should contain numbers (like age) might be stored as text, preventing you from performing calculations. Dates stored as generic strings instead of datetime objects.</li> <li> <strong>Structural Errors:</strong> Extra spaces, typos, wrong delimiters in a CSV file, or incorrect formatting (e.g., phone numbers stored inconsistently).</li> </ol> <p>Recognizing these patterns is the first step to becoming a data whisperer!</p> <h3 id="the-data-cleaning-mindset-explore-decide-act-iterate">The Data Cleaning Mindset: Explore, Decide, Act, Iterate</h3> <p>My personal motto for data cleaning involves four key steps:</p> <ul> <li> <strong>Explore:</strong> Always start with Exploratory Data Analysis (EDA). Visualizations, descriptive statistics (<code class="language-plaintext highlighter-rouge">.describe()</code>, <code class="language-plaintext highlighter-rouge">.info()</code>, <code class="language-plaintext highlighter-rouge">.value_counts()</code>), and simple checks are your best friends here. You can’t clean what you don’t see.</li> <li> <strong>Decide:</strong> Once you identify an issue, decide on the best strategy. There’s rarely a one-size-fits-all solution. Your decision often depends on the specific context, the amount of data affected, and the goal of your analysis.</li> <li> <strong>Act:</strong> Implement your chosen strategy using tools like Pandas, NumPy, or even custom Python scripts.</li> <li> <strong>Iterate:</strong> Data cleaning is not a linear process. Fixing one issue might reveal another, or your initial solution might not be optimal. Be prepared to go back and forth.</li> </ul> <p>Now, let’s dive into some practical strategies!</p> <hr> <h3 id="core-data-cleaning-strategies-in-action">Core Data Cleaning Strategies in Action</h3> <h4 id="1-handling-missing-values-the-datas-silent-gaps">1. Handling Missing Values: The Data’s Silent Gaps</h4> <p>Missing data can wreak havoc on your models, as many algorithms can’t handle <code class="language-plaintext highlighter-rouge">NaN</code>s directly.</p> <p><strong>Identification:</strong> My go-to moves are <code class="language-plaintext highlighter-rouge">df.isnull().sum()</code> to get a count of NaNs per column, and <code class="language-plaintext highlighter-rouge">df.isnull().sum() / len(df) * 100</code> to see the percentage. For a visual flair, <code class="language-plaintext highlighter-rouge">sns.heatmap(df.isnull(), cbar=False)</code> offers a quick glance.</p> <p><strong>Strategies:</strong></p> <ul> <li> <strong>Deletion (Dropping):</strong> <ul> <li> <strong>Drop Rows:</strong> If a row has too many missing values, or if the number of rows with missing values is a tiny fraction of your total dataset, you might drop the entire row using <code class="language-plaintext highlighter-rouge">df.dropna()</code>. Be careful, though! If you drop too many rows, you might lose valuable information or introduce bias.</li> <li> <strong>Drop Columns:</strong> If a column has an overwhelmingly high percentage of missing values (e.g., 70-80% or more), it might be better to drop the entire column using <code class="language-plaintext highlighter-rouge">df.dropna(axis=1)</code>. The column might not provide enough useful information anyway.</li> </ul> </li> <li> <strong>Imputation (Filling):</strong> This is where you replace missing values with estimated ones. <ul> <li> <strong>Mean/Median/Mode Imputation:</strong> <ul> <li>For numerical data, replacing NaNs with the <strong>mean</strong> or <strong>median</strong> of the column is a common, simple approach. The median is more robust to outliers.</li> <li>For categorical data, replacing with the <strong>mode</strong> (most frequent value) is often appropriate.</li> <li>Example (mean imputation for a numerical column ‘Age’): <code class="language-plaintext highlighter-rouge">df['Age'].fillna(df['Age'].mean(), inplace=True)</code> </li> <li>The mathematical idea behind mean imputation for a set of observations $x_1, x_2, \ldots, x_N$ is: $\text{Value}<em>{\text{imputed}} = \frac{1}{N} \sum</em>{i=1}^{N} x_i$ Where $N$ is the number of non-missing observations.</li> </ul> </li> <li> <strong>Forward Fill (ffill) / Backward Fill (bfill):</strong> Especially useful for time-series data. <code class="language-plaintext highlighter-rouge">ffill</code> propagates the last valid observation forward, while <code class="language-plaintext highlighter-rouge">bfill</code> propagates the next valid observation backward. <code class="language-plaintext highlighter-rouge">df['SensorReading'].ffill(inplace=True)</code> </li> <li> <strong>Constant Value Imputation:</strong> Sometimes, you might fill NaNs with a specific constant, like ‘Unknown’ for a categorical column, or 0 for a numerical one if 0 has a specific meaning (e.g., 0 sales when sales data is missing).</li> <li> <strong>More Advanced Imputation:</strong> For more sophisticated scenarios, you could use machine learning models (like K-Nearest Neighbors Imputer or even build a regression model) to predict missing values based on other features. This is often more accurate but also more complex.</li> </ul> </li> </ul> <p>My advice: Start simple. Visualize the distribution of the column before and after imputation to ensure you’re not distorting it too much.</p> <h4 id="2-dealing-with-inconsistent-data-and-formatting-issues-standardizing-the-chaos">2. Dealing with Inconsistent Data and Formatting Issues: Standardizing the Chaos</h4> <p>This is about bringing uniformity to your data, making sure “apples” are always “apples” and not “APPLE” or “Apple “.</p> <p><strong>Strategies:</strong></p> <ul> <li> <strong>Standardization (Case, Whitespace):</strong> <ul> <li>Convert text to a consistent case (e.g., all lowercase or all uppercase): <code class="language-plaintext highlighter-rouge">df['City'].str.lower()</code>.</li> <li>Remove leading/trailing whitespace: <code class="language-plaintext highlighter-rouge">df['Product'].str.strip()</code>.</li> <li>Replace multiple spaces with single ones: <code class="language-plaintext highlighter-rouge">df['Address'].str.replace('\s+', ' ', regex=True)</code>.</li> </ul> </li> <li> <strong>Mapping and Correction:</strong> If you have known variations (e.g., “NY” should be “New York”), create a mapping dictionary and apply it: <code class="language-plaintext highlighter-rouge">city_map = {'NY': 'New York', 'LA': 'Los Angeles'}</code> <code class="language-plaintext highlighter-rouge">df['State'].replace(city_map, inplace=True)</code> </li> <li> <strong>Regular Expressions (Regex):</strong> Powerful for pattern matching and extraction. If you need to extract numbers from a string, validate email formats, or find specific patterns, regex is your friend. <ul> <li>Example: Extracting digits from a messy column: <code class="language-plaintext highlighter-rouge">df['Phone'].str.extract('(\d{3}-\d{3}-\d{4})')</code> </li> </ul> </li> <li> <strong>Categorical Encoding (briefly):</strong> While more of a feature engineering step, standardizing categories often precedes encoding them (e.g., One-Hot Encoding or Label Encoding) for machine learning models.</li> </ul> <p>Always use <code class="language-plaintext highlighter-rouge">.value_counts()</code> before and after these operations to verify the changes.</p> <h4 id="3-taming-outliers-are-they-errors-or-insights">3. Taming Outliers: Are They Errors or Insights?</h4> <p>Outliers are data points significantly different from others. They can be genuine extreme values, or they can be errors. Handling them requires careful consideration because they can drastically skew statistics and impact model performance.</p> <p><strong>Identification:</strong></p> <ul> <li> <strong>Visualization:</strong> Box plots are fantastic for spotting outliers visually. Histograms can also reveal unusual distributions.</li> <li> <strong>Statistical Methods:</strong> <ul> <li> <strong>Z-score:</strong> Measures how many standard deviations a data point is from the mean. A common threshold is $\left|Z\right| &gt; 3$. The Z-score for a data point $x$ in a dataset with mean $\mu$ and standard deviation $\sigma$ is: $Z = \frac{x - \mu}{\sigma}$</li> <li> <strong>IQR (Interquartile Range) Method:</strong> More robust to skewed data than the Z-score. It defines outliers as values falling below $Q_1 - 1.5 \times IQR$ or above $Q_3 + 1.5 \times IQR$. Here, $Q_1$ is the first quartile (25th percentile), $Q_3$ is the third quartile (75th percentile), and $IQR = Q_3 - Q_1$.</li> </ul> </li> </ul> <p><strong>Strategies:</strong></p> <ul> <li> <strong>Removal:</strong> If an outlier is clearly a data entry error (e.g., that “70 feet” height), it’s best to remove it. However, if it’s a genuine extreme value, think twice. Removing too many true outliers can lead to a loss of valuable information or an oversimplified view of reality.</li> <li> <strong>Transformation:</strong> Applying mathematical transformations (like <code class="language-plaintext highlighter-rouge">log</code> or square root) can reduce the impact of extreme values, especially if your data is skewed. <code class="language-plaintext highlighter-rouge">df['Income_Log'] = np.log(df['Income'])</code> </li> <li> <strong>Capping (Winsorization):</strong> This involves setting a ceiling and/or a floor for outliers. Values above the upper cap are replaced by the cap value, and values below the lower cap are replaced by the floor value. <ul> <li>Example: Cap values above the 99th percentile and below the 1st percentile. <code class="language-plaintext highlighter-rouge">upper_bound = df['Price'].quantile(0.99)</code> <code class="language-plaintext highlighter-rouge">lower_bound = df['Price'].quantile(0.01)</code> <code class="language-plaintext highlighter-rouge">df['Price'] = np.where(df['Price'] &gt; upper_bound, upper_bound, df['Price'])</code> <code class="language-plaintext highlighter-rouge">df['Price'] = np.where(df['Price'] &lt; lower_bound, lower_bound, df['Price'])</code> </li> </ul> </li> <li> <strong>Treat as Missing:</strong> If you’re unsure if an outlier is an error or a true extreme, you could treat it as a missing value and then use your preferred imputation strategy.</li> </ul> <p>Always consider the domain context. Is a $1,000,000 salary an outlier in a dataset of students, or a legitimate observation in a dataset of CEOs?</p> <h4 id="4-eliminating-duplicates-ensuring-uniqueness">4. Eliminating Duplicates: Ensuring Uniqueness</h4> <p>Duplicate rows can bias your analysis, inflate counts, and lead to misleading results.</p> <p><strong>Identification:</strong> <code class="language-plaintext highlighter-rouge">df.duplicated()</code> returns a boolean Series indicating whether each row is a duplicate of a previous row. <code class="language-plaintext highlighter-rouge">df.duplicated().sum()</code> gives the total count.</p> <p><strong>Removal:</strong> <code class="language-plaintext highlighter-rouge">df.drop_duplicates(inplace=True)</code> removes all duplicate rows, keeping only the first occurrence. You can also specify a subset of columns to check for duplicates: <code class="language-plaintext highlighter-rouge">df.drop_duplicates(subset=['CustomerID', 'OrderDate'], inplace=True)</code>. This is useful if you want to ensure uniqueness based on certain identifiers, but allow other columns to differ.</p> <h4 id="5-correcting-data-types-the-foundation-of-proper-analysis">5. Correcting Data Types: The Foundation of Proper Analysis</h4> <p>Incorrect data types can prevent calculations, consume more memory, and cause errors in models.</p> <p><strong>Identification:</strong> <code class="language-plaintext highlighter-rouge">df.info()</code> is your best friend here. It shows column names, non-null counts, and their data types (e.g., <code class="language-plaintext highlighter-rouge">object</code>, <code class="language-plaintext highlighter-rouge">int64</code>, <code class="language-plaintext highlighter-rouge">float64</code>, <code class="language-plaintext highlighter-rouge">datetime64</code>).</p> <p><strong>Correction:</strong></p> <ul> <li> <strong>To Numeric:</strong> <code class="language-plaintext highlighter-rouge">pd.to_numeric(df['Price'], errors='coerce')</code> – <code class="language-plaintext highlighter-rouge">errors='coerce'</code> will turn unparseable values into NaNs, which you can then handle.</li> <li> <strong>To Datetime:</strong> Essential for time-series analysis. <code class="language-plaintext highlighter-rouge">pd.to_datetime(df['TransactionDate'], errors='coerce')</code> </li> <li> <strong>To Category:</strong> For columns with a limited number of unique values, converting to <code class="language-plaintext highlighter-rouge">category</code> can save memory and improve performance for some operations. <code class="language-plaintext highlighter-rouge">df['Gender'] = df['Gender'].astype('category')</code> </li> </ul> <p>Correcting data types is a fundamental step that often enables subsequent cleaning and analysis.</p> <h4 id="6-feature-engineering-the-polish-after-the-clean">6. Feature Engineering (The Polish After the Clean)</h4> <p>While not strictly “cleaning,” once your data is clean, you can often derive new, more informative features from existing ones. This is called feature engineering. For example, from a <code class="language-plaintext highlighter-rouge">TransactionDate</code> column, you could extract <code class="language-plaintext highlighter-rouge">DayOfWeek</code>, <code class="language-plaintext highlighter-rouge">Month</code>, <code class="language-plaintext highlighter-rouge">Year</code>, or <code class="language-plaintext highlighter-rouge">Hour</code>. From an <code class="language-plaintext highlighter-rouge">Address</code> column, you might extract <code class="language-plaintext highlighter-rouge">ZipCode</code> or <code class="language-plaintext highlighter-rouge">State</code>. Clean data makes feature engineering much easier and more reliable.</p> <hr> <h3 id="best-practices--the-data-whisperers-mindset">Best Practices &amp; The Data Whisperer’s Mindset</h3> <ul> <li> <strong>Version Control:</strong> Always keep your data cleaning scripts under version control (like Git). This allows you to track changes, revert if needed, and collaborate effectively.</li> <li> <strong>Document Everything:</strong> Make comments in your code. Explain <em>why</em> you made certain cleaning decisions. This is crucial for reproducibility and for anyone else (or future you!) who looks at your work.</li> <li> <strong>Automate When Possible:</strong> If you’re dealing with recurring data, build robust cleaning pipelines that can be run automatically.</li> <li> <strong>Domain Knowledge is Gold:</strong> The best data cleaners aren’t just technical wizards; they also understand the subject matter of their data. Knowing what the data <em>should</em> look like helps immensely in identifying anomalies and choosing the right cleaning strategy.</li> <li> <strong>Garbage In, Garbage Out (GIGO):</strong> This old computing adage holds particularly true for data science. No matter how advanced your machine learning algorithm is, if you feed it dirty, inconsistent, or biased data, your output will be garbage.</li> </ul> <h3 id="conclusion-your-journey-to-becoming-a-data-whisperer">Conclusion: Your Journey to Becoming a Data Whisperer</h3> <p>Data cleaning might not be the flashiest part of data science, but it is, without a doubt, one of the most critical. It’s where you truly get to know your data, understand its quirks, and prepare it to tell its most accurate and compelling story.</p> <p>Mastering these strategies will not only elevate the performance of your machine learning models but also build your confidence as a data professional. It’s a skill that transforms raw, confusing datasets into reliable foundations for insightful decisions.</p> <p>So, roll up your sleeves, embrace the mess, and start whispering to your data. The clearer its voice, the louder its insights will resonate!</p> <p>Happy cleaning!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>