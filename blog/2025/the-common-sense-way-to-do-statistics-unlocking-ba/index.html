<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Common Sense Way to Do Statistics: Unlocking Bayesian Thinking | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/2025/the-common-sense-way-to-do-statistics-unlocking-ba/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Common Sense Way to Do Statistics: Unlocking Bayesian Thinking</h1> <p class="post-meta"> Created on June 10, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/bayesian-statistics"> <i class="fa-solid fa-hashtag fa-sm"></i> Bayesian Statistics</a>   <a href="/blog/blog/tag/probability"> <i class="fa-solid fa-hashtag fa-sm"></i> Probability</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/statistical-inference"> <i class="fa-solid fa-hashtag fa-sm"></i> Statistical Inference</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>Let’s be honest, statistics can sometimes feel like a rigid, rule-bound world, far removed from the messy, uncertain reality we live in. We’re taught about p-values, null hypotheses, and confidence intervals, and while incredibly useful, they don’t always align with how our brains naturally process new information.</p> <p>Imagine this: You’re trying to figure out if a new energy drink actually improves focus for students. Before you even run an experiment, you might have some initial thoughts. Maybe you’ve seen similar products before, or perhaps you’re just generally skeptical. Then, you collect some data – say, 10 students try the drink, and 7 report improved focus. How do you combine your initial gut feeling with this new evidence to form a more refined conclusion?</p> <p>This, my friends, is the heart of <strong>Bayesian Statistics</strong>. It’s a powerful, elegant framework that lets us explicitly incorporate our existing knowledge and continuously update our beliefs as new data comes in. It feels less like a strict scientific test and more like a sophisticated way to reason, much like how we learn throughout life. For anyone in data science or machine learning, understanding this paradigm shift can unlock a deeper, more intuitive approach to problem-solving and decision-making under uncertainty.</p> <h3 id="the-great-divide-a-tale-of-two-statistical-philosophies-briefly">The Great Divide: A Tale of Two Statistical Philosophies (Briefly)</h3> <p>Before we dive deep, it’s worth a tiny detour to understand <em>why</em> Bayesian statistics feels so different.</p> <p>Most of the “traditional” statistics taught in schools is <em>Frequentist</em>. In this view, probabilities are about the long-run frequency of events. If you say a coin has a 50% chance of heads, a frequentist interprets that as: if you flip the coin an infinite number of times, exactly half of them will be heads. Parameters (like the true probability of heads for a specific coin) are fixed but unknown values. You can’t talk about “the probability that the true coin bias is between 0.4 and 0.6” because the true bias <em>is</em> what it is; it’s not random.</p> <p>Bayesian statistics, however, sees probability as a measure of <em>belief</em> or <em>degree of certainty</em>. The probability that the true coin bias is between 0.4 and 0.6 is a perfectly valid statement for a Bayesian. We start with a belief about a parameter, update it with data, and end up with a new, updated belief. It’s statistics that embraces uncertainty rather than trying to eliminate it.</p> <h3 id="at-the-core-bayes-theorem-dont-fear-the-math">At the Core: Bayes’ Theorem (Don’t Fear the Math!)</h3> <p>The entire edifice of Bayesian statistics rests upon one deceptively simple, yet profoundly powerful, formula: <strong>Bayes’ Theorem</strong>.</p> <p>Let’s write it down:</p> <table> <tbody> <tr> <td>$P(H</td> <td>E) = \frac{P(E</td> <td>H) \cdot P(H)}{P(E)}$</td> </tr> </tbody> </table> <p>Whoa, what are all these letters? Let’s break it down intuitively, like learning a new language.</p> <ul> <li> <table> <tbody> <tr> <td>**$P(H</td> <td>E)$ (The Posterior):** This is what we <em>really</em> want to know. It’s the probability of our <strong>Hypothesis (H)</strong> being true, <em>given the Evidence (E)</em> we’ve just observed. This is our updated belief!</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>**$P(E</td> <td>H)$ (The Likelihood):** This tells us how likely it is to observe the <strong>Evidence (E)</strong> if our <strong>Hypothesis (H)</strong> were actually true. Think of it as: “If my hypothesis is correct, how probable is it that I would see this data?”</td> </tr> </tbody> </table> </li> <li> <strong>$P(H)$ (The Prior):</strong> This is our initial <strong>Prior belief</strong> in the <strong>Hypothesis (H)</strong> <em>before</em> we’ve seen any evidence. It’s where we encode our existing knowledge, intuition, or even educated guesses.</li> <li> <table> <tbody> <tr> <td> <strong>$P(E)$ (The Evidence/Marginal Likelihood):</strong> This is the probability of observing the <strong>Evidence (E)</strong>, regardless of whether our hypothesis is true or not. It acts as a normalizing constant to ensure our posterior probability sums to 1. Often, for comparing hypotheses, we can ignore this term and focus on the proportionality: $P(H</td> <td>E) \propto P(E</td> <td>H) \cdot P(H)$.</td> </tr> </tbody> </table> </li> </ul> <p>So, in plain English, Bayes’ Theorem says:</p> <p><strong>“Our updated belief in a hypothesis is proportional to how likely the evidence is under that hypothesis, multiplied by our initial belief in the hypothesis.”</strong></p> <p>It’s a beautiful dance between what we thought before and what the new data tells us.</p> <h3 id="a-walkthrough-the-biased-coin">A Walkthrough: The Biased Coin</h3> <p>Let’s make this concrete with a classic example: estimating the bias of a coin.</p> <p>Imagine you pick up a coin. You suspect it might be biased, meaning the probability of landing heads ($\theta$) isn’t necessarily 0.5. How can you figure out its true bias?</p> <p><strong>1. The Prior ($P(\theta)$): What did you believe before?</strong> Before flipping the coin even once, what’s your initial guess about $\theta$?</p> <ul> <li>Maybe you think it’s probably close to 0.5 (a fair coin).</li> <li>Or maybe you suspect it’s heavily biased one way or another.</li> <li>If you have no strong opinion, you might assume all values of $\theta$ between 0 and 1 are equally likely.</li> </ul> <p>In Bayesian statistics, we express this belief as a <strong>probability distribution</strong>. For probabilities like our coin bias, a fantastic choice for a prior is the <strong>Beta distribution</strong>. It lives between 0 and 1 and can take many shapes depending on its two parameters, $\alpha$ and $\beta$.</p> <p>The probability density function for a Beta distribution is: $P(\theta) \propto \theta^{\alpha-1} (1-\theta)^{\beta-1}$</p> <ul> <li>If you choose $\alpha=1, \beta=1$, it’s a uniform distribution (you think all biases are equally likely – your “no strong opinion” prior).</li> <li>If you choose $\alpha=2, \beta=2$, it peaks nicely around 0.5, reflecting a belief that the coin is probably fair.</li> <li>If you choose $\alpha=10, \beta=2$, it’s heavily skewed towards heads, suggesting you initially believe the coin is biased towards heads.</li> </ul> <p>Let’s start simple: a <strong>uniform prior ($\alpha=1, \beta=1$)</strong>. We’re open-minded.</p> <p><strong>2. The Likelihood ($P(D|\theta)$): What does the data say, given a bias?</strong> Now, you flip the coin 10 times and get 7 heads and 3 tails. This is your <strong>Data (D)</strong>. How likely is it to get 7 heads in 10 flips <em>if</em> the true bias $\theta$ were, say, 0.5? Or 0.7? Or 0.2? This is where the <strong>Binomial distribution</strong> comes in. If a coin has a true bias $\theta$, the probability of getting $k$ heads in $n$ flips is:</p> <table> <tbody> <tr> <td>$P(D</td> <td>\theta) \propto \theta^k (1-\theta)^{n-k}$</td> </tr> </tbody> </table> <p>In our case, $k=7$ (heads) and $n-k=3$ (tails), so the likelihood is proportional to $\theta^7 (1-\theta)^3$.</p> <table> <tbody> <tr> <td>**3. The Posterior ($P(\theta</td> <td>D)$): Combining Prior and Likelihood**</td> <td> </td> </tr> <tr> <td>Now we apply Bayes’ Theorem! Remember: $P(\theta</td> <td>D) \propto P(D</td> <td>\theta) \cdot P(\theta)$.</td> </tr> </tbody> </table> <p>If our prior was Beta($\alpha$, $\beta$) and our likelihood was Binomial($k$, $n-k$), our posterior distribution turns out to be another Beta distribution! This is a super neat property called <strong>conjugacy</strong>, where the prior and posterior belong to the same family of distributions.</p> <p>Specifically, if:</p> <ul> <li>Prior: Beta($\alpha$, $\beta$)</li> <li>Likelihood: Binomial (k heads, n-k tails)</li> <li>Posterior: Beta($\alpha + k$, $\beta + n - k$)</li> </ul> <p>So, with our initial uniform prior ($\alpha=1, \beta=1$) and data (7 heads, 3 tails from 10 flips): Our posterior distribution for $\theta$ is Beta($1+7$, $1+3$) = <strong>Beta(8, 4)</strong>.</p> <p><strong>What does this mean?</strong> The Beta(8,4) distribution has a peak around $\frac{8}{8+4} = \frac{8}{12} \approx 0.67$. This tells us that, after observing 7 heads in 10 flips, our most probable belief for the coin’s true bias is now around 0.67. The distribution also tells us the <em>range</em> of likely values for $\theta$, not just a single point estimate. We can say, “There’s a 95% probability that the coin’s true bias is between X and Y.” This is called a <strong>Credible Interval</strong>, and it’s much more intuitive than a Frequentist Confidence Interval!</p> <p><strong>The Power of Updating:</strong> If we flip the coin another 10 times and get 6 heads, we just update our prior again! Our current posterior Beta(8,4) becomes our new prior. New data: 6 heads, 4 tails. New posterior: Beta($8+6$, $4+4$) = Beta(14, 8). The distribution continues to narrow and shift as we gather more evidence, leading us closer and closer to the true value of $\theta$. This iterative learning process is incredibly powerful.</p> <h3 id="why-go-bayesian-the-advantages">Why Go Bayesian? The Advantages</h3> <ol> <li> <strong>Incorporates Prior Knowledge:</strong> This is perhaps the biggest differentiator. If you have domain expertise or previous experimental results, you can use them! This is especially crucial when data is scarce. For example, in drug trials, you wouldn’t start from scratch; you’d incorporate decades of biological knowledge.</li> <li> <strong>Provides a Full Probability Distribution:</strong> Instead of just a single “best estimate,” Bayesian methods give you a full probability distribution for your parameters (like our Beta(8,4) for $\theta$). This rich output gives you a complete picture of your uncertainty, allowing for more nuanced decision-making.</li> <li> <strong>Intuitive Interpretation:</strong> Credible intervals are easier to explain and understand than confidence intervals. “There’s a 95% chance the true value is in this range” makes more sense than “If I were to repeat this experiment many times, 95% of the intervals calculated this way would contain the true value.”</li> <li> <strong>Handles Small Data Sets:</strong> When you don’t have a lot of data, a well-chosen prior can stabilize your estimates and prevent overfitting, offering more robust conclusions than frequentist approaches might allow.</li> <li> <strong>Directly Answers the Questions We Care About:</strong> Frequentist p-values tell us the probability of observing data <em>at least as extreme as</em> what we saw, <em>assuming the null hypothesis is true</em>. Bayesian methods directly answer questions like “What is the probability that hypothesis A is true given the data?” or “What is the most probable range for this parameter?”</li> </ol> <h3 id="where-does-bayesian-statistics-shine-in-data-science-and-mle">Where Does Bayesian Statistics Shine in Data Science and MLE?</h3> <p>Bayesian thinking isn’t just for academic statisticians; it’s a practical powerhouse for anyone working with data:</p> <ul> <li> <strong>A/B Testing:</strong> Bayesian A/B testing can often lead to faster decisions and more interpretable results. Instead of simply “rejecting a null hypothesis,” you get a direct probability that variant B is better than variant A.</li> <li> <strong>Recommendation Systems:</strong> Bayesian methods can help model user preferences and item characteristics, especially useful in cold-start scenarios where new users or items have little data.</li> <li> <strong>Spam Filtering:</strong> Naive Bayes classifiers are a prime example of a simple yet effective Bayesian algorithm used to classify emails as spam or not spam, using the probabilities of words appearing in spam vs. legitimate emails.</li> <li> <strong>Predictive Modeling (Gaussian Processes, Bayesian Neural Networks):</strong> These advanced machine learning models naturally incorporate uncertainty into their predictions. A Bayesian Neural Network not only predicts an outcome but also tells you <em>how confident it is</em> in that prediction, which is invaluable in high-stakes applications like autonomous driving or medical diagnosis.</li> <li> <strong>Bayesian Optimization:</strong> Used to find the optimal settings for complex systems or hyper-parameters for machine learning models, especially when experiments are expensive or time-consuming. It uses a probabilistic model to intelligently explore the search space.</li> <li> <strong>Financial Modeling:</strong> Assessing risk and uncertainty in stock prices or market movements.</li> <li> <strong>Healthcare:</strong> Estimating drug efficacy, diagnosing diseases, and personalizing treatment plans based on patient-specific data and prior medical knowledge.</li> </ul> <h3 id="challenges-and-considerations">Challenges and Considerations</h3> <p>It’s not all sunshine and posterior distributions. Bayesian methods have their own complexities:</p> <ul> <li> <strong>Choosing Priors:</strong> While a strength, choosing a prior can also be a challenge. An overly strong or misinformed prior can bias results. Thankfully, techniques like <strong>sensitivity analysis</strong> (testing how different priors affect your posterior) can help. For many problems, “weakly informative” or “objective” priors exist that have minimal influence.</li> <li> <strong>Computational Cost:</strong> For complex models or large datasets where conjugate priors don’t exist (most real-world scenarios!), we often can’t calculate the posterior analytically. We resort to <strong>Markov Chain Monte Carlo (MCMC)</strong> methods, which are powerful but can be computationally intensive and require careful tuning.</li> </ul> <h3 id="my-takeaway-for-you">My Takeaway for You</h3> <p>Embracing Bayesian statistics isn’t about abandoning frequentist methods; it’s about adding a powerful, intuitive tool to your analytical toolkit. It teaches you to think probabilistically, to explicitly model uncertainty, and to continually update your understanding as new information becomes available – precisely how we should operate in the dynamic world of data science.</p> <p>For your portfolio, demonstrating an understanding of Bayesian principles shows a deeper appreciation for statistical inference, an ability to reason under uncertainty, and a readiness to tackle complex problems where prior knowledge is invaluable.</p> <p>So, next time you encounter a problem with data, ask yourself: “What do I believe before I see the data? What does the data tell me? How can I combine these to form a more robust, updated belief?” That’s the Bayesian spirit, and it’s a truly powerful way to approach the world.</p> <p>Happy inferring!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>