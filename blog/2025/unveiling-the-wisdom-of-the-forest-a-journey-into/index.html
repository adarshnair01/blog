<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Unveiling the Wisdom of the Forest: A Journey into Random Forests | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/unveiling-the-wisdom-of-the-forest-a-journey-into/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Unveiling the Wisdom of the Forest: A Journey into Random Forests</h1> <p class="post-meta"> Created on February 24, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/random-forests"> <i class="fa-solid fa-hashtag fa-sm"></i> Random Forests</a>   <a href="/blog/blog/tag/ensemble-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Ensemble Learning</a>   <a href="/blog/blog/tag/decision-trees"> <i class="fa-solid fa-hashtag fa-sm"></i> Decision Trees</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>Welcome back to my corner of the internet, where we unravel the mysteries of machine learning, one algorithm at a time. Today, I want to talk about an algorithm that truly captivated me when I first encountered it: <strong>Random Forests</strong>. It’s one of those techniques that feels almost magical in its effectiveness, taking a simple, intuitive concept and supercharging it into a robust, high-performing model.</p> <p>If you’ve been following my posts, you might remember our chat about Decision Trees. They’re wonderfully intuitive, like a flowchart guiding you to a decision. <em>Is the sky blue? Yes -&gt; It’s daytime. No -&gt; It’s night or cloudy.</em> Simple, right? But as much as I adore their interpretability, individual decision trees have a dark side: they can be incredibly <strong>greedy</strong> and prone to <strong>overfitting</strong>.</p> <h4 id="the-peril-of-the-perfect-tree-a-quick-recap-on-decision-trees">The Peril of the Perfect Tree: A Quick Recap on Decision Trees</h4> <p>Imagine you’re trying to predict if a fruit is an apple or an orange based on its color, size, and texture. A decision tree might start by asking: “Is it red?” If yes, it leans towards apple. “Is it small?” If yes, apple. It keeps splitting the data based on features until it can classify every single fruit in your training set perfectly.</p> <p>This perfect classification, however, is often its downfall. It learns the nuances of <em>your specific training data</em> so well that it starts memorizing noise rather than truly understanding the underlying patterns. When a <em>new</em>, unseen fruit comes along, the tree, having memorized every specific branch and leaf of the training data, might stumble. It’s like studying for a test by memorizing the exact answers to last year’s exam, only to find this year’s questions are phrased slightly differently. You might know all the specific answers, but you lack the general understanding to adapt.</p> <p>Mathematically, a single decision tree has high <strong>variance</strong>. This means if you slightly change your training data, the resulting tree can look dramatically different, leading to unstable predictions.</p> <p>So, how do we get the best of both worlds – the intuitive decision-making power without the overfitting woes? Enter <strong>Ensemble Learning</strong> and, specifically, <strong>Random Forests</strong>.</p> <h4 id="the-wisdom-of-the-crowd-introducing-ensemble-learning">The “Wisdom of the Crowd”: Introducing Ensemble Learning</h4> <p>The core idea behind ensemble learning is simple yet profound: <strong>the wisdom of the crowd</strong>. Think about it. If you need to make a really important decision, would you trust the opinion of a single expert, no matter how brilliant, or would you prefer to get opinions from a diverse group of experts and then combine their insights? Most of us would lean towards the group. Diverse opinions tend to cancel out individual biases and errors, leading to a more robust, accurate decision.</p> <p>Ensemble methods apply this principle to machine learning. Instead of training one mighty model, we train <em>multiple</em> simpler models (often called “base estimators”) and then combine their predictions. Random Forests do this exceptionally well using two powerful techniques: <strong>Bagging</strong> and <strong>Feature Randomness</strong>.</p> <h4 id="step-1-bagging-bootstrap-aggregating">Step 1: Bagging (Bootstrap Aggregating)</h4> <p>Let’s break down “Bagging” first. It stands for <strong>Bootstrap Aggregating</strong>.</p> <p><strong>Bootstrap</strong> is a statistical technique that sounds fancy but is quite straightforward: it means <strong>sampling with replacement</strong>. Imagine you have a dataset of 100 fruits. To create a “bootstrap sample,” you randomly pick a fruit, record it, and then <em>put it back</em>. You repeat this 100 times. What happens?</p> <ol> <li>Some fruits from the original dataset might appear multiple times in your bootstrap sample.</li> <li>Some fruits might not appear at all.</li> <li>Each bootstrap sample will be roughly the same size as your original dataset but will contain a slightly different composition of data points.</li> </ol> <p>Why do we do this? By creating <em>many</em> such bootstrap samples (say, 100 of them), we’re essentially creating 100 slightly different versions of our training data.</p> <p>Now, for <strong>Aggregating</strong>: We train a separate decision tree on each of these 100 bootstrap samples. So, you end up with 100 different decision trees, each trained on a slightly different subset of your original data.</p> <p>When it’s time to make a prediction for a new, unseen fruit:</p> <ul> <li>For <strong>classification</strong> tasks (like apple vs. orange), each of the 100 trees makes its own prediction. We then take a <strong>majority vote</strong>. If 70 trees say “apple” and 30 say “orange,” the Random Forest predicts “apple.”</li> <li>For <strong>regression</strong> tasks (like predicting a house price), each tree predicts a price, and we take the <strong>average</strong> of all their predictions.</li> </ul> <p>This aggregation step is crucial. By averaging or voting, we significantly reduce the <strong>variance</strong> that plagues individual decision trees. Individual trees might be overfitted to their specific bootstrap sample, but their individual errors and biases tend to cancel each other out when combined. It’s like having 100 imperfect fortune tellers – no single one is perfectly accurate, but if you ask all of them and average their predictions, you get a much more reliable forecast.</p> <h4 id="step-2-the-random-in-random-forest--feature-randomness">Step 2: The “Random” in Random Forest – Feature Randomness</h4> <p>Bagging alone already makes a powerful ensemble. However, there’s a subtle problem. If you have one very strong feature (e.g., “is it an iPhone?” when predicting phone prices), every single decision tree in your bag might pick that feature as its first split. This means all your trees, despite being trained on different data subsets, would end up looking very similar at their top levels. If they are too similar, their errors might not cancel out effectively. They would be highly <strong>correlated</strong>.</p> <p>This is where the “Random” in Random Forest truly shines and differentiates it from a simple “Bagged Decision Trees” model.</p> <p>When each individual decision tree is being built in a Random Forest, at every single split point (where it decides which feature to use to divide the data), it doesn’t consider <em>all</em> available features. Instead, it only considers a <strong>random subset</strong> of features.</p> <p>For example, if you have 10 features (color, size, texture, weight, price, brand, etc.), a Random Forest might tell each tree: “Okay, at this split, you can only choose from 3 randomly selected features (e.g., color, brand, weight) to make your decision.”</p> <p>Why is this a game-changer?</p> <ol> <li> <strong>Decorrelation</strong>: By forcing each tree to consider only a random subset of features at each split, we ensure that the trees are less correlated with each other. Even if one feature is overwhelmingly strong, not all trees will get to pick it at the top, leading to more diverse and independent base models.</li> <li> <strong>Robustness</strong>: This makes the forest even more robust. If one feature is noisy or misleading, not all trees will be equally affected by it.</li> </ol> <h4 id="how-a-random-forest-is-built-the-full-picture">How a Random Forest is Built (The Full Picture)</h4> <p>Let’s put it all together step-by-step:</p> <ol> <li> <strong>For <code class="language-plaintext highlighter-rouge">n_estimators</code> times (e.g., 100 times):</strong> <ul> <li> <strong>Bootstrap Sampling</strong>: Draw a random sample of data points <em>with replacement</em> from your original training dataset. This sample will be the same size as your original dataset.</li> <li> <strong>Grow a Decision Tree</strong>: Train a decision tree model on this bootstrap sample. <ul> <li> <strong>Feature Randomness</strong>: At each node of the tree, when deciding on the best split, only consider a random subset of the total features. This random subset size is a hyperparameter, often denoted as <code class="language-plaintext highlighter-rouge">max_features</code> or <code class="language-plaintext highlighter-rouge">m</code>. For regression, it’s common to use $m = p/3$ (where $p$ is the total number of features), and for classification, $m = \sqrt{p}$.</li> <li> <strong>Full Growth</strong>: Unlike individual decision trees, these trees are often grown to their maximum depth without pruning. Don’t worry, the aggregation will handle the overfitting!</li> </ul> </li> </ul> </li> <li> <strong>Prediction</strong>: When you need to predict for a new data point: <ul> <li>Pass the data point through <em>all</em> <code class="language-plaintext highlighter-rouge">n_estimators</code> decision trees.</li> <li>For <strong>classification</strong>: Collect the predicted class from each tree and take a majority vote.</li> <li>For <strong>regression</strong>: Collect the predicted value from each tree and take the average.</li> </ul> </li> </ol> <p>This combination of bagging and feature randomness is the secret sauce. By training many deep, somewhat overfit, yet diverse trees and then averaging their predictions, Random Forests manage to maintain low bias (because individual trees are powerful) and significantly reduce variance (because of aggregation and decorrelation).</p> <h4 id="why-does-this-work-so-well-a-peek-at-the-math-intuition">Why Does This Work So Well? A Peek at the Math (Intuition)</h4> <p>Consider the variance of an average of $N$ random variables. If these variables were completely independent, the variance of their average would be $\frac{1}{N}$ times the variance of a single variable. That’s a huge reduction!</p> <p>In Random Forests, our trees aren’t perfectly independent, but the bootstrapping and especially the feature randomness make them <em>less correlated</em>. The less correlated our trees are, the closer we get to that $\frac{1}{N}$ variance reduction.</p> <p>Imagine you have 100 students guessing the number of jelly beans in a jar. If they all collaborate and share information, they might make similar errors. But if they all make independent guesses, their individual overestimates and underestimates will likely cancel out when you average them, leading to a much more accurate overall guess. Random Forests aim to create those “independent” guesses.</p> <h4 id="key-hyperparameters-to-tune">Key Hyperparameters to Tune</h4> <p>To get the most out of a Random Forest, you’ll want to play with a few knobs:</p> <ul> <li> <strong><code class="language-plaintext highlighter-rouge">n_estimators</code></strong>: The number of trees in the forest. More trees generally mean better performance, but also longer training times. There’s usually a diminishing return after a certain point. (e.g., 100, 200, 500)</li> <li> <strong><code class="language-plaintext highlighter-rouge">max_features</code></strong>: The number of features to consider when looking for the best split at each node. This is critical for controlling the randomness and decorrelation of your trees. Common values are <code class="language-plaintext highlighter-rouge">sqrt</code> (for classification) or <code class="language-plaintext highlighter-rouge">0.33</code> (for regression) of the total features.</li> <li> <strong><code class="language-plaintext highlighter-rouge">max_depth</code></strong>: The maximum depth of each tree. While Random Forest trees are often grown deep, limiting depth can sometimes prevent individual trees from becoming <em>too</em> overfit, which might marginally help overall. However, with many trees and proper <code class="language-plaintext highlighter-rouge">max_features</code>, this isn’t always strictly necessary.</li> <li> <strong><code class="language-plaintext highlighter-rouge">min_samples_leaf</code></strong>: The minimum number of samples required to be at a leaf node. Increasing this can prevent individual trees from learning too specific patterns, making them slightly more generalized.</li> </ul> <h4 id="the-superpowers-of-random-forests">The Superpowers of Random Forests</h4> <ol> <li> <strong>High Accuracy</strong>: They are incredibly powerful and often achieve state-of-the-art performance on a wide range of tasks.</li> <li> <strong>Robustness to Overfitting</strong>: Thanks to bagging and feature randomness, they are far less prone to overfitting than individual decision trees.</li> <li> <strong>Handles High Dimensionality</strong>: They can work well with datasets that have a large number of features.</li> <li> <strong>Feature Importance</strong>: Random Forests can tell you which features were most influential in making predictions. This is a huge bonus for understanding your data!</li> <li> <strong>Handles Missing Values (with imputation)</strong>: While not directly, they are less sensitive to missing data if you use appropriate imputation strategies.</li> <li> <strong>Versatile</strong>: Works for both classification and regression problems.</li> <li> <strong>Implicit Scaling</strong>: No need for feature scaling (like normalization or standardization) because decision trees are not sensitive to the scale of features.</li> </ol> <h4 id="any-weaknesses">Any Weaknesses?</h4> <p>Yes, nothing is perfect:</p> <ol> <li> <strong>Less Interpretable</strong>: While individual decision trees are easy to interpret, a forest of hundreds of trees making decisions is much harder to follow. You lose some of that beautiful transparency.</li> <li> <strong>Computationally Intensive</strong>: Training many trees can be slow, especially with very large datasets or many <code class="language-plaintext highlighter-rouge">n_estimators</code>.</li> <li> <strong>Memory Usage</strong>: Storing many trees can consume significant memory.</li> </ol> <h4 id="conclusion-a-forest-of-insight">Conclusion: A Forest of Insight</h4> <p>Random Forests truly are a cornerstone of modern machine learning. They take the intuitive simplicity of decision trees, combine it with the statistical power of ensemble learning, and add a dash of calculated randomness to create a model that is both powerful and robust.</p> <p>From predicting customer churn to classifying medical images, Random Forests find applications across countless industries. They were one of the first algorithms that made me truly appreciate the elegance of combining simple ideas to solve complex problems.</p> <p>So, the next time you’re facing a challenging prediction task, remember the wisdom of the forest. Sometimes, the best way to get a single accurate answer is to ask a diverse crowd of experts, each with a slightly different perspective.</p> <p>What are your thoughts on Random Forests? Have you used them in a project? Let me know in the comments below!</p> <p>Until next time, keep learning, keep building!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>