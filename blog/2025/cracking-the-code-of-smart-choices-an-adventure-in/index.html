<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Cracking the Code of Smart Choices: An Adventure into Q-Learning | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/cracking-the-code-of-smart-choices-an-adventure-in/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Cracking the Code of Smart Choices: An Adventure into Q-Learning</h1> <p class="post-meta"> Created on November 05, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/reinforcement-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Reinforcement Learning</a>   <a href="/blog/blog/tag/q-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Q-Learning</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   <a href="/blog/blog/tag/robotics"> <i class="fa-solid fa-hashtag fa-sm"></i> Robotics</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Remember when you were a little kid, first learning to ride a bike? You didn’t read a manual; you got on, tried to balance, fell down a few times (ouch!), and slowly, surely, your brain learned what worked and what didn’t. Each fall was a “negative reward,” each successful wobble a “positive reward,” and your brain was constantly updating its internal model of “how to ride a bike.”</p> <p>This fundamental process of learning from experience, trying things out, getting feedback, and refining your strategy is precisely what drives a powerful branch of Artificial Intelligence called <strong>Reinforcement Learning (RL)</strong>. And today, we’re going to explore one of its most elegant and foundational algorithms: <strong>Q-Learning</strong>.</p> <h2 id="the-quest-for-intelligent-agents">The Quest for Intelligent Agents</h2> <p>In Reinforcement Learning, we have an <strong>agent</strong> (our learner) operating within an <strong>environment</strong>. The agent’s goal is to learn an optimal <strong>policy</strong> – a fancy word for a strategy or set of rules – that dictates what <strong>action</strong> to take in any given <strong>state</strong> to maximize its cumulative <strong>reward</strong> over time. Think of a robot learning to navigate a factory floor:</p> <ul> <li> <strong>Agent:</strong> The robot.</li> <li> <strong>Environment:</strong> The factory floor (with obstacles, assembly lines, charging stations).</li> <li> <strong>States:</strong> The robot’s current location, battery level, sensor readings.</li> <li> <strong>Actions:</strong> Move forward, turn left, turn right, pick up an item.</li> <li> <strong>Rewards:</strong> +100 for delivering an item, -10 for bumping into an obstacle, -1 for each time step (encouraging efficiency).</li> </ul> <p>The challenge is that the agent doesn’t initially know the “rules” of the environment. It doesn’t have a map or a pre-programmed path. It has to figure it out, just like you figured out how to ride that bike.</p> <h2 id="enter-q-learning-valuing-our-choices">Enter Q-Learning: Valuing Our Choices</h2> <p>At its heart, Q-Learning is a <strong>value-based</strong> algorithm. This means it tries to learn the “value” or “quality” of taking a particular action in a particular state. We often represent this value as a <strong>Q-value</strong>.</p> <p>Imagine you’re trying to find buried treasure on a giant grid. Each square on the grid is a “state.” From each square, you can take actions: move North, South, East, or West. Some paths lead quickly to treasure, others lead to quicksand, and most just lead to more squares.</p> <p>A Q-Learning agent builds an internal “map” (which isn’t really a map, but a table of values) that tells it, for <em>every possible state</em>, how good it is to take <em>every possible action</em>. This “goodness” is the Q-value.</p> <h3 id="the-q-table-our-agents-brain-in-a-spreadsheet">The Q-Table: Our Agent’s Brain (in a Spreadsheet)</h3> <p>For environments with a discrete, manageable number of states and actions, Q-Learning stores these values in a simple table called the <strong>Q-Table</strong>.</p> <table> <thead> <tr> <th style="text-align: left">State (S)</th> <th style="text-align: left">Action (A)</th> <th style="text-align: left">Q-value (S, A)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">State 1</td> <td style="text-align: left">Action 1</td> <td style="text-align: left">0.0</td> </tr> <tr> <td style="text-align: left">State 1</td> <td style="text-align: left">Action 2</td> <td style="text-align: left">0.0</td> </tr> <tr> <td style="text-align: left">…</td> <td style="text-align: left">…</td> <td style="text-align: left">…</td> </tr> <tr> <td style="text-align: left">State N</td> <td style="text-align: left">Action M</td> <td style="text-align: left">0.0</td> </tr> </tbody> </table> <p>Initially, all Q-values are typically set to zero (meaning we have no idea how good any action is). As the agent explores the environment, takes actions, and receives rewards, it continuously updates these Q-values. The goal is that, eventually, this table will reflect the <em>optimal</em> Q-value for every state-action pair – telling the agent the maximum expected future reward it can get by taking that action from that state.</p> <h2 id="the-q-learning-algorithm-the-learning-loop">The Q-Learning Algorithm: The Learning Loop</h2> <p>So, how does the agent actually update this table? This is where the magic happens! Q-Learning is a <strong>model-free</strong> algorithm, meaning it doesn’t need to understand the environment’s full dynamics (like transition probabilities between states). It learns purely from experience.</p> <p>Let’s break down the learning process step-by-step for a single “episode” (a sequence of actions from a start state to a terminal state, like playing a single game):</p> <ol> <li> <p><strong>Initialize the Q-Table:</strong> At the very beginning, our agent is clueless. It sets all Q-values in the table to zero, or small random numbers. $Q(s, a) = 0$ for all states $s$ and actions $a$.</p> </li> <li> <p><strong>Observe the Current State ($S_t$):</strong> The agent looks at its current situation in the environment.</p> </li> <li> <p><strong>Choose an Action ($A_t$):</strong> Now, this is a crucial step! The agent needs to decide what to do. Should it exploit what it <em>thinks</em> it knows (choose the action with the highest Q-value in the current state)? Or should it explore new possibilities (try a random action)?</p> <p>This is the <strong>exploration-exploitation dilemma</strong>, and Q-Learning often tackles it using an <strong>$\epsilon$-greedy strategy</strong>:</p> <ul> <li>With a small probability $\epsilon$ (epsilon), the agent chooses a random action (explores). This helps it discover potentially better paths it hasn’t tried yet.</li> <li>With probability $1 - \epsilon$, the agent chooses the action $A_t$ that has the highest Q-value for the current state $S_t$ in its Q-table (exploits its current knowledge).</li> </ul> <p>Initially, $\epsilon$ is often high (more exploration), and it gradually decreases over time (more exploitation) as the agent learns more about the environment.</p> </li> <li> <p><strong>Execute Action ($A_t$), Observe Reward ($R_{t+1}$), and New State ($S_{t+1}$):</strong> The agent takes the chosen action in the environment. The environment reacts by providing a numerical <strong>reward</strong> ($R_{t+1}$) and transitioning the agent to a <strong>new state</strong> ($S_{t+1}$).</p> </li> <li> <p><strong>Update the Q-Value:</strong> This is the core of Q-Learning, where the agent learns. It updates the Q-value for the <em>action it just took from the state it was in</em> ($Q(S_t, A_t)$) using the following formula:</p> <p>$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma \max_{a} Q(S_{t+1}, a) - Q(S_t, A_t)]$</p> <p>Don’t let the symbols intimidate you! Let’s break down each component:</p> <ul> <li>$Q(S_t, A_t)$: This is the <strong>old Q-value</strong> for the state-action pair we just experienced.</li> <li>$\alpha$ (alpha) - <strong>Learning Rate:</strong> This is a value between 0 and 1. It determines how much of the “new information” we accept. A high $\alpha$ means the agent learns quickly (but might be unstable), while a low $\alpha$ means it learns slowly but more steadily. If $\alpha=0$, the agent learns nothing; if $\alpha=1$, it completely replaces the old value with the new estimate.</li> <li>$R_{t+1}$: This is the <strong>immediate reward</strong> the agent received for taking action $A_t$ from state $S_t$.</li> <li>$\gamma$ (gamma) - <strong>Discount Factor:</strong> Also between 0 and 1. This determines the importance of future rewards. <ul> <li>If $\gamma$ is close to 0, the agent is “short-sighted” and only cares about immediate rewards.</li> <li>If $\gamma$ is close to 1, the agent is “long-sighted” and considers future rewards heavily.</li> </ul> </li> <li>$\max_{a} Q(S_{t+1}, a)$: This is the <strong>maximum expected future reward</strong> from the <em>new state</em> ($S_{t+1}$). The agent looks at all possible actions it could take from the next state and picks the one with the highest Q-value according to its <em>current</em> Q-table. This is the “greedy” part of the update, assuming the agent will act optimally from the next state onward.</li> <li>$[R_{t+1} + \gamma \max_{a} Q(S_{t+1}, a) - Q(S_t, A_t)]$: This entire term inside the brackets is the <strong>Temporal Difference (TD) error</strong>. It represents the difference between what the agent <em>expected</em> to get (the old $Q(S_t, A_t)$) and what it <em>actually got</em> (the immediate reward $R_{t+1}$ plus the discounted maximum future reward from the next state). The agent updates its belief by nudging its old Q-value towards this new, more informed estimate.</li> </ul> </li> <li> <p><strong>Repeat:</strong> Steps 2-5 are repeated for many time steps within an episode, and then for many episodes. Over countless iterations, the Q-values converge towards their optimal values, allowing the agent to eventually make the best decisions consistently.</p> </li> </ol> <h2 id="hyperparameters-tuning-our-agents-brain">Hyperparameters: Tuning Our Agent’s Brain</h2> <p>The Learning Rate ($\alpha$), Discount Factor ($\gamma$), and Exploration Rate ($\epsilon$) are crucial <strong>hyperparameters</strong> that significantly influence how well and how fast our Q-Learning agent learns. Choosing the right values often requires experimentation and understanding the specific problem.</p> <ul> <li> <strong>Alpha ($\alpha$):</strong> How quickly should we forget old beliefs and adopt new ones?</li> <li> <strong>Gamma ($\gamma$):</strong> How much do we care about long-term goals versus immediate gratification?</li> <li> <strong>Epsilon ($\epsilon$):</strong> How often should we try something new versus sticking to what we know works best? (Often, $\epsilon$ starts high and decays over time to encourage exploration initially, then exploitation later.)</li> </ul> <h2 id="a-simple-example-the-frozen-lake">A Simple Example: The Frozen Lake</h2> <p>Imagine a simple grid-world game called “Frozen Lake.” Our agent starts at “S” (Start), needs to reach “G” (Goal), and must avoid “H” (Holes). “F” represents safe, frozen tiles.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>S F F F
F H F H
F F F H
F H F G
</code></pre></div></div> <ul> <li> <strong>States:</strong> Each unique tile on the grid.</li> <li> <strong>Actions:</strong> Move Up, Down, Left, Right.</li> <li> <strong>Rewards:</strong> +1 for reaching ‘G’, -1 for falling into ‘H’, 0 for moving on ‘F’.</li> </ul> <p>The Q-table for this small environment would have <code class="language-plaintext highlighter-rouge">(number of states) x (number of actions)</code> entries. Let’s say it’s a 4x4 grid, so 16 states. With 4 actions per state, the Q-table would have 64 entries.</p> <p>The agent would wander around, sometimes falling in holes, sometimes reaching the goal. With each step, its Q-table entries for the state-action pair it just experienced would get updated. Eventually, after thousands or even millions of trials, the Q-values leading to the goal would become very high, while those leading to holes would become very low. The agent would “learn the path” to the goal without ever being explicitly programmed with directions.</p> <h2 id="when-q-learning-shines-and-its-limitations">When Q-Learning Shines and Its Limitations</h2> <p>Q-Learning is a powerful algorithm, especially effective in:</p> <ul> <li> <strong>Environments with discrete states and actions:</strong> Like grid worlds, simple games (e.g., Tic-Tac-Toe), or robotic tasks with limited, distinct movements.</li> <li> <strong>Model-free scenarios:</strong> When the agent doesn’t have access to the environment’s internal mechanics (how actions influence state transitions or rewards).</li> </ul> <p>However, Q-Learning faces a significant challenge known as the <strong>curse of dimensionality</strong>. What if our “state” isn’t a simple grid tile, but an image from a camera, or a complex robot arm’s joint angles? The number of possible states becomes astronomically large, making a Q-table impossible to create and update. This is where more advanced techniques, like <strong>Deep Q-Networks (DQN)</strong>, come into play, using neural networks to approximate the Q-values instead of storing them in a table.</p> <h2 id="conclusion-the-foundation-of-intelligent-behavior">Conclusion: The Foundation of Intelligent Behavior</h2> <p>Q-Learning, with its intuitive approach of learning from trial and error and continuously updating its value estimates, stands as a cornerstone of Reinforcement Learning. It’s an algorithm that beautifully mirrors how living beings learn to navigate their worlds: by experimenting, making mistakes, celebrating successes, and refining their strategies over time.</p> <p>For anyone venturing into the world of AI, understanding Q-Learning is like learning the alphabet before writing a novel. It provides a solid foundation for grasping more complex RL algorithms and truly appreciating the journey towards creating truly intelligent agents. So, go forth, simulate some environments, and watch your agents learn to make smart choices – it’s a truly rewarding experience!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>