<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Need for Speed: Mastering NumPy Optimization for Blazing Fast Data Science | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/the-need-for-speed-mastering-numpy-optimization-fo/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Need for Speed: Mastering NumPy Optimization for Blazing Fast Data Science</h1> <p class="post-meta"> Created on April 18, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/numpy"> <i class="fa-solid fa-hashtag fa-sm"></i> NumPy</a>   <a href="/blog/blog/tag/optimization"> <i class="fa-solid fa-hashtag fa-sm"></i> Optimization</a>   <a href="/blog/blog/tag/python"> <i class="fa-solid fa-hashtag fa-sm"></i> Python</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/performance"> <i class="fa-solid fa-hashtag fa-sm"></i> Performance</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="the-need-for-speed-mastering-numpy-optimization-for-blazing-fast-data-science">The Need for Speed: Mastering NumPy Optimization for Blazing Fast Data Science</h2> <p>Hello future data scientists and curious coders!</p> <p>If you’ve spent any time working with data in Python, you’ve undoubtedly encountered NumPy. It’s the bedrock for numerical computing, a true superhero that allows us to perform complex mathematical operations on arrays and matrices with impressive speed. But sometimes, even superheroes need a little training to unlock their full potential.</p> <p>I remember my early days, proudly writing Python code to process large datasets. I’d hit “run” and then… wait. And wait. Sometimes I’d even grab a coffee, come back, and it would still be churning! Then I discovered NumPy, and it felt like magic. Loops that took minutes suddenly finished in seconds. But even with NumPy, there comes a point where you need <em>more</em> speed. Where every millisecond counts. That’s when I realized the power of <strong>NumPy Optimization</strong>. It’s not just about using NumPy; it’s about using it <em>smartly</em>.</p> <p>Today, I want to share some of the techniques I’ve learned to squeeze every last drop of performance out of my NumPy code. Think of this as a training manual to turn your data science projects into turbocharged machines!</p> <hr> <h3 id="why-is-numpy-already-so-fast-and-why-isnt-it-always-enough">Why is NumPy Already So Fast (and Why Isn’t it Always Enough)?</h3> <p>Before we dive into making NumPy faster, let’s briefly understand <em>why</em> it’s already a performance marvel compared to standard Python lists.</p> <p>The secret sauce is simple:</p> <ol> <li> <strong>Under the Hood:</strong> NumPy arrays are implemented in C and Fortran, languages famous for their raw speed. When you perform an operation on a NumPy array, you’re essentially calling highly optimized C/Fortran code, not slow Python loops.</li> <li> <strong>Contiguous Memory:</strong> NumPy arrays store their elements in a contiguous block of memory. Imagine all your books perfectly lined up on one long shelf, one after another. This allows the CPU to access data much faster because it knows exactly where the next piece of data is. Python lists, on the other hand, store references to objects scattered throughout memory, making access slower.</li> </ol> <p>So, if it’s already so fast, why optimize? Because it’s easy to accidentally write “Pythonic” code within NumPy that negates its core advantages. We need to learn to <em>think</em> in NumPy.</p> <hr> <h3 id="the-golden-rule-embrace-vectorization-banish-explicit-loops">The Golden Rule: Embrace Vectorization, Banish Explicit Loops!</h3> <p>This is, hands down, the most important lesson. If you take away nothing else, remember this: <strong>avoid explicit Python <code class="language-plaintext highlighter-rouge">for</code> loops when working with NumPy arrays.</strong></p> <p>Let’s see an example. Suppose we want to square every element in a large array.</p> <p><strong>The “Slow” Way (Explicit Python Loop):</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">time</span>

<span class="n">my_array</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">10</span><span class="o">**</span><span class="mi">6</span><span class="p">)</span> <span class="c1"># A million random numbers
</span>
<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="n">result_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">my_array</span><span class="p">:</span>
    <span class="n">result_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Looping took: </span><span class="si">{</span><span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s"> seconds</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><strong>The “Fast” Way (Vectorized NumPy):</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="n">result_numpy</span> <span class="o">=</span> <span class="n">my_array</span><span class="o">**</span><span class="mi">2</span>
<span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Vectorized operation took: </span><span class="si">{</span><span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s"> seconds</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>When you run this, you’ll see a dramatic difference. The vectorized operation will be orders of magnitude faster. Why? Because <code class="language-plaintext highlighter-rouge">my_array**2</code> is a <strong>universal function (ufunc)</strong> in NumPy. It applies the squaring operation ($x^2$) to every element internally using its fast C implementation, without ever touching a Python <code class="language-plaintext highlighter-rouge">for</code> loop.</p> <p>Any operation that works element-wise, like addition ($A+B$), multiplication ($A \times B$), trigonometric functions (<code class="language-plaintext highlighter-rouge">np.sin()</code>), logarithms (<code class="language-plaintext highlighter-rouge">np.log()</code>), or comparisons ($A &gt; B$), should be performed directly on the NumPy array without loops.</p> <hr> <h3 id="technique-1-harness-the-power-of-broadcasting">Technique 1: Harness the Power of Broadcasting</h3> <p>Broadcasting is NumPy’s magical ability to perform operations on arrays of different shapes. It’s incredibly powerful because it often allows you to achieve complex operations without explicitly creating large, temporary arrays.</p> <p>Imagine you have a matrix and you want to add a different value to each row.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">matrix</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                   <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span>
                   <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]])</span>

<span class="n">row_additions</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">])</span> <span class="c1"># Add 10 to row 0, 20 to row 1, etc.
</span></code></pre></div></div> <p>If you tried <code class="language-plaintext highlighter-rouge">matrix + row_additions</code>, it would raise an error because their shapes are incompatible. But if <code class="language-plaintext highlighter-rouge">row_additions</code> had a shape of <code class="language-plaintext highlighter-rouge">(3, 1)</code> or <code class="language-plaintext highlighter-rouge">(1, 3)</code> (depending on the desired operation), NumPy could “broadcast” it.</p> <p>To add <code class="language-plaintext highlighter-rouge">[10, 10, 10]</code> to the first row, <code class="language-plaintext highlighter-rouge">[20, 20, 20]</code> to the second, and <code class="language-plaintext highlighter-rouge">[30, 30, 30]</code> to the third, we can reshape <code class="language-plaintext highlighter-rouge">row_additions</code>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># We want to add [10, 20, 30] to *each column* of its respective row.
# This means we need to broadcast a (3,) array across the columns of a (3,3) array.
# The rules state that dimensions are compared from trailing end.
# (3,3) vs (3,) -&gt; (3,3) vs (1,3) after internal prep.
</span>
<span class="c1"># A more common example: adding a single value to each row
</span><span class="n">addition_vector</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">])</span> <span class="c1"># Shape (3,)
</span><span class="n">result_broadcast</span> <span class="o">=</span> <span class="n">matrix</span> <span class="o">+</span> <span class="n">addition_vector</span><span class="p">[:,</span> <span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">]</span> <span class="c1"># Reshapes to (3,1)
</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Original Matrix:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">matrix</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Addition Vector:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">addition_vector</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Result with Broadcasting:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">result_broadcast</span><span class="p">)</span>
<span class="c1"># Output:
# Original Matrix:
#  [[1 2 3]
#   [4 5 6]
#   [7 8 9]]
# Addition Vector:
#  [10 20 30]
# Result with Broadcasting:
#  [[11 12 13]
#   [24 25 26]
#   [37 38 39]]
</span></code></pre></div></div> <p>NumPy intelligently stretches the smaller array to match the shape of the larger one during the operation, without actually duplicating the data in memory. This saves a huge amount of memory and computation time. Learn the <a href="https://numpy.org/doc/stable/user/basics.broadcasting.html" rel="external nofollow noopener" target="_blank">broadcasting rules</a> – they’re a game-changer!</p> <hr> <h3 id="technique-2-mind-your-data-types-dtypes">Technique 2: Mind Your Data Types (Dtypes)</h3> <p>Data types (dtypes) are crucial for both memory efficiency and speed. By default, NumPy often uses <code class="language-plaintext highlighter-rouge">float64</code> for floating-point numbers and <code class="language-plaintext highlighter-rouge">int64</code> for integers. While these provide high precision, they also consume 8 bytes of memory per element.</p> <p>If you know your data doesn’t require such high precision (e.g., pixel values from 0-255, counts that won’t exceed 32,000, or floating-point numbers where <code class="language-plaintext highlighter-rouge">float32</code> is sufficient), you can specify smaller dtypes:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">np.float32</code> (4 bytes) instead of <code class="language-plaintext highlighter-rouge">np.float64</code> (8 bytes)</li> <li> <code class="language-plaintext highlighter-rouge">np.int8</code> (1 byte), <code class="language-plaintext highlighter-rouge">np.int16</code> (2 bytes), <code class="language-plaintext highlighter-rouge">np.int32</code> (4 bytes) instead of <code class="language-plaintext highlighter-rouge">np.int64</code> (8 bytes)</li> <li> <code class="language-plaintext highlighter-rouge">np.uint8</code> (unsigned, 1 byte, good for images)</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Default float64
</span><span class="n">arr_default</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">10</span><span class="o">**</span><span class="mi">6</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Default float64 size: </span><span class="si">{</span><span class="n">arr_default</span><span class="p">.</span><span class="n">nbytes</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> MB</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># ~7.63 MB
</span>
<span class="c1"># Using float32
</span><span class="n">arr_float32</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">10</span><span class="o">**</span><span class="mi">6</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Float32 size: </span><span class="si">{</span><span class="n">arr_float32</span><span class="p">.</span><span class="n">nbytes</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> MB</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># ~3.81 MB
</span>
<span class="c1"># An array of integers
</span><span class="n">arr_int_large</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">10</span><span class="o">**</span><span class="mi">6</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">int64</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Int64 size: </span><span class="si">{</span><span class="n">arr_int_large</span><span class="p">.</span><span class="n">nbytes</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> MB</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># ~7.63 MB
</span>
<span class="c1"># If numbers are small, use a smaller int type
</span><span class="n">arr_int_small</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">10</span><span class="o">**</span><span class="mi">6</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Int32 size: </span><span class="si">{</span><span class="n">arr_int_small</span><span class="p">.</span><span class="n">nbytes</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> MB</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># ~3.81 MB
</span></code></pre></div></div> <p>Half the memory often means faster operations because more data can fit into CPU caches, reducing trips to slower main memory. For very large datasets, this can be a crucial optimization.</p> <hr> <h3 id="technique-3-pre-allocation-for-efficiency">Technique 3: Pre-allocation for Efficiency</h3> <p>Imagine you’re building a house. Would you prefer to have all your bricks delivered at once, or would you ask for one brick at a time as you need it, and then realize you need to expand your storage every time you get more bricks?</p> <p>In programming, this translates to <strong>pre-allocation</strong>. When you know the final size of an array, it’s always better to create an empty (or zero-filled) array of that size upfront, and then fill it with values. This is much faster than repeatedly appending to a list (which often requires reallocating memory for the entire list) or dynamically resizing a NumPy array.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># The "Slow" Way (appending to a Python list):
</span><span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="n">results_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">results_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">i</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">List append took: </span><span class="si">{</span><span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s"> seconds</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># The "Fast" Way (pre-allocating a NumPy array):
</span><span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="n">results_numpy</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">empty</span><span class="p">(</span><span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span> <span class="c1"># or np.zeros
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">results_numpy</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="mi">2</span>
<span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">NumPy pre-allocation took: </span><span class="si">{</span><span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s"> seconds</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><em>(Note: Even better would be <code class="language-plaintext highlighter-rouge">np.arange(10\*\*5) _ 2</code> for full vectorization, but this example focuses on the pre-allocation concept when you _must</em> loop or fill iteratively)*</p> <hr> <h3 id="technique-4-efficient-data-access-memory-layout-considerations">Technique 4: Efficient Data Access (Memory Layout Considerations)</h3> <p>Remember how NumPy stores data contiguously? This means accessing elements that are next to each other in memory is much faster than jumping around.</p> <p>NumPy arrays typically store data in <strong>row-major order</strong> (C-contiguous), meaning elements of a row are adjacent in memory. If you iterate over columns, you might be forcing your CPU to jump around memory more, which is slower.</p> <p>Consider a 2D array:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[A, B, C],
 [D, E, F],
 [G, H, I]]
</code></pre></div></div> <p>In C-contiguous order, the elements are stored as <code class="language-plaintext highlighter-rouge">A, B, C, D, E, F, G, H, I</code> in memory.</p> <ul> <li>Accessing <code class="language-plaintext highlighter-rouge">arr[row, col]</code> then <code class="language-plaintext highlighter-rouge">arr[row, col+1]</code> (moving along a row) is fast.</li> <li>Accessing <code class="language-plaintext highlighter-rouge">arr[row, col]</code> then <code class="language-plaintext highlighter-rouge">arr[row+1, col]</code> (moving down a column) is slower because it has to skip over entire rows to get to the next element.</li> </ul> <p>While for most common operations NumPy handles this efficiently, if you’re writing custom, element-wise loops (which, ideally, you’re avoiding!), be mindful of how you access elements. Always try to iterate or access data in the order it’s stored for maximum cache efficiency.</p> <hr> <h3 id="pro-tip-leverage-built-in-functions-and-the-out-parameter">Pro Tip: Leverage Built-in Functions and the <code class="language-plaintext highlighter-rouge">out</code> Parameter</h3> <p>NumPy’s built-in functions like <code class="language-plaintext highlighter-rouge">np.sum()</code>, <code class="language-plaintext highlighter-rouge">np.mean()</code>, <code class="language-plaintext highlighter-rouge">np.dot()</code>, <code class="language-plaintext highlighter-rouge">np.max()</code>, etc., are highly optimized. Always use them instead of trying to roll your own logic.</p> <p>Furthermore, some NumPy functions allow you to specify an <code class="language-plaintext highlighter-rouge">out</code> parameter. This means instead of creating a <em>new</em> array to store the result, NumPy will place the result directly into a pre-existing array you provide. This avoids unnecessary memory allocations and deallocations, which can be beneficial for performance, especially in loops or memory-constrained environments.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">10</span><span class="o">**</span><span class="mi">6</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">10</span><span class="o">**</span><span class="mi">6</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">empty_like</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="c1"># Pre-allocate an array for the result
</span>
<span class="c1"># Without 'out' parameter (creates a new 'sum_result' array)
# sum_result = a + b
</span>
<span class="c1"># With 'out' parameter (stores result directly into 'result')
</span><span class="n">np</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">result</span><span class="p">)</span>

<span class="c1"># This is equivalent to `result = a + b` if 'result' was not pre-allocated
# The performance gain here is marginal for single operations,
# but can add up in tight loops or for very large arrays.
</span></code></pre></div></div> <hr> <h3 id="measuring-performance-your-best-friend-timeit">Measuring Performance: Your Best Friend, <code class="language-plaintext highlighter-rouge">%timeit</code> </h3> <p>How do you know if your optimizations are working? You measure them! In Jupyter notebooks (or IPython), the magical <code class="language-plaintext highlighter-rouge">%timeit</code> command is your best friend. It runs a piece of code multiple times and gives you the average execution time.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%</span><span class="n">timeit</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">10</span><span class="o">**</span><span class="mi">6</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<span class="o">%</span><span class="n">timeit</span> <span class="p">[</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">10</span><span class="o">**</span><span class="mi">6</span><span class="p">)]</span>
</code></pre></div></div> <p>The output will clearly show the difference, giving you empirical evidence for your optimization efforts.</p> <hr> <h3 id="beyond-numpy-when-even-optimization-isnt-enough">Beyond NumPy: When Even Optimization Isn’t Enough</h3> <p>Sometimes, even after applying all these NumPy optimization tricks, your code might still be too slow. This usually happens when:</p> <ul> <li>You have inherently sequential operations that can’t be fully vectorized.</li> <li>Your data is too large to fit into memory, or processing is incredibly CPU-intensive.</li> </ul> <p>In such cases, you might look into:</p> <ul> <li> <strong>Numba:</strong> A JIT (Just-In-Time) compiler that can take Python functions (especially those with loops) and compile them into fast machine code, often rivaling C/Fortran performance.</li> <li> <strong>Dask:</strong> For “out-of-core” (data too big for RAM) or parallel computing, Dask scales NumPy-like operations across multiple cores or even clusters.</li> <li> <strong>Cython:</strong> Allows you to write C extensions for Python directly, giving you ultimate control over performance.</li> </ul> <p>These are more advanced topics, but it’s good to know they exist when you hit the limits of pure NumPy.</p> <hr> <h3 id="wrapping-up-the-journey-of-a-high-performance-coder">Wrapping Up: The Journey of a High-Performance Coder</h3> <p>NumPy optimization isn’t just a set of tricks; it’s a mindset. It’s about understanding how NumPy works under the hood and writing code that leverages its strengths rather than fighting against them.</p> <p>As you continue your journey in data science and machine learning, you’ll encounter bigger datasets and more complex computations. Mastering these optimization techniques will not only save you time (and many coffee breaks!) but also make your code more efficient, scalable, and professional.</p> <p>So go forth, experiment, profile your code, and unleash the inner cheetah in your NumPy arrays! Happy coding!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>