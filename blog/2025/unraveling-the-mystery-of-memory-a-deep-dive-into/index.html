<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Unraveling the Mystery of Memory: A Deep Dive into Recurrent Neural Networks | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/unraveling-the-mystery-of-memory-a-deep-dive-into/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Unraveling the Mystery of Memory: A Deep Dive into Recurrent Neural Networks</h1> <p class="post-meta"> Created on April 09, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/neural-networks"> <i class="fa-solid fa-hashtag fa-sm"></i> Neural Networks</a>   <a href="/blog/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> NLP</a>   <a href="/blog/blog/tag/rnns"> <i class="fa-solid fa-hashtag fa-sm"></i> RNNs</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>As a kid, I remember struggling with those “What comes next?” puzzles. You know, <code class="language-plaintext highlighter-rouge">apple, banana, cherry, ____</code>. My brain would instantly scan for patterns, remembering the previous fruits to deduce the logical next step. If it was a simple alphabetical sequence, it was easy. But what if it was more complex, like a story? To understand “The cat sat on the…”, my brain <em>needs</em> to remember “The cat sat on” to predict “the mat” or “the sofa,” not “the sky.”</p> <p>This fundamental human ability to process information <em>sequentially</em>, remembering context from the past, is something traditional neural networks often struggle with. They’re like brilliant amnesiacs – they can learn incredibly complex patterns, but only by looking at each input in isolation. For tasks involving sequences – text, speech, time series data – that’s a huge problem.</p> <p>Imagine trying to understand a conversation if you only heard one word at a time, completely forgetting the words that came before. You’d be lost! This is where Recurrent Neural Networks (RNNs) step in, bringing the power of <em>memory</em> to the world of deep learning.</p> <h3 id="the-limits-of-stateless-networks">The Limits of “Stateless” Networks</h3> <p>Before we dive into RNNs, let’s quickly recap what makes standard feedforward neural networks (FNNs) fall short for sequential data. An FNN takes an input, passes it through a series of layers with activation functions, and produces an output. Each input is treated independently. There’s no inherent mechanism for information from one input to influence the processing of the next.</p> <p>Consider tasks like:</p> <ul> <li> <strong>Language Translation:</strong> How do you translate “I am hungry” if you only see “hungry” and forget “I am”? The grammatical structure and meaning depend on the entire sequence.</li> <li> <strong>Stock Price Prediction:</strong> Predicting tomorrow’s stock price without considering yesterday’s or last week’s trends would be pure guesswork.</li> <li> <strong>Speech Recognition:</strong> Deciphering a sentence from an audio stream requires understanding how sounds connect and form words and phrases.</li> </ul> <p>FNNs excel at tasks where inputs are self-contained, like classifying an image of a cat. Whether you saw an image of a dog before has no bearing on classifying the current cat image. But for anything involving a sequence, we need something more. We need a network with a memory.</p> <h3 id="enter-recurrent-neural-networks-the-architects-of-memory">Enter Recurrent Neural Networks: The Architects of Memory</h3> <p>The core idea behind RNNs is deceptively simple yet profoundly powerful: give the neural network a <strong>memory</strong>. This “memory” is often referred to as a <strong>hidden state</strong> or <strong>context vector</strong>, which captures information about the sequence processed so far.</p> <p>Think of it like this: a regular neural network takes an input and processes it. An RNN takes an input, processes it, and then <em>passes a summary of what it just learned</em> (its hidden state) to itself for the next step in the sequence. It’s a network with a loop!</p> <p>![RNN Unrolling Diagram - Conceptual representation] <em>(Imagine a diagram here showing a basic RNN cell on the left, with an input $x_t$, an output $y_t$, and a hidden state $h_t$ looping back to itself. On the right, the same RNN is “unrolled” over time, showing $x_0, x_1, x_2, …$ feeding into successive cells, each passing $h_{t-1}$ to $h_t$ to $h_{t+1}$)</em></p> <p>When we “unroll” the RNN over time, it looks like a deep feedforward network where each layer corresponds to a time step. The crucial part is that the <em>weights and biases</em> across all these “time steps” are <strong>shared</strong>. This sharing of weights is what allows the network to learn sequential patterns effectively and keeps the number of parameters manageable.</p> <p>Let’s look at the core equations for a simple RNN:</p> <ol> <li> <strong>Calculating the Hidden State:</strong> The hidden state at time $t$, denoted $h_t$, is a function of the current input $x_t$ and the previous hidden state $h_{t-1}$. \(h_t = \tanh(W_{hh}h_{t-1} + W_{xh}x_t + b_h)\) <ul> <li>$h_t$: The hidden state at the current time step $t$. This is our “memory” of the sequence up to this point.</li> <li>$h_{t-1}$: The hidden state from the previous time step.</li> <li>$x_t$: The input at the current time step $t$.</li> <li>$W_{hh}$: Weight matrix for the recurrent connection (how much the previous hidden state influences the current one).</li> <li>$W_{xh}$: Weight matrix for the input (how much the current input influences the current hidden state).</li> <li>$b_h$: Bias vector for the hidden state.</li> <li>$\tanh$: An activation function (often hyperbolic tangent) that squashes the values between -1 and 1, introducing non-linearity.</li> </ul> </li> <li> <strong>Calculating the Output:</strong> The output at time $t$, denoted $y_t$, is typically a function of the current hidden state $h_t$. \(y_t = W_{hy}h_t + b_y\) <ul> <li>$y_t$: The output at the current time step $t$.</li> <li>$W_{hy}$: Weight matrix for the output.</li> <li>$b_y$: Bias vector for the output.</li> <li>(An activation function like softmax for classification or linear for regression might be applied after this, depending on the task).</li> </ul> </li> </ol> <p>The initial hidden state $h_0$ is usually initialized to a vector of zeros. As the network processes each element in the sequence, the hidden state $h_t$ updates, continuously building a more nuanced representation of the context.</p> <h3 id="rnns-in-action-predicting-the-next-word">RNNs in Action: Predicting the Next Word</h3> <p>Let’s revisit our sentence example: “The cat sat on the…”</p> <ol> <li> <strong>Input “The”:</strong> The RNN takes “The” as $x_0$. It computes an initial $h_0$ (from $h_{-1}$ being zeros) and potentially an output $y_0$.</li> <li> <strong>Input “cat”:</strong> The RNN takes “cat” as $x_1$. Crucially, it combines “cat” with $h_0$ (the memory from “The”) to compute $h_1$. This $h_1$ now encodes information about “The cat.”</li> <li> <strong>Input “sat”:</strong> Takes “sat” as $x_2$, combines it with $h_1$ to get $h_2$. Now $h_2$ knows about “The cat sat.”</li> <li>…and so on.</li> <li> <strong>Input “the”:</strong> Takes “the” as $x_4$, combines it with $h_3$ (which remembers “The cat sat on”) to get $h_4$.</li> <li> <strong>Prediction:</strong> From $h_4$, the RNN can now make a much more informed prediction for the next word. Because $h_4$ has processed “The cat sat on the”, it’s far more likely to predict “mat”, “rug”, or “chair” than “sky” or “banana”. The hidden state holds the story so far.</li> </ol> <p>This ability to leverage past information makes RNNs incredibly powerful for a wide range of tasks.</p> <h3 id="where-rnns-shine-applications">Where RNNs Shine: Applications</h3> <p>RNNs have revolutionized many fields, particularly in areas involving sequential data:</p> <ul> <li> <strong>Natural Language Processing (NLP):</strong> <ul> <li> <strong>Language Modeling:</strong> Predicting the next word or character in a sequence (like predictive text on your phone).</li> <li> <strong>Machine Translation:</strong> Translating text from one language to another (e.g., Google Translate).</li> <li> <strong>Sentiment Analysis:</strong> Determining the emotional tone of text (positive, negative, neutral).</li> <li> <strong>Named Entity Recognition:</strong> Identifying proper nouns like people, places, or organizations in text.</li> </ul> </li> <li> <strong>Speech Recognition:</strong> Converting spoken language into text.</li> <li> <strong>Time Series Prediction:</strong> Forecasting stock prices, weather patterns, or energy consumption.</li> <li> <strong>Music Generation:</strong> Composing new melodies or extending existing ones.</li> <li> <strong>Video Analysis:</strong> Understanding actions and events in video sequences frame by frame.</li> </ul> <h3 id="the-achilles-heel-vanishing-and-exploding-gradients">The Achilles’ Heel: Vanishing and Exploding Gradients</h3> <p>While groundbreaking, simple RNNs have a significant limitation: they struggle with <strong>long-term dependencies</strong>. That is, they find it hard to connect information from many steps back in the sequence to the current prediction.</p> <p>Imagine trying to predict the last word in a long sentence like: “The boy, who grew up in a small town in France, loved to play soccer, but his real passion was <em>astronomy</em>.” To predict “astronomy,” you need to remember “passion” from earlier in the sentence, possibly dozens of words ago. Simple RNNs often forget this distant information.</p> <p>This problem stems from the way neural networks learn: <strong>backpropagation</strong>. In RNNs, this is called <strong>Backpropagation Through Time (BPTT)</strong>. When the error signal is propagated backward through many time steps, the gradients (which guide weight updates) can either:</p> <ol> <li> <p><strong>Vanishing Gradients:</strong> Become incredibly small. This happens when the activation function (like $\tanh$) squashes values into a very flat region, and repeated multiplication by small numbers during backpropagation causes the gradient to shrink exponentially. If gradients vanish, the network learns very little about long-term dependencies because the updates to the initial weights become negligible. It’s like whispering a secret down a very long line of people – by the end, the message is barely audible or completely lost.</p> </li> <li> <p><strong>Exploding Gradients:</strong> Become extremely large. This occurs when gradients accumulate rapidly, leading to unstable learning and possibly NaN values. This is less common than vanishing gradients and can often be mitigated using techniques like <strong>gradient clipping</strong>, where gradients are scaled down if they exceed a certain threshold.</p> </li> </ol> <p>The vanishing gradient problem, in particular, was a major hurdle for RNNs in practical applications, preventing them from effectively modeling sequences where context might span hundreds or thousands of time steps.</p> <h3 id="the-evolution-lstms-and-grus">The Evolution: LSTMs and GRUs</h3> <p>To overcome the vanishing gradient problem and allow RNNs to learn much longer-term dependencies, researchers developed more sophisticated architectures. The two most prominent are <strong>Long Short-Term Memory (LSTM) networks</strong> and <strong>Gated Recurrent Units (GRUs)</strong>.</p> <h4 id="long-short-term-memory-lstm">Long Short-Term Memory (LSTM)</h4> <p>Introduced in 1997 by Hochreiter and Schmidhuber, LSTMs are a special kind of RNN designed specifically to remember information for extended periods. The magic lies in their internal structure, particularly the concept of a <strong>cell state</strong> and <strong>gates</strong>.</p> <ul> <li> <strong>Cell State ($C_t$):</strong> This is essentially a “conveyor belt” that runs through the entire sequence. It carries information relevant to long-term dependencies. Critically, information can be added to or removed from the cell state via gates, but its flow is largely uninterrupted.</li> <li> <strong>Gates:</strong> LSTMs employ three types of gates, each controlled by a sigmoid neural network layer (which outputs values between 0 and 1, acting like a “switch” to let information through or block it): <ul> <li> <strong>Forget Gate ($f_t$):</strong> Decides what information from the previous cell state $C_{t-1}$ should be thrown away. A 0 means “forget completely,” a 1 means “keep completely.”</li> <li> <strong>Input Gate ($i_t$):</strong> Decides what new information from the current input $x_t$ and previous hidden state $h_{t-1}$ should be stored in the cell state.</li> <li> <strong>Output Gate ($o_t$):</strong> Decides what part of the cell state $C_t$ should be output as the hidden state $h_t$.</li> </ul> </li> </ul> <p>These gates allow LSTMs to selectively read, write, and erase information from the cell state, effectively creating a much more robust memory mechanism that combats vanishing gradients. They can “remember” a piece of information for thousands of time steps, making them incredibly powerful for complex sequential tasks.</p> <h4 id="gated-recurrent-unit-gru">Gated Recurrent Unit (GRU)</h4> <p>Developed in 2014 by Cho et al., GRUs are a slightly simplified version of LSTMs. They combine the forget and input gates into a single <strong>update gate</strong> and also merge the cell state and hidden state.</p> <ul> <li> <strong>Update Gate ($z_t$):</strong> Decides how much of the previous hidden state to carry over and how much of the new candidate hidden state to incorporate.</li> <li> <strong>Reset Gate ($r_t$):</strong> Decides how much of the previous hidden state to “forget” when calculating the new candidate hidden state.</li> </ul> <p>GRUs have fewer parameters than LSTMs, which can sometimes lead to faster training and slightly less computational overhead. Despite their simplicity, they often perform comparably to LSTMs on many tasks. The choice between LSTMs and GRUs often comes down to experimental results for a specific problem.</p> <h3 id="the-legacy-and-evolution">The Legacy and Evolution</h3> <p>Recurrent Neural Networks, particularly their gated variants like LSTMs and GRUs, were a monumental leap forward in deep learning. They enabled machines to understand and generate sequential data with unprecedented accuracy, powering advancements in everything from voice assistants to machine translation.</p> <p>While newer architectures like <strong>Transformers</strong> have largely superseded RNNs in many state-of-the-art NLP tasks due to their ability to process sequences in parallel and handle much longer dependencies, RNNs (and especially LSTMs/GRUs) remain fundamental. They are still highly relevant in specific domains, especially where memory efficiency, real-time processing of streaming data, or shorter sequences are critical. Moreover, understanding RNNs is a crucial stepping stone to grasping the more advanced concepts behind Transformers.</p> <h3 id="wrapping-up">Wrapping Up</h3> <p>From predicting the next word in a sentence to forecasting complex time series, Recurrent Neural Networks empowered AI with the crucial ability to <strong>remember</strong>. They taught us that context matters, and by introducing a hidden state that evolves over time, we could build models that understood the intricate dance of sequences.</p> <p>The journey from simple RNNs to the sophisticated LSTMs and GRUs highlights the iterative nature of research and the constant drive to overcome limitations. So, the next time you marvel at a language model generating coherent text, remember the humble but powerful RNN, the unsung hero that first brought “memory” to the machine. Go forth, explore, and build your own sequential wonders!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>