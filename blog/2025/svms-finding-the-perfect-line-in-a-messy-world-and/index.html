<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> SVMs: Finding the Perfect Line in a Messy World (and Beyond!) | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/svms-finding-the-perfect-line-in-a-messy-world-and/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">SVMs: Finding the Perfect Line in a Messy World (and Beyond!)</h1> <p class="post-meta"> Created on February 11, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/svm"> <i class="fa-solid fa-hashtag fa-sm"></i> SVM</a>   <a href="/blog/blog/tag/classification"> <i class="fa-solid fa-hashtag fa-sm"></i> Classification</a>   <a href="/blog/blog/tag/optimization"> <i class="fa-solid fa-hashtag fa-sm"></i> Optimization</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>As a budding data scientist, I’m constantly fascinated by how seemingly complex problems can be distilled into elegant mathematical solutions. One algorithm that truly captured my imagination early on was the Support Vector Machine, or SVM. It’s a classic, a workhorse in the machine learning toolkit, and it beautifully illustrates the power of optimization and geometry in building predictive models.</p> <p>Today, I want to take you on a deep dive into SVMs. We’ll strip away some of the mystique, explore the core intuition, dive into the underlying math, and even discover how it tackles non-linear problems. My goal is to make this journey accessible whether you’re just starting out in data science or looking for a clearer picture of how these fascinating algorithms work.</p> <h3 id="the-problem-drawing-a-line-in-the-sand">The Problem: Drawing a Line in the Sand</h3> <p>Imagine you have a scatter plot of data points. Some points represent “cats” (let’s say, circles) and others represent “dogs” (triangles). Your task is to draw a line that separates the cats from the dogs. Simple enough, right?</p> <p><img src="https://i.imgur.com/kP1oR2e.png" alt="Simple 2D Separation"> <em>(Imagine circles on one side, triangles on the other. A straight line separates them.)</em></p> <p>But then you look closer. There isn’t just <em>one</em> line that separates them. There are <em>many</em> possible lines!</p> <p><img src="https://i.imgur.com/8Q9K3H5.png" alt="Multiple Separating Lines"> <em>(Same circles and triangles, but now with three different lines, all separating them.)</em></p> <p>So, which line is the <em>best</em> line? This isn’t just an academic question; it’s fundamental to building a robust classifier. A “good” line should not only separate the data we’ve seen, but also generalize well to <em>new</em>, unseen data. This is where the genius of SVMs truly shines.</p> <h3 id="the-intuition-maximizing-the-margin">The Intuition: Maximizing the Margin</h3> <p>The core idea behind SVMs is elegant: Instead of just finding <em>any</em> line that separates the classes, SVMs find the line that maximizes the “margin” between the closest data points of each class.</p> <p>Think of it like building a road. You want to build a road that separates two towns, but you also want that road to be as wide as possible, with ample space (a “shoulder”) on either side before you hit the nearest house in either town. This ensures that your road is robust and doesn’t accidentally clip a house if a new one is built slightly off course.</p> <p>In SVM terms:</p> <ul> <li> <strong>Hyperplane</strong>: The “road” or decision boundary that separates the classes. In 2D, it’s a line; in 3D, it’s a plane; in higher dimensions, it’s called a hyperplane.</li> <li> <strong>Margin</strong>: The region between the two closest data points from each class. SVM aims to make this region as wide as possible.</li> <li> <strong>Support Vectors</strong>: These are the crucial data points that lie closest to the hyperplane. They “support” the hyperplane, meaning if you move or remove them, the hyperplane’s position and orientation might change. All other data points are less influential.</li> </ul> <p><img src="https://i.imgur.com/d9jY52v.png" alt="SVM Margin and Support Vectors"> <em>(Diagram showing a separating hyperplane, the margin, and the support vectors (highlighted) on the edges of the margin.)</em></p> <p>By maximizing this margin, SVMs achieve two crucial things:</p> <ol> <li> <strong>Robustness</strong>: A larger margin means the classifier is less sensitive to small perturbations in the data.</li> <li> <strong>Generalization</strong>: It tends to perform better on unseen data because it’s found a more “confident” separation.</li> </ol> <h3 id="the-math-behind-the-magic-linear-svm">The Math Behind the Magic: Linear SVM</h3> <p>Alright, let’s get a little bit mathematical. Don’t worry, we’ll break it down step-by-step.</p> <p>A hyperplane can be represented by the equation: \(w \cdot x + b = 0\) Where:</p> <ul> <li>$w$ is a vector perpendicular (normal) to the hyperplane. It dictates the orientation.</li> <li>$x$ is a data point.</li> <li>$b$ is the bias term (intercept). It dictates the position of the hyperplane.</li> </ul> <p>For any data point $x_i$, its predicted class is determined by the sign of $w \cdot x_i + b$. Let’s say we assign class labels $y_i = +1$ for cats and $y_i = -1$ for dogs. A correctly classified point will satisfy:</p> <ul> <li>$w \cdot x_i + b &gt; 0$ if $y_i = +1$</li> <li>$w \cdot x_i + b &lt; 0$ if $y_i = -1$</li> </ul> <p>We can combine these into a single condition: $y_i(w \cdot x_i + b) &gt; 0$.</p> <p>Now, remember the support vectors? These are the points closest to the hyperplane. For these points, we want them to be exactly at a distance of 1 from the hyperplane. We can scale $w$ and $b$ such that for the support vectors:</p> <ul> <li>$w \cdot x_+ + b = +1$ (for the positive class support vectors)</li> <li>$w \cdot x_- + b = -1$ (for the negative class support vectors)</li> </ul> <table> <tbody> <tr> <td>These two equations define the boundaries of our margin. The distance between these two parallel hyperplanes ($w \cdot x + b = 1$ and $w \cdot x + b = -1$) is given by $\frac{2}{</td> <td> </td> <td>w</td> <td> </td> <td>}$.</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>Our goal is to maximize this distance, which is equivalent to minimizing $</td> <td> </td> <td>w</td> <td> </td> <td>$. Mathematically, it’s often easier to minimize $\frac{1}{2}</td> <td> </td> <td>w</td> <td> </td> <td>^2$ (the $\frac{1}{2}$ and the square are for mathematical convenience during optimization, making the derivative simpler).</td> </tr> </tbody> </table> <p>So, the optimization problem for a <strong>Linear SVM</strong> with a hard margin (no misclassifications allowed) is:</p> <p>Minimize: \(\frac{1}{2} ||w||^2\)</p> <p>Subject to: \(y_i(w \cdot x_i + b) \ge 1 \quad \text{for all } i\)</p> <p>This is a quadratic programming problem – a type of convex optimization problem – which means we can find a unique global minimum. This mathematical formulation elegantly captures the intuition of maximizing the margin!</p> <h3 id="dealing-with-imperfection-soft-margin-svm">Dealing with Imperfection: Soft Margin SVM</h3> <p>The hard margin SVM works perfectly when your data is <strong>linearly separable</strong> – meaning you can draw a perfect straight line to separate the classes. But what happens in the real world? Data is often messy and overlapping!</p> <p><img src="https://i.imgur.com/K1Lg5kL.png" alt="Overlapping Data"> <em>(Circles and triangles, but now some triangles are mixed in with circles, and vice versa.)</em></p> <p>Trying to force a perfect separation would lead to a very wiggly, complex line that overfits the training data. It wouldn’t generalize well to new data.</p> <p>This is where the <strong>Soft Margin SVM</strong> comes to the rescue. It introduces some tolerance for misclassification or for points to be within the margin. We introduce “slack variables,” $\xi_i$ (pronounced “xi”).</p> <ul> <li>If $\xi_i = 0$, the point is correctly classified and outside the margin.</li> <li>If $0 &lt; \xi_i &lt; 1$, the point is correctly classified but within the margin.</li> <li>If $\xi_i \ge 1$, the point is misclassified.</li> </ul> <p>Our objective now includes a penalty for these slack variables. The new optimization problem becomes:</p> <p>Minimize: \(\frac{1}{2} ||w||^2 + C \sum\_{i=1}^{n} \xi_i\)</p> <p>Subject to: \(y_i(w \cdot x_i + b) \ge 1 - \xi_i \quad \text{for all } i\) \(\xi_i \ge 0 \quad \text{for all } i\)</p> <p>The hyperparameter $C$ is crucial here. It controls the trade-off between maximizing the margin and minimizing the misclassification errors.</p> <ul> <li> <strong>High C</strong>: Small tolerance for misclassification. The model tries hard to correctly classify all training points, potentially leading to a smaller margin and overfitting.</li> <li> <strong>Low C</strong>: Large tolerance for misclassification. The model prioritizes a larger margin, potentially accepting more training errors but achieving better generalization (less overfitting).</li> </ul> <p>Understanding $C$ is key to tuning your SVM model. It’s a classic example of the bias-variance trade-off in machine learning.</p> <h3 id="beyond-lines-the-kernel-trick">Beyond Lines: The Kernel Trick</h3> <p>So far, we’ve talked about separating data with a straight line (or hyperplane). But what if your data isn’t linearly separable even with a soft margin? Imagine data shaped like concentric circles: you can’t draw a straight line to separate the inner circle from the outer one.</p> <p><img src="https://i.imgur.com/z4V8Z8D.png" alt="Non-linearly Separable Data"> <em>(Circles and triangles, with circles forming an inner circle and triangles forming an outer ring.)</em></p> <p>This is where the true power and elegance of SVMs for non-linear classification come into play with the <strong>Kernel Trick</strong>.</p> <p>The idea is breathtakingly simple yet profoundly impactful:</p> <ol> <li>Map your original data points ($x$) from their current low-dimensional space to a much higher-dimensional feature space ($\phi(x)$).</li> <li>In this higher-dimensional space, the data might <em>become</em> linearly separable!</li> <li>Then, a linear SVM can be used to find a hyperplane in this new space, which corresponds to a non-linear decision boundary in the original space.</li> </ol> <p>The “trick” is that we don’t actually need to compute $\phi(x)$ explicitly for every data point. That would be computationally expensive, especially in very high dimensions. Instead, SVMs only need the <strong>dot product</strong> of these transformed features: $\phi(x_i) \cdot \phi(x_j)$.</p> <p>A <strong>kernel function</strong>, $K(x_i, x_j)$, allows us to directly compute this dot product in the higher-dimensional space without ever actually performing the explicit transformation! \(K(x_i, x_j) = \phi(x_i) \cdot \phi(x_j)\)</p> <p>This is like folding a piece of paper (your 2D data). Points that were close in 2D might still be close. But if you draw a line through the folded paper, when you unfold it, that line becomes a curve. The kernel trick achieves this “folding” without you ever having to physically fold the paper.</p> <p>Common Kernel Functions:</p> <ul> <li> <strong>Polynomial Kernel</strong>: $K(x_i, x_j) = (x_i \cdot x_j + c)^d$ <ul> <li>This maps data into a polynomial feature space. $d$ is the degree of the polynomial.</li> </ul> </li> <li> <strong>Radial Basis Function (RBF) Kernel</strong> (also known as Gaussian Kernel): \(K(x_i, x_j) = \exp(-\gamma ||x_i - x_j||^2)\) <ul> <li>This is perhaps the most popular kernel. It maps data into an infinite-dimensional space. $\gamma$ (gamma) is another important hyperparameter. A high $\gamma$ means points far apart contribute very little, leading to a “tighter” decision boundary that might overfit. A low $\gamma$ means points far apart still have some influence, leading to a smoother boundary.</li> </ul> </li> <li> <strong>Sigmoid Kernel</strong>: $K(x_i, x_j) = \tanh(\alpha x_i \cdot x_j + c)$ (often used in neural networks).</li> </ul> <p>The choice of kernel is crucial and depends on the nature of your data. The RBF kernel is a good default for many tasks.</p> <h3 id="why-svms-are-powerful-and-their-limitations">Why SVMs are Powerful (and Their Limitations)</h3> <p><strong>Strengths:</strong></p> <ul> <li> <strong>Effective in High Dimensions</strong>: SVMs work remarkably well in spaces with many features, even when the number of features exceeds the number of samples.</li> <li> <strong>Memory Efficient</strong>: Since only the support vectors are needed to define the hyperplane, SVMs are memory efficient once trained.</li> <li> <strong>Versatile with Kernels</strong>: The kernel trick allows them to model complex non-linear relationships.</li> <li> <strong>Robustness</strong>: The maximum margin principle makes them robust against small changes in data.</li> </ul> <p><strong>Limitations:</strong></p> <ul> <li> <strong>Computational Cost</strong>: Can be computationally intensive and slow to train on very large datasets (millions of samples), especially with non-linear kernels.</li> <li> <strong>Sensitivity to Hyperparameters</strong>: Performance is highly dependent on the choice of $C$ (and $\gamma$ for RBF kernel), which often requires careful tuning via techniques like cross-validation.</li> <li> <strong>Feature Scaling</strong>: SVMs are sensitive to the scaling of features. It’s almost always a good idea to standardize or normalize your data before training an SVM.</li> <li> <strong>Lack of Probabilistic Output</strong>: Unlike logistic regression, SVMs don’t inherently provide probability estimates for class membership (though extensions exist to approximate them).</li> </ul> <h3 id="my-takeaway--your-next-step">My Takeaway &amp; Your Next Step</h3> <p>My journey with SVMs showed me that some of the most elegant solutions in machine learning come from a beautiful blend of geometry and optimization. From simply drawing a line to intelligently transforming data into higher dimensions, SVMs offer a powerful and robust way to classify data.</p> <p>While newer algorithms like deep learning often grab the headlines, understanding the foundational algorithms like SVMs provides an invaluable bedrock for any data scientist. They teach us about margin maximization, regularization (via the C parameter), and the incredible power of the kernel trick.</p> <p>Now that you’ve got a grasp of the “why” and “how” of SVMs, I encourage you to:</p> <ol> <li> <strong>Code it up!</strong> Use scikit-learn’s <code class="language-plaintext highlighter-rouge">SVC</code> (Support Vector Classifier) in Python and experiment with different kernels and hyperparameters.</li> <li> <strong>Explore different datasets!</strong> See how SVMs perform on various real-world problems.</li> <li> <strong>Read more!</strong> Dive into the dual form of the SVM optimization problem for an even deeper mathematical understanding.</li> </ol> <p>Happy machine learning!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>