<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Markov Chains: How Simple Rules Predict Complex Futures (Even When They Forget!) | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/markov-chains-how-simple-rules-predict-complex-fut/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Markov Chains: How Simple Rules Predict Complex Futures (Even When They Forget!)</h1> <p class="post-meta"> Created on December 26, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/markov-chains"> <i class="fa-solid fa-hashtag fa-sm"></i> Markov Chains</a>   <a href="/blog/blog/tag/probability"> <i class="fa-solid fa-hashtag fa-sm"></i> Probability</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/time-series"> <i class="fa-solid fa-hashtag fa-sm"></i> Time Series</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>As a budding data scientist and machine learning enthusiast, one of the concepts that truly captivated me early on was the Markov Chain. It’s one of those beautiful mathematical ideas that, at first glance, seems almost too simple to be powerful, yet it underpins so many sophisticated algorithms we use today. From predicting the next word in a sentence to modeling how stock prices might fluctuate, Markov Chains offer a powerful lens through which to understand sequential, probabilistic systems.</p> <p>I remember first encountering them during a course on stochastic processes, and the idea of a system “forgetting” its past to predict its future struck me as profoundly counter-intuitive, yet deeply elegant. Let’s peel back the layers and see what makes these chains so fascinating and widely applicable.</p> <h3 id="the-heart-of-the-matter-states-transitions-and-the-memoryless-property">The Heart of the Matter: States, Transitions, and the “Memoryless” Property</h3> <p>At its core, a Markov Chain is a stochastic model describing a sequence of possible events where the probability of each event depends <em>only</em> on the state attained in the previous event. This isn’t just a fancy way of saying “what happened before affects what happens now”; it’s much more specific.</p> <p>Imagine a system that can be in one of several <em>states</em>. For example:</p> <ul> <li> <strong>Weather:</strong> Sunny, Cloudy, Rainy.</li> <li> <strong>A Light Switch:</strong> On, Off.</li> <li> <strong>Your Mood:</strong> Happy, Neutral, Sad.</li> <li> <strong>A Word in a Sentence:</strong> “The”, “cat”, “sat”, “on”, “the”, “mat”.</li> </ul> <p>A Markov Chain describes how the system <em>transitions</em> from one state to another over time. The crucial part, the “memoryless” property (also known as the <strong>Markov Property</strong>), is what truly defines it:</p> <p><strong>The probability of transitioning to any particular state depends only on the current state, and not on the sequence of states that preceded it.</strong></p> <p>Think of it like playing a board game where your next move only depends on the square you’re currently on, not on the path you took to get there. Whether you landed on “Start” via rolling a six or via rolling a one doesn’t matter for your next roll. All that matters is your current position. This simplification is incredibly powerful because it makes complex systems tractable.</p> <h3 id="the-math-behind-the-magic-transition-probabilities-and-matrices">The Math Behind the Magic: Transition Probabilities and Matrices</h3> <p>Let’s formalize this a bit. If $X_t$ represents the state of our system at time $t$, then the Markov property states:</p> <table> <tbody> <tr> <td>$P(X_{t+1} = j</td> <td>X_t = i, X_{t-1} = k, \dots) = P(X_{t+1} = j</td> <td>X_t = i)$</td> </tr> </tbody> </table> <p>This simply means the probability of being in state $j$ at time $t+1$, given the entire history, is the same as the probability of being in state $j$ given <em>only</em> that you were in state $i$ at time $t$.</p> <p>These transition probabilities are often constant over time, meaning the system is <em>time-homogeneous</em>. We can organize all these probabilities into a very important structure: the <strong>Transition Matrix (P)</strong>.</p> <p>Let’s use our simple weather example: States: $S_1$ = Sunny, $S_2$ = Rainy.</p> <p>Imagine these probabilities:</p> <ul> <li>If today is Sunny, there’s a 90% chance tomorrow is Sunny, and 10% chance it’s Rainy.</li> <li>If today is Rainy, there’s a 30% chance tomorrow is Sunny, and 70% chance it’s Rainy.</li> </ul> <p>We can represent this as a matrix:</p> <table> <tbody> <tr> <td>$P = \begin{pmatrix} P(S_1</td> <td>S_1) &amp; P(S_2</td> <td>S_1) \ P(S_1</td> <td>S_2) &amp; P(S_2</td> <td>S_2) \end{pmatrix} = \begin{pmatrix} 0.9 &amp; 0.1 \ 0.3 &amp; 0.7 \end{pmatrix}$</td> </tr> </tbody> </table> <p>Each row in the matrix represents the current state, and the elements in that row are the probabilities of transitioning to each possible next state. Notice that the sum of probabilities in each row must equal 1 (since you <em>must</em> transition to <em>some</em> state).</p> <p>Now, if we know the probability distribution of being in each state at time $t$, represented by a <strong>state vector</strong> $\pi_t = (\pi_{t,S_1}, \pi_{t,S_2})$, we can predict the distribution at time $t+1$ by simply multiplying:</p> <p>$\pi_{t+1} = \pi_t P$</p> <p>For example, if today there’s a 70% chance it’s Sunny and 30% chance it’s Rainy: $\pi_0 = (0.7, 0.3)$. Tomorrow’s probabilities would be: $\pi_1 = (0.7, 0.3) \begin{pmatrix} 0.9 &amp; 0.1 \ 0.3 &amp; 0.7 \end{pmatrix}$ $\pi_1 = ((0.7 \times 0.9) + (0.3 \times 0.3), (0.7 \times 0.1) + (0.3 \times 0.7))$ $\pi_1 = (0.63 + 0.09, 0.07 + 0.21)$ $\pi_1 = (0.72, 0.28)$</p> <p>So, after one day, there’s a 72% chance of Sun and 28% chance of Rain. Pretty neat for a simple calculation!</p> <h3 id="the-long-run-steady-state-and-ergodicity">The Long Run: Steady State and Ergodicity</h3> <p>What happens if we keep multiplying by $P$ day after day, year after year? Does the weather prediction eventually settle into a stable pattern, regardless of the initial probabilities? For many Markov Chains, the answer is yes!</p> <p>This long-term distribution is called the <strong>steady-state distribution</strong> or <strong>stationary distribution</strong>, denoted as $\pi^*$. It’s the probability distribution over states where, once reached, it remains constant over time. Mathematically, it satisfies:</p> <p>$\pi^* = \pi^* P$</p> <p>Along with the condition that the probabilities sum to one: $\sum_{i} \pi^*_i = 1$.</p> <p>Finding $\pi^<em>$ involves solving a system of linear equations. For our weather example: Let $\pi^</em> = (s, r)$, where $s$ is the long-term probability of Sunny and $r$ is the long-term probability of Rainy. We have: $(s, r) = (s, r) \begin{pmatrix} 0.9 &amp; 0.1 \ 0.3 &amp; 0.7 \end{pmatrix}$ This gives us two equations:</p> <ol> <li>$s = 0.9s + 0.3r$</li> <li>$r = 0.1s + 0.7r$ And our additional constraint:</li> <li>$s + r = 1$</li> </ol> <p>From equation 1: $0.1s = 0.3r \Rightarrow s = 3r$. Substitute this into equation 3: $3r + r = 1 \Rightarrow 4r = 1 \Rightarrow r = 0.25$. Then, $s = 3 \times 0.25 = 0.75$.</p> <p>So, the steady-state distribution is $(0.75, 0.25)$. In the long run, regardless of what the weather is like today, there’s a 75% chance of it being Sunny and a 25% chance of it being Rainy on any given day. This holds true as long as the transition probabilities remain constant.</p> <p>For a Markov Chain to converge to a unique steady state, it typically needs to be <strong>irreducible</strong> (meaning you can get from any state to any other state, possibly in multiple steps) and <strong>aperiodic</strong> (meaning it doesn’t get stuck in cycles of fixed length, like always being sunny, then rainy, then sunny, then rainy). Our weather example satisfies these conditions.</p> <h3 id="where-do-we-see-them-real-world-applications">Where Do We See Them? Real-World Applications</h3> <p>The simplicity of Markov Chains belies their incredible utility across various domains:</p> <ol> <li> <p><strong>Google PageRank Algorithm:</strong> This is perhaps the most famous application. Imagine the internet as a massive Markov Chain. Each webpage is a state, and a hyperlink from one page to another is a transition. Google’s algorithm essentially calculates the stationary distribution of this web surfer model. Pages with higher steady-state probabilities are considered more important, leading to their higher ranking in search results. It’s an elegant way to quantify the “importance” of a node in a massive network.</p> </li> <li> <strong>Natural Language Processing (NLP):</strong> <ul> <li> <strong>Text Generation:</strong> Markov Chains are the backbone of simple text generators. Given a word, a unigram (order 0 Markov Chain) might just pick any word from the vocabulary. A bigram (order 1 Markov Chain) predicts the next word based <em>only</em> on the current word, just like our weather example. Trigrams (order 2) consider the two previous words. This is how many autocomplete features or simple predictive text models work.</li> <li> <strong>Speech Recognition:</strong> Hidden Markov Models (HMMs), an extension of Markov Chains, are widely used in speech recognition. Here, the observed sounds are ‘hidden’ and the underlying spoken words are the states we infer.</li> <li> <strong>Spam Filtering:</strong> Analyzing word sequences can help classify emails.</li> </ul> </li> <li> <p><strong>Reinforcement Learning (RL):</strong> Many RL problems are framed as Markov Decision Processes (MDPs), which are an extension of Markov Chains where an agent also makes decisions (actions) that influence the transitions between states, often with associated rewards. The memoryless property is crucial here for defining the environment.</p> </li> <li> <p><strong>Biology and Genetics:</strong> Markov Chains are used to model the evolution of DNA sequences, protein folding, and population dynamics. For instance, you can model the probability of a base pair (A, T, C, G) changing over evolutionary time.</p> </li> <li> <p><strong>Finance:</strong> While financial markets are notoriously complex and often don’t strictly adhere to the memoryless property, simplified Markov models can be used to model stock price movements or credit risk ratings transitions.</p> </li> <li> <strong>Queueing Theory:</strong> Modeling waiting lines in supermarkets, call centers, or computer networks often employs Markov Chains to predict wait times and system throughput.</li> </ol> <p>I’ve personally found them incredibly useful when trying to get a baseline understanding of sequential data. Before jumping into complex recurrent neural networks, a simple Markov Chain can provide a fantastic first-pass model, highlighting inherent patterns in transitions.</p> <h3 id="limitations-and-extensions">Limitations and Extensions</h3> <p>While powerful, the Markov property—that strict “forgetfulness”—is also the main limitation. Many real-world phenomena do depend on more than just the immediate past. The stock market, for instance, might be influenced by trends over weeks or months, not just yesterday’s closing price. Similarly, human conversation often builds on a much richer context than just the last word spoken.</p> <p>Furthermore, we’ve discussed <em>discrete-time</em> Markov Chains with <em>discrete</em> states. There are also:</p> <ul> <li> <strong>Continuous-time Markov Chains:</strong> Where transitions can happen at any point in time.</li> <li> <strong>Hidden Markov Models (HMMs):</strong> Where the states themselves are not directly observable, but their influence on observable outputs is modeled probabilistically. This is very common in NLP and speech recognition.</li> <li> <strong>Markov Decision Processes (MDPs):</strong> As mentioned, these add actions and rewards, forming the foundation of much of Reinforcement Learning.</li> </ul> <h3 id="conclusion">Conclusion</h3> <p>Markov Chains offer a beautifully elegant and surprisingly robust framework for modeling systems that evolve probabilistically over time. Their underlying principle – that the future depends only on the present – simplifies complex dynamics, making them accessible for analysis and prediction.</p> <p>From understanding the long-term behavior of a weather system to ranking websites on the internet and generating human-like text, Markov Chains are a testament to how simple mathematical ideas can yield profound insights and powerful applications in data science and machine learning. So, the next time you see a prediction, pause for a moment. Perhaps, somewhere in its heart, a Markov Chain is quietly doing its work, remembering just enough to forget the rest, and still pointing us towards the future.</p> <p>What Markov Chain have you encountered in your life? Or perhaps, what system could you model with one? The possibilities are surprisingly vast!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>