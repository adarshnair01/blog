<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Demystifying the Magic: My Journey Into How Large Language Models Think | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/2025/demystifying-the-magic-my-journey-into-how-large-l/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Demystifying the Magic: My Journey Into How Large Language Models Think</h1> <p class="post-meta"> Created on November 19, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> NLP</a>   <a href="/blog/blog/tag/transformers"> <i class="fa-solid fa-hashtag fa-sm"></i> Transformers</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>When I first encountered Large Language Models (LLMs), like GPT-3 or even the models powering everyday AI assistants, I was captivated. It felt like magic – a system that could not only understand complex human language but also generate coherent, creative, and contextually relevant text. From writing poetry to debugging code, their capabilities seemed boundless. But as a budding data scientist, my curiosity quickly shifted from “Wow!” to “How?”</p> <p>This post is an exploration into that “how.” It’s my attempt to demystify these incredible systems, breaking down their core components in a way that’s accessible enough for high school students, yet deep enough to pique the interest of anyone curious about the cutting edge of AI. Think of it as peeking under the hood of a very sophisticated language engine, shared through my personal lens of discovery.</p> <h3 id="what-are-large-language-models-anyway">What ARE Large Language Models, Anyway?</h3> <p>At its heart, a Large Language Model is a type of artificial intelligence designed to understand, generate, and process human language. But calling them “chatbots” would be like calling a rocket a “fancy car.” They are far more sophisticated.</p> <p>The fundamental, mind-blowingly simple task an LLM learns during its initial training is to <strong>predict the next word</strong> in a sequence. Imagine you’re playing a game: “The cat sat on the…” What’s the most likely next word? “Mat,” “couch,” “rug.” An LLM does this, but on an unprecedented scale, across billions of words and complex contexts.</p> <p>The “Large” in LLM refers to two main things:</p> <ol> <li> <strong>The amount of data they’re trained on</strong>: Billions, even trillions, of words from the internet – books, articles, websites, conversations. This vast exposure to human language allows them to learn incredibly intricate patterns.</li> <li> <strong>The number of parameters they possess</strong>: These are the internal variables that the model adjusts during training, essentially its “knowledge knobs.” Modern LLMs can have hundreds of billions, even over a trillion, parameters. More parameters generally mean a greater capacity to learn and store information.</li> </ol> <p>It’s this combination of vast data and immense computational capacity that gives LLMs their seemingly magical ability to perform a wide range of language tasks, from translation and summarization to creative writing and complex reasoning.</p> <h3 id="the-building-blocks-a-peek-under-the-hood">The Building Blocks: A Peek Under the Hood</h3> <p>So, how does a computer “understand” words? It doesn’t, not in the human sense. Instead, it transforms words into numbers and uses mathematical operations to find patterns.</p> <h4 id="1-words-to-numbers-tokens-and-embeddings">1. Words to Numbers: Tokens and Embeddings</h4> <p>Before an LLM can do anything, it needs to convert human-readable text into a format a computer can process. This involves two key steps:</p> <ul> <li> <strong>Tokenization</strong>: Imagine breaking down a sentence into its fundamental units. These units are called <strong>tokens</strong>. A token might be a whole word (“apple”), a subword part (“ing”), or even a single character (“!”). For example, “unbelievable” might be tokenized as “un”, “believe”, “able”. This allows the model to handle rare words and variations efficiently. <ul> <li> <em>My personal insight</em>: This is like giving the computer a very precise set of LEGO bricks to build with, rather than just whole, rigid structures.</li> </ul> </li> <li> <strong>Embeddings</strong>: Once we have tokens, how do we represent their meaning? This is where <strong>embeddings</strong> come in. An embedding is a vector (a list of numbers) that represents a token. The magic here is that words with similar meanings will have embedding vectors that are “close” to each other in a multi-dimensional space. <ul> <li>For instance, the embedding vector for “king” might be very similar to “queen” but different from “apple.” The beauty is that relationships can also be encoded: the vector difference between “king” and “man” might be similar to the vector difference between “queen” and “woman.”</li> <li>Mathematically, if you imagine each word as a point in a high-dimensional space (say, 768 dimensions for many models), similar words cluster together. This isn’t just arbitrary; it’s learned from the contexts in which words appear in the training data.</li> </ul> </li> </ul> <h4 id="2-the-transformer-architecture-the-engine-of-modern-llms">2. The Transformer Architecture: The Engine of Modern LLMs</h4> <p>For a long time, models called Recurrent Neural Networks (RNNs) and Long Short-Term Memory networks (LSTMs) were the go-to for sequential data like text. They processed words one by one, maintaining a “memory” of previous words. However, they struggled with very long sentences, often forgetting information from the beginning of a text.</p> <p>Then came the <strong>Transformer architecture</strong> in 2017, introduced in the seminal paper “Attention Is All You Need.” This was a game-changer, and it’s the backbone of virtually all modern LLMs (GPT, BERT, LLaMA, etc.). The core innovation? The <strong>Attention Mechanism</strong>.</p> <h5 id="the-magic-of-attention">The Magic of Attention</h5> <p>Think about reading a long paragraph. When you encounter a pronoun like “it,” your brain automatically looks back to find what “it” refers to. The Attention Mechanism allows LLMs to do something similar: when processing a word, the model “pays attention” to other relevant words in the input sequence, no matter how far apart they are.</p> <p>This is achieved using three concepts for each word (or token) in the input:</p> <ul> <li> <strong>Query (Q)</strong>: What am I looking for? (e.g., “what does ‘it’ refer to?”)</li> <li> <strong>Key (K)</strong>: What do I have? (e.g., “I have ‘dog’, ‘ball’, ‘park’”)</li> <li> <strong>Value (V)</strong>: The actual information associated with the Key.</li> </ul> <p>Here’s a simplified breakdown of how it works:</p> <ol> <li>For each token, the model generates a Query vector ($Q$), a Key vector ($K$), and a Value vector ($V$). These are derived from the token’s embedding.</li> <li>To decide how much attention to pay to other tokens, we calculate a <strong>similarity score</strong> between the current token’s Query and all other tokens’ Keys. A common way to do this is using the dot product: $Q \cdot K$. A higher dot product means higher similarity.</li> <li>These scores are then passed through a <strong>softmax</strong> function. Softmax converts the scores into probabilities that sum to 1, indicating the <em>weight</em> or <em>importance</em> of each token. This scaling is often done by dividing by the square root of the dimension of the key vectors, $\sqrt{d_k}$, to prevent very large dot products from causing unstable gradients during training. So the attention score for a given Query $Q_i$ and Keys $K_j$ is $\text{softmax}(\frac{Q_i K_j^T}{\sqrt{d_k}})$.</li> <li>Finally, these attention weights are multiplied by the Value vectors ($V$) of each token and summed up. This weighted sum becomes the new, context-aware representation of the current token. It effectively blends the information from all other relevant tokens into the current one. <ul> <li>The full mathematical representation of scaled dot-product attention for a set of queries, keys, and values is: $Attention(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$ <em>Where $Q$, $K$, and $V$ are matrices containing the query, key, and value vectors for all tokens in the sequence.</em> </li> </ul> </li> </ol> <ul> <li> <em>My “Aha!” moment</em>: This means every word isn’t processed in isolation; it’s processed in the context of <em>every other word</em> in the sentence, simultaneously. This is why Transformers can handle long-range dependencies so well!</li> </ul> <h5 id="multi-head-attention-and-positional-encoding">Multi-Head Attention and Positional Encoding</h5> <p>To make attention even more powerful:</p> <ul> <li> <strong>Multi-Head Attention</strong>: Instead of just one set of Q, K, V, the model uses several independent “attention heads.” Each head learns to focus on different aspects of the relationships between words (e.g., one head might look for grammatical relationships, another for semantic ones). The outputs from these heads are then concatenated and linearly transformed.</li> <li> <strong>Positional Encoding</strong>: Since the Attention mechanism processes all words simultaneously (without a strict left-to-right order), the model needs a way to understand the sequence of words. Positional encodings are vectors added to the word embeddings that provide information about each word’s position in the sequence. This ensures that “Dog bites man” is understood differently from “Man bites dog.”</li> </ul> <p>After the attention layers, the Transformer also uses simple <strong>feed-forward neural networks</strong> to process each position independently, adding more non-linearity and representational capacity. These blocks are stacked many times (up to hundreds of layers in large models) to create deep networks that can learn very complex patterns.</p> <h4 id="3-pre-training-and-fine-tuning-the-learning-journey">3. Pre-training and Fine-tuning: The Learning Journey</h4> <p>LLMs learn in two major phases:</p> <ul> <li> <strong>Pre-training</strong>: This is the heavy lifting. The model is trained on massive datasets (the entire internet, essentially) in an <strong>unsupervised</strong> manner. The goal is simple: predict the next word, or fill in masked words. By doing this billions of times, the model learns grammar, facts, common sense, different writing styles, and even basic reasoning abilities purely from statistical patterns in language.</li> <li> <strong>Fine-tuning</strong>: After pre-training, the general-purpose model can be adapted for specific tasks (e.g., sentiment analysis, question answering, summarization) using smaller, <strong>supervised</strong> datasets. This step helps the model specialize its broad knowledge to particular applications. More recently, techniques like Reinforcement Learning from Human Feedback (RLHF) have been used to fine-tune models to be more helpful, harmless, and honest, as seen in models like ChatGPT.</li> </ul> <h3 id="the-power-of-scale-why-large-matters">The Power of Scale: Why “Large” Matters</h3> <p>The sheer scale of LLMs – billions of parameters, trained on trillions of tokens – leads to <strong>emergent abilities</strong>. These are capabilities that aren’t explicitly programmed but <em>emerge</em> as the model gets bigger. A small language model might just predict the next word; a large one can write a coherent essay, translate languages flawlessly, or even explain complex scientific concepts. It’s like reaching a critical mass where quantity transforms into a new quality.</p> <h3 id="challenges-and-ethical-considerations">Challenges and Ethical Considerations</h3> <p>While LLMs are astonishing, they are not without flaws.</p> <ul> <li> <strong>Bias</strong>: They learn from the internet, which unfortunately contains human biases. LLMs can inadvertently perpetuate stereotypes or generate toxic content if not carefully managed.</li> <li> <strong>Hallucinations</strong>: Sometimes, an LLM will confidently generate information that is completely false or nonsensical. They are statistical engines, not truth-tellers; they predict what <em>looks</em> like a plausible sequence of words, even if those words describe something that doesn’t exist.</li> <li> <strong>Computational Cost</strong>: Training and running these models requires immense computing power and energy, raising environmental concerns.</li> <li> <strong>Misinformation and Misuse</strong>: The ability to generate highly realistic text can be misused to create deepfakes, spread propaganda, or automate spam.</li> <li> <strong>Interpretability</strong>: Understanding <em>why</em> an LLM makes a particular decision is still a significant research challenge. They are often “black boxes.”</li> </ul> <h3 id="my-take-and-the-road-ahead">My Take and the Road Ahead</h3> <p>Exploring the inner workings of LLMs has been a profoundly insightful journey for me. From the simple idea of “next word prediction” to the elegant complexity of the Transformer’s attention mechanism, it reveals how sophisticated engineering, coupled with massive data and computational power, can lead to seemingly intelligent behavior.</p> <p>What truly fascinates me is the potential for these models to revolutionize how we interact with information, learn, and create. They’re not just tools; they’re collaborators, educators, and powerful assistants. But with great power comes great responsibility. As data scientists and engineers, it’s our duty to not only understand how these models work but also to guide their development ethically and responsibly.</p> <p>If you’re reading this as a high school student, I hope this peek behind the curtain sparks your curiosity. The field of AI and machine learning is constantly evolving, and there’s so much more to discover. Whether it’s learning to code, diving deeper into linear algebra, or simply experimenting with an LLM, your journey into this fascinating world is just beginning. The magic isn’t in what LLMs do, but in the ingenuity that built them – and what <em>we</em> will build with them next.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>