<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Unlocking the Machine's Eye: My Journey into Computer Vision | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/unlocking-the-machines-eye-my-journey-into-compute/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Unlocking the Machine's Eye: My Journey into Computer Vision</h1> <p class="post-meta"> Created on October 20, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/computer-vision"> <i class="fa-solid fa-hashtag fa-sm"></i> Computer Vision</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>From the moment we open our eyes, we’re bombarded with visual information. Our brains effortlessly interpret shapes, colors, movements, and expressions, making sense of a complex world. We rarely stop to think about the intricate dance of light hitting our retinas, signals firing in our optic nerves, and higher-level processing in our visual cortex that allows us to recognize a friend, avoid a pothole, or appreciate a beautiful sunset. It’s an astounding feat of biological engineering.</p> <p>Now, imagine giving that power – the power of sight and understanding – to a machine. That, in essence, is the grand ambition of <strong>Computer Vision</strong>.</p> <h3 id="what-exactly-is-computer-vision">What Exactly <em>Is</em> Computer Vision?</h3> <p>At its core, Computer Vision is a field of artificial intelligence that enables computers and systems to derive meaningful information from digital images, videos, and other visual inputs, and to take actions or make recommendations based on that information. It’s not just about “seeing” in the literal sense (which is just collecting pixels), but about <em>understanding</em> what those pixels represent.</p> <p>Think about it:</p> <ul> <li>How does a self-driving car differentiate between a pedestrian, a traffic light, and a tree?</li> <li>How does a medical imaging system detect a tumor in an X-ray?</li> <li>How does your smartphone unlock itself just by looking at your face?</li> <li>How do online stores recommend clothes similar to ones you’ve browsed?</li> </ul> <p>All of these incredible applications are powered by Computer Vision. For me, as someone passionate about Data Science and Machine Learning, the idea of teaching a machine to interpret the richness of visual data is utterly captivating. It feels like we’re giving machines a fundamental sense, opening up a whole new dimension of interaction with the world.</p> <h3 id="the-world-through-a-machines-eye-pixels-and-numbers">The World Through a Machine’s “Eye”: Pixels and Numbers</h3> <p>Before we dive into how computers <em>understand</em> images, let’s first consider how they <em>see</em> them. When you look at a beautiful photograph on your screen, what the computer actually “sees” is a grid of numbers.</p> <p>Every digital image is composed of tiny squares called <strong>pixels</strong>. Each pixel has a specific color value. In a typical color image, this value is often represented by three channels: Red, Green, and Blue (RGB). So, for any given pixel at coordinate $(x,y)$, its color can be described by three intensity values:</p> <p>$I(x,y) = (R_{intensity}, G_{intensity}, B_{intensity})$</p> <p>where each intensity typically ranges from 0 to 255. A value of (0, 0, 0) would be black, and (255, 255, 255) would be white. A typical image might be 1920 pixels wide by 1080 pixels high, meaning it’s an enormous array of numbers ($1920 \times 1080 \times 3$ numbers!).</p> <p>So, for a computer, an image is just a giant matrix (or a stack of three matrices, one for each color channel) of numbers. The challenge then becomes: how do we transform this raw numerical data into something meaningful, like “there’s a cat in this picture” or “this is a stop sign”?</p> <h3 id="the-deep-learning-revolution-enter-the-cnns">The Deep Learning Revolution: Enter the CNNs</h3> <p>For decades, Computer Vision relied on hand-crafted features and complex algorithms to detect edges, corners, and textures. Researchers would spend countless hours designing specific mathematical filters to find patterns. While these methods were ingenious, they often struggled with variability – a cat seen from a different angle, in different lighting, or partially obscured, might not be recognized.</p> <p>Then came the <strong>Deep Learning Revolution</strong>, and with it, <strong>Convolutional Neural Networks (CNNs)</strong>. CNNs didn’t just improve Computer Vision; they fundamentally transformed it. Instead of us telling the computer <em>what</em> features to look for, CNNs learn these features directly from the data.</p> <p>Think of a CNN as a stack of intelligent filters that learn to identify increasingly complex patterns within an image. Let’s break down the key players:</p> <h4 id="1-the-convolutional-layer-the-feature-detectives">1. The Convolutional Layer: The Feature Detectives</h4> <p>This is the heart of a CNN. Imagine a small magnifying glass, called a <strong>filter</strong> or <strong>kernel</strong>, sliding over every part of your image. This filter is itself a small matrix of numbers. At each position, it performs a mathematical operation called a <strong>convolution</strong> with the underlying pixels.</p> <p>The operation itself is a sum of products. For a given pixel $(x,y)$ in the output feature map, the convolution with a kernel $K$ is calculated as:</p> <p>$ (I * K)(x,y) = \sum_u \sum_v I(x-u, y-v) K(u,v) $</p> <p>where $I$ is the input image, and the sums are over the dimensions of the kernel.</p> <p>What does this do? Different filters are designed (or rather, <em>learned</em>) to detect different kinds of features. One filter might activate strongly when it encounters a vertical edge, another for a horizontal edge, another for a specific texture. The output of a convolutional layer isn’t just one number, but a <strong>feature map</strong> – essentially a new “image” where bright spots indicate where that particular feature was detected in the original image.</p> <h4 id="2-the-activation-function-adding-non-linearity">2. The Activation Function: Adding Non-Linearity</h4> <p>After a convolution, the output often passes through an <strong>activation function</strong>. A popular choice is the <strong>Rectified Linear Unit (ReLU)</strong>:</p> <p>$f(x) = \max(0, x)$</p> <p>In simple terms, ReLU just converts any negative values to zero and keeps positive values as they are. Why is this important? It introduces <em>non-linearity</em> into the network. Without non-linearity, no matter how many layers you stack, the network would only be able to learn linear relationships, which are insufficient to model the complexity of real-world images. It helps the network learn more intricate patterns.</p> <h4 id="3-the-pooling-layer-zooming-out-and-generalizing">3. The Pooling Layer: Zooming Out and Generalizing</h4> <p>After convolution and activation, the feature maps can be quite large. <strong>Pooling layers</strong>, commonly <strong>Max Pooling</strong>, help reduce the spatial dimensions (width and height) of the feature map. It works by taking a small window (e.g., 2x2 pixels) and picking the maximum value within that window, then moving the window to the next non-overlapping region.</p> <p>Think of it like this: if a feature (like an edge) is detected strongly in a small area, Max Pooling ensures that this strong signal is carried forward, even if the exact pixel location shifts slightly. This makes the network more robust to small variations and reduces the number of parameters, making the model faster and less prone to overfitting.</p> <h4 id="4-stacking-layers-building-a-hierarchy-of-understanding">4. Stacking Layers: Building a Hierarchy of Understanding</h4> <p>The real magic happens when you stack these layers. Early convolutional layers might learn very basic features like edges and corners. As you go deeper into the network, subsequent layers combine these basic features to detect more complex patterns:</p> <ul> <li>Layer 1: Edges, lines</li> <li>Layer 2: Simple shapes, textures (e.g., a circle, a brick pattern)</li> <li>Layer 3: Parts of objects (e.g., an eye, a wheel)</li> <li>Layer 4+: Full objects (e.g., a face, a car)</li> </ul> <p>This hierarchical learning is what gives CNNs their incredible power and allows them to understand images at a profound level.</p> <h4 id="5-the-fully-connected-layer-making-the-final-decision">5. The Fully Connected Layer: Making the Final Decision</h4> <p>Finally, after several convolutional and pooling layers have extracted high-level features, these features are “flattened” into a single vector and fed into one or more <strong>fully connected layers</strong>. These are like the traditional neural network layers where every neuron is connected to every neuron in the next layer. This part of the network takes all the learned features and uses them to make a final prediction – perhaps classifying the image (“cat,” “dog,” “bird”) or detecting specific objects with bounding boxes.</p> <h4 id="how-do-they-learn-the-training-process">How Do They Learn? The Training Process</h4> <p>A CNN learns by being shown millions of examples (images with their correct labels). If it makes a wrong prediction, a <strong>loss function</strong> calculates how “wrong” it was. This error signal is then propagated backward through the network (<strong>backpropagation</strong>), gently adjusting the thousands or even millions of internal parameters (the numbers in the filters) so that it’s more likely to make the correct prediction next time. It’s an iterative process of trial, error, and refinement, allowing the network to “tune” itself to recognize patterns with astonishing accuracy.</p> <h3 id="beyond-classification-key-applications-and-my-fascination">Beyond Classification: Key Applications and My Fascination</h3> <p>The capabilities of CNNs have unlocked an explosion of applications across countless industries:</p> <ul> <li> <strong>Object Detection:</strong> Identifying and localizing multiple objects within an image (e.g., self-driving cars recognizing other vehicles, pedestrians, traffic signs).</li> <li> <strong>Image Classification:</strong> Categorizing an entire image into a predefined class (e.g., “this is a picture of a landscape”).</li> <li> <strong>Semantic Segmentation:</strong> Assigning a class label to <em>every single pixel</em> in an image (e.g., marking all pixels belonging to the “road,” “sky,” or “car”). This is crucial for detailed scene understanding.</li> <li> <strong>Facial Recognition and Emotion Detection:</strong> Identifying individuals and inferring emotions from facial expressions.</li> <li> <strong>Medical Imaging:</strong> Assisting doctors in diagnosing diseases by analyzing X-rays, MRIs, and CT scans for anomalies.</li> <li> <strong>Augmented Reality (AR) &amp; Virtual Reality (VR):</strong> Understanding the real-world environment to seamlessly overlay virtual objects.</li> <li> <strong>Quality Control in Manufacturing:</strong> Automated inspection of products for defects at high speed.</li> </ul> <p>What truly excites me about Computer Vision is its potential to solve real-world problems. From enhancing accessibility for the visually impaired to revolutionizing healthcare and making our cities smarter, the impact is immense. The interdisciplinary nature, combining mathematics, statistics, programming, and an understanding of human perception, makes it an intellectually stimulating field.</p> <h3 id="challenges-and-the-road-ahead">Challenges and the Road Ahead</h3> <p>While Computer Vision has achieved incredible feats, it’s not without its challenges:</p> <ul> <li> <strong>Data Scarcity and Bias:</strong> High-quality, labeled datasets are crucial but often expensive to obtain. Biases in training data can lead to models that perform poorly or unfairly for certain demographics.</li> <li> <strong>Robustness and Adversarial Attacks:</strong> Models can sometimes be surprisingly fragile. Tiny, imperceptible changes to an image can completely fool a sophisticated model, a significant concern for security-critical applications.</li> <li> <strong>Interpretability:</strong> Understanding <em>why</em> a complex deep learning model makes a particular decision can be difficult, often referred to as the “black box” problem.</li> <li> <strong>Computational Cost:</strong> Training state-of-the-art models requires massive computational resources and energy.</li> <li> <strong>Ethical Considerations:</strong> As the technology becomes more powerful, concerns around privacy (e.g., pervasive facial recognition), surveillance, and the potential misuse of CV systems become increasingly important.</li> </ul> <p>Addressing these challenges is vital for the responsible and effective deployment of Computer Vision technology. It’s a call to action for every data scientist and machine learning engineer entering this field.</p> <h3 id="my-vision-for-the-future">My Vision for the Future</h3> <p>My journey into Computer Vision has just begun, but I’m already fascinated by its complexity and potential. As I build my portfolio, I’m eager to dive deeper, perhaps exploring projects in areas like medical image analysis or developing more robust and fair vision models. The continuous advancements, the open-source community, and the sheer intellectual horsepower driving this field make it an incredibly exciting space to be in.</p> <p>Computer Vision isn’t just about teaching machines to “see”; it’s about enabling them to understand, interact, and ultimately assist us in navigating and improving our world. It’s a testament to human ingenuity, and I can’t wait to be a part of its unfolding story. If you’re intrigued, I encourage you to grab some images, explore Python libraries like OpenCV and TensorFlow/PyTorch, and start your own journey into giving machines the gift of sight!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>