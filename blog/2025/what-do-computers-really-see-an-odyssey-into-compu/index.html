<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> What Do Computers *Really* See? An Odyssey into Computer Vision | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/what-do-computers-really-see-an-odyssey-into-compu/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">What Do Computers *Really* See? An Odyssey into Computer Vision</h1> <p class="post-meta"> Created on November 01, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/computer-vision"> <i class="fa-solid fa-hashtag fa-sm"></i> Computer Vision</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   <a href="/blog/blog/tag/image-processing"> <i class="fa-solid fa-hashtag fa-sm"></i> Image Processing</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>As a budding data scientist and machine learning enthusiast, few fields have captivated my imagination quite like Computer Vision. It’s a discipline that seeks to give machines the ultimate human sense: sight. But not just sight in the literal sense of perceiving light, rather, the ability to <em>understand</em> what they see – to identify objects, recognize faces, interpret scenes, and even infer actions.</p> <p>It started with a simple question: How does my phone <em>know</em> that’s <em>my</em> face? This seemingly trivial daily interaction sparked a fascination, pulling me down a rabbit hole of pixels, matrices, and neural networks. What I discovered was a field brimming with innovation, blending complex mathematics with elegant algorithms to solve some of the most challenging problems in AI.</p> <h3 id="the-world-through-a-computers-eyes-beyond-the-pixel">The World Through a Computer’s Eyes: Beyond the Pixel</h3> <p>Imagine looking at a picture of a cat. You instantly recognize it: furry, pointy ears, whiskers, probably plotting world domination. You perceive it as a cohesive entity, separate from the background. Now, how does a computer “see” that same image?</p> <p>For a computer, an image is nothing more than a grid of numbers, a massive matrix of pixels. A typical color image is represented by three such grids (or “channels”), one each for Red, Green, and Blue light intensities. Each pixel’s value usually ranges from 0 to 255. So, a $1000 \times 1000$ pixel color image isn’t a cat; it’s a $1000 \times 1000 \times 3$ array of numbers!</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Conceptual Python representation
</span><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># A grayscale image (height x width)
</span><span class="n">grayscale_image</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mi">60</span><span class="p">,</span> <span class="mi">70</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">80</span><span class="p">,</span> <span class="mi">90</span><span class="p">,</span> <span class="mi">100</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">110</span><span class="p">,</span> <span class="mi">120</span><span class="p">,</span> <span class="mi">130</span><span class="p">]</span>
<span class="p">])</span>

<span class="c1"># A color image (height x width x channels)
</span><span class="n">color_image</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span>
    <span class="p">[[</span><span class="mi">255</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span>  <span class="c1"># Red, Green pixels
</span>    <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">],</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">]]</span> <span class="c1"># Blue, Gray pixels
</span><span class="p">])</span>
</code></pre></div></div> <p>The fundamental challenge of Computer Vision is bridging this enormous gap: transforming raw pixel values into meaningful, semantic understanding. How do we get from “a bunch of numbers” to “that’s a cat wearing a tiny hat”?</p> <h3 id="the-early-days-handcrafting-features">The Early Days: Handcrafting Features</h3> <p>In the early days of Computer Vision, researchers tried to explicitly tell computers what to look for. They engineered “features” – specific patterns or characteristics – that might indicate the presence of an object.</p> <ul> <li> <p><strong>Edge Detection:</strong> One common technique involved finding sharp changes in pixel intensity, which usually correspond to edges of objects. Algorithms like Sobel or Canny filters would essentially “sweep” a small matrix (a “kernel” or “filter”) over the image, performing calculations to highlight these changes.</p> <p>Imagine a $3 \times 3$ kernel like this:</p> \[K_x = \begin{pmatrix} -1 &amp; 0 &amp; 1 \\ -2 &amp; 0 &amp; 2 \\ -1 &amp; 0 &amp; 1 \end{pmatrix}\] <p>When this kernel is applied (convolved) over an image, it enhances vertical edges. Similarly, a $K_y$ kernel would enhance horizontal edges.</p> </li> <li> <p><strong>Feature Descriptors:</strong> More complex descriptors like SIFT (Scale-Invariant Feature Transform) or HOG (Histogram of Oriented Gradients) were developed to identify robust features that remained recognizable even if an object was rotated, scaled, or viewed under different lighting.</p> </li> </ul> <p>These methods were ingenious but came with significant limitations. They often struggled with variations in viewpoint, lighting, and clutter. Crafting these features was a laborious, domain-specific task, and the results weren’t always robust enough for real-world applications.</p> <h3 id="the-game-changer-deep-learning-and-cnns">The Game Changer: Deep Learning and CNNs</h3> <p>The landscape of Computer Vision dramatically transformed with the advent of deep learning, particularly with a specific type of neural network called the <strong>Convolutional Neural Network (CNN)</strong>. Instead of painstakingly designing features, CNNs learn to extract features directly from the data. This paradigm shift was nothing short of revolutionary.</p> <h4 id="what-makes-a-cnn-so-special">What Makes a CNN So Special?</h4> <p>Let’s break down the core components of a typical CNN, which often feel like the secret sauce behind modern visual AI.</p> <ol> <li> <p><strong>Convolutional Layers: The Feature Detectors</strong> This is where the magic really begins. Like the edge detection filters mentioned earlier, convolutional layers use small learnable filters (kernels) that sweep across the input image. However, unlike traditional methods where these filters are predefined, in a CNN, the filters’ values are <em>learned</em> during training.</p> <p>Each filter specializes in detecting a particular feature: maybe a horizontal edge, a specific texture, a corner, or even more abstract patterns. As a filter slides over the image (this operation is called <strong>convolution</strong>), it calculates a dot product between its values and the corresponding pixel values in the input patch. The result is a single number in an “output feature map.”</p> <p>The formula for a 2D convolution at position $(i, j)$ of the output feature map $O$ for an input image $I$ and a filter $K$ is:</p> \[O(i, j) = \sum_{m=0}^{F-1} \sum_{n=0}^{F-1} I(i+m, j+n) K(m, n)\] <p>Where $F$ is the size of the filter.</p> <p>This process is repeated across the entire image, generating a new, smaller representation (the feature map) that highlights where that specific feature is present in the original image. Multiple filters are used in a layer, each generating its own feature map. Stacking these layers allows the network to learn increasingly complex and abstract features. Early layers might detect simple edges, while deeper layers combine these to detect shapes, then parts of objects (e.g., an eye, a wheel), and finally, entire objects.</p> </li> <li> <p><strong>Activation Functions: Introducing Non-Linearity</strong> After a convolution operation, an activation function is applied to the feature map. The most common one is the <strong>ReLU (Rectified Linear Unit)</strong>: $f(x) = \max(0, x)$.</p> <p>Why non-linearity? If all operations were linear, stacking multiple layers would just be equivalent to a single linear layer. Non-linearities allow the network to learn complex, non-linear relationships in the data, which are crucial for understanding real-world images.</p> </li> <li> <p><strong>Pooling Layers: Downsampling for Robustness</strong> Pooling layers, often inserted between convolutional layers, serve to reduce the spatial dimensions (width and height) of the feature maps. The most popular is <strong>Max Pooling</strong>. For a given window (e.g., $2 \times 2$), it takes the maximum value from that window.</p> <p>Why pool?</p> <ul> <li> <strong>Dimensionality Reduction:</strong> It reduces the number of parameters and computational cost, preventing overfitting.</li> <li> <strong>Translational Invariance:</strong> It makes the network more robust to slight shifts or distortions in the input image. If an object shifts slightly, its maximum feature activation will still likely be picked up in the pooled output, making the network less sensitive to exact positions.</li> </ul> </li> <li> <p><strong>Fully Connected Layers: The Classifier</strong> After several alternating convolutional and pooling layers, the high-level features extracted are “flattened” into a single vector. This vector is then fed into one or more fully connected (dense) layers, similar to a traditional artificial neural network. These layers are responsible for taking the learned features and making a final classification (e.g., “cat,” “dog,” “car”).</p> <p>The final layer typically uses a softmax activation function to output probabilities for each possible class.</p> </li> </ol> <h3 id="training-a-cnn-learning-to-see">Training a CNN: Learning to See</h3> <p>So, how do these filters and connections learn? Through a process called <strong>training</strong>, guided by data.</p> <ol> <li> <strong>Data, Data, Data:</strong> We feed the CNN millions of labeled images (e.g., “this image is a cat,” “this image is a dog”). Datasets like ImageNet, with millions of images across thousands of categories, were instrumental in the CNN revolution.</li> <li> <strong>Loss Function:</strong> For each image, the CNN makes a prediction. A <strong>loss function</strong> (e.g., Cross-Entropy Loss) quantifies how “wrong” that prediction is compared to the true label.</li> <li> <strong>Optimization (Gradient Descent &amp; Backpropagation):</strong> The goal is to minimize this loss. An optimization algorithm like <strong>Stochastic Gradient Descent (SGD)</strong>, powered by <strong>backpropagation</strong>, calculates how much each weight (including the values in our convolutional filters) contributes to the total loss. It then adjusts these weights incrementally in the direction that reduces the loss. This iterative process allows the CNN to “learn” the optimal filter values and connections that can accurately classify images.</li> </ol> <p>It’s truly remarkable: the network, starting with random weights, progressively discovers intricate patterns and hierarchical representations of the visual world, entirely on its own!</p> <h3 id="computer-vision-in-action-impacting-our-world">Computer Vision in Action: Impacting Our World</h3> <p>The breakthroughs in CNNs have unleashed a torrent of applications, many of which we interact with daily:</p> <ul> <li> <strong>Image Classification:</strong> Identifying the primary subject in an image (e.g., identifying spam images, content moderation).</li> <li> <strong>Object Detection:</strong> Not just <em>what</em> is in an image, but <em>where</em> it is. This involves drawing bounding boxes around objects. Think self-driving cars identifying pedestrians, vehicles, and traffic signs in real-time (YOLO, Faster R-CNN are popular architectures here).</li> <li> <strong>Semantic Segmentation:</strong> Taking object detection a step further, this task assigns a class label to <em>every single pixel</em> in an image. It’s like painting different objects with different colors. Crucial for medical imaging (segmenting tumors) and advanced robotics.</li> <li> <strong>Facial Recognition:</strong> From unlocking your phone to security systems, identifying individuals based on unique facial features.</li> <li> <strong>Medical Imaging Analysis:</strong> Aiding doctors in diagnosing diseases by analyzing X-rays, MRIs, and CT scans to detect anomalies, often with superhuman accuracy.</li> <li> <strong>Augmented Reality (AR):</strong> Understanding the real-world environment to seamlessly overlay virtual objects (think Pokémon Go or Snapchat filters).</li> <li> <strong>Robotics:</strong> Giving robots the ability to perceive their environment, navigate, and interact with objects safely.</li> </ul> <h3 id="the-road-ahead-challenges-and-ethical-considerations">The Road Ahead: Challenges and Ethical Considerations</h3> <p>While Computer Vision has made incredible strides, it’s far from a solved problem. My journey continues to uncover fascinating challenges:</p> <ul> <li> <strong>Data Scarcity:</strong> While large datasets exist, many niche applications still suffer from a lack of labeled data. Techniques like few-shot learning and synthetic data generation are active research areas.</li> <li> <strong>Robustness and Adversarial Attacks:</strong> CNNs can be surprisingly fragile. Tiny, imperceptible perturbations to an image can trick a model into misclassifying it completely. Ensuring robustness is critical for safety-critical applications.</li> <li> <strong>Bias:</strong> If the training data is biased (e.g., underrepresents certain demographics), the models will inherit and amplify that bias, leading to unfair or incorrect predictions. Addressing data bias and ensuring fairness is a paramount ethical concern.</li> <li> <strong>Explainability (XAI):</strong> Why did the model make that decision? Understanding the “black box” of deep neural networks is crucial for trust, debugging, and scientific discovery.</li> <li> <strong>Real-time Performance:</strong> Many applications, like autonomous driving, demand lightning-fast processing, pushing the boundaries of hardware and efficient model design.</li> </ul> <h3 id="my-continuing-journey">My Continuing Journey</h3> <p>Exploring Computer Vision has been an exhilarating experience, blending the precision of mathematics with the creative problem-solving of engineering. From understanding how pixels transform into probabilities to appreciating the elegant simplicity of a convolutional filter, it’s a field that constantly challenges and inspires.</p> <p>The ability to teach machines to see, understand, and interact with our visual world is not just about technological advancement; it’s about expanding human capabilities, enhancing safety, and opening doors to applications we can barely imagine today. As I continue to build my skills in data science and machine learning, I’m eager to contribute to this vibrant field and help shape the future where AI truly has its eyes wide open.</p> <p>Perhaps your journey into Computer Vision will start with a similar question, or simply by observing the marvels around you that are powered by this incredible technology. The tools are more accessible than ever, and the problems waiting to be solved are limitless. So, what will <em>you</em> teach computers to see next?</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>