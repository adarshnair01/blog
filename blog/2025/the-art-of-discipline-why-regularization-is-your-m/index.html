<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Art of Discipline: Why Regularization is Your Model's Best Friend Against Overfitting | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/the-art-of-discipline-why-regularization-is-your-m/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Art of Discipline: Why Regularization is Your Model's Best Friend Against Overfitting</h1> <p class="post-meta"> Created on June 20, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/regularization"> <i class="fa-solid fa-hashtag fa-sm"></i> Regularization</a>   <a href="/blog/blog/tag/overfitting"> <i class="fa-solid fa-hashtag fa-sm"></i> Overfitting</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/model-training"> <i class="fa-solid fa-hashtag fa-sm"></i> Model Training</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey there, fellow data explorer!</p> <p>It’s late, and I’m staring at another model’s performance report. It’s doing <em>exceptionally</em> well on the training data, metrics are through the roof! But then I test it on some new, unseen data, and… <em>oof</em>. The performance drops like a stone. It’s a familiar sting for anyone diving deep into machine learning: the dreaded <strong>overfitting</strong>.</p> <p>It’s like preparing for a big exam. If you just memorize every single practice question, word for word, you might ace <em>those specific questions</em>. But if the actual exam throws even a slightly different phrasing or a new problem, you’re lost because you didn’t truly understand the underlying concepts. Your model, in this scenario, just “memorized” the training data, including all its quirks and noise, instead of learning the general patterns.</p> <p>This phenomenon, where a model performs excellently on the data it was trained on but poorly on new data, is the bane of many a data scientist’s existence. Our goal isn’t just to make a model perform well on what it’s seen; it’s to make it <strong>generalize</strong> well to data it <em>hasn’t</em> seen. And that, my friends, is where <strong>regularization</strong> steps onto the stage.</p> <h3 id="the-overfitting-dilemma-too-much-of-a-good-thing">The Overfitting Dilemma: Too Much of a Good Thing</h3> <p>Imagine you’re trying to draw a line through a set of data points that represent, say, the relationship between hours studied and exam scores.</p> <p>A simple straight line (a linear model) might capture the general trend: more studying, higher scores. But it won’t hit every single point perfectly. This model is <strong>simple</strong>, and while it might have some <strong>bias</strong> (it might not perfectly capture all the nuances), it has low <strong>variance</strong> (it won’t change drastically if we get new data).</p> <p>Now, imagine drawing a really wiggly, complex curve that passes through <em>every single data point</em>. It’s perfect for the training data! But if you get a new data point, this wild curve might predict something completely outlandish because it’s been influenced too much by the noise or specific anomalies in the training data. This is a <strong>complex</strong> model; it has low bias (it fits the training data perfectly) but very high <strong>variance</strong> (it’s extremely sensitive to new data and will likely perform poorly).</p> <p>Overfitting happens when our model becomes too complex. It’s like a journalist who reports every single detail, every rumor, every personal anecdote – they capture <em>everything</em> about one specific event, but miss the bigger story or context, making their reporting unreliable for future, similar events.</p> <p>So, how do we rein in this complexity? How do we tell our model, “Hey, focus on the big picture, not every little speck of dust”?</p> <h3 id="enter-regularization-the-models-disciplinarian">Enter Regularization: The Model’s Disciplinarian</h3> <p>Regularization is essentially a technique that adds a “penalty” to our model’s complexity. It discourages the model from assigning excessively large weights to features, which often leads to those overly wiggly, high-variance curves. Think of it as giving our model a stern talking-to: “You can be good, but you can’t be <em>too</em> good by memorizing everything.”</p> <p>The core idea is to modify the model’s <strong>loss function</strong>. The loss function is what the model tries to minimize during training; it quantifies how “wrong” the model’s predictions are.</p> <p>Normally, for a simple linear regression, our model tries to minimize something like the Mean Squared Error (MSE):</p> <p>$ \text{Minimize: } J(\mathbf{w}, b) = \frac{1}{m} \sum_{i=1}^{m} (y^{(i)} - (\mathbf{w}^T \mathbf{x}^{(i)} + b))^2 $</p> <p>Here, $\mathbf{w}$ represents the weights (or coefficients) that our model learns for each feature, $b$ is the bias term, $\mathbf{x}^{(i)}$ is the input features for the $i$-th data point, $y^{(i)}$ is the actual output, and $m$ is the number of data points.</p> <p>Regularization adds an extra term to this loss function:</p> <p>$ \text{Minimize: } J<em>{\text{regularized}}(\mathbf{w}, b) = \frac{1}{m} \sum</em>{i=1}^{m} (y^{(i)} - (\mathbf{w}^T \mathbf{x}^{(i)} + b))^2 + \text{Regularization Term} $</p> <p>This “Regularization Term” is what penalizes large weights. The larger the weights, the more complex the model is considered to be, and the higher the penalty. This forces the model to find a balance: fit the data well <em>but</em> keep the weights as small as possible.</p> <p>There’s also a hyperparameter, typically denoted by $\lambda$ (lambda), that controls the strength of this penalty.</p> <ul> <li>If $\lambda$ is $0$, there’s no penalty, and we’re back to an unregularized model (prone to overfitting).</li> <li>If $\lambda$ is very large, the penalty for large weights becomes so significant that the model will prioritize keeping weights small, potentially sacrificing too much fit to the data (leading to underfitting – too simple a model).</li> <li>Our goal is to find the “Goldilocks” $\lambda$ that’s just right.</li> </ul> <p>Let’s dive into the two most common types of regularization: L1 and L2.</p> <h3 id="l2-regularization-ridge-regression-the-team-player-penalty">L2 Regularization: Ridge Regression (The “Team Player” Penalty)</h3> <p>Imagine a sports team. You want players who contribute, but you don’t want one superstar trying to do <em>everything</em> and hogging the ball. L2 regularization works similarly. It penalizes the <em>square</em> of the magnitude of the weights.</p> <p>The regularization term for L2 is: $\lambda \sum_{j=1}^{n} w_j^2$</p> <p>So, the full L2-regularized (Ridge Regression) loss function looks like this:</p> <p>$ J<em>{\text{Ridge}}(\mathbf{w}, b) = \frac{1}{m} \sum</em>{i=1}^{m} (y^{(i)} - (\mathbf{w}^T \mathbf{x}^{(i)} + b))^2 + \lambda \sum_{j=1}^{n} w_j^2 $</p> <p>Notice how we sum the <em>squares</em> of the weights ($w_j^2$). Squaring big numbers makes them even bigger, so this penalty heavily discourages extremely large weights.</p> <p><strong>What does L2 regularization do?</strong></p> <ul> <li> <strong>Shrinks weights towards zero:</strong> It pushes all the weights closer to zero, but it rarely makes them <em>exactly</em> zero. This means that all features still contribute to the model, just with smaller, more controlled impacts.</li> <li> <strong>Smooths the model:</strong> By keeping weights small, it prevents the model from making sharp turns or highly sensitive predictions based on individual features, leading to a smoother, less complex decision boundary.</li> <li> <strong>Distributes impact:</strong> Instead of one feature having an enormous weight, L2 regularization encourages a more even distribution of smaller weights across multiple features. It’s like telling all your team members to pass the ball around and contribute, rather than relying on one player for all the goals.</li> </ul> <p>L2 regularization is excellent when you suspect that <em>all</em> your features are somewhat relevant, and you just want to prevent any single feature from dominating the prediction too much due to noise.</p> <h3 id="l1-regularization-lasso-regression-the-feature-selector-penalty">L1 Regularization: Lasso Regression (The “Feature Selector” Penalty)</h3> <p>Now, imagine a different scenario for your sports team. You have a huge roster of players, but you suspect some of them aren’t really contributing much at all – maybe some are even just getting in the way. You want to identify the truly essential players and sideline the others. That’s where L1 regularization comes in.</p> <p>It penalizes the <em>absolute value</em> of the magnitude of the weights.</p> <table> <tbody> <tr> <td>The regularization term for L1 is: $\lambda \sum_{j=1}^{n}</td> <td>w_j</td> <td>$</td> </tr> </tbody> </table> <p>So, the full L1-regularized (Lasso Regression) loss function looks like this:</p> <table> <tbody> <tr> <td>$ J<em>{\text{Lasso}}(\mathbf{w}, b) = \frac{1}{m} \sum</em>{i=1}^{m} (y^{(i)} - (\mathbf{w}^T \mathbf{x}^{(i)} + b))^2 + \lambda \sum_{j=1}^{n}</td> <td>w_j</td> <td>$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>Notice the absolute value ($</td> <td>w_j</td> <td>$). This seemingly small change has a profound effect.</td> </tr> </tbody> </table> <p><strong>What does L1 regularization do?</strong></p> <ul> <li> <strong>Shrinks weights to zero:</strong> Unlike L2, L1 regularization has a tendency to shrink some weights <em>exactly</em> to zero. When a weight becomes zero, the corresponding feature is effectively removed from the model.</li> <li> <strong>Feature Selection:</strong> This ability to drive weights to zero means L1 regularization performs automatic feature selection. It’s incredibly useful when you have many features, and you suspect only a subset of them are truly important. It’s like pruning your team down to only the top performers.</li> <li> <strong>Sparse Models:</strong> Models with many zero weights are called “sparse models.” They are simpler, more interpretable, and computationally more efficient because they rely on fewer features.</li> </ul> <p>L1 regularization is your go-to when you believe many features might be irrelevant or redundant, and you want your model to be simpler and easier to understand.</p> <h3 id="elastic-net-regularization-the-best-of-both-worlds">Elastic Net Regularization: The Best of Both Worlds</h3> <p>What if you want the feature-selecting power of L1 but also the group effect (not knocking out correlated features completely) of L2? Enter <strong>Elastic Net Regularization</strong>. It’s a hybrid that combines both L1 and L2 penalties:</p> <table> <tbody> <tr> <td>$ J<em>{\text{Elastic Net}}(\mathbf{w}, b) = \frac{1}{m} \sum</em>{i=1}^{m} (y^{(i)} - (\mathbf{w}^T \mathbf{x}^{(i)} + b))^2 + \lambda<em>1 \sum</em>{j=1}^{n}</td> <td>w*j</td> <td>+ \lambda_2 \sum*{j=1}^{n} w_j^2 $</td> </tr> </tbody> </table> <p>(Often, $\lambda_1$ and $\lambda_2$ are combined into a single $\lambda$ and a ratio $\alpha$, but the idea remains: it’s a mix).</p> <p>Elastic Net is particularly useful when you have a dataset with many features, some of which are highly correlated. L1 might randomly pick one of the correlated features and zero out the others, which isn’t always ideal. Elastic Net helps group correlated features together, keeping them in the model collectively while still performing selection.</p> <h3 id="regularization-beyond-simple-regression">Regularization Beyond Simple Regression</h3> <p>While we’ve discussed regularization mainly in the context of linear regression, its principles apply broadly across machine learning.</p> <ul> <li> <strong>Neural Networks:</strong> In neural networks, L1 and L2 regularization are often referred to as “L1/L2 weight decay.” They prevent individual neurons from becoming too specialized to specific training examples. Another very popular regularization technique in neural networks is <strong>Dropout</strong>, where during training, a random subset of neurons are “dropped out” (temporarily ignored). This prevents neurons from co-adapting too much and forces the network to learn more robust features.</li> <li> <strong>Support Vector Machines (SVMs):</strong> The C parameter in SVMs is conceptually similar to the inverse of $\lambda$. A smaller C (larger $\lambda$) encourages a wider margin, accepting more misclassifications but leading to a more generalized model.</li> </ul> <h3 id="finding-the-right-discipline-hyperparameter-tuning">Finding the Right Discipline: Hyperparameter Tuning</h3> <p>How do we choose the right $\lambda$ (or $\lambda_1$, $\lambda_2$, or dropout rate)? This is where <strong>hyperparameter tuning</strong> comes in, typically using techniques like <strong>cross-validation</strong>.</p> <p>We split our training data into several folds. We train the model on a subset of these folds and evaluate its performance on the remaining fold. We repeat this process multiple times with different $\lambda$ values and pick the one that gives the best average performance on the validation sets. This ensures our chosen $\lambda$ leads to a model that generalizes well.</p> <h3 id="the-art-of-balance">The Art of Balance</h3> <p>Regularization isn’t about making your model “worse” at fitting the training data; it’s about making it “smarter” and more robust for the real world. It forces your model to find simpler, more general patterns, reducing its tendency to memorize noise.</p> <p>In essence, regularization helps us navigate the crucial bias-variance trade-off. We introduce a little bit of bias (by restricting the model’s complexity) to significantly reduce variance, ultimately leading to models that perform reliably on unseen data.</p> <p>So, the next time your model is performing suspiciously well on your training set, remember regularization. It’s the silent hero that ensures your model isn’t just a memorization machine, but a true learner ready for anything the real world throws its way.</p> <p>Keep exploring, keep building, and keep your models well-disciplined!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>