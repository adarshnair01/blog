<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Beyond 'You Might Also Like': A Data Scientist's Journey into Recommender Systems | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/beyond-you-might-also-like-a-data-scientists-journ/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Beyond 'You Might Also Like': A Data Scientist's Journey into Recommender Systems</h1> <p class="post-meta"> Created on August 24, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/recommender-systems"> <i class="fa-solid fa-hashtag fa-sm"></i> Recommender Systems</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/collaborative-filtering"> <i class="fa-solid fa-hashtag fa-sm"></i> Collaborative Filtering</a>   <a href="/blog/blog/tag/content-based-filtering"> <i class="fa-solid fa-hashtag fa-sm"></i> Content-Based Filtering</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>As a data scientist, I often find myself pondering the algorithms that shape our daily lives. One particular area that never ceases to fascinate me is <strong>Recommender Systems</strong>. You interact with them constantly, perhaps without even realizing it. That perfect movie suggestion on Netflix, the next song Spotify queues up, the product Amazon thinks you <em>just might need</em> – all powered by these intelligent systems.</p> <p>It’s easy to dismiss these as simple suggestions, but behind that “You Might Also Like” label lies a sophisticated blend of statistics, linear algebra, and machine learning. Today, I want to take you on a journey to demystify this magic, breaking down how these systems work, what challenges they face, and where they might be headed. Think of this as my personal journal entry, exploring one of the coolest applications of data science.</p> <h3 id="the-why-behind-the-magic-information-overload">The “Why” Behind the Magic: Information Overload</h3> <p>Imagine walking into a massive library with millions of books, or a colossal store with billions of products. Without help, finding something you’d genuinely love is like finding a needle in an infinite haystack. This is the challenge of information overload that recommender systems are built to solve. They act as your personal curator, sifting through vast amounts of data to present you with tailored options, enhancing your experience and often introducing you to things you wouldn’t have discovered otherwise.</p> <p>So, how do they do it? Let’s dive into the core methodologies.</p> <h3 id="the-two-big-ideas-content-vs-collaboration">The Two Big Ideas: Content vs. Collaboration</h3> <p>At their heart, most recommender systems fall into one of two main categories: <strong>Content-Based Filtering</strong> or <strong>Collaborative Filtering</strong>.</p> <h4 id="1-content-based-filtering-if-you-liked-this-youll-like-that">1. Content-Based Filtering: “If you liked this, you’ll like that!”</h4> <p><strong>The Idea:</strong> This approach is perhaps the most intuitive. It works by recommending items that are similar to items a user has liked in the past. It’s all about analyzing the <strong>features</strong> of the items themselves and comparing them to a user’s historical preferences.</p> <p><strong>How it Works (My Analogy):</strong> Imagine you’re a movie buff, and I’m building a content-based recommender for you. I’d look at the movies you’ve rated highly. Let’s say you loved “Dune” and “Interstellar.” Both are sci-fi, have epic visuals, and complex plots. My system would then build a “profile” of your taste based on these features: high preference for “sci-fi,” “epic,” “complex storyline.” Then, it would search for other movies with similar features that you haven’t seen yet – perhaps “Arrival” or “Blade Runner 2049.”</p> <p><strong>In Action:</strong></p> <ol> <li> <strong>Item Representation:</strong> Each item is described by a set of attributes (e.g., for movies: genre, director, actors, keywords). We can represent these as vectors.</li> <li> <strong>User Profile:</strong> A user’s profile is built from the features of items they’ve positively interacted with (e.g., averaging the feature vectors of liked movies).</li> <li> <strong>Recommendation:</strong> The system recommends items whose features best match the user’s profile, typically using similarity metrics like <strong>Cosine Similarity</strong> (which we’ll touch on soon!).</li> </ol> <p><strong>Pros:</strong></p> <ul> <li> <strong>No “Cold Start” for items:</strong> Can recommend new items as long as their features are known.</li> <li> <strong>User Independence:</strong> Doesn’t need other users’ data, only the individual user’s history.</li> <li> <strong>Transparency:</strong> Easy to explain <em>why</em> an item was recommended (e.g., “Because you liked other sci-fi movies”).</li> </ul> <p><strong>Cons:</strong></p> <ul> <li> <strong>Limited Diversity:</strong> Tends to recommend items very similar to what a user already likes, potentially creating a “filter bubble.”</li> <li> <strong>Feature Engineering:</strong> Requires detailed, structured data about items, which can be hard to obtain or define.</li> <li> <strong>“Cold Start” for Users:</strong> Can’t recommend to new users without any history.</li> </ul> <h4 id="2-collaborative-filtering-people-like-you-like-this">2. Collaborative Filtering: “People like you, like this!”</h4> <p>This is where things get really interesting and a bit more subtle. Collaborative filtering doesn’t rely on item features at all. Instead, it leverages the collective behavior and preferences of users. It’s about finding patterns in how users interact with items. There are two main flavors:</p> <h5 id="a-user-user-collaborative-filtering-finding-your-taste-alikes">a) User-User Collaborative Filtering: Finding Your Taste-Alikes</h5> <p><strong>The Idea:</strong> “Tell me who has similar taste to me, and I’ll see what they liked.” If I, a data scientist who loves indie rock, have a similar taste profile to another data scientist who also loves indie rock, then there’s a good chance I’ll like the new indie rock band they just discovered.</p> <p><strong>How it Works:</strong></p> <ol> <li> <strong>Find Similar Users:</strong> The system identifies users whose past ratings or interactions are similar to yours.</li> <li> <strong>Recommend:</strong> It then recommends items that these “similar users” have liked but you haven’t seen yet.</li> </ol> <p>To find similar users, we need to quantify “similarity.” Two common metrics are <strong>Cosine Similarity</strong> and <strong>Pearson Correlation</strong>.</p> <ul> <li> <p><strong>Cosine Similarity:</strong> This measures the cosine of the angle between two vectors (e.g., rating vectors of two users). A smaller angle (cosine closer to 1) means higher similarity. It’s great for comparing users in a high-dimensional space where most values are zero (sparse data).</p> <table> <tbody> <tr> <td>$cosine_similarity(A, B) = \frac{A \cdot B}{</td> <td> </td> <td>A</td> <td> </td> <td>\cdot</td> <td> </td> <td>B</td> <td> </td> <td>} = \frac{\sum_{i=1}^n A_i B_i}{\sqrt{\sum_{i=1}^n A_i^2} \sqrt{\sum_{i=1}^n B_i^2}}$</td> </tr> </tbody> </table> <p>Here, $A$ and $B$ are the rating vectors for user A and user B across all common items.</p> </li> <li> <p><strong>Pearson Correlation:</strong> This measures the linear relationship between two datasets. It’s particularly useful because it adjusts for differences in rating scales (e.g., if one user always rates highly and another always rates low, but they agree on <em>relative</em> preferences).</p> <p>$pearson_correlation(A, B) = \frac{\sum_{i=1}^n (A_i - \bar{A})(B_i - \bar{B})}{\sqrt{\sum_{i=1}^n (A_i - \bar{A})^2} \sqrt{\sum_{i=1}^n (B_i - \bar{B})^2}}$</p> <p>Where $\bar{A}$ and $\bar{B}$ are the average ratings of user A and user B, respectively.</p> </li> </ul> <p><strong>Pros:</strong></p> <ul> <li> <strong>Discoverability:</strong> Can recommend entirely new genres or types of items that a user hasn’t explicitly shown interest in, simply because their “taste-alikes” enjoyed them.</li> <li> <strong>No Item Features Needed:</strong> Doesn’t require any prior information about the items themselves.</li> </ul> <p><strong>Cons:</strong></p> <ul> <li> <strong>Scalability:</strong> Finding similar users among millions can be computationally very expensive, especially for large user bases.</li> <li> <strong>Sparsity:</strong> Many users rate very few items, making it hard to find enough common items to accurately calculate similarity.</li> <li> <strong>“Cold Start” for Users:</strong> New users have no rating history, so no similar users can be found.</li> </ul> <h5 id="b-item-item-collaborative-filtering-people-who-liked-this-also-liked-that">b) Item-Item Collaborative Filtering: “People who liked <em>this</em>, also liked <em>that</em>!”</h5> <p><strong>The Idea:</strong> Instead of finding similar users, let’s find similar <em>items</em>. If I liked “The Lord of the Rings,” what other movies were also liked by people who liked “The Lord of the Rings”? This tends to be more stable because item similarity is generally less volatile than user similarity.</p> <p><strong>How it Works:</strong></p> <ol> <li> <strong>Find Similar Items:</strong> The system calculates how similar each item is to every other item, based on how users have rated them. (e.g., if many users who rated movie A highly also rated movie B highly, then A and B are similar).</li> <li> <strong>Recommend:</strong> When you view an item, the system recommends other items that are similar to it.</li> </ol> <p><strong>Example:</strong> You just finished watching “The Crown” on Netflix. The system looks at all the users who also watched and liked “The Crown” and then sees what <em>other</em> shows those users liked. If a significant number of them also enjoyed “Downton Abbey” or “Peaky Blinders,” those would be strong recommendations for you.</p> <p><strong>Pros:</strong></p> <ul> <li> <strong>Scalability:</strong> Item similarity can often be pre-computed and is more stable over time, making it faster for large user bases.</li> <li> <strong>Less Affected by Sparsity:</strong> Item similarities are often denser than user similarities.</li> <li> <strong>“Cold Start” for Users (partial solution):</strong> If a new user rates even one item, we can instantly recommend items similar to it.</li> </ul> <p><strong>Cons:</strong></p> <ul> <li> <strong>“Cold Start” for Items:</strong> New items have no interaction data, so their similarity to other items can’t be computed.</li> <li><strong>Still suffers from the fundamental ‘cold start’ for brand new users with <em>no</em> ratings.</strong></li> </ul> <h3 id="a-peek-into-matrix-factorization-unveiling-latent-factors">A Peek into Matrix Factorization: Unveiling Latent Factors</h3> <p>Collaborative filtering, while powerful, can struggle with scalability and sparsity. Enter <strong>Matrix Factorization</strong>, a more advanced technique that tries to uncover the “secret ingredients” behind user preferences and item characteristics.</p> <p>Imagine a giant table (matrix) where rows are users and columns are items. Each cell contains a user’s rating for an item, but most cells are empty because users only rate a tiny fraction of all available items. This is our <strong>User-Item Interaction Matrix</strong>, $R$.</p> <p>Matrix factorization aims to decompose this sparse matrix $R$ into two lower-rank matrices:</p> <ol> <li> <strong>User Matrix ($P$):</strong> Each row represents a user, and columns represent a set of <strong>latent factors</strong> (e.g., “action-loving,” “sci-fi enthusiast,” “drama-follower”). We don’t explicitly define these factors; the algorithm discovers them.</li> <li> <strong>Item Matrix ($Q$):</strong> Each row represents an item, and columns represent how strongly that item exhibits each latent factor.</li> </ol> <p>The idea is that if we multiply these two matrices, $P$ and $Q^T$, we can reconstruct our original rating matrix $R$, but <em>with the missing ratings filled in!</em></p> <p>$\hat{R} \approx P \cdot Q^T$</p> <p>Where $\hat{R}$ is our predicted rating matrix. For a specific user $u$ and item $i$, the predicted rating $\hat{r}_{ui}$ would be the dot product of user $u$’s latent factor vector ($p_u$) and item $i$’s latent factor vector ($q_i$):</p> <p>$\hat{r}<em>{ui} = p_u \cdot q_i^T = \sum</em>{k=1}^K p_{uk}q_{ik}$</p> <p>Here, $K$ is the number of latent factors we choose (e.g., 50, 100).</p> <p><strong>How it Works (Intuitively):</strong> Think of these latent factors as hidden “personality traits” for users and “characteristic components” for items. The algorithm learns these factors by trying to minimize the difference between the actual known ratings and the ratings predicted by the dot product of the factor vectors. This is typically done using optimization algorithms like <strong>Stochastic Gradient Descent (SGD)</strong>, which iteratively adjusts the values in $P$ and $Q$ to get closer to the true ratings.</p> <p><strong>Pros:</strong></p> <ul> <li> <strong>Handles Sparsity:</strong> Can make accurate predictions even with very sparse rating data.</li> <li> <strong>Scalability:</strong> Once the matrices $P$ and $Q$ are learned, predicting ratings is very fast.</li> <li> <strong>Deep Understanding:</strong> Uncovers hidden relationships and patterns that are not obvious from raw data.</li> </ul> <p><strong>Cons:</strong></p> <ul> <li> <strong>Interpretability:</strong> Latent factors are abstract; it’s hard to explain <em>why</em> a specific factor exists or what it represents.</li> <li> <strong>“Cold Start” (still):</strong> Hard to assign latent factors to new users or items without any historical data.</li> </ul> <h3 id="hybrid-approaches-best-of-both-worlds">Hybrid Approaches: Best of Both Worlds</h3> <p>Given the pros and cons of each method, real-world recommender systems often combine techniques. <strong>Hybrid systems</strong> leverage the strengths of content-based filtering (e.g., for new items) and collaborative filtering (e.g., for discoverability), often resulting in more robust and accurate recommendations. For example, Netflix famously used a hybrid approach as part of its $1 million prize competition-winning algorithm.</p> <h3 id="navigating-the-rocky-roads-challenges-in-recommender-systems">Navigating the Rocky Roads: Challenges in Recommender Systems</h3> <p>Building a truly effective recommender system isn’t without its hurdles:</p> <ol> <li> <strong>Cold Start Problem:</strong> <ul> <li> <strong>New Users:</strong> If a user has no history, how do you know what to recommend? (Solution: popularity-based, content-based initially, ask for preferences).</li> <li> <strong>New Items:</strong> If an item has just been added, it has no ratings. How do you recommend it? (Solution: content-based, promote new items, expert reviews).</li> </ul> </li> <li> <p><strong>Sparsity:</strong> Most users interact with only a tiny fraction of available items. This makes the user-item matrix very sparse, challenging similarity calculations and matrix factorization.</p> </li> <li> <p><strong>Scalability:</strong> For platforms with millions of users and items, computing similarities or factorizing matrices can be computationally intensive and require distributed computing infrastructures.</p> </li> <li> <strong>Bias &amp; Filter Bubbles:</strong> Recommending only what’s similar can lead to a narrow range of suggestions, reinforcing existing preferences and potentially creating “filter bubbles” where users are only exposed to information that confirms their existing views. Introducing <strong>diversity</strong> and <strong>serendipity</strong> (recommending surprisingly good items) is a constant challenge.</li> </ol> <h3 id="measuring-success-how-do-we-know-its-good">Measuring Success: How Do We Know It’s Good?</h3> <p>To evaluate how well a recommender system performs, we use various metrics:</p> <ul> <li> <strong>RMSE (Root Mean Squared Error):</strong> For predicting ratings, this measures the average magnitude of the errors. A lower RMSE means more accurate predictions.</li> <li> <strong>Precision@K &amp; Recall@K:</strong> For top-N recommendations, these metrics measure how many of the top K recommended items are truly relevant to the user (precision) and how many relevant items were captured in the top K (recall).</li> </ul> <h3 id="the-road-ahead-the-future-of-recommendations">The Road Ahead: The Future of Recommendations</h3> <p>Recommender systems are a rapidly evolving field. We’re seeing exciting advancements with:</p> <ul> <li> <strong>Deep Learning:</strong> Neural networks can learn complex, non-linear relationships in user-item interactions, often outperforming traditional methods, especially with rich, unstructured data (text, images, audio).</li> <li> <strong>Reinforcement Learning:</strong> Treating recommendations as a sequence of actions, where the system learns to maximize long-term user satisfaction.</li> <li> <strong>Context-Aware Recommendations:</strong> Incorporating external factors like time of day, location, mood, or companion into recommendations.</li> <li> <strong>Ethical AI:</strong> Addressing issues of fairness, transparency, and preventing harmful biases or addiction loops.</li> </ul> <h3 id="conclusion-your-personal-guide-to-discovery">Conclusion: Your Personal Guide to Discovery</h3> <p>From understanding your past preferences to discovering the hidden “personality traits” of items and users, recommender systems are a powerful testament to how data science can enhance our daily lives. They transform overwhelming choice into curated discovery, making our digital experiences more personal and enjoyable.</p> <p>This journey has only scratched the surface. The world of recommender systems is vast and ever-expanding, filled with fascinating mathematical challenges and endless opportunities for innovation. So next time Netflix queues up your perfect movie, take a moment to appreciate the intricate dance of algorithms working tirelessly behind the scenes – a dance choreographed by data scientists like us, pushing the boundaries of what’s possible.</p> <p>What’s <em>your</em> favorite recommendation you’ve ever received? And what kinds of recommendations would you like to see next? Let me know!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>