<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Invisible Hand of Learning: Demystifying Backpropagation | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/the-invisible-hand-of-learning-demystifying-backpr/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Invisible Hand of Learning: Demystifying Backpropagation</h1> <p class="post-meta"> Created on July 13, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/neural-networks"> <i class="fa-solid fa-hashtag fa-sm"></i> Neural Networks</a>   <a href="/blog/blog/tag/backpropagation"> <i class="fa-solid fa-hashtag fa-sm"></i> Backpropagation</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/gradient-descent"> <i class="fa-solid fa-hashtag fa-sm"></i> Gradient Descent</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>As a budding data scientist, I’ve always been fascinated by how neural networks, these incredibly powerful mathematical constructs, manage to ‘learn’ and adapt. It feels like magic, doesn’t it? One moment, a network is clueless; the next, it’s recognizing cats, translating languages, or even generating art. This isn’t magic, however, but brilliant engineering, powered by a single, elegant algorithm: <strong>Backpropagation</strong>.</p> <p>Today, I want to take you on a journey to demystify this “invisible hand” that guides our neural networks to intelligence. Whether you’re a high school student with a knack for puzzles or a fellow aspiring ML engineer, understanding Backpropagation will be a cornerstone of your deep learning adventure.</p> <h3 id="the-stage-our-neural-network">The Stage: Our Neural Network</h3> <p>Before we dive into the ‘how,’ let’s quickly remind ourselves of the ‘what.’ Imagine a neural network as a series of interconnected nodes, or ‘neurons,’ organized into layers:</p> <ol> <li> <strong>Input Layer:</strong> Where our data enters (e.g., pixel values of an image).</li> <li> <strong>Hidden Layers:</strong> The computational engine, where the magic happens. Each neuron in a layer receives inputs from the previous layer, performs a calculation, and passes its output to the next.</li> <li> <strong>Output Layer:</strong> The final result of the network (e.g., “cat” or “dog”).</li> </ol> <p>Inside each neuron, two primary operations occur:</p> <ul> <li>A <strong>weighted sum</strong> of its inputs ($z = \sum (w_i x_i) + b$), where $w_i$ are <strong>weights</strong> (the strength of connection), $x_i$ are inputs, and $b$ is a <strong>bias</strong> (an offset).</li> <li>An <strong>activation function</strong> (e.g., ReLU, Sigmoid) applies a non-linear transformation to this sum ($a = \sigma(z)$). This non-linearity is crucial for the network to learn complex patterns.</li> </ul> <p>When we feed data through the network, from input to output, this is called the <strong>forward pass</strong>. At the end of it, our network makes a prediction.</p> <h3 id="the-problem-when-predictions-go-wrong">The Problem: When Predictions Go Wrong</h3> <p>Initially, a neural network is like a newborn baby. Its weights and biases are randomly initialized – it has no idea what it’s doing! Its predictions will almost certainly be wrong.</p> <p>To quantify “wrongness,” we use a <strong>Loss Function</strong> (or Cost Function). This function measures the difference between our network’s prediction ($\hat{y}$) and the actual truth ($y$). Common loss functions include Mean Squared Error for regression ($L = (\hat{y} - y)^2$) or Cross-Entropy for classification. Our goal? To minimize this loss.</p> <h3 id="the-quest-finding-the-path-to-improvement">The Quest: Finding the Path to Improvement</h3> <p>Imagine you’re standing on a mountain, blindfolded. Your goal is to reach the lowest point in the valley. How do you do it? You feel around with your feet. If the ground slopes down in one direction, you take a small step that way. If it slopes up, you know not to go that way. You keep taking small steps in the steepest downhill direction until you reach the bottom.</p> <p>This is the essence of <strong>Gradient Descent</strong>. The “mountain” is our loss function, and the “direction of steepest descent” is given by the <strong>gradient</strong>. The gradient is a vector of partial derivatives, telling us how much the loss changes with respect respect to each weight and bias in our network.</p> <p>Mathematically, for a single weight $w$, we want to calculate $\frac{\partial L}{\partial w}$. Once we have this, we update the weight:</p> <p>$w_{new} = w_{old} - \eta \frac{\partial L}{\partial w}$</p> <p>Here, $\eta$ (eta) is the <strong>learning rate</strong>, a small positive number that determines the size of our steps. If $\eta$ is too large, we might overshoot the minimum; if too small, we might take forever to get there.</p> <p>The challenge? A typical neural network can have millions, even billions, of weights and biases. Calculating all these partial derivatives independently for every single parameter would be computationally impossible!</p> <h3 id="the-aha-moment-backpropagation">The “Aha!” Moment: Backpropagation</h3> <p>This is where Backpropagation steps in, a brilliant algorithm discovered independently multiple times, but popularized in the context of neural networks by Rumelhart, Hinton, and Williams in 1986.</p> <p>Instead of recalculating everything from scratch for each parameter, Backpropagation exploits a fundamental rule of calculus: <strong>the Chain Rule</strong>. It allows us to calculate the gradient of the loss with respect to <em>each</em> parameter by propagating the error signal <em>backward</em> through the network, layer by layer.</p> <h4 id="the-chain-rule-our-mathematical-swiss-army-knife">The Chain Rule: Our Mathematical Swiss Army Knife</h4> <p>Let’s refresh our memory on the chain rule with a simple example. If you have $y = f(u)$ and $u = g(x)$, then the derivative of $y$ with respect to $x$ is:</p> <p>$\frac{dy}{dx} = \frac{dy}{du} \cdot \frac{du}{dx}$</p> <p>In essence, it says: “To find out how much <code class="language-plaintext highlighter-rouge">x</code> affects <code class="language-plaintext highlighter-rouge">y</code>, first find out how much <code class="language-plaintext highlighter-rouge">x</code> affects <code class="language-plaintext highlighter-rouge">u</code>, and then how much <code class="language-plaintext highlighter-rouge">u</code> affects <code class="language-plaintext highlighter-rouge">y</code>, and multiply these effects.”</p> <p>This is precisely what Backpropagation does. It takes the “blame” (the error signal) from the final output, and distributes it backward, layer by layer, through the network. Each neuron calculates how much it contributed to the error, and then passes its share of the blame to the neurons in the <em>previous</em> layer.</p> <h4 id="step-by-step-breakdown-simplified">Step-by-Step Breakdown (Simplified)</h4> <p>Let’s consider a simple feed-forward network for clarity.</p> <p><strong>1. The Forward Pass:</strong></p> <ul> <li>Input $x$ passes through layer 1, activating $a^{(1)}$.</li> <li>$a^{(1)}$ passes through layer 2, activating $a^{(2)}$ (our prediction $\hat{y}$).</li> <li>We calculate the loss $L(\hat{y}, y)$.</li> </ul> <p><strong>2. The Backward Pass (Backpropagation begins!):</strong></p> <p><strong>a. Output Layer’s Blame:</strong> First, we need to know how much the loss changes with respect to the output of our last layer. This is the initial error signal we propagate backward:</p> <p>$\frac{\partial L}{\partial a^{(L)}}$ (where $L$ denotes the output layer)</p> <p>Next, for the weights and biases <em>connecting to the output layer</em> (let’s say $W^{(L)}$ and $b^{(L)}$):</p> <ul> <li> <p><strong>Bias update for output layer:</strong> To find out how much the loss changes with respect to a bias $b^{(L)}<em>j$ in the output layer, we use the chain rule: $\frac{\partial L}{\partial b^{(L)}_j} = \frac{\partial L}{\partial a^{(L)}_j} \cdot \frac{\partial a^{(L)}_j}{\partial z^{(L)}_j} \cdot \frac{\partial z^{(L)}_j}{\partial b^{(L)}_j}$ Since $z^{(L)}_j = \sum w^{(L)}</em>{jk} a^{(L-1)}_k + b^{(L)}_j$, we have $\frac{\partial z^{(L)}_j}{\partial b^{(L)}_j} = 1$. So, $\frac{\partial L}{\partial b^{(L)}_j} = \frac{\partial L}{\partial a^{(L)}_j} \cdot \sigma’(z^{(L)}_j)$ (where $\sigma’$ is the derivative of the activation function).</p> </li> <li> <p><strong>Weight update for output layer:</strong> Similarly, for a weight $w^{(L)}<em>{jk}$ connecting neuron $k$ in the previous layer to neuron $j$ in the output layer: $\frac{\partial L}{\partial w^{(L)}</em>{jk}} = \frac{\partial L}{\partial a^{(L)}<em>j} \cdot \frac{\partial a^{(L)}_j}{\partial z^{(L)}_j} \cdot \frac{\partial z^{(L)}_j}{\partial w^{(L)}</em>{jk}}$ Since $z^{(L)}<em>j = \sum w^{(L)}</em>{jk} a^{(L-1)}<em>k + b^{(L)}_j$, we have $\frac{\partial z^{(L)}_j}{\partial w^{(L)}</em>{jk}} = a^{(L-1)}<em>k$. So, $\frac{\partial L}{\partial w^{(L)}</em>{jk}} = \frac{\partial L}{\partial a^{(L)}_j} \cdot \sigma’(z^{(L)}_j) \cdot a^{(L-1)}_k$</p> </li> </ul> <p>Notice a pattern? The term $\frac{\partial L}{\partial a^{(L)}_j} \cdot \sigma’(z^{(L)}_j)$ appears in both. This is often called the <strong>error term</strong> for neuron $j$ in layer $L$, denoted as $\delta^{(L)}_j$.</p> <p>$\delta^{(L)}_j = \frac{\partial L}{\partial a^{(L)}_j} \cdot \sigma’(z^{(L)}_j)$</p> <p>Now, the gradients are simply: $\frac{\partial L}{\partial b^{(L)}<em>j} = \delta^{(L)}_j$ $\frac{\partial L}{\partial w^{(L)}</em>{jk}} = \delta^{(L)}_j \cdot a^{(L-1)}_k$</p> <p><strong>b. Propagating Blame to the Hidden Layers:</strong> This is the core of Backpropagation. How do we get the error term for a hidden layer, say layer $(L-1)$? We use the chain rule again!</p> <p>The error $\delta^{(L-1)}_k$ for a neuron $k$ in layer $(L-1)$ depends on how much its output $a^{(L-1)}_k$ influenced the errors in the <em>next</em> layer (layer $L$).</p> <p>$\delta^{(L-1)}<em>k = \left( \sum</em>{j} w^{(L)}_{jk} \delta^{(L)}_j \right) \cdot \sigma’(z^{(L-1)}_k)$</p> <p>Let’s break this down:</p> <ul> <li>$\sum_{j} w^{(L)}<em>{jk} \delta^{(L)}_j$: This term sums up the error signals ($\delta^{(L)}_j$) from all neurons $j$ in the *next* layer, weighted by the strength of their connection ($w^{(L)}</em>{jk}$) to our current neuron $k$. This effectively tells us how much neuron $k$’s output contributed to the <em>total error</em> propagated backward from the next layer.</li> <li>$\sigma’(z^{(L-1)}_k)$: This multiplies the summed error by the derivative of the activation function at neuron $k$. It accounts for how sensitive neuron $k$’s output was to its input. If the activation function was flat (derivative close to zero), even a large error coming back won’t change this neuron’s internal state much.</li> </ul> <p>With $\delta^{(L-1)}_k$, we can calculate the gradients for $W^{(L-1)}$ and $b^{(L-1)}$ just as before:</p> <p>$\frac{\partial L}{\partial b^{(L-1)}<em>k} = \delta^{(L-1)}_k$ $\frac{\partial L}{\partial w^{(L-1)}</em>{ki}} = \delta^{(L-1)}_k \cdot a^{(L-2)}_i$</p> <p>This process continues, layer by layer, backwards through the network until we reach the input layer. At each step, we calculate the error term for the current layer and then use it to compute the gradients for its weights and biases, and also to compute the error term for the <em>previous</em> layer.</p> <h3 id="why-backpropagation">Why “Backpropagation”?</h3> <p>The name perfectly describes the process: we <strong>propagate</strong> the error signal <strong>back</strong> through the network. It’s an incredibly efficient way to compute all the necessary gradients, allowing neural networks to learn even with vast numbers of parameters.</p> <h3 id="the-learning-loop">The Learning Loop</h3> <p>So, a single training iteration (or <em>epoch</em>) for a neural network looks like this:</p> <ol> <li> <strong>Forward Pass:</strong> Feed input data through the network, calculate outputs.</li> <li> <strong>Calculate Loss:</strong> Compare predictions to actual labels using the loss function.</li> <li> <strong>Backward Pass (Backpropagation):</strong> Use the chain rule to calculate the gradients of the loss with respect to all weights and biases, propagating error backward.</li> <li> <strong>Update Parameters:</strong> Adjust weights and biases using Gradient Descent (or its variants like Adam, RMSProp) based on the calculated gradients and the learning rate.</li> <li> <strong>Repeat:</strong> Go back to step 1 with the next batch of data, continually refining the network’s understanding.</li> </ol> <h3 id="beyond-the-basics">Beyond the Basics</h3> <p>While Backpropagation is conceptually straightforward, its implementation and performance have practical challenges:</p> <ul> <li> <strong>Vanishing/Exploding Gradients:</strong> In very deep networks, gradients can become extremely small (vanishing) or extremely large (exploding) as they are propagated backward, hindering learning. This led to innovations like ReLU activation functions, careful weight initialization, and batch normalization.</li> <li> <strong>Optimizers:</strong> Simple Gradient Descent can be slow. Advanced optimizers like Adam, RMSProp, and Adagrad use more sophisticated methods to adjust learning rates per parameter, significantly speeding up training.</li> </ul> <h3 id="my-takeaway-and-yours">My Takeaway and Yours</h3> <p>Learning about Backpropagation was a true “aha!” moment for me. It transformed the seemingly magical process of neural network learning into an elegant, understandable mathematical dance. It showed me that even the most complex AI systems are built upon fundamental, beautiful principles.</p> <p>Backpropagation is not just an algorithm; it’s the bedrock upon which modern deep learning stands. Understanding it deeply gives you an incredible advantage in debugging models, choosing activation functions, and truly comprehending the “why” behind what works in neural networks.</p> <p>So, next time you see an AI perform an amazing feat, remember the invisible hand of Backpropagation, meticulously guiding the network, one tiny gradient step at a time, towards intelligence. Dive deeper, experiment, and keep learning – the world of AI is yours to explore!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>