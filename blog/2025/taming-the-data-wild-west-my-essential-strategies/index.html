<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Taming the Data Wild West: My Essential Strategies for Squeaky Clean Insights | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/taming-the-data-wild-west-my-essential-strategies/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Taming the Data Wild West: My Essential Strategies for Squeaky Clean Insights</h1> <p class="post-meta"> Created on November 02, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/data-cleaning"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Cleaning</a>   <a href="/blog/blog/tag/data-preprocessing"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Preprocessing</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/python"> <i class="fa-solid fa-hashtag fa-sm"></i> Python</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>It’s a common dream for aspiring data scientists, isn’t it? You picture yourself diving deep into complex algorithms, building mind-bending predictive models, and unearthing revolutionary insights. I certainly did. But then, I bumped into a harsh reality that quickly became my most frequent — and often most rewarding — challenge: <strong>messy data.</strong></p> <p>My first real data science project felt less like a sophisticated intellectual pursuit and more like an archaeological dig through a digital landfill. Missing values, inconsistent formats, duplicate entries, bizarre outliers… it was all there, a glorious, chaotic mess. I spent more time wrestling with the data itself than with any fancy machine learning model. And you know what? That’s perfectly normal.</p> <p>The adage “Garbage In, Garbage Out” isn’t just a catchy phrase; it’s the absolute truth. No matter how cutting-edge your algorithm, if the data feeding it is flawed, your results will be, too. My journey taught me that data cleaning isn’t a chore to be rushed through; it’s an art, a science, and arguably, the most critical phase of any data project.</p> <p>So, I wanted to share my tried-and-true data cleaning strategies. Think of this as my personal toolkit, refined through countless battles with messy datasets. Whether you’re just starting out or looking to sharpen your existing skills, these approaches will help you transform raw, unruly data into a pristine foundation for powerful insights.</p> <h3 id="the-why-behind-the-mess-a-little-empathy-goes-a-long-way">The “Why” Behind the Mess: A Little Empathy Goes a Long Way</h3> <p>Before we dive into the “how,” let’s quickly understand <em>why</em> data gets messy in the first place. It’s rarely malicious; more often, it’s a byproduct of real-world operations:</p> <ul> <li> <strong>Human Error:</strong> Typos, incorrect entries, inconsistent formatting by different users.</li> <li> <strong>Faulty Instruments/Sensors:</strong> Devices malfunction, leading to erroneous or missing readings.</li> <li> <strong>Data Integration Challenges:</strong> Merging datasets from different sources with varying schemas, naming conventions, or data types.</li> <li> <strong>Legacy Systems:</strong> Old databases often lack modern validation rules, allowing for inconsistencies to creep in.</li> <li> <strong>Data Collection Issues:</strong> Poorly designed surveys, optional fields left blank, or incomplete data exports.</li> </ul> <p>Recognizing these sources helps me anticipate potential issues and approach the cleaning process with a problem-solving mindset rather than just frustration.</p> <h3 id="my-data-cleaning-toolkit-strategies-for-squeaky-clean-insights">My Data Cleaning Toolkit: Strategies for Squeaky Clean Insights</h3> <h4 id="i-understanding-your-data-the-first-commandment-of-cleaning">I. Understanding Your Data: The First Commandment of Cleaning</h4> <p>You can’t clean what you don’t understand. My first step, <em>always</em>, is a deep dive into Exploratory Data Analysis (EDA). This is where I become a data detective, looking for clues, patterns, and anomalies.</p> <p><strong>What I do:</strong></p> <ul> <li> <strong>Initial Inspection:</strong> <ul> <li> <code class="language-plaintext highlighter-rouge">df.info()</code>: Tells me data types, non-null counts, and memory usage. A quick scan often reveals columns with many missing values or incorrect data types (e.g., numbers stored as objects/strings).</li> <li> <code class="language-plaintext highlighter-rouge">df.describe()</code>: Provides statistical summaries (mean, min, max, quartiles) for numerical columns. This is great for spotting unusually large/small values or potential outliers.</li> <li> <code class="language-plaintext highlighter-rouge">df.isnull().sum()</code>: A simple yet powerful command to see the total number of missing values per column. I often visualize this as a bar chart to quickly grasp the scale of the problem.</li> <li> <code class="language-plaintext highlighter-rouge">df.head()</code> and <code class="language-plaintext highlighter-rouge">df.sample()</code>: Just looking at raw data samples can reveal formatting issues, extra spaces, or inconsistent capitalization.</li> </ul> </li> <li> <strong>Value Counts:</strong> For categorical features, <code class="language-plaintext highlighter-rouge">df['column'].value_counts()</code> is invaluable. It quickly highlights inconsistent spellings (“USA”, “U.S.A.”, “United States”), typos, or categories that should be grouped.</li> <li> <strong>Visualizations:</strong> <ul> <li> <strong>Histograms and Density Plots:</strong> For numerical data, these help me understand the distribution and spot outliers or skewness.</li> <li> <strong>Box Plots:</strong> Excellent for identifying outliers, especially across different groups.</li> <li> <strong>Scatter Plots:</strong> Useful for understanding relationships between two numerical variables and spotting unusual data points.</li> </ul> </li> </ul> <p><strong>My takeaway:</strong> This exploratory phase isn’t just about identifying problems; it’s about building intuition about the data. I’m trying to understand its story, its quirks, and what “normal” looks like.</p> <h4 id="ii-handling-missing-values-to-fill-or-not-to-fill">II. Handling Missing Values: To Fill or Not To Fill?</h4> <p>Missing data (often represented as <code class="language-plaintext highlighter-rouge">NaN</code>, <code class="language-plaintext highlighter-rouge">null</code>, or <code class="language-plaintext highlighter-rouge">None</code>) is perhaps the most common headache. My strategy here depends heavily on the <em>nature</em> of the missingness and the <em>amount</em> of data I have.</p> <p><strong>1. Deletion (When to bravely let go):</strong></p> <ul> <li> <strong>Row-wise Deletion (<code class="language-plaintext highlighter-rouge">df.dropna(axis=0)</code>):</strong> If a significant portion of a row’s values are missing, or if only a tiny fraction of your <em>total</em> dataset has missing values, dropping rows might be acceptable. <ul> <li> <strong>Caution:</strong> This can lead to significant data loss, potentially biasing your analysis if the missingness isn’t random. Imagine you’re studying health data and only drop rows where blood pressure is missing – you might inadvertently remove all hypertensive patients who didn’t get their blood pressure recorded!</li> </ul> </li> <li> <strong>Column-wise Deletion (<code class="language-plaintext highlighter-rouge">df.dropna(axis=1)</code>):</strong> If a column has an overwhelmingly large percentage of missing values (e.g., &gt;70-80%), it might not be useful for analysis, and dropping the entire column could be the best option.</li> </ul> <p><strong>2. Imputation (When to smartly estimate):</strong></p> <p>This is where we fill in the blanks using estimates.</p> <ul> <li> <strong>Simple Imputation:</strong> <ul> <li> <strong>Mean:</strong> For numerical features with a relatively normal distribution. It’s fast and simple. The mean is calculated as: $ \bar{x} = \frac{1}{n}\sum_{i=1}^{n} x_i $</li> <li> <strong>Median:</strong> For numerical features that are skewed or contain outliers. The median is more robust to extreme values than the mean.</li> <li> <strong>Mode:</strong> For categorical or numerical features where the most frequent value makes sense (e.g., filling in a missing ‘color’ with the most common color).</li> <li> <strong>Constant Value:</strong> Sometimes, filling with ‘0’, ‘-1’, or ‘Unknown’ makes sense, especially if the missingness itself conveys information.</li> </ul> </li> <li> <strong>Advanced Imputation:</strong> <ul> <li> <strong>Forward-Fill/Backward-Fill:</strong> Useful for time-series data where the value at the previous or next timestamp is a reasonable estimate.</li> <li> <strong>K-Nearest Neighbors (KNN) Imputation:</strong> This is cooler! For each missing value, it finds the ‘k’ most similar data points (neighbors) based on other features and then imputes the missing value based on the average (for numerical) or mode (for categorical) of those neighbors. It’s more sophisticated but computationally intensive.</li> <li> <strong>Regression Imputation:</strong> Treat the column with missing values as your target variable and use other columns to build a regression model to predict the missing values.</li> </ul> </li> </ul> <p><strong>My takeaway:</strong> There’s no one-size-fits-all. I weigh the potential data loss against the potential for bias created by imputation. Understanding the domain and the reason for missingness is paramount.</p> <h4 id="iii-dealing-with-outliers-separating-the-signal-from-the-noise">III. Dealing with Outliers: Separating the Signal from the Noise</h4> <p>Outliers are data points that significantly deviate from other observations. They can be legitimate but extreme values, or they can be errors. Either way, they can severely skew statistical analyses and degrade model performance.</p> <p><strong>1. Detection:</strong></p> <ul> <li> <strong>Visualizations:</strong> My go-to is always a <strong>Box Plot</strong>. It visually highlights points beyond the “whiskers.” Histograms can also reveal long tails or isolated points.</li> <li> <strong>Statistical Methods:</strong> <ul> <li> <strong>Z-score:</strong> For data that is approximately normally distributed. A Z-score measures how many standard deviations a data point is from the mean. $Z = \frac{x - \mu}{\sigma}$ I typically flag values with $|Z| &gt; 3$ as potential outliers.</li> <li> <strong>Interquartile Range (IQR):</strong> This is robust to skewed data and is what box plots use. First, calculate $IQR = Q3 - Q1$ (where $Q1$ is the 25th percentile and $Q3$ is the 75th percentile). Values outside the range $[Q1 - 1.5 \times IQR, Q3 + 1.5 \times IQR]$ are considered outliers.</li> </ul> </li> </ul> <p><strong>2. Treatment:</strong></p> <ul> <li> <strong>Removal:</strong> If an outlier is clearly a data entry error or extremely rare and not representative of the population you’re studying, removing it might be appropriate. Again, caution about data loss and bias.</li> <li> <strong>Transformation:</strong> For skewed distributions, transformations like <strong>log transformation</strong> ($log(x)$) or <strong>square root transformation</strong> ($\sqrt{x}$) can compress the range of values, bringing outliers closer to the distribution.</li> <li> <strong>Capping (Winsorization):</strong> This involves replacing outlier values with a specified percentile value (e.g., replace all values above the 99th percentile with the value at the 99th percentile, and all values below the 1st percentile with the value at the 1st percentile). This keeps the data points but reduces their extreme influence.</li> <li> <strong>Treat as Missing:</strong> Sometimes, if an outlier seems completely out of place and you’re unsure if it’s an error, you can treat it as a missing value and then use imputation techniques.</li> </ul> <p><strong>My takeaway:</strong> Outliers aren’t always bad! They can sometimes represent crucial information (e.g., fraud detection, disease outbreaks). My first step is always to investigate <em>why</em> a point is an outlier before deciding how to treat it.</p> <h4 id="iv-handling-inconsistent-data-and-duplicates-the-standardization-imperative">IV. Handling Inconsistent Data and Duplicates: The Standardization Imperative</h4> <p>This category often involves cleaning categorical data or textual fields and ensuring each record is unique.</p> <p><strong>1. Inconsistent Data:</strong></p> <ul> <li> <strong>Standardizing Text:</strong> <ul> <li> <strong>Case Conversion:</strong> Convert all text to lowercase or uppercase (<code class="language-plaintext highlighter-rouge">df['column'].str.lower()</code>). This ensures “Apple”, “apple”, and “APPLE” are treated as the same category.</li> <li> <strong>Whitespace Removal:</strong> Strip leading/trailing whitespaces (<code class="language-plaintext highlighter-rouge">df['column'].str.strip()</code>).</li> <li> <strong>Typos and Variations:</strong> Use <code class="language-plaintext highlighter-rouge">value_counts()</code> to identify variations like “NY”, “New York”, “N.Y.”. I then map these to a consistent format (<code class="language-plaintext highlighter-rouge">df['column'].replace({'NY': 'New York', 'N.Y.': 'New York'})</code>). For complex cases, fuzzy matching libraries (like <code class="language-plaintext highlighter-rouge">fuzzywuzzy</code>) can help.</li> </ul> </li> <li> <strong>Data Type Conversion:</strong> Ensure columns are of the correct data type. Numbers shouldn’t be strings, dates shouldn’t be objects. <code class="language-plaintext highlighter-rouge">pd.to_numeric()</code>, <code class="language-plaintext highlighter-rouge">pd.to_datetime()</code> are my best friends here.</li> <li> <strong>Encoding Categorical Data:</strong> For machine learning models, categorical variables (like “Red”, “Green”, “Blue”) need to be converted into numerical representations, such as <strong>One-Hot Encoding</strong> or <strong>Label Encoding</strong>. This ensures consistency for the models.</li> </ul> <p><strong>2. Duplicate Records:</strong></p> <ul> <li> <strong>Identifying Duplicates:</strong> <code class="language-plaintext highlighter-rouge">df.duplicated()</code> returns a boolean Series indicating whether each row is a duplicate of a previous row.</li> <li> <strong>Removing Duplicates:</strong> <code class="language-plaintext highlighter-rouge">df.drop_duplicates()</code> is fantastic. <ul> <li>I often use the <code class="language-plaintext highlighter-rouge">subset</code> parameter to specify which columns to consider when looking for duplicates (e.g., <code class="language-plaintext highlighter-rouge">df.drop_duplicates(subset=['customer_id', 'order_date'])</code> to find duplicate orders for a customer).</li> <li>The <code class="language-plaintext highlighter-rouge">keep</code> parameter (‘first’, ‘last’, <code class="language-plaintext highlighter-rouge">False</code>) allows me to decide which duplicate to keep or remove all of them.</li> </ul> </li> </ul> <p><strong>My takeaway:</strong> Consistency is key for reliable analysis and model performance. This step often feels like meticulous data housekeeping, but it pays off immensely.</p> <h3 id="the-iterative-nature-of-cleaning-a-continuous-cycle">The Iterative Nature of Cleaning: A Continuous Cycle</h3> <p>It’s vital to remember that data cleaning is rarely a linear process. It’s iterative. I often find myself circling back:</p> <ol> <li><strong>Clean a bit.</strong></li> <li> <strong>Re-do EDA</strong> on the cleaned portion to see the impact.</li> <li> <strong>Identify new issues</strong> that were masked by previous problems.</li> <li><strong>Repeat.</strong></li> </ol> <p>Sometimes, fixing missing values reveals new outliers. Or standardizing text might expose inconsistencies in another column. Embrace this cyclical nature; it’s how you build a truly robust dataset.</p> <h3 id="conclusion-you-are-the-data-architect">Conclusion: You Are the Data Architect</h3> <p>Data cleaning might not have the same flashy appeal as neural networks or complex AI, but its importance cannot be overstated. It’s the bedrock upon which all reliable data science is built. Every clean dataset is a testament to the meticulous effort and thoughtful decisions of a data professional.</p> <p>My journey through messy data has taught me patience, critical thinking, and the immense satisfaction of transforming chaos into clarity. It empowers me to trust my models and stand by my insights. So, as you embark on your own data science adventures, remember: you’re not just a coder or an analyst; you are a data janitor, an architect, a detective – shaping raw information into a powerful tool for discovery.</p> <p>Happy cleaning! Now go forth and tame that data wild west.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>