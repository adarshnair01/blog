<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Unraveling Time's Secrets: A Deep Dive into Time Series Analysis | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/unraveling-times-secrets-a-deep-dive-into-time-ser/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Unraveling Time's Secrets: A Deep Dive into Time Series Analysis</h1> <p class="post-meta"> Created on May 30, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/time-series"> <i class="fa-solid fa-hashtag fa-sm"></i> Time Series</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/forecasting"> <i class="fa-solid fa-hashtag fa-sm"></i> Forecasting</a>   <a href="/blog/blog/tag/arima"> <i class="fa-solid fa-hashtag fa-sm"></i> ARIMA</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>My first encounter with data was like looking at a static photograph. Every data point felt isolated, a snapshot in time. But then, I stumbled upon a different kind of data – one where the order <em>mattered</em>. It was like flipping through a stop-motion animation, where each frame influences the next. That, my friends, was my introduction to <strong>Time Series Analysis</strong>, and it completely changed how I saw the world.</p> <p>Imagine trying to predict the price of a stock, the energy consumption of a city, or even the number of ice cream sales next summer. All these aren’t just random numbers; they have a history, a rhythm, a memory. This is the essence of time series data: observations collected sequentially over time. And understanding this sequence is where the magic begins.</p> <h3 id="what-is-time-series-data-anyway">What is Time Series Data, Anyway?</h3> <p>At its core, time series data is just a sequence of data points indexed (or listed) in time order. Think of it like a diary: each entry is dated, and the events of one day often influence the next.</p> <p><strong>Examples abound:</strong></p> <ul> <li> <strong>Economic data:</strong> GDP, inflation rates, stock prices.</li> <li> <strong>Environmental data:</strong> Temperature readings, rainfall, air quality.</li> <li> <strong>Business data:</strong> Sales figures, website traffic, customer service calls.</li> <li> <strong>Medical data:</strong> Heart rate monitoring, disease progression.</li> </ul> <p>The fascinating thing about time series is that unlike regular datasets where observations are often assumed to be independent, here, each observation is explicitly dependent on the previous ones. The past truly informs the future.</p> <h3 id="why-do-we-even-care-the-power-of-prediction-and-understanding">Why Do We Even Care? The Power of Prediction and Understanding</h3> <p>The primary goals of time series analysis are twofold:</p> <ol> <li> <strong>Forecasting (Prediction):</strong> “What will happen next?” This is often the most exciting part – predicting future values. Think weather forecasting or predicting next quarter’s sales.</li> <li> <strong>Understanding (Description):</strong> “What happened, and why?” This involves identifying patterns, trends, and relationships within the data to gain insights. For example, understanding why sales peak during certain months.</li> </ol> <p>My journey into time series felt like becoming a detective, looking for clues in the chronological order of events.</p> <h3 id="deconstructing-time-the-fundamental-components">Deconstructing Time: The Fundamental Components</h3> <p>Before we even think about building models, we need to understand the building blocks of a time series. Most time series can be broken down into four main components:</p> <ol> <li> <strong>Trend ($T_t$):</strong> This is the long-term increase or decrease in the data over time. Is the overall temperature of the planet going up? Are sales generally growing year over year? That’s a trend. It doesn’t have to be linear; it can be curved or segmented.</li> <li> <strong>Seasonality ($S_t$):</strong> These are patterns that repeat over fixed periods of time. Think of daily cycles (rush hour traffic), weekly cycles (weekend sales boost), or yearly cycles (Christmas shopping, summer travel). It’s a predictable, repeating up-and-down movement.</li> <li> <strong>Cyclicity ($C_t$):</strong> Often confused with seasonality, cycles are also up-and-down patterns, but they don’t have a fixed frequency. They usually span longer periods than seasonal patterns (e.g., business cycles that might last anywhere from 2 to 10 years). The key difference: <strong>seasonality has a fixed period; cyclicity does not.</strong> </li> <li> <strong>Noise/Residuals ($E_t$):</strong> This is the random, irregular variation in the data that can’t be explained by trend, seasonality, or cyclicity. It’s the unpredictable “leftover” part.</li> </ol> <p>We can combine these components in different ways, typically using an <strong>additive model</strong> or a <strong>multiplicative model</strong>:</p> <ul> <li> <strong>Additive Model:</strong> $Y_t = T_t + S_t + C_t + E_t$ <ul> <li>Useful when the magnitude of the seasonal fluctuations or residuals doesn’t change with the level of the time series.</li> </ul> </li> <li> <strong>Multiplicative Model:</strong> $Y_t = T_t \times S_t \times C_t \times E_t$ <ul> <li>Useful when the magnitude of the seasonal fluctuations or residuals increases as the series level increases (e.g., sales increase, and so do the seasonal peaks).</li> </ul> </li> </ul> <p>Visualizing these components is usually the first step in any time series analysis. Tools like Python’s <code class="language-plaintext highlighter-rouge">statsmodels</code> library can automatically decompose your series and show you these individual patterns. It’s like taking a complex song and isolating the bassline, drums, and vocals.</p> <h3 id="the-cornerstone-stationarity">The Cornerstone: Stationarity</h3> <p>This concept initially stumped me, but it’s <em>crucial</em> for many classical time series models. A time series is said to be <strong>stationary</strong> if its statistical properties (like mean, variance, and autocorrelation) do not change over time.</p> <p>Imagine a calm river: its average depth, width, and current speed remain roughly the same as you observe it over days. That’s stationary. Now imagine a river during a flood: its depth and speed fluctuate wildly. That’s non-stationary.</p> <p><strong>Why is stationarity important?</strong> Many time series models (especially ARIMA, which we’ll discuss) assume that the underlying process generating the data is stationary. If your data isn’t stationary, your model might capture spurious relationships or produce unreliable forecasts.</p> <p><strong>How do we check for it?</strong></p> <ol> <li> <strong>Visual inspection:</strong> Plotting the data can often reveal obvious trends or changing variance.</li> <li> <strong>ACF/PACF plots:</strong> These help identify if autocorrelation decays quickly (a sign of stationarity).</li> <li> <strong>Statistical tests:</strong> The <strong>Augmented Dickey-Fuller (ADF) test</strong> is a popular one. It tests the null hypothesis that a unit root is present in the time series (meaning it’s non-stationary). A low p-value (typically $&lt; 0.05$) suggests stationarity.</li> </ol> <p><strong>How do we make non-stationary data stationary?</strong> The most common technique is <strong>differencing</strong>. This involves calculating the difference between consecutive observations. For a first-order difference:</p> <p>$Y’<em>t = Y_t - Y</em>{t-1}$</p> <p>If there’s a strong trend, differencing once might remove it. If there’s strong seasonality, you might use seasonal differencing, where you subtract the observation from the same period in the previous cycle (e.g., $Y_t - Y_{t-12}$ for monthly data). You might need to difference multiple times (e.g., second-order differencing: $(Y_t - Y_{t-1}) - (Y_{t-1} - Y_{t-2})$).</p> <h3 id="exploring-relationships-autocorrelation">Exploring Relationships: Autocorrelation</h3> <p>Since time series data is sequential, observations are correlated with past observations. This is called <strong>autocorrelation</strong>.</p> <ul> <li>The <strong>Autocorrelation Function (ACF)</strong> plot shows the correlation of the time series with its own lagged values (e.g., $Y_t$ with $Y_{t-1}$, $Y_t$ with $Y_{t-2}$, etc.).</li> <li>The <strong>Partial Autocorrelation Function (PACF)</strong> plot shows the correlation of the time series with its own lagged values <em>after removing the effects of intermediate lags</em>. For example, PACF at lag 2 shows the correlation between $Y_t$ and $Y_{t-2}$ after accounting for the effect of $Y_{t-1}$.</li> </ul> <p>These plots are incredibly useful for identifying the order of AR and MA components in ARIMA models, which we’ll get to next! A stationary series’ ACF plot should generally decay rapidly to zero.</p> <h3 id="the-models-from-simple-to-sophisticated">The Models: From Simple to Sophisticated</h3> <p>Now that we understand the building blocks, let’s talk about the tools we use to model and forecast.</p> <h4 id="1-naive-and-simple-averaging-methods">1. Naive and Simple Averaging Methods</h4> <ul> <li> <strong>Naive Forecast:</strong> $\hat{Y}_{t+1} = Y_t$ (Tomorrow will be exactly like today). Surprisingly useful as a baseline!</li> <li> <strong>Simple Average:</strong> $\hat{Y}<em>{t+1} = \frac{1}{t} \sum</em>{i=1}^{t} Y_i$ (Tomorrow will be the average of all past values).</li> <li> <strong>Moving Average (MA) Forecast:</strong> $\hat{Y}<em>{t+1} = \frac{1}{k} \sum</em>{i=0}^{k-1} Y_{t-i}$ (Tomorrow will be the average of the last ‘k’ values). This gives more weight to recent observations than the simple average.</li> </ul> <h4 id="2-exponential-smoothing-methods">2. Exponential Smoothing Methods</h4> <p>These methods give exponentially decreasing weights to older observations. More recent data points are considered more important.</p> <ul> <li> <p><strong>Simple Exponential Smoothing (SES):</strong> For data with no trend or seasonality. $\hat{Y}<em>{t+1} = \alpha Y_t + (1-\alpha) \hat{Y}_t$ Here, $\hat{Y}</em>{t+1}$ is the forecast for the next period, $Y_t$ is the actual observation at time $t$, and $\hat{Y}_t$ is the forecast for time $t$. $\alpha$ (alpha) is the smoothing parameter (between 0 and 1). A higher $\alpha$ means more weight given to the most recent observation.</p> </li> <li> <strong>Holt’s Method:</strong> Extends SES to handle trends by adding a second smoothing parameter for the trend component.</li> <li> <strong>Holt-Winters Method:</strong> Further extends Holt’s to handle seasonality, adding a third smoothing parameter for the seasonal component. This method is often very effective for series with both trend and seasonality.</li> </ul> <h4 id="3-arima-the-workhorse-model">3. ARIMA: The Workhorse Model</h4> <p>The <strong>AutoRegressive Integrated Moving Average (ARIMA)</strong> model is a powerful and widely used method for time series forecasting. It’s a combination of three components, defined by three parameters: $(p, d, q)$.</p> <ul> <li> <p><strong>AR (AutoRegressive) - $p$:</strong> The “p” indicates the number of lagged (past) observations to include in the model. An AR(p) model looks like this: $Y_t = c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + … + \phi_p Y_{t-p} + \epsilon_t$ Where $Y_t$ is the current value, $Y_{t-i}$ are past values, $\phi_i$ are coefficients, $c$ is a constant, and $\epsilon_t$ is the white noise error term. It essentially says, “The current value depends on some linear combination of its past values.” You can often determine ‘p’ from the PACF plot by looking for where the significant spikes cut off.</p> </li> <li> <p><strong>I (Integrated) - $d$:</strong> The “d” represents the number of times the raw observations are differenced to make the series stationary. As we discussed, differencing helps stabilize the mean of the time series by removing trends and seasonality. A $d=1$ means first-order differencing was applied, $d=2$ means second-order, and so on.</p> </li> <li> <p><strong>MA (Moving Average) - $q$:</strong> The “q” indicates the number of lagged forecast errors that should go into the ARIMA model. An MA(q) model looks like this: $Y_t = c + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + … + \theta_q \epsilon_{t-q} + \epsilon_t$ Where $\epsilon_{t-i}$ are past error terms. It says, “The current value depends on some linear combination of its past forecast errors.” You can often determine ‘q’ from the ACF plot by looking for where the significant spikes cut off.</p> </li> </ul> <p>Putting it all together, an <strong>ARIMA(p, d, q)</strong> model integrates these components.</p> <p><strong>How do you choose p, d, and q?</strong></p> <ol> <li> <strong>Determine ‘d’:</strong> Use the ADF test and visual inspection to find the minimum number of differences needed to achieve stationarity.</li> <li> <strong>Determine ‘p’ and ‘q’:</strong> Once stationary, examine the ACF and PACF plots of the <em>differenced</em> series. <ul> <li>If the PACF cuts off sharply after lag ‘p’ (and ACF decays slowly), it suggests an AR(p) model.</li> <li>If the ACF cuts off sharply after lag ‘q’ (and PACF decays slowly), it suggests an MA(q) model.</li> <li>If both decay, you might need both AR and MA components.</li> </ul> </li> <li> <strong>Model Selection Criteria:</strong> Use metrics like AIC (Akaike Information Criterion) or BIC (Bayesian Information Criterion). Lower AIC/BIC values generally indicate a better model.</li> </ol> <p>For time series with strong seasonality, you’d use a <strong>SARIMA (Seasonal ARIMA)</strong> model, which has additional seasonal parameters (P, D, Q, S).</p> <h3 id="evaluating-your-forecasts">Evaluating Your Forecasts</h3> <p>After building a model, how do you know if it’s any good? We use various metrics:</p> <ul> <li> <strong>Mean Absolute Error (MAE):</strong> Average of the absolute differences between actual values and forecasts. Easy to interpret.</li> <li> <strong>Mean Squared Error (MSE):</strong> Average of the squared differences. Penalizes large errors more heavily.</li> <li> <strong>Root Mean Squared Error (RMSE):</strong> Square root of MSE. In the same units as the original data, making it easier to compare.</li> <li> <strong>Mean Absolute Percentage Error (MAPE):</strong> Expresses error as a percentage of the actual value, useful for comparing forecasts across different scales.</li> </ul> <p>The choice of metric depends on the problem and what types of errors are most critical.</p> <h3 id="beyond-the-classics-a-glimpse-into-advanced-horizons">Beyond the Classics: A Glimpse into Advanced Horizons</h3> <p>While ARIMA and Exponential Smoothing are powerful, the world of time series analysis doesn’t stop there.</p> <ul> <li> <strong>Prophet:</strong> Developed by Facebook, this model is designed for business forecasts and handles seasonality, trends, and holidays automatically. It’s often easier to use for non-experts.</li> <li> <strong>Vector Autoregression (VAR):</strong> For multivariate time series, where you have multiple interacting series (e.g., how interest rates and inflation affect each other).</li> <li> <strong>Deep Learning for Time Series:</strong> Recurrent Neural Networks (RNNs), especially Long Short-Term Memory (LSTM) networks, and even Transformer models are now being used to capture complex, long-range dependencies in time series data, particularly for very long sequences.</li> </ul> <h3 id="my-personal-takeaway">My Personal Takeaway</h3> <p>Time series analysis isn’t just a set of statistical tools; it’s a way of thinking about data, seeing the invisible threads that connect moments in time. It taught me patience, the importance of understanding underlying patterns before jumping to conclusions, and the immense power of historical data.</p> <p>Whether you’re predicting stock market fluctuations, understanding climate change patterns, or optimizing business operations, time series analysis offers a robust framework. So, next time you see data unfolding sequentially, remember: it’s not just a series of events, it’s a story waiting to be told, and you now have some of the keys to unlock its secrets. Happy forecasting!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>