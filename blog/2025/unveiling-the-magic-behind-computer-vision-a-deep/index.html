<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Unveiling the Magic Behind Computer Vision: A Deep Dive into Convolutional Neural Networks | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/unveiling-the-magic-behind-computer-vision-a-deep/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Unveiling the Magic Behind Computer Vision: A Deep Dive into Convolutional Neural Networks</h1> <p class="post-meta"> Created on March 03, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/computer-vision"> <i class="fa-solid fa-hashtag fa-sm"></i> Computer Vision</a>   <a href="/blog/blog/tag/cnns"> <i class="fa-solid fa-hashtag fa-sm"></i> CNNs</a>   <a href="/blog/blog/tag/neural-networks"> <i class="fa-solid fa-hashtag fa-sm"></i> Neural Networks</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>It’s amazing, isn’t it? Just a few years ago, the idea of a computer accurately identifying a cat in a photo, transcribing handwriting, or even powering a self-driving car seemed like pure science fiction. Yet, today, these are realities we interact with daily. As someone who’s always been fascinated by how we can teach machines to perform human-like tasks, diving into the world of Artificial Intelligence felt like unlocking a secret superpower. And among the many tools in the AI arsenal, one particular type of neural network has always struck me as exceptionally elegant and powerful: <strong>Convolutional Neural Networks (CNNs)</strong>.</p> <p>Think about it: Your brain processes visual information almost instantly and effortlessly. You see a fluffy, four-legged creature and <em>know</em> it’s a cat, regardless of its angle, lighting, or if it’s partially hidden. How can we possibly empower a machine to do the same? This was the grand challenge, and CNNs emerged as a brilliant solution.</p> <h3 id="the-problem-with-regular-neural-networks-for-images">The Problem with “Regular” Neural Networks for Images</h3> <p>Before we dive into what makes CNNs special, let’s briefly consider why traditional, fully-connected (FC) neural networks struggle with image data.</p> <p>Imagine a simple image, say a grayscale 28x28 pixel picture of a handwritten digit. That’s 784 pixels. If we feed this into a regular neural network, each pixel would be an individual input neuron. If our first hidden layer had, say, 128 neurons, that’s $784 \times 128 = 100,352$ weights just between the input and the first hidden layer! Now scale that up to a color image (three channels: Red, Green, Blue) of a more realistic size, like 224x224 pixels. That’s $224 \times 224 \times 3 = 150,528$ pixels. The number of weights explodes exponentially, leading to:</p> <ol> <li> <strong>Too many parameters:</strong> A massive number of weights makes the network incredibly slow to train, prone to overfitting (memorizing the training data instead of learning general patterns), and computationally expensive.</li> <li> <strong>Loss of spatial information:</strong> A regular FC network treats each pixel as an independent feature. It completely ignores the crucial spatial relationships between pixels. The fact that a pixel’s neighbors are also pixels is incredibly important for forming shapes, edges, and textures. A regular network just sees a long list of numbers.</li> </ol> <p>This is where CNNs come in, like a specialized magnifying glass designed specifically for images.</p> <h3 id="enter-the-convolutional-layer-the-feature-detectives">Enter the Convolutional Layer: The Feature Detectives</h3> <p>The magic of CNNs begins with the <strong>convolutional layer</strong>. Instead of treating every pixel individually, this layer employs a clever trick: it scans the image with a small “filter” or “kernel.”</p> <p>Imagine you’re looking for a specific pattern, like an edge, in a large puzzle. You wouldn’t look at the entire puzzle at once. Instead, you’d take a small magnifying glass and systematically scan it across the puzzle, looking for your pattern. That’s essentially what a convolutional layer does.</p> <h4 id="the-kernel-filter">The Kernel (Filter)</h4> <p>At the heart of a convolutional layer is a <strong>kernel</strong>, which is just a small matrix of numbers (e.g., 3x3 or 5x5). This kernel acts as a feature detector. When we train the CNN, these numbers in the kernel <em>learn</em> to identify specific visual features like:</p> <ul> <li>Horizontal edges</li> <li>Vertical edges</li> <li>Diagonal lines</li> <li>Corners</li> <li>Blobs of color</li> <li>And eventually, more complex patterns!</li> </ul> <h4 id="the-convolution-operation">The Convolution Operation</h4> <p>The kernel “convolves” (slides) over the input image. At each position, it performs an element-wise multiplication with the corresponding patch of pixels in the image and then sums up the results into a single number. This single number becomes a pixel in the output, which we call a <strong>feature map</strong> or <strong>activation map</strong>.</p> <p>Let’s illustrate with a simplified conceptual math. For a given output pixel at position $(i, j)$ in the feature map, the operation would look something like this:</p> <p>$ (\text{Feature Map})<em>{ij} = \sum</em>{m=0}^{K<em>h-1} \sum</em>{n=0}^{K<em>w-1} (\text{Input Image})</em>{i+m, j+n} \cdot (\text{Kernel})_{m,n} $</p> <p>Where $K_h$ and $K_w$ are the height and width of the kernel.</p> <p>By sliding this kernel across the entire image, we generate a new image (the feature map) where bright pixels indicate strong detection of the feature the kernel is looking for, and dark pixels indicate weak or no detection.</p> <p><strong>Key advantages of convolution:</strong></p> <ul> <li> <strong>Parameter Sharing:</strong> The same kernel (set of weights) is applied across the entire image. This drastically reduces the number of parameters compared to FC networks. Think of it: if you’re looking for an edge, that edge can appear anywhere in the image, and you don’t need a different detector for each possible location.</li> <li> <strong>Translation Invariance:</strong> Because the kernel slides across the entire image, if a feature (like a cat’s eye) shifts slightly in the input image, the CNN can still detect it. This is a crucial property for robust computer vision.</li> <li> <strong>Local Receptive Fields:</strong> Each neuron in a convolutional layer is only connected to a small, local region of the input. This reflects how biological vision works, where neurons respond to stimuli in a limited region of the visual field.</li> </ul> <h4 id="hyperparameters-of-convolutional-layers">Hyperparameters of Convolutional Layers</h4> <ul> <li> <strong>Stride:</strong> How many pixels the kernel shifts at each step. A stride of 1 means it moves one pixel at a time. A stride of 2 means it skips a pixel, which effectively downsamples the feature map.</li> <li> <strong>Padding:</strong> When the kernel moves to the edges of an image, it might not perfectly align with the remaining pixels. Padding involves adding extra “dummy” pixels (usually zeros) around the border of the input image to ensure the kernel can cover all parts of the image and maintain the desired output size. Common types are ‘valid’ (no padding, output shrinks) and ‘same’ (output size is the same as input).</li> <li> <strong>Number of Filters:</strong> A convolutional layer typically uses multiple kernels, each learning to detect a different feature. If we use 32 filters, we’ll get 32 different feature maps as output.</li> </ul> <h3 id="the-role-of-non-linearity-the-activation-layer">The Role of Non-linearity: The Activation Layer</h3> <p>After the convolution operation, the output (feature map) is typically passed through an <strong>activation function</strong>. The most popular choice for CNNs is the <strong>Rectified Linear Unit (ReLU)</strong>:</p> <p>$ f(x) = \max(0, x) $</p> <p>ReLU simply outputs the input if it’s positive, and zero otherwise. Why is this important? Without non-linear activation functions, stacking multiple convolutional layers would just result in a fancy linear transformation. Non-linearity introduces the ability for the network to learn complex, non-linear relationships in the data, which are essential for recognizing intricate patterns in images.</p> <h3 id="summarizing-information-the-pooling-layer">Summarizing Information: The Pooling Layer</h3> <p>After a convolutional layer and an activation function, it’s common to add a <strong>pooling layer</strong>. The primary purpose of pooling layers is to reduce the spatial dimensions (width and height) of the feature maps, which in turn:</p> <ol> <li> <strong>Reduces computational cost:</strong> Less data to process in subsequent layers.</li> <li> <strong>Reduces overfitting:</strong> By summarizing information, it forces the network to focus on the presence of features rather than their exact location.</li> <li> <strong>Increases translation invariance:</strong> It makes the network even more robust to small shifts or distortions in the input image.</li> </ol> <p>The most common type is <strong>Max Pooling</strong>. Imagine a 2x2 window sliding over the feature map. For each window, Max Pooling simply takes the maximum value within that window and uses it as the single output for that region.</p> <p>For example, if you have a 4x4 feature map and apply a 2x2 max pooling with a stride of 2:</p> <p>Original 4x4 Feature Map:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[1, 1, 2, 4],
 [5, 6, 7, 8],
 [3, 2, 1, 0],
 [1, 2, 3, 4]]
</code></pre></div></div> <p>After 2x2 Max Pooling with stride 2:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[6, 8],
 [3, 4]]
</code></pre></div></div> <p>(because max(1,1,5,6)=6, max(2,4,7,8)=8, etc.)</p> <p>Other pooling types exist, like Average Pooling, but Max Pooling generally performs better in practice for capturing dominant features.</p> <h3 id="building-blocks-the-cnn-architecture">Building Blocks: The CNN Architecture</h3> <p>So, how do all these pieces fit together? A typical CNN architecture involves stacking multiple convolutional layers, activation functions (like ReLU), and pooling layers.</p> <p>A common pattern is: <strong>[CONV -&gt; ReLU -&gt; POOL] -&gt; [CONV -&gt; ReLU -&gt; POOL] -&gt; … -&gt; [FULLY CONNECTED LAYERS] -&gt; [SOFTMAX/OUTPUT]</strong></p> <ol> <li> <strong>Initial Layers:</strong> The first few convolutional layers learn to detect very basic, low-level features like edges, corners, and simple textures.</li> <li> <strong>Deeper Layers:</strong> As you go deeper into the network, the convolutional layers learn to combine these basic features into more complex and abstract representations. For instance, an intermediate layer might detect parts of objects (e.g., a wheel, an eye, a nose), and very deep layers might detect entire objects (a car, a face, a dog). This hierarchical learning is incredibly powerful!</li> <li> <strong>Flattening:</strong> After several rounds of convolution and pooling, the final feature maps are “flattened” into a single, long vector of numbers. This vector represents a high-level, abstract summary of the entire image’s content.</li> <li> <strong>Fully Connected Layers:</strong> This flattened vector is then fed into one or more traditional fully connected (FC) neural network layers. These layers act as classifiers, taking the learned features and using them to make predictions (e.g., “This image is 90% a cat, 8% a dog, 2% a bird”).</li> <li> <strong>Softmax/Output Layer:</strong> The final FC layer often uses a Softmax activation function for multi-class classification, outputting probabilities for each possible class.</li> </ol> <h3 id="training-a-cnn-the-learning-process">Training a CNN: The Learning Process</h3> <p>Just like other neural networks, CNNs learn through a process called <strong>backpropagation</strong> and <strong>gradient descent</strong>. During training, the network is fed millions of labeled images. It makes predictions, compares them to the actual labels (the “ground truth”), calculates the error (loss), and then adjusts all its internal weights (including those in the kernels!) to minimize that error. This iterative process allows the kernels to <em>learn</em> what patterns are important for classification.</p> <h3 id="why-cnns-are-so-powerful">Why CNNs Are So Powerful</h3> <p>To summarize, CNNs excel in computer vision tasks due to several key innovations:</p> <ul> <li> <strong>Parameter Sharing:</strong> Drastically reduces the number of trainable parameters, making models lighter and faster.</li> <li> <strong>Local Receptive Fields:</strong> Focus on local patterns, mimicking biological vision.</li> <li> <strong>Hierarchical Feature Learning:</strong> Builds complex features from simple ones, allowing the network to understand objects at various levels of abstraction.</li> <li> <strong>Translation Invariance:</strong> Detects features regardless of their position in the image.</li> </ul> <p>These properties make CNNs incredibly effective for tasks that require understanding visual patterns.</p> <h3 id="real-world-applications">Real-World Applications</h3> <p>CNNs aren’t just theoretical concepts; they are the backbone of countless modern applications:</p> <ul> <li> <strong>Image Classification:</strong> Identifying objects, animals, or scenes in images (e.g., Google Photos, Instagram filters).</li> <li> <strong>Object Detection:</strong> Locating and identifying multiple objects within an image (e.g., self-driving cars recognizing pedestrians and other vehicles, security cameras).</li> <li> <strong>Facial Recognition:</strong> Unlocking your phone, tagging friends on social media.</li> <li> <strong>Medical Imaging:</strong> Detecting tumors or diseases in X-rays, MRIs, and CT scans.</li> <li> <strong>Image Generation:</strong> Creating realistic fake images (deepfakes) or transferring artistic styles.</li> <li> <strong>Image Segmentation:</strong> Identifying which pixels belong to which object in an image.</li> </ul> <h3 id="wrapping-up">Wrapping Up</h3> <p>Diving into Convolutional Neural Networks really cemented my appreciation for the ingenuity in Deep Learning. It’s a field that, while complex, offers elegant solutions to problems that once seemed insurmountable. From understanding the limitations of traditional networks to appreciating the elegant dance of kernels, activation functions, and pooling layers, CNNs truly revolutionize how computers “see” the world.</p> <p>I hope this journey into CNNs has sparked your curiosity as much as it has mine! The world of AI is constantly evolving, and CNNs are just one powerful example of the incredible progress being made. What will you build or discover next with this knowledge? The possibilities are truly limitless!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>