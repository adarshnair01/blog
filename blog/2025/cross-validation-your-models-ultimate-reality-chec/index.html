<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Cross-Validation: Your Model's Ultimate Reality Check | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/cross-validation-your-models-ultimate-reality-chec/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Cross-Validation: Your Model's Ultimate Reality Check</h1> <p class="post-meta"> Created on September 09, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/cross-validation"> <i class="fa-solid fa-hashtag fa-sm"></i> Cross-Validation</a>   <a href="/blog/blog/tag/model-evaluation"> <i class="fa-solid fa-hashtag fa-sm"></i> Model Evaluation</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/overfitting"> <i class="fa-solid fa-hashtag fa-sm"></i> Overfitting</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey there, fellow data adventurer!</p> <p>I remember a time when I first started tinkering with machine learning models. I’d split my data into a training set and a test set, train a cool algorithm, and boom! My accuracy would be through the roof, like 95% or even 98%! I’d pat myself on the back, convinced I was a genius. Then, I’d deploy the model or try it on some truly new data, and the performance would often drop significantly. My “genius” model suddenly looked… well, not so genius.</p> <p>Sound familiar? This frustrating experience is common for anyone starting out in data science. It’s the classic tale of a model that <em>overfits</em> to its training data, meaning it learned the specific nuances and noise of its training examples too well, and thus struggles to generalize to new, unseen information. It’s like a student who memorizes answers to a single practice test but fails the real exam because the questions are slightly different.</p> <p>So, how do we build models that are not just good on <em>one</em> test, but reliably good on <em>any</em> test? How do we get a truly honest assessment of our model’s capabilities before unleashing it into the wild?</p> <p>Enter <strong>Cross-Validation</strong>, our trusty companion in the quest for robust and reliable machine learning models.</p> <h2 id="the-problem-with-a-simple-train-test-split-its-not-enough">The Problem with a Simple Train-Test Split (It’s Not Enough!)</h2> <p>Before we dive into cross-validation, let’s briefly revisit the standard practice: the train-test split.</p> <p>You take your entire dataset and usually split it into two parts:</p> <ol> <li> <strong>Training Set:</strong> Used to train your machine learning model. The model learns patterns and relationships from this data.</li> <li> <strong>Test Set:</strong> Used to evaluate how well your trained model performs on data it has <em>never seen before</em>. This gives you an estimate of its generalization ability.</li> </ol> <p>This is a good first step, and much better than evaluating on the training data itself (which would almost always yield artificially high scores). You might use a function like <code class="language-plaintext highlighter-rouge">train_test_split</code> from <code class="language-plaintext highlighter-rouge">scikit-learn</code> to do this, often with a 70/30 or 80/20 split.</p> <p>The issue, however, is that this split is <em>random</em>. What if, by sheer luck, your random split creates a test set that’s particularly easy for your model? Or, conversely, one that’s unusually difficult? Or perhaps one that doesn’t fully represent the diversity of your overall dataset?</p> <p>In any of these scenarios, the single performance score you get from your test set might be misleading. It’s just one data point, one perspective. It’s like asking only one person to review your new movie – their opinion might not reflect what the general audience thinks.</p> <h2 id="cross-validation-the-robust-tester">Cross-Validation: The Robust Tester</h2> <p>Cross-validation is a more sophisticated and reliable technique for evaluating model performance. Instead of a single split, we perform <em>multiple</em> splits and multiple training/evaluation cycles. The core idea is simple: <strong>don’t just evaluate your model once; evaluate it many times, on different subsets of your data, and then average the results.</strong></p> <p>This multi-faceted evaluation gives us:</p> <ol> <li>A more stable and reliable estimate of the model’s true generalization performance.</li> <li>A measure of the variability of the model’s performance (how much its performance changes depending on the data it sees).</li> </ol> <p>Let’s look at the most common type: <strong>K-Fold Cross-Validation</strong>.</p> <h3 id="how-k-fold-cross-validation-works-the-most-popular-kind">How K-Fold Cross-Validation Works (The Most Popular Kind)</h3> <p>Imagine you have a deck of cards (your dataset). K-Fold Cross-Validation works like this:</p> <ol> <li> <strong>Divide into K Folds:</strong> First, you shuffle your entire dataset randomly. Then, you divide it into $K$ equally sized “folds” or subsets. Common choices for $K$ are 5 or 10. Let’s say we choose $K=5$. <ul> <li>Your data is now split into 5 pieces: Fold 1, Fold 2, Fold 3, Fold 4, Fold 5.</li> </ul> </li> <li> <strong>The Iteration Game (K Times):</strong> Now, we’ll run K rounds of training and testing: <ul> <li> <strong>Round 1:</strong> <ul> <li>We designate <strong>Fold 1 as our test set</strong>.</li> <li>The remaining folds (Fold 2, Fold 3, Fold 4, Fold 5) are combined to form our <strong>training set</strong>.</li> <li>We train our machine learning model on this training set and then evaluate its performance (e.g., accuracy, precision, F1-score) on Fold 1. Let’s call this score $S_1$.</li> </ul> </li> <li> <strong>Round 2:</strong> <ul> <li>We designate <strong>Fold 2 as our test set</strong>.</li> <li>The remaining folds (Fold 1, Fold 3, Fold 4, Fold 5) become our <strong>training set</strong>.</li> <li>Train the model again, evaluate on Fold 2. Get score $S_2$.</li> </ul> </li> <li> <p>…and so on, until…</p> </li> <li> <strong>Round K (Round 5 in our example):</strong> <ul> <li>We designate <strong>Fold K (Fold 5) as our test set</strong>.</li> <li>The remaining folds (Fold 1, Fold 2, Fold 3, Fold 4) become our <strong>training set</strong>.</li> <li>Train the model again, evaluate on Fold 5. Get score $S_K$.</li> </ul> </li> </ul> </li> <li> <p><strong>Average the Scores:</strong> After completing all $K$ rounds, you will have $K$ different performance scores ($S_1, S_2, …, S_K$). To get a single, robust estimate of your model’s performance, you simply calculate the average of these scores:</p> <p>$ \bar{S} = \frac{1}{K} \sum_{i=1}^{K} S_i $</p> <p>You might also calculate the standard deviation of these scores to understand how much the performance varies across different splits:</p> <p>$ \text{Standard Deviation} = \sqrt{\frac{1}{K-1} \sum_{i=1}^{K} (S_i - \bar{S})^2} $</p> <p>A small standard deviation indicates that your model’s performance is quite consistent, regardless of the specific data it’s tested on. A large standard deviation might suggest instability or that your dataset has high variance.</p> </li> </ol> <p><strong>Visualizing it:</strong> Imagine your dataset as a long strip.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>|------|------|------|------|------|  &lt;-- 5 Folds
Fold 1 Fold 2 Fold 3 Fold 4 Fold 5

Iteration 1:
[TEST ] [TRAIN] [TRAIN] [TRAIN] [TRAIN] -&gt; Score S1

Iteration 2:
[TRAIN] [TEST ] [TRAIN] [TRAIN] [TRAIN] -&gt; Score S2

... and so on ...

Iteration 5:
[TRAIN] [TRAIN] [TRAIN] [TRAIN] [TEST ] -&gt; Score S5
</code></pre></div></div> <h3 id="the-benefits-why-k-fold-cv-is-a-game-changer">The Benefits: Why K-Fold CV is a Game-Changer</h3> <ol> <li> <strong>More Reliable Performance Estimate:</strong> By averaging scores over multiple test sets, we get a much more stable and less biased estimate of our model’s true performance on unseen data. It reduces the impact of a “lucky” or “unlucky” single split.</li> <li> <strong>Better Generalization Assessment:</strong> It helps to detect overfitting. If your model performs exceptionally well on the training folds but poorly on the test folds during each iteration, that’s a red flag for overfitting.</li> <li> <strong>Full Data Utilization:</strong> Every data point in your dataset gets to be in a test set exactly once, and in a training set $K-1$ times. This is great for smaller datasets where you want to maximize the information used for both training and testing.</li> <li> <strong>Hyperparameter Tuning:</strong> Cross-validation is absolutely crucial for hyperparameter tuning (the process of finding the best configuration for your model, e.g., the learning rate for a neural network or the number of trees in a Random Forest). Tools like <code class="language-plaintext highlighter-rouge">GridSearchCV</code> or <code class="language-plaintext highlighter-rouge">RandomizedSearchCV</code> in <code class="language-plaintext highlighter-rouge">scikit-learn</code> use cross-validation internally to compare different hyperparameter combinations and select the best one based on robust performance estimates.</li> </ol> <h3 id="variations-of-cross-validation-briefly">Variations of Cross-Validation (Briefly)</h3> <p>While K-Fold CV is the workhorse, there are other types for specific situations:</p> <ul> <li> <strong>Stratified K-Fold Cross-Validation:</strong> Essential for classification problems, especially with imbalanced datasets. It ensures that each fold maintains the same proportion of target class labels as the overall dataset. So, if 10% of your data belongs to class A, then each fold will also have roughly 10% from class A.</li> <li> <strong>Leave-One-Out Cross-Validation (LOOCV):</strong> This is an extreme form where $K$ is equal to the number of data points $N$. Each data point becomes its own test set in turn, with the remaining $N-1$ points forming the training set. It’s computationally very expensive for large datasets but provides a nearly unbiased estimate of performance.</li> <li> <strong>Time Series Cross-Validation (Walk-Forward Validation):</strong> For time-dependent data (like stock prices or weather forecasts), you <em>cannot</em> randomly shuffle and split the data, as it would break the temporal order. Time series CV involves training on a historical period and testing on a subsequent period, then “walking forward” in time. For example, train on Jan-March, test on April; then train on Jan-April, test on May, and so on.</li> </ul> <h3 id="practical-considerations">Practical Considerations</h3> <ul> <li> <strong>Computational Cost:</strong> Yes, running your training and testing $K$ times takes longer than just once. But consider it an investment in building a reliable model. For very large datasets, you might opt for a smaller $K$ (e.g., 3) or stick to a single train-test split for initial experimentation, then use CV for final evaluation.</li> <li> <strong>Choice of K:</strong> There’s no magic number for $K$. 5 or 10 are common heuristics. A higher $K$ means smaller test sets in each fold, potentially leading to more bias but lower variance in the overall performance estimate. A lower $K$ means larger test sets, potentially lower bias but higher variance.</li> <li> <strong>Always use it!</strong> Whenever you’re evaluating a model, comparing different models, or tuning hyperparameters, make cross-validation your default method. It’s a hallmark of rigorous machine learning practice.</li> </ul> <h2 id="my-final-thoughts-trust-but-verify">My Final Thoughts: Trust, But Verify!</h2> <p>Learning about cross-validation was a pivotal moment in my data science journey. It transformed my approach from hoping my models were good to knowing they were reliably good. It taught me the importance of skepticism and thoroughness.</p> <p>Cross-validation isn’t just a technique; it’s a mindset. It embodies the scientific principle of reproducible results and robust findings. By consistently applying cross-validation, you move beyond mere performance numbers and gain a deeper understanding of your model’s true capabilities and limitations.</p> <p>So, next time you’re building a machine learning model, remember to give it the ultimate reality check. Embrace cross-validation, and build models that don’t just look good on paper, but truly shine in the real world! Happy modeling!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>