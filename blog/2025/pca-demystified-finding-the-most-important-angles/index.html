<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> PCA Demystified: Finding the Most Important Angles in Your Data | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/pca-demystified-finding-the-most-important-angles/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">PCA Demystified: Finding the Most Important Angles in Your Data</h1> <p class="post-meta"> Created on November 26, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/dimensionality-reduction"> <i class="fa-solid fa-hashtag fa-sm"></i> Dimensionality Reduction</a>   <a href="/blog/blog/tag/pca"> <i class="fa-solid fa-hashtag fa-sm"></i> PCA</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/statistics"> <i class="fa-solid fa-hashtag fa-sm"></i> Statistics</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Ever stared at a spreadsheet with hundreds, maybe thousands, of columns, each representing a “feature” of your data? Perhaps it was customer demographics, sensor readings, or gene expressions. As data scientists and machine learning engineers, we often face this “curse of dimensionality” – too many features can make models slow, prone to overfitting, and incredibly hard to interpret.</p> <p>What if there was a way to simplify this chaotic information jungle without losing its essence? What if we could distill the most important patterns, the core story, into a much smaller set of variables?</p> <p>Enter <strong>Principal Component Analysis (PCA)</strong>, a fundamental technique in data science that feels like magic but is rooted in elegant linear algebra. It’s not about throwing away features randomly; it’s about intelligently compressing your data, identifying new dimensions that capture the maximum amount of variation.</p> <h3 id="the-less-is-more-philosophy-smart-data-compression">The “Less is More” Philosophy: Smart Data Compression</h3> <p>Imagine you’re trying to describe a cloud of points floating in 3D space. You could list the (x, y, z) coordinates for every single point. But what if the cloud, despite being in 3D, mostly lies along a somewhat flat, elongated sheet? You could describe its overall shape and orientation much more simply by just specifying that sheet and how the points are spread along it.</p> <p>PCA does exactly this. It looks for directions (or “components”) in your data along which the data varies the most. These new directions are called <strong>Principal Components</strong>.</p> <p>Why do we care about variance? Because variance signifies information. If a feature has very little variance, it means all data points are very similar along that dimension, providing little distinguishing information. Conversely, high variance means the data points are spread out, showing distinct differences that are often crucial for understanding the underlying patterns.</p> <h3 id="the-intuition-finding-the-best-angles">The Intuition: Finding the Best Angles</h3> <p>Let’s ground this with an analogy. Imagine you’re an artist trying to sketch a person. You don’t try to capture every single hair, every pore. Instead, you focus on the major lines, the overall posture, the most defining features that immediately convey the person’s identity. These are your “principal components” of the sketch.</p> <p>PCA works similarly:</p> <ol> <li> <strong>Finding the First Principal Component (PC1):</strong> PCA searches for a direction (a line) through your data that best explains the spread, or variance, of the data. If you project all your data points onto this line, they would be as spread out as possible. This direction captures the most “information.”</li> <li> <strong>Finding the Second Principal Component (PC2):</strong> After finding PC1, PCA then looks for another direction that also explains a lot of the remaining variance, but with a crucial constraint: it must be <strong>orthogonal</strong> (perpendicular) to PC1. Why orthogonal? To ensure that PC2 captures <em>new</em>, non-redundant information that wasn’t already captured by PC1.</li> <li> <strong>And so on…:</strong> This process continues, finding subsequent principal components that are orthogonal to all previous ones and capture the maximum remaining variance.</li> </ol> <p>Each Principal Component is a linear combination of your original features. It’s like saying, “This new dimension is 30% Feature A, 50% Feature B, and 20% Feature C.”</p> <h3 id="the-math-behind-the-magic-a-step-by-step-glimpse">The Math Behind the Magic: A Step-by-Step Glimpse</h3> <p>While the full mathematical derivation involves some heavy lifting, understanding the steps conceptually will give you a solid grasp:</p> <h4 id="step-1-standardize-your-data">Step 1: Standardize Your Data</h4> <p>Before anything else, we need to ensure all features are on a level playing field. If one feature ranges from 0-1 (e.g., probability) and another from 0-1,000,000 (e.g., income), the latter will dominate the variance calculation just because of its scale.</p> <p>So, we <strong>standardize</strong> each feature: subtract its mean and divide by its standard deviation. This transforms the data so each feature has a mean of 0 and a standard deviation of 1.</p> \[z = \frac{x - \mu}{\sigma}\] <p>Where $x$ is the original value, $\mu$ is the mean, and $\sigma$ is the standard deviation.</p> <h4 id="step-2-calculate-the-covariance-matrix">Step 2: Calculate the Covariance Matrix</h4> <p>The covariance matrix tells us how much each pair of features varies together.</p> <ul> <li> <strong>Positive covariance</strong> means if one feature increases, the other tends to increase.</li> <li> <strong>Negative covariance</strong> means if one feature increases, the other tends to decrease.</li> <li> <strong>Zero covariance</strong> means there’s no linear relationship between them.</li> </ul> <p>The diagonal elements of the covariance matrix are the variances of each individual feature, while the off-diagonal elements are the covariances between pairs of features. This matrix is symmetric. Understanding these relationships is crucial because PCA aims to find new directions that <em>de-correlate</em> these features.</p> <p>For two variables, $X$ and $Y$, the covariance is: \(Cov(X, Y) = E[(X - E[X])(Y - E[Y])]\)</p> <h4 id="step-3-find-the-eigenvectors-and-eigenvalues">Step 3: Find the Eigenvectors and Eigenvalues</h4> <p>This is the mathematical core of PCA.</p> <ul> <li> <strong>Eigenvectors</strong>: Imagine a special kind of vector that, when transformed by a matrix (in our case, the covariance matrix), only gets stretched or shrunk, but doesn’t change its direction. These special vectors are called <strong>eigenvectors</strong>. In PCA, the eigenvectors of the covariance matrix are our <strong>Principal Components</strong>. They point in the directions of maximum variance in the data.</li> <li> <strong>Eigenvalues</strong>: Each eigenvector has a corresponding <strong>eigenvalue</strong>, which tells us the magnitude of the “stretch” or “shrink.” In PCA, an eigenvalue quantifies the amount of variance captured along its corresponding eigenvector (Principal Component). A larger eigenvalue means that eigenvector captures more variance, hence more “information.”</li> </ul> <p>The fundamental equation describing this relationship is: \(Av = \lambda v\) Here, $A$ is our covariance matrix, $v$ is an eigenvector, and $\lambda$ is its corresponding eigenvalue.</p> <h4 id="step-4-sort-and-select-principal-components">Step 4: Sort and Select Principal Components</h4> <p>We now have a set of eigenvectors (our potential principal components) and their corresponding eigenvalues (the variance each component explains). We sort them in descending order based on their eigenvalues. The eigenvector with the largest eigenvalue is PC1, the one with the second largest is PC2, and so on.</p> <p>You then decide how many principal components to keep. A common approach is to look at the “explained variance ratio” of each component. This tells you the proportion of total variance explained by each principal component. You might choose to keep enough components to explain, say, 95% of the total variance, or you might look for an “elbow” in a scree plot (a plot of eigenvalues) where the explained variance drops off sharply.</p> <h4 id="step-5-project-data-onto-new-dimensions">Step 5: Project Data onto New Dimensions</h4> <p>Finally, you transform your original standardized data into the new, lower-dimensional space defined by your chosen principal components. This involves multiplying your original data by the matrix formed by the selected eigenvectors. Each original data point will now have new coordinates along these principal component axes.</p> <h3 id="the-superpowers-of-pca-why-bother">The Superpowers of PCA: Why Bother?</h3> <ol> <li> <strong>Dimensionality Reduction:</strong> This is the most obvious benefit. By reducing the number of features, you make your datasets smaller, which leads to: <ul> <li>Faster training times for machine learning models.</li> <li>Reduced storage requirements.</li> </ul> </li> <li> <strong>Noise Reduction:</strong> Features with low variance often represent noise. By focusing on components with high variance, PCA inherently reduces the impact of this noise, potentially improving model performance.</li> <li> <strong>Visualization:</strong> It’s impossible to visualize data with hundreds of dimensions. PCA allows you to reduce complex datasets to 2 or 3 principal components, making them plottable and much easier to gain insights from.</li> <li> <strong>Improved Model Performance (Sometimes):</strong> While not always guaranteed, reducing dimensionality can help combat overfitting, especially when you have many highly correlated features. Simpler models often generalize better.</li> </ol> <h3 id="the-catch-limitations-and-considerations">The Catch: Limitations and Considerations</h3> <p>PCA is powerful, but not a silver bullet:</p> <ul> <li> <strong>Linearity Assumption:</strong> PCA only finds <em>linear</em> relationships between features. If the true underlying relationships are non-linear (e.g., curved patterns), PCA might not be the most effective method. Kernel PCA is an extension that can handle non-linearity.</li> <li> <strong>Interpretability:</strong> The new principal components are linear combinations of original features. This means PC1 might be “0.4 _ income + 0.3 _ age - 0.2 * education.” While mathematically sound, interpreting what this new combined feature *means* in real-world terms can sometimes be challenging.</li> <li> <strong>Scaling Sensitivity:</strong> As we saw in Step 1, PCA is highly sensitive to the scaling of your features. Always standardize your data before applying PCA.</li> <li> <strong>Information Loss:</strong> By reducing dimensionality, you <em>do</em> lose some information. The goal is to lose the least important information while retaining the most.</li> </ul> <h3 id="when-to-use-pca">When to Use PCA</h3> <p>PCA is a fantastic tool for:</p> <ul> <li> <strong>Exploratory Data Analysis (EDA):</strong> Visualizing high-dimensional data.</li> <li> <strong>Preprocessing for Machine Learning Models:</strong> Reducing the number of input features for models like SVMs, neural networks, or logistic regression.</li> <li> <strong>Image Compression:</strong> Reducing the number of pixels while retaining visual quality.</li> <li> <strong>Facial Recognition:</strong> Extracting key “eigenfaces” from image data.</li> <li> <strong>Genomics and Bioinformatics:</strong> Analyzing gene expression data.</li> </ul> <h3 id="a-peek-at-the-code-python-with-scikit-learn">A Peek at the Code (Python with scikit-learn)</h3> <p>In the real world, you don’t calculate eigenvectors and eigenvalues by hand. Libraries do the heavy lifting:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="c1"># Assume you have a DataFrame called 'df_features'
# and you want to reduce its dimensionality
</span>
<span class="c1"># 1. Standardize the data
</span><span class="n">scaler</span> <span class="o">=</span> <span class="nc">StandardScaler</span><span class="p">()</span>
<span class="n">scaled_data</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">df_features</span><span class="p">)</span>

<span class="c1"># 2. Apply PCA
# Let's say we want to reduce to 2 principal components
</span><span class="n">pca</span> <span class="o">=</span> <span class="nc">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">principal_components</span> <span class="o">=</span> <span class="n">pca</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">scaled_data</span><span class="p">)</span>

<span class="c1"># Create a new DataFrame for the principal components
</span><span class="n">pca_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">principal_components</span><span class="p">,</span>
                      <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">Principal Component 1</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Principal Component 2</span><span class="sh">'</span><span class="p">])</span>

<span class="c1"># You can also check the explained variance ratio
</span><span class="nf">print</span><span class="p">(</span><span class="n">pca</span><span class="p">.</span><span class="n">explained_variance_ratio_</span><span class="p">)</span>
<span class="c1"># Output might be something like [0.65, 0.20] meaning PC1 explains 65% of variance, PC2 explains 20%
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Total explained variance: </span><span class="si">{</span><span class="nf">sum</span><span class="p">(</span><span class="n">pca</span><span class="p">.</span><span class="n">explained_variance_ratio_</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">%</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Now 'pca_df' has your data in a reduced, 2-dimensional form!
</span></code></pre></div></div> <h3 id="conclusion-your-datas-best-storyteller">Conclusion: Your Data’s Best Storyteller</h3> <p>PCA is more than just a trick to make your data smaller; it’s a powerful lens to peer into the inherent structure of your datasets. It helps us cut through the noise, focus on the most meaningful patterns, and ultimately tell a clearer, more concise story with our data.</p> <p>Understanding PCA’s core principles – maximizing variance, maintaining orthogonality, and leveraging the magic of eigenvectors and eigenvalues – equips you with a fundamental tool in your data science toolkit. So, the next time you face a high-dimensional dataset, remember PCA: your guide to finding the most important angles and revealing the true essence within. Go forth and simplify!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>