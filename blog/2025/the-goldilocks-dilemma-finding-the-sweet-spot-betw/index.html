<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Goldilocks Dilemma: Finding the Sweet Spot Between Overfitting and Underfitting | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/the-goldilocks-dilemma-finding-the-sweet-spot-betw/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Goldilocks Dilemma: Finding the Sweet Spot Between Overfitting and Underfitting</h1> <p class="post-meta"> Created on October 26, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/overfitting"> <i class="fa-solid fa-hashtag fa-sm"></i> Overfitting</a>   <a href="/blog/blog/tag/underfitting"> <i class="fa-solid fa-hashtag fa-sm"></i> Underfitting</a>   <a href="/blog/blog/tag/model-evaluation"> <i class="fa-solid fa-hashtag fa-sm"></i> Model Evaluation</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey there, fellow explorers of the data frontier!</p> <p>My journey into the captivating world of Machine Learning (ML) has been a rollercoaster of “aha!” moments and head-scratching puzzles. One of the earliest, and perhaps most fundamental, lessons I learned was the critical distinction between <strong>overfitting</strong> and <strong>underfitting</strong>. It’s like the Goldilocks story for algorithms: finding the model that’s “just right.”</p> <p>I remember grappling with this concept, seeing models perform brilliantly on the data I used to train them, only to utterly fail when presented with new, unseen information. It was frustrating, perplexing, and a huge hurdle in building truly useful predictive systems. If you’ve felt that same sting of disappointment, you’re in good company. Understanding these two phenomena isn’t just academic; it’s the bedrock of building robust, generalizable ML models.</p> <p>So, let’s dive in and demystify these two crucial concepts, exploring why they happen, how to spot them, and what we can do to guide our models towards that elusive “just right” state.</p> <h3 id="the-core-idea-whats-our-model-trying-to-do">The Core Idea: What’s Our Model Trying to Do?</h3> <p>At its heart, a Machine Learning model is an attempt to find patterns and relationships within data. We feed it a bunch of examples (our <strong>training data</strong>), and it tries to learn a function that can map inputs ($x$) to outputs ($y$). Once it has “learned” these patterns, our ultimate goal is for it to make accurate predictions on <em>new, unseen data</em>. This ability to perform well on data it hasn’t encountered before is called <strong>generalization</strong>.</p> <p>Imagine you’re teaching a student about different types of animals. You show them pictures of cats, dogs, and birds (training data) and tell them their names. Then, you show them a <em>new</em> picture (test data) and ask them to identify it.</p> <p>This process involves:</p> <ol> <li> <strong>Training Data:</strong> The dataset used to teach the model.</li> <li> <strong>Validation Data (Optional but Recommended):</strong> A subset of data used to tune hyperparameters and evaluate the model <em>during</em> training to prevent overfitting early on.</li> <li> <strong>Test Data:</strong> A completely separate, unseen dataset used to evaluate the model’s final performance <em>after</em> training. This is our true measure of generalization.</li> </ol> <p>The core objective is to minimize a <strong>loss function</strong> (or cost function), which quantifies how “wrong” our model’s predictions are. For a simple regression problem, a common loss function is the Mean Squared Error (MSE):</p> <p>$J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})^2$</p> <p>Here, $h_\theta(x^{(i)})$ is our model’s prediction for the $i$-th example, $y^{(i)}$ is the actual value, $m$ is the number of training examples, and $\theta$ represents the model’s parameters (the ‘knowledge’ it learns). The goal is to find the $\theta$ that minimizes $J(\theta)$.</p> <h3 id="the-problem-of-being-too-simple-underfitting">The Problem of Being Too Simple: Underfitting</h3> <p>Let’s start with the simpler problem: underfitting.</p> <p><strong>Imagine this:</strong> You’ve hired an artist to draw a detailed portrait of a complex landscape with rolling hills, a winding river, and a bustling city in the distance. But the artist, for some reason, decides to use only a few broad strokes, drawing a flat line for the hills, a single squiggle for the river, and a block for the city.</p> <p>That’s underfitting in a nutshell.</p> <p><strong>Technically speaking, an underfit model is too simple to capture the underlying patterns in the training data.</strong> It fails to learn the relationships between the input features and the target variable effectively. This means it performs poorly not just on new data but <em>even on the data it was trained on</em>.</p> <p><strong>Characteristics of Underfitting:</strong></p> <ul> <li> <strong>High Bias:</strong> The model makes strong assumptions about the data that are often incorrect. It’s too rigid.</li> <li> <strong>Poor performance on both training and test sets:</strong> If your model’s accuracy is low on your training data, it’s a huge red flag for underfitting.</li> <li> <strong>Simple model, complex data:</strong> You might be using a linear model for data that clearly has a non-linear relationship.</li> </ul> <p><strong>Visualizing Underfitting:</strong> Consider a dataset where the true relationship between $x$ and $y$ is clearly curved. If we try to fit a simple linear regression model ($h_\theta(x) = \theta_0 + \theta_1 x$):</p> <p><img src="https://i.imgur.com/example_underfitting.png" alt="Example of Underfitting: A straight line trying to fit curved data points."> </p> <p>The straight line ($h_\theta(x)$) simply cannot capture the curve. The loss $J(\theta)$ will be high, indicating poor fit.</p> <h3 id="the-problem-of-being-too-detailed-overfitting">The Problem of Being Too Detailed: Overfitting</h3> <p>Now, let’s flip the coin. What if our artist, instead of being too simplistic, became obsessed with every single blade of grass, every tiny pebble, every individual brick? They draw every single detail perfectly, even the smudges on your photo reference. The result is a portrait that’s stunningly accurate for <em>that specific photo</em>, but it looks bizarre and unnatural if you compare it to the actual landscape or if the lighting subtly changes.</p> <p>This is overfitting.</p> <p><strong>An overfit model is overly complex; it learns not only the underlying patterns but also the noise and random fluctuations present in the training data.</strong> It essentially memorizes the training examples rather than understanding the general rules. While it performs exceptionally well on the training data (sometimes perfectly!), its performance drastically drops when exposed to new, unseen data because it’s learned irrelevant specifics that don’t generalize.</p> <p><strong>Characteristics of Overfitting:</strong></p> <ul> <li> <strong>High Variance:</strong> The model is too flexible and sensitive to small fluctuations in the training data.</li> <li> <strong>Excellent performance on training set, poor performance on test set:</strong> This is the classic symptom. Your model looks great during development but flops in production.</li> <li> <strong>Complex model, limited data:</strong> Often occurs with models that have many parameters or degrees of freedom (e.g., very deep neural networks, high-degree polynomial regression) and not enough data to constrain them.</li> </ul> <p><strong>Visualizing Overfitting:</strong> Using the same curved dataset, what if we try to fit a very high-degree polynomial regression model ($h_\theta(x) = \theta_0 + \theta_1 x + \theta_2 x^2 + … + \theta_n x^n$, where $n$ is very large):</p> <p><img src="https://i.imgur.com/example_overfitting.png" alt="Example of Overfitting: A wiggly, high-degree polynomial curve hitting every single data point, including noise."> </p> <p>The curve wiggles perfectly through every single data point. It might even hit outliers! The training loss $J(\theta)$ will be very low (close to zero), but when you introduce new points that deviate slightly from these exact locations, the model’s predictions will be way off.</p> <h3 id="the-goldilocks-zone-just-right">The Goldilocks Zone: Just Right!</h3> <p>Our goal is to find a model that is neither too simple (underfit) nor too complex (overfit). We want a model that captures the true underlying patterns in the data without getting distracted by the noise. This is the “Goldilocks Zone” – a model with a good balance of bias and variance, capable of <strong>generalizing</strong> well.</p> <p><img src="https://i.imgur.com/example_justright.png" alt="Ideal Fit: A smooth curve that captures the general trend of the data without being too simple or too wiggly."> </p> <p>Here, the curve captures the true relationship without being unduly influenced by individual points. It might not hit <em>every</em> training point perfectly, but it performs well on average across the entire dataset, including unseen data.</p> <h3 id="how-do-we-diagnose-them">How Do We Diagnose Them?</h3> <p>It’s one thing to understand underfitting and overfitting; it’s another to spot them in the wild.</p> <ol> <li> <strong>Training vs. Test Performance:</strong> <ul> <li> <strong>Underfitting:</strong> Low training accuracy, low test accuracy.</li> <li> <strong>Overfitting:</strong> High training accuracy, low test accuracy.</li> <li> <strong>Just Right:</strong> High training accuracy, high test accuracy (with a small, acceptable gap).</li> </ul> </li> <li> <strong>Learning Curves:</strong> These are powerful diagnostic tools. They plot the model’s performance (e.g., loss or accuracy) on the training set and validation/test set as a function of the number of training examples or training iterations. <ul> <li> <p><strong>Underfitting (High Bias):</strong> Both training and validation error are high and converge to a similar, high value. Adding more data won’t help much because the model fundamentally can’t learn the pattern. <img src="https://i.imgur.com/learning_curve_underfit.png" alt="Learning Curve for High Bias (Underfitting)"> </p> </li> <li> <p><strong>Overfitting (High Variance):</strong> The training error is very low, while the validation error is significantly higher, and there’s a large gap between them. As you add more data, the validation error might decrease, and the gap might narrow. <img src="https://i.imgur.com/learning_curve_overfit.png" alt="Learning Curve for High Variance (Overfitting)"> </p> </li> <li> <p><strong>Just Right:</strong> Both training and validation errors are low, close to each other, and converge as more data is used. <img src="https://i.imgur.com/learning_curve_justright.png" alt="Learning Curve for Just Right Fit"> </p> </li> </ul> </li> </ol> <h3 id="how-do-we-mitigate-them">How Do We Mitigate Them?</h3> <p>Once we’ve diagnosed the problem, we can apply specific strategies:</p> <h4 id="mitigating-underfitting-reducing-bias">Mitigating Underfitting (Reducing Bias):</h4> <p>When your model is too simple:</p> <ol> <li> <strong>Increase Model Complexity:</strong> <ul> <li>For linear models, add polynomial features (e.g., instead of $x$, use $x, x^2, x^3$).</li> <li>Use a more complex algorithm (e.g., switch from linear regression to a Random Forest or Neural Network).</li> <li>Increase the number of layers or neurons in a neural network.</li> </ul> </li> <li> <strong>Add More Relevant Features:</strong> If your current features aren’t enough, engineer new ones or gather more data with richer information.</li> <li> <strong>Decrease Regularization:</strong> Regularization (which we’ll discuss next) can prevent a model from becoming too complex. If a model is underfitting, it might be over-regularized.</li> </ol> <h4 id="mitigating-overfitting-reducing-variance">Mitigating Overfitting (Reducing Variance):</h4> <p>When your model is too complex and memorizing noise:</p> <ol> <li> <strong>More Data:</strong> The single best defense against overfitting is to provide more diverse training data. More examples help the model learn general patterns instead of specific quirks.</li> <li> <strong>Feature Selection/Reduction:</strong> Remove irrelevant or redundant features that might just be adding noise.</li> <li> <strong>Regularization:</strong> This is a powerful technique that adds a penalty to the loss function for large model parameters, effectively discouraging overly complex models. <ul> <li> <table> <tbody> <tr> <td> <strong>L1 Regularization (Lasso):</strong> Adds a penalty proportional to the absolute value of the coefficients: $J(\theta) + \lambda \sum_{j=1}^{n}</td> <td>\theta_j</td> <td>$. It can drive some coefficients to zero, effectively performing feature selection.</td> </tr> </tbody> </table> </li> <li> <strong>L2 Regularization (Ridge/Weight Decay):</strong> Adds a penalty proportional to the square of the coefficients: $J(\theta) + \lambda \sum_{j=1}^{n} \theta_j^2$. It shrinks coefficients but rarely makes them exactly zero.</li> <li>The hyperparameter $\lambda$ (lambda) controls the strength of the regularization. A larger $\lambda$ means more regularization (simpler model), a smaller $\lambda$ means less.</li> </ul> </li> <li> <strong>Cross-Validation:</strong> Techniques like K-Fold Cross-Validation help in getting a more robust estimate of model performance and prevent selecting a model that just happened to do well on a single validation set.</li> <li> <strong>Simplify Model Architecture:</strong> Reduce the complexity of your model (e.g., fewer polynomial degrees, shallower neural networks, fewer trees in a forest).</li> <li> <strong>Early Stopping:</strong> For iterative training algorithms (like neural networks), stop training when the performance on the validation set starts to degrade, even if the training set performance is still improving.</li> <li> <strong>Dropout (for Neural Networks):</strong> Randomly “turns off” a fraction of neurons during training, forcing the network to learn more robust features.</li> </ol> <h3 id="a-balancing-act-not-a-one-time-fix">A Balancing Act, Not a One-Time Fix</h3> <p>Finding the perfect balance between bias and variance is an iterative process. It’s not about eliminating bias or variance entirely, but about finding the optimal trade-off for your specific problem and dataset. Every dataset is unique, and every model will require careful tuning and evaluation.</p> <p>My journey taught me that humility is key. No model is perfect, and the goal isn’t perfection, but rather robust generalization. By understanding and actively addressing overfitting and underfitting, you’re not just building models; you’re building reliable tools that can solve real-world problems.</p> <p>So, next time your model isn’t performing as expected, ask yourself: Is it being too simplistic? Or is it trying too hard to please its teacher? The answer lies in those training and testing scores, and the path to improvement is often revealed by those insightful learning curves.</p> <p>Happy modeling!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>