<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Finding Your Tribe: A Journey into K-Means Clustering | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/2025/finding-your-tribe-a-journey-into-k-means-clusteri/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Finding Your Tribe: A Journey into K-Means Clustering</h1> <p class="post-meta"> Created on January 17, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/k-means"> <i class="fa-solid fa-hashtag fa-sm"></i> K-Means</a>   <a href="/blog/blog/tag/clustering"> <i class="fa-solid fa-hashtag fa-sm"></i> Clustering</a>   <a href="/blog/blog/tag/unsupervised-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Unsupervised Learning</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hello fellow data adventurers!</p> <p>Today, I want to share a story about finding order in chaos, about making sense of the jumbled pieces of information that often land on our desks. Imagine you’re a librarian with a brand new, massive shipment of books, but none of them have labels indicating their genre. They’re just… books. How would you start organizing them so patrons can find what they’re looking for? You’d probably start by looking at titles, cover art, maybe reading a few pages, and then grouping similar books together. That’s the essence of what we’re going to explore today: <strong>K-Means Clustering</strong>.</p> <p>It’s one of those elegant algorithms in machine learning that feels almost magical in its simplicity, yet incredibly powerful in its applications. It’s a cornerstone of what we call <strong>unsupervised learning</strong>, a branch of AI where we don’t have predefined “answers” or “labels” for our data. Instead, our goal is to discover hidden patterns and structures <em>within</em> the data itself.</p> <h3 id="whats-this-clustering-all-about">What’s This “Clustering” All About?</h3> <p>At its heart, clustering is the task of dividing the dataset into groups, or “clusters,” such that data points within the same cluster are more similar to each other than to those in other clusters. Think of it like sorting your diverse collection of LEGO bricks by color, or separating different species of flowers based on their petal and sepal measurements. You’re creating natural groupings without anyone telling you explicitly what those groups should be.</p> <h3 id="enter-k-means-the-k-and-the-means">Enter K-Means: The “K” and the “Means”</h3> <p>K-Means is perhaps the most popular and widely used clustering algorithm. The name itself gives us two vital clues:</p> <ol> <li> <strong>K:</strong> This refers to the number of clusters we want to find. It’s a hyperparameter we need to decide <em>before</em> running the algorithm. Choosing the right ‘K’ is often a bit of an art and a science, which we’ll touch upon later.</li> <li> <strong>Means:</strong> This hints at how the clusters are formed. Each cluster is represented by its “mean” or <strong>centroid</strong>, which is essentially the average position of all data points belonging to that cluster.</li> </ol> <p>The goal of K-Means is to partition our data into <code class="language-plaintext highlighter-rouge">K</code> clusters, ensuring that each data point belongs to the cluster with the nearest mean (centroid). We want to minimize the “spread” or “variance” within each cluster, making the points inside a cluster as close to their centroid as possible.</p> <h3 id="the-k-means-algorithm-a-step-by-step-dance">The K-Means Algorithm: A Step-by-Step Dance</h3> <p>Let’s break down how K-Means actually works. It’s an iterative process, meaning it repeats a set of steps until it reaches a stable solution.</p> <p><strong>Step 1: Initialization – “Planting the Seeds”</strong> First, we randomly select <code class="language-plaintext highlighter-rouge">K</code> data points from our dataset to serve as the initial centroids for our <code class="language-plaintext highlighter-rouge">K</code> clusters. Imagine scattering <code class="language-plaintext highlighter-rouge">K</code> flags randomly across your data landscape.</p> <p><strong>Step 2: Assignment Step (E-step) – “Gathering Around the Flags”</strong> Now, for every single data point in our dataset, we calculate its distance to <em>all</em> <code class="language-plaintext highlighter-rouge">K</code> centroids. The data point is then assigned to the cluster whose centroid is closest to it.</p> <p>How do we measure “closest”? Most commonly, we use the <strong>Euclidean distance</strong>. For two points, $\mathbf{x} = (x_1, x_2, \ldots, x_D)$ and $\mathbf{c} = (c_1, c_2, \ldots, c_D)$ in a D-dimensional space, the Euclidean distance is:</p> <p>$d(\mathbf{x}, \mathbf{c}) = \sqrt{\sum_{i=1}^{D} (x_i - c_i)^2}$</p> <p>This step effectively partitions the data space into <code class="language-plaintext highlighter-rouge">K</code> regions, where each region consists of points closer to one centroid than any other. These regions are called <strong>Voronoi cells</strong>.</p> <p><strong>Step 3: Update Step (M-step) – “Moving the Flags”</strong> Once all data points have been assigned to a cluster, we recalculate the position of each of the <code class="language-plaintext highlighter-rouge">K</code> centroids. The new centroid for a cluster is simply the <strong>mean</strong> (average) of all the data points that were assigned to that cluster in the previous step.</p> <p>If $C_j$ represents the set of data points assigned to cluster $j$, then the new centroid $\mathbf{c}_j$ is calculated as:</p> <table> <tbody> <tr> <td>$\mathbf{c}_j = \frac{1}{</td> <td>C_j</td> <td>} \sum_{\mathbf{x} \in C_j} \mathbf{x}$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>Where $</td> <td>C_j</td> <td>$ is the number of data points in cluster $j$. This makes perfect sense: the center of a cluster should be where its members are most densely located.</td> </tr> </tbody> </table> <p><strong>Step 4: Repeat Until Convergence – “Settling Down”</strong> We go back to Step 2 (assignment) and Step 3 (update), repeating this process until one of two conditions is met:</p> <ul> <li>The centroids no longer move significantly between iterations.</li> <li>A maximum number of iterations has been reached.</li> </ul> <p>When the centroids stop moving, it means the clusters have stabilized, and we’ve found our <code class="language-plaintext highlighter-rouge">K</code> groups!</p> <h3 id="the-objective-function-what-k-means-tries-to-minimize">The Objective Function: What K-Means Tries to Minimize</h3> <p>Behind these steps, K-Means is diligently working to minimize a specific objective function. This function is often called the <strong>Sum of Squared Errors (SSE)</strong> or <strong>Inertia</strong>. It measures the sum of the squared distances between each data point and its assigned centroid across all clusters.</p> <table> <tbody> <tr> <td>$J = \sum_{j=1}^{K} \sum_{\mathbf{x} \in C_j}</td> <td> </td> <td>\mathbf{x} - \mathbf{c}_j</td> <td> </td> <td>^2$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>Here, $C_j$ is the set of points in cluster $j$, $\mathbf{x}$ is a data point, and $\mathbf{c}_j$ is the centroid of cluster $j$. The squared Euclidean distance $</td> <td> </td> <td>…</td> <td> </td> <td>^2$ makes sure that points closer to the centroid contribute less to the error, and it penalizes points further away more heavily. By minimizing this value, K-Means ensures that points within a cluster are as close to their cluster’s center as possible, leading to compact and well-defined clusters.</td> </tr> </tbody> </table> <h3 id="key-considerations-and-challenges">Key Considerations and Challenges</h3> <p>While elegant, K-Means isn’t without its quirks:</p> <ol> <li> <strong>Choosing K:</strong> This is perhaps the biggest challenge. How do you know how many groups are “natural” in your data? <ul> <li> <strong>Elbow Method:</strong> A popular heuristic is to run K-Means for a range of <code class="language-plaintext highlighter-rouge">K</code> values (e.g., from 1 to 10) and plot the SSE for each <code class="language-plaintext highlighter-rouge">K</code>. The SSE generally decreases as <code class="language-plaintext highlighter-rouge">K</code> increases. We look for the “elbow” point in the graph where the rate of decrease dramatically slows down, suggesting that adding more clusters beyond this point doesn’t significantly improve the clustering quality.</li> <li> <strong>Silhouette Score:</strong> Another metric that measures how similar a data point is to its own cluster compared to other clusters. A higher silhouette score generally indicates better-defined clusters.</li> </ul> </li> <li> <strong>Initialization Sensitivity:</strong> Because Step 1 (random initialization of centroids) is, well, random, different runs of K-Means on the same dataset can lead to different final clusterings. This is because K-Means can get stuck in “local optima” rather than finding the globally best solution. <ul> <li> <strong>K-Means++:</strong> To mitigate this, a smarter initialization technique called K-Means++ is often used. Instead of picking all centroids randomly, it tries to spread them out by selecting initial centroids that are far apart from each other. Most modern K-Means implementations use K-Means++ by default.</li> <li> <strong>Multiple Runs:</strong> It’s also common practice to run K-Means multiple times (e.g., 10 or 100 times) with different random initializations and pick the clustering result that has the lowest SSE.</li> </ul> </li> <li> <strong>Assumptions and Limitations:</strong> <ul> <li> <strong>Spherical Clusters:</strong> K-Means works best when clusters are roughly spherical and similar in size and density. It struggles with clusters that are elongated, irregularly shaped, or have varying densities.</li> <li> <strong>Feature Scaling:</strong> K-Means relies on distance calculations. If your features have very different scales (e.g., one feature ranges from 0-100 and another from 0-1), features with larger scales can dominate the distance calculations. It’s crucial to <strong>scale your data</strong> (e.g., using StandardScaler) before applying K-Means.</li> <li> <strong>Outliers:</strong> K-Means can be sensitive to outliers because they can significantly pull the centroid towards them, distorting the clusters.</li> <li> <strong>Requires K:</strong> As mentioned, you need to specify <code class="language-plaintext highlighter-rouge">K</code> upfront. For some problems, this isn’t known.</li> </ul> </li> </ol> <h3 id="where-can-you-find-your-k-means-tribe">Where Can You Find Your K-Means Tribe?</h3> <p>Despite its limitations, K-Means is incredibly versatile and widely used due to its simplicity and efficiency (especially for large datasets). You’ll find it applied in many real-world scenarios:</p> <ul> <li> <strong>Customer Segmentation:</strong> Grouping customers based on their purchasing behavior, demographics, or website activity to target marketing campaigns more effectively.</li> <li> <strong>Document Clustering:</strong> Organizing large corpuses of text documents into topics or categories.</li> <li> <strong>Image Compression:</strong> Reducing the number of colors in an image by grouping similar colors together.</li> <li> <strong>Anomaly Detection:</strong> Identifying data points that don’t fit into any of the established clusters.</li> <li> <strong>Recommendation Systems:</strong> Suggesting products or content based on user groups.</li> </ul> <h3 id="a-powerful-tool-in-the-unsupervised-arsenal">A Powerful Tool in the Unsupervised Arsenal</h3> <p>K-Means is a fantastic entry point into the world of unsupervised learning. It teaches us to look for inherent structures in data, to let the data “speak for itself” when labels are scarce or non-existent. It empowers us to gain insights and make data-driven decisions without needing pre-classified examples.</p> <p>So, the next time you encounter a seemingly chaotic dataset, remember K-Means. It might just be the perfect tool to help you find its hidden tribes and bring order to its beautiful complexity.</p> <p>Happy clustering, and may your data always find its perfect group!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>