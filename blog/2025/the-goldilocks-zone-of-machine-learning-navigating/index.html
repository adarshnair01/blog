<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Goldilocks Zone of Machine Learning: Navigating Overfitting and Underfitting | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/the-goldilocks-zone-of-machine-learning-navigating/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Goldilocks Zone of Machine Learning: Navigating Overfitting and Underfitting</h1> <p class="post-meta"> Created on April 13, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/overfitting"> <i class="fa-solid fa-hashtag fa-sm"></i> Overfitting</a>   <a href="/blog/blog/tag/underfitting"> <i class="fa-solid fa-hashtag fa-sm"></i> Underfitting</a>   <a href="/blog/blog/tag/model-evaluation"> <i class="fa-solid fa-hashtag fa-sm"></i> Model Evaluation</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>Welcome back to my corner of the data science universe. Today, I want to unravel a concept that lies at the very heart of building effective machine learning models: the delicate balance between <strong>overfitting</strong> and <strong>underfitting</strong>. It’s a challenge every aspiring data scientist, from high school students tinkering with their first Python scripts to seasoned MLEs deploying large-scale systems, must grapple with. Think of it as finding the “Goldilocks Zone” for your model – not too simple, not too complex, but <em>just right</em>.</p> <h3 id="the-ultimate-goal-generalization">The Ultimate Goal: Generalization</h3> <p>Before we dive into the pitfalls, let’s remember our main objective. When we train a machine learning model, we’re not just trying to make it ace a pop quiz on the data it’s already seen. No, our true goal is for the model to perform well on <em>new, unseen data</em>. This ability is called <strong>generalization</strong>.</p> <p>Imagine you’re studying for a math test. You don’t just memorize the answers to the practice problems; you learn the <em>methods</em> and <em>concepts</em> so you can solve any new problem thrown your way. In machine learning, our model needs to learn the underlying patterns, not just memorize the training examples. If it can generalize, it’s a truly useful model. If not, well, that’s where overfitting and underfitting come in.</p> <h3 id="the-problem-of-underfitting-the-too-simple-model">The Problem of Underfitting: The “Too Simple” Model</h3> <p>Let’s start with the easier-to-spot issue: <strong>underfitting</strong>.</p> <p><strong>Analogy Time:</strong> Think of a student who barely studies for a big exam. They might glance at the textbook, vaguely understand a few main ideas, but they haven’t delved deep enough to grasp the nuances. When the exam comes, they struggle with most questions, even the ones that are similar to what they briefly saw.</p> <p>In machine learning, an underfit model is like that student. It’s too simple to capture the underlying structure of the data. It hasn’t learned enough from the training data to make accurate predictions, even on that same training data.</p> <p><strong>What it Looks Like:</strong></p> <ul> <li><strong>Poor performance on the training data.</strong></li> <li><strong>Equally poor performance on new, unseen data.</strong></li> </ul> <p>Visually, imagine you have a scatter plot of data points that clearly follow a curve, but your model tries to fit a straight line through them. That straight line will miss most of the points, indicating it’s too simple to represent the true relationship.</p> <p><strong>Why Does It Happen?</strong> Underfitting is often due to:</p> <ol> <li> <strong>Model is too simple:</strong> Using a linear model for inherently non-linear data. For example, trying to predict house prices (which might grow exponentially with size) using only a simple linear relationship.</li> <li> <strong>Insufficient features:</strong> Not providing the model with enough relevant information. If you’re predicting a car’s fuel efficiency, but only give it color and make, it’s missing crucial factors like engine size or weight.</li> <li> <strong>Too much regularization:</strong> Regularization is a technique we’ll discuss shortly, but if it’s applied too aggressively, it can overly simplify the model.</li> </ol> <p><strong>Mathematical Intuition (High Bias):</strong> Underfitting is primarily associated with <strong>high bias</strong>. Bias refers to the error introduced by approximating a real-world problem, which may be complex, by a simplified model. A high-bias model makes strong assumptions about the data’s relationship, often oversimplifying it. It consistently misses the mark because its core assumptions are flawed for the given data.</p> <p><strong>How to Fix Underfitting:</strong></p> <ul> <li> <strong>Increase model complexity:</strong> Use a more flexible model (e.g., polynomial regression instead of linear, decision tree with more depth, a neural network with more layers/neurons).</li> <li> <strong>Add more relevant features:</strong> Feature engineering is key! Derive new features from existing ones, or gather more data with different attributes.</li> <li> <strong>Reduce regularization:</strong> If you’re using regularization, try decreasing its strength.</li> <li> <strong>Train longer:</strong> For iterative models (like neural networks), sometimes the model just needs more “study time” to learn.</li> </ul> <h3 id="the-problem-of-overfitting-the-too-complex-model">The Problem of Overfitting: The “Too Complex” Model</h3> <p>Now, let’s flip to the other side of the coin: <strong>overfitting</strong>. This is often more insidious and harder to spot without proper evaluation.</p> <p><strong>Analogy Time:</strong> Imagine a student who doesn’t just study for the exam, but <em>memorizes every single example problem, every footnote, every diagram</em> in the textbook, including any typos or obscure details. They know those specific examples perfectly. But if the exam asks a question phrased slightly differently, or presents a new problem that requires applying the underlying concept rather than recalling a specific solution, they might struggle. They’ve memorized the answers rather than truly understanding the principles.</p> <p>An overfit model is like this student. It has learned the training data <em>too well</em>, including the random noise, errors, or unique quirks present in that specific dataset. It’s essentially memorized the training examples, rather than learning the general patterns.</p> <p><strong>What it Looks Like:</strong></p> <ul> <li><strong>Excellent (or even perfect) performance on the training data.</strong></li> <li><strong>Poor performance on new, unseen data.</strong></li> </ul> <p>Visually, if your data points follow a general curve but have some random jiggles, an overfit model might draw a highly complex, wiggly line that perfectly passes through <em>every single training point</em>, even the outliers. While impressive on the training data, this wiggly line is unlikely to represent the true underlying relationship and will probably perform poorly on new data that doesn’t follow those specific jiggles.</p> <p><strong>Why Does It Happen?</strong> Overfitting typically occurs when:</p> <ol> <li> <strong>Model is too complex:</strong> The model has too many parameters or too much flexibility relative to the amount of training data.</li> <li> <strong>Too little training data:</strong> With insufficient data, a complex model can easily memorize the few examples it has.</li> <li> <strong>Too many features:</strong> If you have many features, especially irrelevant ones, the model might latch onto spurious correlations.</li> </ol> <p><strong>Mathematical Intuition (High Variance):</strong> Overfitting is primarily associated with <strong>high variance</strong>. Variance refers to the model’s sensitivity to small fluctuations or noise in the training data. A high-variance model will change significantly if a different training dataset is used (even if drawn from the same underlying distribution). It’s too adaptable and essentially captures the noise along with the signal.</p> <p><strong>How to Fix Overfitting:</strong></p> <ul> <li> <strong>Simplify the model:</strong> Reduce the number of parameters (e.g., fewer layers or neurons in a neural network, pruning a decision tree).</li> <li> <strong>More data:</strong> The most straightforward solution! With more diverse training data, the model has a harder time memorizing specific examples and is forced to learn general patterns.</li> <li> <strong>Feature selection/engineering:</strong> Remove irrelevant or redundant features, or combine existing features to create more meaningful ones.</li> <li> <strong>Regularization:</strong> This is a powerful technique to prevent overfitting. It adds a penalty to the loss function based on the magnitude of the model’s parameters, discouraging them from taking on extreme values. <ul> <li> <strong>L1 Regularization (Lasso):</strong> Adds a penalty proportional to the absolute value of the coefficients: $L1 \text{ penalty} = \lambda \sum_{i=1}^{n} |\theta_i|$ It tends to shrink less important feature coefficients to zero, effectively performing feature selection.</li> <li> <strong>L2 Regularization (Ridge):</strong> Adds a penalty proportional to the square of the coefficients: $L2 \text{ penalty} = \lambda \sum_{i=1}^{n} \theta_i^2$ It shrinks coefficients towards zero, making the model simpler without necessarily eliminating features entirely.</li> <li> <strong>Dropout (for Neural Networks):</strong> Randomly “turns off” a fraction of neurons during training, forcing the network to learn more robust features.</li> </ul> </li> <li> <strong>Early Stopping:</strong> For iterative models, monitor the model’s performance on a separate validation set. Stop training when the validation error starts to increase, even if the training error is still decreasing. This signals the onset of overfitting.</li> <li> <strong>Cross-validation:</strong> A robust technique to estimate a model’s performance on unseen data by training and testing on different subsets of the data multiple times. This helps detect if a model is merely good on one specific split.</li> </ul> <h3 id="the-bias-variance-trade-off-the-balancing-act">The Bias-Variance Trade-off: The Balancing Act</h3> <p>Underfitting (high bias, low variance) and overfitting (low bias, high variance) are two sides of the same coin, intrinsically linked by the <strong>bias-variance trade-off</strong>.</p> <ul> <li> <strong>Bias:</strong> Error from erroneous assumptions in the learning algorithm. High bias means the model is too simple and misses the underlying trends.</li> <li> <strong>Variance:</strong> Error from sensitivity to small fluctuations in the training dataset. High variance means the model is too complex and learns the noise, not just the signal.</li> <li> <strong>Irreducible Error:</strong> This is the error that cannot be reduced by any model. It’s due to inherent noise in the data itself (e.g., measurement errors).</li> </ul> <p>The total expected prediction error for a model can be theoretically decomposed as: $Total \text{ Error} = (\text{Bias})^2 + \text{Variance} + \text{Irreducible Error}$</p> <p>The challenge is that as you decrease bias (make the model more complex to capture more patterns), you often increase variance (make it more sensitive to noise). Conversely, as you decrease variance (simplify the model to reduce sensitivity), you often increase bias (make it too simple to capture true patterns).</p> <p>Our “Goldilocks Zone” is the sweet spot where both bias and variance are acceptably low, leading to the lowest possible total error on unseen data.</p> <h3 id="how-to-detect-it-the-power-of-data-splitting-and-learning-curves">How to Detect It: The Power of Data Splitting and Learning Curves</h3> <p>The crucial tool for navigating this trade-off is proper data splitting:</p> <ol> <li> <strong>Training Set:</strong> The data your model learns from.</li> <li> <strong>Validation Set:</strong> A separate dataset used to tune hyperparameters and make decisions about the model (e.g., early stopping). This set <em>simulates</em> unseen data during the development phase.</li> <li> <strong>Test Set:</strong> A completely independent dataset, used <em>only once</em> at the very end to evaluate the final model’s performance. It gives you an unbiased estimate of generalization.</li> </ol> <p>By monitoring your model’s performance on both the <strong>training set</strong> and the <strong>validation set</strong> during training, you can diagnose overfitting and underfitting:</p> <ul> <li> <strong>Underfitting:</strong> Both training error and validation error are high and roughly similar. The model isn’t learning well.</li> <li> <strong>Overfitting:</strong> Training error is low (sometimes almost zero), but validation error is significantly higher and often starts to increase after a certain point. The model is memorizing.</li> <li> <strong>Just Right:</strong> Training error is low, and validation error is also low and similar to the training error, with both converging.</li> </ul> <p>Plotting these errors over training iterations or model complexity creates <strong>learning curves</strong>, which are incredibly insightful for diagnosis.</p> <h3 id="conclusion-its-a-journey-not-a-destination">Conclusion: It’s a Journey, Not a Destination</h3> <p>Understanding overfitting and underfitting is fundamental to becoming an effective data scientist. It’s not just about knowing the definitions, but internalizing the implications and developing an intuition for how to detect and mitigate them.</p> <p>Finding that “just right” model is rarely a one-shot deal. It involves iterative experimentation: trying different models, tuning hyperparameters, engineering features, and constantly evaluating against fresh data. It’s a continuous journey of balancing complexity and simplicity, making sure your model learns the true signal without getting lost in the noise.</p> <p>So, next time your model isn’t performing as expected, ask yourself: Is it underfitting, needing more complexity or features? Or is it overfitting, needing regularization or simplification? With these tools in your arsenal, you’re well on your way to building robust and reliable machine learning systems.</p> <p>Happy modeling!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>