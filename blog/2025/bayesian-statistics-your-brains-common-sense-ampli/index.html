<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Bayesian Statistics: Your Brain's Common Sense, Amplified by Data | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/bayesian-statistics-your-brains-common-sense-ampli/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Bayesian Statistics: Your Brain's Common Sense, Amplified by Data</h1> <p class="post-meta"> Created on December 31, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/bayesian-statistics"> <i class="fa-solid fa-hashtag fa-sm"></i> Bayesian Statistics</a>   <a href="/blog/blog/tag/probability"> <i class="fa-solid fa-hashtag fa-sm"></i> Probability</a>   <a href="/blog/blog/tag/statistical-inference"> <i class="fa-solid fa-hashtag fa-sm"></i> Statistical Inference</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Imagine you’re trying to figure something out – maybe whether a new movie is worth watching, or if your favorite sports team will win their next game. You start with some initial gut feeling, right? Then, you gather more information: reviews for the movie, the team’s recent performance, injury reports. As you get more data, your initial gut feeling, your “belief,” shifts. Sometimes it strengthens, sometimes it changes completely.</p> <p>This process of starting with a belief, seeing new evidence, and then updating your belief is fundamentally human. What if I told you there’s a whole branch of statistics that formalizes this exact process, giving us a powerful mathematical tool to make better decisions with data? Welcome to the fascinating world of <strong>Bayesian Statistics</strong>.</p> <p>For us in data science and machine learning, understanding Bayesian thinking isn’t just an academic exercise; it’s a superpower for building more robust models, making more informed predictions, and understanding uncertainty in a profoundly intuitive way.</p> <h3 id="the-two-sides-of-the-statistical-coin-frequentist-vs-bayesian">The Two Sides of the Statistical Coin: Frequentist vs. Bayesian</h3> <p>Before we dive into the Bayesian rabbit hole, it’s useful to briefly acknowledge its counterpart: <strong>Frequentist Statistics</strong>. Most of the statistics you might encounter in introductory courses – things like p-values, confidence intervals, and null hypothesis testing – fall under the frequentist umbrella.</p> <p>The core difference often boils down to how they view <strong>probability</strong> and <strong>parameters</strong>:</p> <ul> <li> <strong>Frequentist View</strong>: Parameters (like the true average height of people, or the true success rate of a drug) are fixed, but unknown constants. Probability is defined by the long-run frequency of an event if an experiment were repeated infinitely many times. Data is random; parameters are not.</li> <li> <strong>Bayesian View</strong>: Parameters are not fixed; they are themselves random variables with their own probability distributions. Probability represents our <em>degree of belief</em> in an event. Data is fixed (what we’ve observed); parameters are what we’re uncertain about, and we update our beliefs about them.</li> </ul> <p>Think of it this way: a Frequentist asks, “Given that the coin is fair, how likely is it that I observe 7 heads out of 10 tosses?” A Bayesian asks, “Given that I observed 7 heads out of 10 tosses, how likely is it that the coin is actually fair?” See the subtle but profound shift in perspective?</p> <h3 id="the-heartbeat-of-bayesian-statistics-bayes-theorem">The Heartbeat of Bayesian Statistics: Bayes’ Theorem</h3> <p>The entire edifice of Bayesian statistics rests upon a single, elegant formula: <strong>Bayes’ Theorem</strong>. It’s named after Thomas Bayes, an 18th-century Presbyterian minister and mathematician.</p> <p>Let’s write it down:</p> \[P(A|B) = \frac{P(B|A)P(A)}{P(B)}\] <p>At first glance, it might look like a jumble of letters and symbols. But let’s break down each component, as if we’re dissecting a fascinating puzzle:</p> <ul> <li> <strong>$P(A)$ (The Prior Probability)</strong>: This is your <em>initial belief</em> or hypothesis about event A, <em>before</em> you’ve seen any new evidence. It’s what you think is true based on existing knowledge, past data, or even educated guesses. In our movie example, this might be your initial feeling about the movie’s quality based on the director or genre.</li> <li> <table> <tbody> <tr> <td>**$P(B</td> <td>A)$ (The Likelihood)**: This tells you “how likely is the evidence $B$, given that your hypothesis $A$ is true?” It measures how well your hypothesis $A$ explains the data $B$ you just observed. If the movie is truly good ($A$), how likely is it to get overwhelmingly positive reviews ($B$)?</td> </tr> </tbody> </table> </li> <li> <strong>$P(B)$ (The Marginal Likelihood or Evidence)</strong>: This is the overall probability of observing the evidence $B$, regardless of whether $A$ is true or not. It acts as a normalizing constant, ensuring that your updated probabilities sum up to 1. Conceptually, it averages out the likelihood of the evidence across all possible hypotheses.</li> <li> <table> <tbody> <tr> <td>**$P(A</td> <td>B)$ (The Posterior Probability)**: This is the star of the show! It’s your <em>updated belief</em> about event $A$, <em>after</em> taking into account the new evidence $B$. This is where the learning happens. It tells you “what is the probability of my hypothesis $A$ being true, given that I’ve observed the evidence $B$?” This is your refined opinion about the movie’s quality after reading the reviews.</td> </tr> </tbody> </table> </li> </ul> <p>In plain English, Bayes’ Theorem says:</p> <table> <tbody> <tr> <td>**“Your updated belief about something ($P(A</td> <td>B)$) is proportional to your initial belief about it ($P(A)$) multiplied by how well that belief explains the new data you’ve seen ($P(B</td> <td>A)$).”**</td> </tr> </tbody> </table> <p>The $P(B)$ term simply scales it correctly.</p> <h3 id="a-powerful-example-the-rare-disease-test">A Powerful Example: The Rare Disease Test</h3> <p>Let’s illustrate this with a classic, often counter-intuitive example: medical diagnostic testing. This scenario perfectly highlights the importance of our prior beliefs.</p> <p>Imagine a very rare disease that affects <strong>1 in 1,000 people</strong> ($P(D) = 0.001$). A new test for this disease is developed. It’s quite accurate:</p> <ul> <li> <table> <tbody> <tr> <td>If a person <em>has</em> the disease, the test correctly identifies them as positive <strong>99% of the time</strong> ($P(T+</td> <td>D) = 0.99$).</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>If a person is <em>healthy</em>, the test correctly identifies them as negative <strong>98% of the time</strong> ($P(T-</td> <td>H) = 0.98$). This means it incorrectly gives a positive result (a false positive) 2% of the time ($P(T+</td> <td>H) = 0.02$).</td> </tr> </tbody> </table> </li> </ul> <p>Now, let’s say a random person takes the test, and it comes back <strong>positive</strong>. How worried should they be? What is the probability that they <em>actually have the disease</em>, given a positive test result?</p> <table> <tbody> <tr> <td>This is exactly what Bayes’ Theorem is designed to answer: we want to find $P(D</td> <td>T+)$.</td> </tr> </tbody> </table> <p>Let’s define our terms for the formula:</p> <ul> <li>$A$ = Has the disease ($D$)</li> <li>$B$ = Tests positive ($T+$)</li> </ul> <table> <tbody> <tr> <td>So we want to calculate $P(D</td> <td>T+) = \frac{P(T+</td> <td>D)P(D)}{P(T+)}$</td> </tr> </tbody> </table> <p>Let’s plug in what we know and calculate what we need:</p> <ol> <li> <p><strong>Prior Probability ($P(D)$)</strong>: Our initial belief that a random person has the disease. $P(D) = 0.001$ (1 in 1000) This also means the probability of a random person being healthy ($H$) is $P(H) = 1 - P(D) = 0.999$.</p> </li> <li> <table> <tbody> <tr> <td>**Likelihood ($P(T+</td> <td>D)$)**: The probability of testing positive if you <em>do</em> have the disease.</td> </tr> <tr> <td>$P(T+</td> <td>D) = 0.99$</td> </tr> </tbody> </table> </li> <li> <strong>Marginal Likelihood ($P(T+)$)</strong>: This is the tricky one. What’s the overall probability of <em>anyone</em> testing positive? A person can test positive in two ways: <ul> <li>They have the disease AND test positive ($D$ and $T+$)</li> <li>They are healthy AND test positive ($H$ and $T+$ - a false positive)</li> </ul> <table> <tbody> <tr> <td>So, $P(T+) = P(T+</td> <td>D)P(D) + P(T+</td> <td>H)P(H)$</td> </tr> <tr> <td>We know $P(T+</td> <td>H) = 1 - P(T-</td> <td>H) = 1 - 0.98 = 0.02$.</td> </tr> </tbody> </table> <p>Let’s calculate $P(T+)$: $P(T+) = (0.99 \times 0.001) + (0.02 \times 0.999)$ $P(T+) = 0.00099 + 0.01998$ $P(T+) = 0.02097$</p> <p>This means about 2.1% of all people taking the test will get a positive result.</p> </li> <li> <table> <tbody> <tr> <td>**Now, for the Posterior Probability ($P(D</td> <td>T+)$)**:</td> <td> </td> </tr> <tr> <td>$P(D</td> <td>T+) = \frac{P(T+</td> <td>D)P(D)}{P(T+)}$</td> </tr> <tr> <td>$P(D</td> <td>T+) = \frac{0.99 \times 0.001}{0.02097}$</td> <td> </td> </tr> <tr> <td>$P(D</td> <td>T+) = \frac{0.00099}{0.02097} \approx 0.0472$</td> <td> </td> </tr> </tbody> </table> </li> </ol> <p>What does this number mean? Even though the test is 99% accurate, if you test positive, your probability of actually having this rare disease is only about <strong>4.72%</strong>!</p> <p>This often surprises people. Why so low? Because the disease is so incredibly rare (your prior belief was very, very low) that even a small false positive rate for healthy individuals ($2\%$ of $999$ healthy people out of 1000) swamps the true positives ($99\%$ of $1$ diseased person out of 1000). Your initial low prior belief played a massive role in the updated posterior.</p> <p>This example dramatically demonstrates how Bayes’ Theorem elegantly combines our initial knowledge (the prior prevalence of the disease) with new evidence (the test result) to give us a far more accurate, nuanced understanding of the situation.</p> <h3 id="why-bayesian-statistics-is-a-data-science-superpower">Why Bayesian Statistics is a Data Science Superpower</h3> <p>For anyone building intelligent systems, working with data, or trying to make sense of uncertainty, Bayesian methods offer significant advantages:</p> <ol> <li> <strong>Incorporating Prior Knowledge</strong>: Unlike frequentist methods that often start from a “blank slate,” Bayesian statistics allows us to explicitly incorporate what we already know (or believe) into our models. This is invaluable when working with specialized domains where expert knowledge exists, or when dealing with limited data.</li> <li> <strong>Quantifying Uncertainty</strong>: Instead of just giving a single “point estimate” (like a frequentist mean), Bayesian methods often provide entire probability distributions for parameters. This means we get a full picture of the uncertainty, not just a best guess. We can say “there’s a 95% chance the true value is between X and Y,” which is much richer than just “the value is X.”</li> <li> <strong>Small Data Advantage</strong>: When you have very little data, frequentist methods can struggle to produce reliable results. Bayesian approaches, by allowing the inclusion of priors, can often generate more stable and sensible inferences even with sparse datasets.</li> <li> <table> <tbody> <tr> <td> <strong>Intuitive Interpretation</strong>: The posterior probability $P(A</td> <td>B)$ directly answers the question most people intuitively want to ask: “What’s the probability of my hypothesis given the data?” This is often more straightforward to explain and understand than complex p-values.</td> </tr> </tbody> </table> </li> <li> <strong>Flexibility in Modeling</strong>: Bayesian models can be incredibly flexible, allowing for complex hierarchical structures and the modeling of relationships that are difficult for frequentist approaches to handle. This is especially useful in areas like personalized recommendations, A/B testing, and even spam filtering.</li> </ol> <h3 id="some-considerations">Some Considerations</h3> <p>While powerful, Bayesian statistics isn’t without its challenges:</p> <ul> <li> <strong>Choosing Priors</strong>: While a strength, selecting an appropriate prior can also be a source of debate. If your prior is too strong or misinformed, it can skew your results. However, techniques exist for choosing “non-informative” priors when you genuinely have little prior knowledge.</li> <li> <strong>Computational Complexity</strong>: For many real-world problems, calculating the $P(B)$ (the marginal likelihood) can be computationally intensive, especially with complex models or high-dimensional data. This often necessitates the use of advanced sampling techniques like Markov Chain Monte Carlo (MCMC), which can require significant computational resources and expertise.</li> </ul> <h3 id="your-journey-into-data-driven-wisdom">Your Journey into Data-Driven Wisdom</h3> <p>Bayesian statistics isn’t just a set of equations; it’s a philosophy for how we learn from data. It formalizes the way we instinctively update our beliefs, offering a robust framework for making decisions in a world full of uncertainty.</p> <p>Whether you’re predicting stock prices, optimizing a recommendation engine, or simply trying to figure out if that new movie is worth watching, Bayesian thinking equips you with a powerful way to integrate new information and refine your understanding. As you continue your journey in data science and machine learning, keep this fundamental idea close: start with what you know, observe the world, and let the data intelligently update your beliefs. That, in essence, is the art and science of Bayesian statistics.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>