<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Seeing Beyond Pixels: My Journey into the Heart of Computer Vision | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/seeing-beyond-pixels-my-journey-into-the-heart-of/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Seeing Beyond Pixels: My Journey into the Heart of Computer Vision</h1> <p class="post-meta"> Created on March 28, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/computer-vision"> <i class="fa-solid fa-hashtag fa-sm"></i> Computer Vision</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/image-processing"> <i class="fa-solid fa-hashtag fa-sm"></i> Image Processing</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>From the moment we open our eyes, we’re inundated with visual information. Our brains effortlessly process complex scenes, recognize faces, distinguish objects, and navigate our environment. It’s a miracle we often take for granted. But what if we wanted to give machines this incredible ability? How would a computer, fundamentally a device that understands only numbers, begin to “see” a cat, a car, or a tumor in an X-ray?</p> <p>This question sparked my initial fascination with <strong>Computer Vision (CV)</strong>, a field that sits at the thrilling intersection of artificial intelligence and digital image processing. It’s about empowering computers to derive meaningful information from digital images, videos, and other visual inputs, and then take action or make recommendations based on that understanding.</p> <h3 id="the-pixel-problem-how-a-computer-sees">The “Pixel Problem”: How a Computer Sees</h3> <p>Imagine showing a photograph of a sunset to a friend. They instantly grasp the beauty, the colors, the mood. Now, show that same photograph to a computer. What does it see? Numbers. Lots and lots of numbers.</p> <p>A digital image, at its most fundamental level, is a grid of tiny squares called <strong>pixels</strong>. Each pixel holds numerical values representing its color intensity. For a common RGB (Red, Green, Blue) image, each pixel is a combination of three values, typically ranging from 0 to 255, for each color channel. So, a simple 100x100 pixel image isn’t a picture of a sunset to a computer; it’s a 100x100x3 array of numbers.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Conceptual representation of a small image
# Let's say a 2x2 image with RGB values
</span><span class="n">image_data</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[[</span><span class="mi">255</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="mi">0</span><span class="p">]],</span>  <span class="c1"># Top row: Red pixel, Green pixel
</span>    <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">],</span> <span class="p">[</span><span class="mi">255</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]</span>  <span class="c1"># Bottom row: Blue pixel, Yellow pixel
</span><span class="p">]</span>
<span class="c1"># This is just a tiny fraction of the data for a real image!
</span></code></pre></div></div> <p>The core challenge of Computer Vision is to bridge this massive gap: transforming raw pixel data into high-level, semantic understanding. How do we teach a machine that a specific pattern of red, green, and blue numbers, arranged in a particular way, represents an “eye,” which is part of a “face,” which is a “human”?</p> <h3 id="early-attempts-handcrafting-features">Early Attempts: Handcrafting Features</h3> <p>In the early days of CV, researchers tried to define rules and extract features manually. They’d write algorithms to detect edges (sudden changes in pixel intensity), corners, or blobs. They might hand-engineer features like “Histogram of Oriented Gradients (HOG)” to describe shapes, or “Scale-Invariant Feature Transform (SIFT)” to find distinctive points in an image.</p> <p>This approach worked for very specific, controlled problems. But it was brittle. If the lighting changed, if the object rotated slightly, or if the background was too cluttered, these handcrafted rules often failed. It was like trying to teach someone to recognize all dog breeds by giving them a rulebook for each individual hair color, ear shape, and tail length – an endless, exhausting, and ultimately unscalable task.</p> <h3 id="the-machine-learning-revolution-learning-from-data">The Machine Learning Revolution: Learning from Data</h3> <p>The paradigm shifted with the advent of Machine Learning. Instead of programming explicit rules, we started feeding computers massive datasets of images labeled with what they contained. The idea was to let the machine <em>learn</em> the patterns and features directly from the data. Algorithms like Support Vector Machines (SVMs) or Random Forests, when fed with the aforementioned handcrafted features, showed promise.</p> <p>However, the bottleneck remained: human effort was still required to extract those “good” features. We needed a way for the machine to not just learn to classify, but also to learn <em>what features were important</em> for classification.</p> <h3 id="deep-learning-and-cnns-the-game-changer">Deep Learning and CNNs: The Game Changer</h3> <p>This is where Deep Learning, and specifically <strong>Convolutional Neural Networks (CNNs)</strong>, burst onto the scene and utterly transformed Computer Vision. CNNs are a special type of neural network designed to process data that has a known grid-like topology, such as images.</p> <p>Think of a CNN as a stack of specialized “detectives,” each looking for increasingly complex patterns.</p> <ol> <li> <strong>Low-level detectives:</strong> Look for basic patterns like edges, lines, and simple textures.</li> <li> <strong>Mid-level detectives:</strong> Combine these basic patterns to find shapes, corners, and parts of objects (e.g., an eye, a wheel).</li> <li> <strong>High-level detectives:</strong> Assemble these parts into complete objects (e.g., a face, a car).</li> </ol> <p>Let’s break down the core components of a CNN:</p> <h4 id="1-the-convolutional-layer-the-feature-extractors">1. The Convolutional Layer: The Feature Extractors</h4> <p>This is the heart of a CNN. Instead of processing every pixel individually, a convolutional layer uses a small matrix of numbers called a <strong>kernel</strong> (or filter). This kernel slides across the entire image, performing a mathematical operation called <strong>convolution</strong>.</p> <p>Imagine the kernel as a small magnifying glass looking for a specific pattern. When the pattern under the magnifying glass matches what the kernel is looking for (e.g., a vertical edge), it produces a strong signal in the output. If it doesn’t match, the signal is weak.</p> <p>Mathematically, a 2D convolution operation can be expressed as: \((I \* K)(i, j) = \sum_m \sum_n I(i-m, j-n)K(m, n)\) Where $I$ is the input image, $K$ is the kernel, and $(i, j)$ are the coordinates of the output pixel. This operation effectively creates a <strong>feature map</strong>, which highlights where that specific pattern was detected in the original image.</p> <p>A CNN uses <em>many</em> different kernels, each designed to detect a different feature (horizontal edges, vertical edges, diagonal lines, blobs, etc.). These kernels aren’t designed by humans; they are <em>learned</em> by the network during training!</p> <h4 id="2-activation-functions-relu-adding-non-linearity">2. Activation Functions (ReLU): Adding Non-linearity</h4> <p>After convolution, the output often passes through an activation function. The most popular one in CNNs is the <strong>Rectified Linear Unit (ReLU)</strong>, defined as: \(ReLU(x) = \max(0, x)\) ReLU simply sets all negative values to zero and keeps positive values as they are. Why is this important? It introduces <em>non-linearity</em> into the network. Without non-linearity, stacking multiple convolutional layers would just result in another linear transformation, limiting the network’s ability to learn complex patterns. ReLU allows the network to model highly complex, non-linear relationships in the data, which are essential for understanding real-world images.</p> <h4 id="3-pooling-layers-downsampling-and-invariance">3. Pooling Layers: Downsampling and Invariance</h4> <p>Next comes the pooling layer, typically <strong>Max Pooling</strong>. This layer reduces the spatial dimensions (width and height) of the feature maps, making the network more efficient and robust.</p> <p>How does Max Pooling work? It slides a small window (e.g., 2x2) over the feature map and picks the maximum value within that window. This has several benefits:</p> <ul> <li> <strong>Dimensionality Reduction:</strong> It reduces the number of parameters and computation, preventing overfitting.</li> <li> <strong>Translation Invariance:</strong> By taking the maximum value, the exact position of a feature becomes less important. If an edge shifts slightly, the max pooling output might still be the same, making the network less sensitive to minor shifts or distortions in the input image.</li> </ul> <h4 id="4-fully-connected-layers-classification">4. Fully Connected Layers: Classification</h4> <p>After several stacked convolutional and pooling layers, the network has learned to extract a rich set of high-level features. These features are then “flattened” into a single vector and fed into one or more <strong>Fully Connected (FC) layers</strong>, similar to a traditional neural network. These layers are responsible for making the final classification decision (e.g., “this is a cat,” “this is a car,” “this is a human face”). The output layer typically uses a softmax activation function to give probabilities for each possible class.</p> <h4 id="how-cnns-learn-backpropagation-and-optimization">How CNNs Learn: Backpropagation and Optimization</h4> <p>The entire CNN architecture is trained using a process called <strong>backpropagation</strong>. Initially, the kernels and weights in the FC layers are random. When an image is fed through the network, it makes a prediction. If this prediction is wrong, a <strong>loss function</strong> calculates how far off the prediction was. This error signal is then propagated backward through the network, allowing an <strong>optimizer</strong> (like Adam or SGD) to slightly adjust the kernel values and weights to reduce the error for future predictions. This iterative process, repeated over millions of images, is how the CNN learns to “see.”</p> <p>This hierarchical learning, where early layers learn simple features and deeper layers combine them into more abstract representations, is the magic behind CNNs’ success.</p> <h3 id="real-world-applications-seeing-the-impact">Real-World Applications: Seeing the Impact</h3> <p>The impact of CNNs and Computer Vision is everywhere:</p> <ul> <li> <strong>Autonomous Vehicles:</strong> Object detection (cars, pedestrians, traffic signs), lane keeping, pedestrian tracking.</li> <li> <strong>Medical Imaging:</strong> Detecting tumors in X-rays or MRIs, diagnosing diseases from microscopic images, surgical assistance.</li> <li> <strong>Facial Recognition:</strong> Unlocking phones, security surveillance, identifying individuals.</li> <li> <strong>Augmented Reality (AR):</strong> Overlaying digital information onto the real world (e.g., Snapchat filters, IKEA Place app).</li> <li> <strong>Retail:</strong> Analyzing customer behavior, inventory management, frictionless checkout stores.</li> <li> <strong>Manufacturing:</strong> Quality control, anomaly detection in production lines.</li> <li> <strong>Agriculture:</strong> Monitoring crop health, detecting pests.</li> </ul> <p>I’ve personally applied these techniques in projects ranging from classifying different types of plant diseases from leaf images to developing a custom object detection model for specific tools in a workshop setting. The ability to leverage pre-trained models like ResNet or YOLO and fine-tune them for specific tasks is incredibly powerful and democratizes access to this cutting-edge technology.</p> <h3 id="challenges-and-the-future-of-seeing-machines">Challenges and the Future of Seeing Machines</h3> <p>Despite its incredible progress, Computer Vision still faces challenges:</p> <ul> <li> <strong>Data Scarcity and Bias:</strong> High-quality, labeled data is expensive and time-consuming to acquire. Biases in training data can lead to unfair or inaccurate predictions, especially in sensitive applications like facial recognition.</li> <li> <strong>Explainability (XAI):</strong> Deep learning models can be “black boxes.” Understanding <em>why</em> a CNN made a particular prediction is crucial for trust and debugging, especially in critical domains like healthcare.</li> <li> <strong>Robustness:</strong> CNNs can be vulnerable to “adversarial attacks” – tiny, imperceptible changes to an image that can trick a model into misclassifying it.</li> <li> <strong>Computational Cost:</strong> Training large CNNs requires significant computational resources.</li> </ul> <p>The future of Computer Vision is bustling with innovation. Researchers are working on more efficient architectures, self-supervised learning (where models learn from unlabeled data), robust models against adversarial attacks, and techniques for greater explainability. The emergence of <strong>Generative Adversarial Networks (GANs)</strong> and <strong>Diffusion Models</strong> allows machines to not just understand images but also to <em>create</em> incredibly realistic ones, pushing the boundaries of what’s possible.</p> <h3 id="conclusion-my-ongoing-vision-quest">Conclusion: My Ongoing Vision Quest</h3> <p>From understanding pixel matrices to unraveling the layers of a CNN, my journey into Computer Vision has been nothing short of exhilarating. It’s a field that continues to evolve at a breakneck pace, constantly challenging our understanding of intelligence and perception. As a data scientist, contributing to this field means not just building powerful models, but also understanding their ethical implications and striving to create systems that are fair, transparent, and beneficial to all.</p> <p>The ability to give machines the gift of sight is profoundly transformative, and I believe we’ve only just begun to scratch the surface of what’s possible. The journey of teaching computers to “see” continues, and I’m excited to be a part of it.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>