<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Cracking the Code of Cognition: A Dive into Neural Networks | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/cracking-the-code-of-cognition-a-dive-into-neural/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Cracking the Code of Cognition: A Dive into Neural Networks</h1> <p class="post-meta"> Created on May 29, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/neural-networks"> <i class="fa-solid fa-hashtag fa-sm"></i> Neural Networks</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="cracking-the-code-of-cognition-a-dive-into-neural-networks">Cracking the Code of Cognition: A Dive into Neural Networks</h2> <p>Remember the first time you tried to teach a pet a trick? Or when you finally grasped a complex concept in math or science? That moment of understanding, of making sense of the world, feels inherently human. But what if I told you that we’ve found a way to instill a similar, albeit artificial, learning capability into machines? This isn’t science fiction anymore; it’s the reality powered by <strong>Neural Networks</strong>.</p> <p>For a long time, the idea of machines “thinking” or “learning” felt like a distant dream. Traditional programming was about explicit rules: <code class="language-plaintext highlighter-rouge">IF X THEN Y</code>. But how do you write rules for recognizing a cat in an image, understanding sarcasm, or predicting stock market trends? It’s simply too complex for human-defined rules. This is where Neural Networks step in, offering a paradigm shift: instead of telling the machine <em>how</em> to solve a problem, we show it <em>examples</em> and let it figure out the rules for itself.</p> <p>As someone who constantly explores the bleeding edge of data science and machine learning, I remember the initial intimidation of diving into neural networks. The jargon, the math, the seemingly magical results. But once you peel back the layers, you discover an elegant, intuitive framework that’s as profound as it is powerful. My goal here is to demystify these incredible systems, making them accessible whether you’re just starting your journey or looking to solidify your understanding.</p> <h3 id="the-neuron-the-brains-basic-building-block-and-ais-too">The Neuron: The Brain’s Basic Building Block (and AI’s too!)</h3> <p>Our journey begins with the simplest component: the <strong>artificial neuron</strong>, often called a <strong>perceptron</strong>. Inspired by biological neurons in our brains, these tiny computational units are the fundamental building blocks of any neural network.</p> <p>Imagine a single neuron as a mini decision-maker. It receives several inputs, processes them, and then decides whether to “fire” an output. Let’s break this down:</p> <ol> <li> <strong>Inputs ($x_1, x_2, …, x_n$):</strong> These are numerical values fed into the neuron. In an image recognition task, these could be the pixel values of an image.</li> <li> <strong>Weights ($w_1, w_2, …, w_n$):</strong> Each input is multiplied by a corresponding weight. Think of weights as the neuron’s “attention” or “importance” assigned to each input. A higher weight means that input has a stronger influence on the neuron’s output. Initially, these are random values, but they are what the network <em>learns</em>.</li> <li> <strong>Bias ($b$):</strong> This is an additional value added to the sum of weighted inputs. It acts like a kind of threshold or an adjustable constant that allows the neuron to activate even if all inputs are zero, or to suppress activation even with strong inputs. It essentially shifts the activation function, giving the neuron more flexibility.</li> <li> <strong>Summation:</strong> The neuron sums up all the weighted inputs and adds the bias. Mathematically, this linear combination looks like: \(z = \sum\_{i=1}^n w_i x_i + b\) Or, in vector form, $z = \mathbf{w}^T \mathbf{x} + b$.</li> <li> <p><strong>Activation Function ($f$):</strong> This is the crucial non-linear step. The sum $z$ is passed through an activation function, which determines the neuron’s final output. Without non-linear activation functions, even a deep network would just be a linear model, incapable of learning complex patterns.</p> <p>A popular activation function is the <strong>Rectified Linear Unit (ReLU)</strong>: \(f(z) = \max(0, z)\) This simply outputs the input if it’s positive, and zero otherwise. It’s computationally efficient and helps networks learn faster. Another common one is the Sigmoid function, which squashes the output between 0 and 1, useful for probabilities.</p> </li> </ol> <p>So, the output ($y$) of a single artificial neuron can be represented as: \(y = f(\sum\_{i=1}^n w_i x_i + b)\) Simple, right? This one equation is the atomic unit of the neural network universe.</p> <h3 id="from-neurons-to-networks-the-architecture">From Neurons to Networks: The Architecture</h3> <p>Now, imagine connecting hundreds, thousands, or even millions of these simple neurons together. That’s a neural network! They are typically organized into layers:</p> <ol> <li> <strong>Input Layer:</strong> This layer receives the raw data (e.g., pixel values, sensor readings, word embeddings). It doesn’t perform any computation, it just passes the inputs to the first hidden layer.</li> <li> <strong>Hidden Layers:</strong> These are the “thinking” layers. Each neuron in a hidden layer takes inputs from the previous layer, performs its weighted sum and activation, and then passes its output to the next layer. The “deep” in “Deep Learning” simply refers to networks with many hidden layers. Each hidden layer learns increasingly complex representations of the input data. For example, the first hidden layer might detect edges in an image, the second might combine edges to detect shapes, and so on.</li> <li> <strong>Output Layer:</strong> This layer produces the final result. The number of neurons here depends on the problem. For classifying an image as a “cat” or “dog,” you might have two output neurons (or one, representing the probability of “cat”). For predicting a house price, you’d have one output neuron.</li> </ol> <p>The information flows in one direction, from the input layer, through the hidden layers, to the output layer. This is called a <strong>feedforward network</strong>. It’s like an assembly line, where each station (neuron/layer) performs a specific transformation on the data before passing it down the line.</p> <h3 id="the-magic-of-learning-backpropagation-and-gradient-descent">The Magic of Learning: Backpropagation and Gradient Descent</h3> <p>Okay, we have our network architecture. We feed it data, and it spits out an answer. But how does it learn? How do those initial random weights ($w$) and biases ($b$) become intelligent? This is where the real magic happens, through a process called <strong>training</strong>.</p> <p>The core idea is simple:</p> <ol> <li> <strong>Make a prediction:</strong> Given an input, the network makes an output prediction ($\hat{y}$).</li> <li> <strong>Measure the error:</strong> We compare this prediction to the <em>actual</em> correct answer ($y$). The difference is our error or <strong>loss</strong>.</li> <li> <strong>Adjust the weights:</strong> Based on this error, we slightly tweak the weights and biases throughout the network to make a more accurate prediction next time.</li> <li> <strong>Repeat:</strong> We repeat this process millions of times with many examples.</li> </ol> <h4 id="measuring-error-the-loss-function">Measuring Error: The Loss Function</h4> <p>To measure “how wrong” our network is, we use a <strong>loss function</strong> (or cost function). For example, if we’re predicting a numerical value (like a house price), a common loss function is the Mean Squared Error (MSE): \(L(\hat{y}, y) = (\hat{y} - y)^2\) Here, $\hat{y}$ is our network’s prediction, and $y$ is the true value. Our ultimate goal is to minimize this loss.</p> <h4 id="finding-the-right-path-gradient-descent">Finding the Right Path: Gradient Descent</h4> <p>Imagine the loss function as a mountainous landscape, where valleys represent low loss (good performance) and peaks represent high loss (bad performance). Our goal is to find the lowest point in this landscape.</p> <p>This is where <strong>Gradient Descent</strong> comes in. If you’ve studied calculus, you know that the derivative of a function tells you the slope (or gradient) at any given point. In our multi-dimensional loss landscape, the gradient points in the direction of the steepest ascent. To minimize loss, we want to go in the opposite direction – the direction of the steepest descent!</p> <p>So, we update each weight ($w$) and bias ($b$) by taking a small step in the direction opposite to the gradient of the loss function with respect to that weight/bias: \(w*{new} = w*{old} - \alpha \frac{\partial L}{\partial w\_{old}}\) Here, $\alpha$ is the <strong>learning rate</strong>, a small positive number that controls the size of our steps. If $\alpha$ is too large, we might overshoot the minimum; if it’s too small, learning will be very slow.</p> <h4 id="the-aha-moment-backpropagation">The “Aha!” Moment: Backpropagation</h4> <p>Now, the critical question: how do we calculate $\frac{\partial L}{\partial w}$ for <em>every</em> single weight in the entire network, especially for weights in earlier hidden layers? This is notoriously difficult because a weight in an early layer indirectly affects the final loss through many subsequent layers.</p> <p>This is where <strong>Backpropagation</strong> (short for “backward propagation of errors”) shines. It’s an ingenious algorithm that uses the chain rule from calculus to efficiently calculate the gradient of the loss function with respect to every single weight and bias in the network, propagating the error <em>backward</em> from the output layer through the hidden layers to the input layer.</p> <p>Here’s the simplified intuition:</p> <ol> <li> <strong>Forward Pass:</strong> Data flows from input to output, generating a prediction and a loss.</li> <li> <strong>Backward Pass:</strong> The error signal starts at the output layer. Each neuron in the output layer knows how much it contributed to the overall error.</li> <li> <strong>Distribute Responsibility:</strong> This error is then “back-propagated” to the previous hidden layer. Each neuron in that hidden layer receives an error signal weighted by how much it contributed to the neurons in the next layer. It’s like telling a student, “Your answer was wrong by this much, and you’re responsible for this portion of the mistake.”</li> <li> <strong>Update Weights:</strong> Once each neuron knows its “share” of the error (its gradient), it adjusts its weights and bias using the gradient descent rule.</li> </ol> <p>This iterative process of forward pass, loss calculation, backward pass, and weight update is repeated thousands or millions of times over vast datasets. Each repetition is like a study session for the network, incrementally refining its understanding of the underlying patterns in the data.</p> <h3 id="why-are-neural-networks-so-powerful">Why are Neural Networks so Powerful?</h3> <p>The combination of simple neurons, layered architectures, non-linear activation functions, and the efficient learning algorithm of backpropagation gives neural networks incredible power:</p> <ol> <li> <strong>Feature Learning:</strong> Unlike traditional machine learning where you manually design “features” (e.g., “is there an eye in this image?”), neural networks <em>learn</em> relevant features directly from the raw data. Each hidden layer extracts increasingly abstract and useful representations.</li> <li> <strong>Universal Function Approximators:</strong> Theoretically, a sufficiently large neural network with at least one hidden layer can approximate any continuous function to arbitrary precision. This means they can model incredibly complex, non-linear relationships in data.</li> <li> <strong>Scalability:</strong> With enough data and computational power (GPUs are particularly well-suited for matrix multiplications), neural networks can scale to solve problems of immense complexity that are intractable for other methods.</li> <li> <strong>Adaptability:</strong> Different architectures (like Convolutional Neural Networks for images or Recurrent Neural Networks for sequences) allow them to excel in diverse domains.</li> </ol> <h3 id="beyond-the-basics">Beyond the Basics</h3> <p>What we’ve covered is the bedrock of neural networks, often referred to as <strong>Multilayer Perceptrons (MLPs)</strong> or <strong>feedforward neural networks</strong>. But the field is vast and rapidly evolving! Here are just a few avenues for further exploration:</p> <ul> <li> <strong>Convolutional Neural Networks (CNNs):</strong> Revolutionized computer vision by using specialized “convolutional” layers to automatically detect spatial hierarchies of features.</li> <li> <strong>Recurrent Neural Networks (RNNs):</strong> Designed for sequential data (like text or time series) by having connections that allow information to persist from one step to the next.</li> <li> <strong>Transformers:</strong> The current state-of-the-art for natural language processing, leveraging “attention mechanisms” to weigh the importance of different parts of the input sequence.</li> <li> <strong>Optimizers:</strong> More sophisticated versions of gradient descent (like Adam, RMSprop) that adapt the learning rate during training for faster and more stable convergence.</li> <li> <strong>Regularization Techniques:</strong> Methods like dropout, L1/L2 regularization to prevent networks from “memorizing” the training data (overfitting) and improve their generalization to new, unseen data.</li> </ul> <h3 id="conclusion">Conclusion</h3> <p>From the humble artificial neuron to the complex, multi-layered architectures capable of feats once thought impossible, neural networks are a testament to the power of mimicking nature’s designs. They are the engine behind self-driving cars, personalized recommendations, intelligent assistants, and breakthroughs in medicine.</p> <p>Understanding neural networks isn’t just about memorizing formulas; it’s about grasping an elegant framework for learning that continually amazes me with its potential. The journey from initial input to a sophisticated understanding, guided by the relentless pursuit of minimizing error through backpropagation, is truly captivating.</p> <p>If you’ve followed along, you now have a solid foundation for understanding how these “brains” of AI operate. This is just the beginning of a fascinating journey into deep learning, and I encourage you to build your own simple neural networks, experiment with different parameters, and witness their learning capabilities firsthand. The future of AI is being built on these principles, and your understanding is a crucial step in shaping that future. Keep learning, keep exploring!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>