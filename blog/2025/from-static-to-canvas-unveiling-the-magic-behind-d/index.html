<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> From Static to Canvas: Unveiling the Magic Behind Diffusion Models | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/2025/from-static-to-canvas-unveiling-the-magic-behind-d/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">From Static to Canvas: Unveiling the Magic Behind Diffusion Models</h1> <p class="post-meta"> Created on September 13, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/diffusion-models"> <i class="fa-solid fa-hashtag fa-sm"></i> Diffusion Models</a>   <a href="/blog/blog/tag/generative-ai"> <i class="fa-solid fa-hashtag fa-sm"></i> Generative AI</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/ai-art"> <i class="fa-solid fa-hashtag fa-sm"></i> AI Art</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>As a budding data scientist and machine learning enthusiast, few things have captivated my imagination quite like the recent explosion of generative AI. You’ve seen them, I’m sure: DALL-E, Stable Diffusion, Midjourney – tools that can turn your wildest text prompts into breathtaking visual art. For a long time, these felt like pure magic. But then I started digging, and what I found wasn’t a wizard’s spell, but an elegant dance between mathematics, probability, and neural networks: <strong>Diffusion Models</strong>.</p> <p>If you’re anything like me, you’re probably curious about the “how.” How do these models learn to create? How do they “understand” what a cat wearing a spacesuit looks like? In this post, I want to take you on my personal exploration of Diffusion Models, breaking down their core mechanics in a way that’s both accessible and deep. Think of it as our joint journal entry into the heart of generative AI.</p> <h3 id="the-spark-of-creation-an-intuitive-beginning">The Spark of Creation: An Intuitive Beginning</h3> <p>Imagine you have a beautiful, pristine photograph. Now, imagine someone gradually sprinkles a tiny bit of random noise (like static on an old TV) onto it, then a bit more, and a bit more, until eventually, all you see is pure, undifferentiated static. The original image is completely gone, swallowed by randomness.</p> <p>This “noising” process is easy to do. We know exactly how much noise we’re adding at each step. The real genius of Diffusion Models lies in <em>reversing</em> this process. What if we could learn to take that pure static and, step by tiny step, remove the noise until a clear image emerges? This, in essence, is what a Diffusion Model does: it learns to reverse the entropy, to turn chaos back into order, static back into a masterpiece.</p> <p>It’s like a sculptor who learns by watching a perfect statue slowly crumble into dust. The sculptor doesn’t just learn how to make dust; they learn the <em>precise steps</em> needed to <em>reverse</em> that crumbling, allowing them to eventually sculpt a new statue from a pile of dust.</p> <h3 id="part-1-the-forward-diffusion-process-the-crumbling-statue">Part 1: The Forward Diffusion Process (The Crumbling Statue)</h3> <p>Let’s get a little more technical, but don’t worry, we’ll keep it as clear as possible. The forward process is often called the <strong>noising process</strong>. It’s a fixed Markov chain that gradually adds Gaussian noise to an image $x_0$ over $T$ time steps.</p> <p>At each step $t$, we take the slightly noisy image $x_{t-1}$ and add a small amount of Gaussian noise to get $x_t$. Mathematically, this looks like:</p> \[q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t I)\] <p>Here’s what each part means:</p> <ul> <li>$x_0$: Our original, pristine image.</li> <li>$x_t$: The image at time step $t$, which is slightly noisier than $x_{t-1}$.</li> <li>$\mathcal{N}(x; \mu, \Sigma)$: This denotes a normal (Gaussian) distribution with mean $\mu$ and covariance $\Sigma$.</li> <li>$\beta_t$: A small, pre-defined variance schedule. It’s usually small at the beginning and increases towards the end, meaning we add more noise as time progresses.</li> <li>$I$: The identity matrix, meaning the noise is added independently to each pixel.</li> </ul> <p>This formula essentially says: to get $x_t$, we take a small piece of $x_{t-1}$ (scaled by $\sqrt{1-\beta_t}$) and add a bit of random Gaussian noise (with variance $\beta_t$).</p> <p>One amazing property of this forward process is that we can directly sample $x_t$ from $x_0$ for any arbitrary time step $t$. This means we don’t have to simulate the noise addition step-by-step to get $x_t$. We can jump directly there!</p> <p>Let $\alpha_t = 1 - \beta_t$ and $\bar{\alpha}<em>t = \prod</em>{s=1}^t \alpha_s$. Then, we can directly sample $x_t$ as:</p> \[q(x_t | x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t}x_0, (1-\bar{\alpha}_t)I)\] <p>This equation is crucial! It tells us that we can generate $x_t$ from $x_0$ by:</p> \[x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon\] <p>where $\epsilon \sim \mathcal{N}(0, I)$ is pure Gaussian noise.</p> <p>So, the forward process is simple, controllable, and we know exactly how to get $x_t$ from $x_0$ and some noise $\epsilon$. The goal of the model is to learn how to reverse this.</p> <h3 id="part-2-the-reverse-diffusion-process-the-sculptors-craft">Part 2: The Reverse Diffusion Process (The Sculptor’s Craft)</h3> <p>Now for the hard part, the part that requires machine learning magic! We want to learn how to go from $x_t$ (a noisy image) back to $x_{t-1}$ (a slightly less noisy image). This is called the <strong>reverse diffusion process</strong>.</p> <table> <tbody> <tr> <td>The true reverse probability $q(x_{t-1}</td> <td>x_t)$ is incredibly complex and intractable to compute directly. This is where our neural network comes in. We train a model to approximate this reverse step: $p_\theta(x_{t-1}</td> <td>x_t)$.</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>Crucially, it turns out that if $\beta_t$ is small enough (which it is), $q(x_{t-1}</td> <td>x_t)$ can also be approximated by a Gaussian distribution. This means our model just needs to learn the <em>mean</em> and <em>variance</em> of this Gaussian to go backwards.</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>Amazingly, it’s been shown that this reverse transition $q(x_{t-1}</td> <td>x_t, x_0)$ (if we knew $x_0$) has a mean that depends directly on the <em>noise</em> that was added to get $x_t$. So, instead of trying to predict $x_{t-1}$ directly, our neural network $\epsilon_\theta(x_t, t)$ is trained to predict the <strong>noise component</strong> $\epsilon$ from $x_t$ and the current time step $t$.</td> </tr> </tbody> </table> <p>Let’s elaborate on that: From our forward process, we know $x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon$. We can rearrange this to express $x_0$ in terms of $x_t$ and $\epsilon$: \(x_0 = \frac{1}{\sqrt{\bar{\alpha}_t}}(x_t - \sqrt{1-\bar{\alpha}_t}\epsilon)\)</p> <table> <tbody> <tr> <td>Using this, a lot of mathematical wizardry (which you can find in the original DDPM paper if you’re really keen!), the mean of the reverse step $p_\theta(x_{t-1}</td> <td>x_t)$ can be simplified. It turns out that if our model can accurately predict $\epsilon$ (the noise that was added), we can then calculate a good estimate for $x_{t-1}$.</td> </tr> </tbody> </table> <p>The neural network, often a <strong>U-Net</strong> architecture (more on this in a bit), takes the noisy image $x_t$ and the current time step $t$ as input, and outputs its best guess for the noise $\epsilon$ that was used to get $x_t$ from $x_0$. We call this predicted noise $\epsilon_\theta(x_t, t)$.</p> <p>The objective function, or loss, for training is wonderfully simple: we want the predicted noise to be as close as possible to the actual noise that was added.</p> \[L_t = ||\epsilon - \epsilon_\theta(x_t, t)||^2\] <p>This is just a mean squared error. The model learns to be an expert “noise predictor” at every possible stage of noisiness.</p> <h3 id="training-a-diffusion-model-learning-the-anti-noise">Training a Diffusion Model: Learning the Anti-Noise</h3> <p>So, how do we actually train this $\epsilon_\theta$ network?</p> <ol> <li> <strong>Start with a real image:</strong> Pick an image $x_0$ from your dataset (e.g., a photo of a cat).</li> <li> <strong>Pick a random time step:</strong> Choose a random $t$ between $1$ and $T$ (where $T$ is the total number of steps, typically a few hundred or a thousand).</li> <li> <strong>Generate noise:</strong> Sample some pure Gaussian noise $\epsilon \sim \mathcal{N}(0, I)$.</li> <li> <strong>Create a noisy image:</strong> Use the direct forward process formula to get $x_t$: \(x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon\) Now we have $x_t$, a version of our cat photo with $t$ steps of noise added. We also know the <em>exact</em> noise $\epsilon$ that was used to create it.</li> <li> <strong>Predict the noise:</strong> Feed $x_t$ and $t$ into our neural network $\epsilon_\theta$. The network tries to predict the noise: $\epsilon_\theta(x_t, t)$.</li> <li> <table> <tbody> <tr> <td> <strong>Calculate the loss:</strong> Compare the network’s prediction $\epsilon_\theta(x_t, t)$ with the actual noise $\epsilon$ using the simple mean squared error: $L_t =</td> <td> </td> <td>\epsilon - \epsilon_\theta(x_t, t)</td> <td> </td> <td>^2$.</td> </tr> </tbody> </table> </li> <li> <strong>Update the network:</strong> Use gradient descent to adjust the weights of $\epsilon_\theta$ to minimize this loss.</li> </ol> <p>Repeat these steps millions of times with countless images, and the network slowly but surely learns to accurately predict the noise for any given noisy image $x_t$ at any time step $t$. It becomes incredibly good at “seeing” the original image through the static.</p> <h3 id="generating-new-images-the-masterpiece-unfolds">Generating New Images: The Masterpiece Unfolds</h3> <p>Once our $\epsilon_\theta$ network is trained, the exciting part begins: generating new images! This is the <strong>sampling process</strong>.</p> <ol> <li> <strong>Start with pure noise:</strong> Begin with a completely random Gaussian noise image $x_T \sim \mathcal{N}(0, I)$. This is our blank canvas, our pile of dust.</li> <li> <strong>Iterative Denoising:</strong> Now, we iterate backwards from $t=T$ down to $t=1$: <ul> <li> <strong>Predict the noise:</strong> Use our trained network to predict the noise component $\epsilon_t$ in $x_t$: $\epsilon_t = \epsilon_\theta(x_t, t)$.</li> <li> <strong>Estimate $x_0$ (temporarily):</strong> We can use our predicted noise to make a temporary estimation of the original, clean image $x_0$ at this step: \(\hat{x}_0 = \frac{1}{\sqrt{\bar{\alpha}_t}}(x_t - \sqrt{1-\bar{\alpha}_t}\epsilon_t)\)</li> <li> <strong>Calculate the next, less noisy image $x_{t-1}$:</strong> Using $\hat{x}<em>0$ and some known parameters from the forward process, we can construct $x</em>{t-1}$. A common formulation looks something like this (simplified): \(x_{t-1} = \frac{1}{\sqrt{\alpha_t}} \left(x_t - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}}\epsilon_t\right) + \sigma_t z\) where $\sigma_t$ is a specific variance for the reverse step, and $z \sim \mathcal{N}(0, I)$ is a bit of random noise added back in. This randomness is crucial; without it, the model would always generate the <em>same</em> image from the <em>same</em> starting noise, losing its creative spark.</li> </ul> </li> <li> <strong>The Final Image:</strong> After $T$ steps, we arrive at $x_0$, a brand new, generated image that was conjured from pure static!</li> </ol> <h3 id="the-u-net-the-brain-of-the-operation">The U-Net: The Brain of the Operation</h3> <p>I mentioned the U-Net architecture. Why is it so effective here?</p> <ul> <li> <strong>Encoder-Decoder Structure:</strong> It’s a type of convolutional neural network designed to process images. It has a “contracting path” (encoder) that downsamples the image, extracting high-level features, and an “expanding path” (decoder) that upsamples, reconstructing the image while incorporating these features.</li> <li> <strong>Skip Connections:</strong> This is the “U” part. It directly connects layers from the encoder to corresponding layers in the decoder. This allows the network to combine coarse-grained semantic information (from deep in the encoder) with fine-grained spatial details (from shallow in the encoder), which is vital for precise image reconstruction.</li> <li> <strong>Time Embeddings:</strong> Since the amount of noise and the task of the network changes with time step $t$, we can’t just feed $t$ as a number. Instead, $t$ is typically converted into a high-dimensional vector (a “time embedding”) and added to the intermediate layers of the U-Net. This tells the network “at what point in the denoising process” it is.</li> </ul> <p>The U-Net is perfect for Diffusion Models because it needs to take a noisy image and output another image (the predicted noise) of the exact same size, while understanding both the global context and local details.</p> <h3 id="why-diffusion-models-are-so-powerful">Why Diffusion Models Are So Powerful</h3> <ol> <li> <strong>Excellent Image Quality:</strong> By gradually refining an image, Diffusion Models can produce incredibly high-quality, realistic, and diverse outputs.</li> <li> <strong>Stable Training:</strong> Unlike some other generative models (like GANs) that can be notoriously hard to train, Diffusion Models have a relatively stable training objective (the simple mean squared error).</li> <li> <strong>Flexibility:</strong> They are easily adaptable for conditional generation. Want an image of “a cat astronaut”? You just feed a text embedding (a numerical representation of the text) into the U-Net alongside $x_t$ and $t$. The model learns to generate images that match the provided condition. This is what powers DALL-E and Stable Diffusion!</li> <li> <strong>Mathematical Elegance:</strong> The forward process is well-defined and analytically tractable, making the training process robust.</li> </ol> <h3 id="wrapping-up-from-static-to-canvas">Wrapping Up: From Static to Canvas</h3> <p>So, there you have it. The magic of AI art isn’t magic at all, but a brilliant application of probability, calculus, and deep learning. Diffusion Models are a testament to the power of breaking down a complex problem (generating an image) into many tiny, manageable steps (denoising).</p> <p>It’s been a fascinating journey for me to understand these models, moving from awe to a deeper appreciation for their elegant mechanics. We started with the simple idea of adding noise to an image, then built a neural network to learn how to precisely reverse that noise-adding process. By iteratively removing predicted noise, these models can literally sculpt new images from nothing but pure static.</p> <p>As we continue to push the boundaries of AI, I find immense excitement in understanding the foundational concepts like Diffusion Models. They’re not just tools; they’re windows into new ways of thinking about data, probability, and creation itself. Keep exploring, keep learning, and who knows what beautiful creations you might unleash next!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>