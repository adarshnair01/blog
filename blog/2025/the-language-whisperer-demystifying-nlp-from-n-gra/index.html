<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Language Whisperer: Demystifying NLP from N-grams to Transformers | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/the-language-whisperer-demystifying-nlp-from-n-gra/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Language Whisperer: Demystifying NLP from N-grams to Transformers</h1> <p class="post-meta"> Created on January 20, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/natural-language-processing"> <i class="fa-solid fa-hashtag fa-sm"></i> Natural Language Processing</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   <a href="/blog/blog/tag/python"> <i class="fa-solid fa-hashtag fa-sm"></i> Python</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="the-language-whisperer-demystifying-nlp-from-n-grams-to-transformers">The Language Whisperer: Demystifying NLP from N-grams to Transformers</h2> <p>Hello there, fellow explorers of data and technology! If you’re anything like me, you’re constantly amazed by how seamlessly technology integrates into our lives, often anticipating our needs or understanding our quirky requests. From asking Siri about the weather to having Grammarly polish your essays, or even watching Google Translate conjure up perfect sentences in a foreign language – there’s a quiet revolution happening behind the scenes. This magic, my friends, is largely powered by <strong>Natural Language Processing (NLP)</strong>.</p> <p>NLP is, at its heart, the bridge between human language and computer understanding. It’s the field that gives machines the ability to read, comprehend, and even generate human languages. For someone like me, who’s always been fascinated by both the nuances of language and the power of algorithms, NLP feels like the ultimate intersection. It’s where the art of communication meets the science of computation.</p> <p>In this post, I want to take you on a journey through NLP – from its humble, rule-based beginnings to the astonishing deep learning models that are reshaping our digital world. We’ll peel back the layers of this “magic trick” and see how it all works. Don’t worry, we’ll keep it accessible, even if you’re just starting your data science adventure, but we’ll also go deep enough to appreciate the technical marvels involved.</p> <h3 id="the-early-days-teaching-machines-to-speak-sort-of">The Early Days: Teaching Machines to Speak (Sort Of)</h3> <p>Imagine trying to teach a computer to understand English. Where do you even begin? In the early days, researchers often approached this like creating a massive dictionary and a colossal rulebook.</p> <p><strong>Rule-Based Systems:</strong> Think of a very simple chatbot. It might look for keywords and respond with pre-programmed sentences.</p> <ul> <li>If input contains “hello” or “hi”, respond with “Hello there! How can I help?”</li> <li>If input contains “weather”, respond with “I cannot check the weather right now.”</li> </ul> <p>While seemingly clever, these systems were incredibly brittle. They couldn’t handle synonyms, misspellings, or even slightly different sentence structures. A tiny deviation from the expected input would break them. It was like teaching a child only to understand specific phrases, rather than the concept behind them.</p> <p><strong>Statistical NLP: The Shift to Data:</strong> The next big leap came from realizing that language isn’t just about rules; it’s about patterns and probabilities. Instead of explicitly programming every rule, what if we let the computer <em>learn</em> these patterns from large amounts of text? This gave birth to <strong>Statistical NLP</strong>.</p> <p>One of the foundational concepts here is the <strong>N-gram model</strong>. It’s a fancy name for a simple idea: predicting the next word in a sequence based on the previous $N-1$ words.</p> <ul> <li>A <strong>bigram</strong> (N=2) looks at the previous word to predict the current one. If you see “I am”, what’s the most likely next word? “happy”, “going”, “hungry”?</li> <li>A <strong>trigram</strong> (N=3) looks at the two previous words. “I am very…” - the context narrows down the possibilities.</li> </ul> <p>The probability of a word $w_i$ given the preceding words $(w_{i-N+1}, \dots, w_{i-1})$ can be estimated using basic counts:</p> <table> <tbody> <tr> <td>$P(w_i</td> <td>w_{i-N+1}, \dots, w_{i-1}) = \frac{\text{count}(w_{i-N+1}, \dots, w_i)}{\text{count}(w_{i-N+1}, \dots, w_{i-1})}$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>*For example, to find the probability of “happy” after “I am” ($P(\text{happy}</td> <td>\text{I am})$), you’d count how many times “I am happy” appears in a large text dataset and divide it by how many times “I am” appears.*</td> </tr> </tbody> </table> <p>N-grams were a huge step forward, enabling basic language models for spell-checking and simple speech recognition. However, they faced limitations:</p> <ol> <li> <strong>Sparsity</strong>: What if a sequence of words has never appeared in the training data? The model would assign it a zero probability, which isn’t helpful.</li> <li> <strong>Context Window</strong>: N-grams only look at a very short window of history. They can’t understand long-range dependencies or the deeper meaning of a sentence. “The boy who ate the apple <em>yesterday</em> is hungry today” – a trigram might not connect “boy” to “hungry today”.</li> </ol> <h3 id="enter-machine-learning-learning-from-data-intelligently">Enter Machine Learning: Learning from Data, Intelligently</h3> <p>With the rise of Machine Learning, NLP practitioners started to leverage algorithms like Naïve Bayes, Support Vector Machines (SVMs), and Logistic Regression to tackle tasks like spam detection or sentiment analysis.</p> <p>The key idea was to represent text in a numerical format that these algorithms could understand. One common approach was the <strong>Bag-of-Words (BoW)</strong> model. Imagine taking all the words in a document and throwing them into a “bag,” counting how many times each word appears, and then discarding their order.</p> <ul> <li>Sentence 1: “I love this movie.”</li> <li>Sentence 2: “This movie is great, I love it.”</li> </ul> <p>Using BoW, both sentences might be represented by vectors like <code class="language-plaintext highlighter-rouge">[I:1, love:1, this:1, movie:1]</code> and <code class="language-plaintext highlighter-rouge">[this:1, movie:1, is:1, great:1, I:1, love:1, it:1]</code> respectively. While effective for some tasks, BoW still suffered because it completely ignored word order and semantic meaning. “A dog bit a man” and “A man bit a dog” would have identical BoW representations, despite completely different meanings. Also, words like “good” and “excellent” were treated as distinct, unrelated entities, not as synonyms or words with similar connotations.</p> <h3 id="the-deep-learning-revolution-meaning-in-vectors-and-sequences">The Deep Learning Revolution: Meaning in Vectors and Sequences</h3> <p>The true paradigm shift in NLP came with <strong>Deep Learning</strong>. This is where machines started to develop a more nuanced understanding of language, moving beyond surface-level statistics.</p> <p><strong>1. Word Embeddings: Giving Words Meaning</strong> The real game-changer was the concept of <strong>word embeddings</strong>. Instead of treating each word as an isolated unit, what if we represented words as dense numerical vectors (lists of numbers) in a continuous space? The magic here is that words with similar meanings or that appear in similar contexts would have similar vectors, meaning they’d be “closer” to each other in this high-dimensional space.</p> <p>Models like <strong>Word2Vec</strong> (developed by Google) and <strong>GloVe</strong> (Global Vectors for Word Representation) learned these embeddings by analyzing massive amounts of text. They predict a word’s context based on its neighbors or predict a word given its context.</p> <p>A classic example illustrates the power of embeddings:</p> <ul> <li>Vector(“King”) - Vector(“Man”) + Vector(“Woman”) $\approx$ Vector(“Queen”)</li> </ul> <p>This shows that embeddings can capture complex semantic relationships! We can even quantify how similar two words are using <strong>cosine similarity</strong>:</p> <table> <tbody> <tr> <td>$\text{cosine_similarity}(A, B) = \frac{A \cdot B}{</td> <td> </td> <td>A</td> <td> </td> <td>\cdot</td> <td> </td> <td>B</td> <td> </td> <td>}$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>Here, $A$ and $B$ are the word vectors, $A \cdot B$ is their dot product, and $</td> <td> </td> <td>A</td> <td> </td> <td>$ and $</td> <td> </td> <td>B</td> <td> </td> <td>$ are their magnitudes. A higher cosine similarity (closer to 1) means the words are more semantically similar.</td> </tr> </tbody> </table> <p><strong>2. Recurrent Neural Networks (RNNs): Remembering Sequences</strong> Since language is sequential, deep learning models needed a way to process information over time. <strong>Recurrent Neural Networks (RNNs)</strong> were designed for this. Unlike traditional neural networks, RNNs have “memory” – they pass information from one step to the next in a sequence. This allowed them to understand dependencies between words, even if they weren’t immediately adjacent.</p> <p>However, basic RNNs struggled with “long-term dependencies” (the vanishing gradient problem). Imagine a very long sentence; an RNN might forget information from the beginning by the time it reaches the end. This led to the development of more sophisticated RNN variants:</p> <ul> <li><strong>Long Short-Term Memory (LSTM) networks</strong></li> <li><strong>Gated Recurrent Units (GRUs)</strong></li> </ul> <p>These models introduced “gates” that control what information is remembered or forgotten, allowing them to selectively retain relevant context over long sequences. LSTMs and GRUs were instrumental in tasks like machine translation, where an “encoder” RNN would read the source sentence and generate a context vector, which a “decoder” RNN would then use to generate the target sentence.</p> <h3 id="the-transformer-era-parallel-power-and-attention">The Transformer Era: Parallel Power and Attention</h3> <p>While RNNs with LSTMs/GRUs were powerful, they had a fundamental limitation: they processed information sequentially. This made them slow for very long sequences and difficult to parallelize effectively on modern hardware (like GPUs). The need for a faster, more effective architecture became apparent.</p> <p>Enter the <strong>Attention Mechanism</strong>, first introduced in 2017 with the seminal paper “Attention Is All You Need.” This concept revolutionized NLP. Instead of processing words strictly one after another, attention allows the model to “look at” and “weigh” the importance of different words in the input sequence when processing a particular word.</p> <p><strong>Imagine you’re translating a sentence like “The cat sat on the mat.”</strong> When translating “mat,” the model doesn’t just look at “the” immediately preceding it; it also “pays attention” to “cat” and “sat” to understand the full context.</p> <p>The simplified core idea of attention can be thought of as mapping a <strong>query</strong> (the word we’re currently processing) and a set of <strong>key-value</strong> pairs (all other words in the sequence) to an output. The model calculates a “similarity score” between the query and each key, then uses these scores to create a weighted sum of the values.</p> <p>A simplified version of the Scaled Dot-Product Attention mechanism, central to Transformers, looks like this:</p> <p>$\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$</p> <p>Where:</p> <ul> <li>$Q$ (Query), $K$ (Key), $V$ (Value) are matrices derived from the input embeddings.</li> <li>$Q K^T$ calculates the similarity scores (how much each word should “attend” to others).</li> <li>$\sqrt{d_k}$ is a scaling factor to prevent large dot products from pushing the softmax into regions with tiny gradients.</li> <li>$\text{softmax}$ normalizes these scores into a probability distribution.</li> <li>The result is a weighted sum of the Value vectors, capturing the “attended” context.</li> </ul> <p><strong>The Transformer Architecture:</strong> The Transformer fully embraced the attention mechanism, replacing recurrent layers entirely. It consists of stacked “encoder” and “decoder” blocks, each heavily relying on multiple “self-attention” layers (where queries, keys, and values all come from the same input sequence) and “multi-head attention” (running several attention mechanisms in parallel to capture different aspects of relationships).</p> <p>The key advantages of Transformers:</p> <ol> <li> <strong>Parallelization</strong>: Unlike RNNs, the attention mechanism can compute dependencies between all words in parallel, leading to much faster training times.</li> <li> <strong>Long-Range Dependencies</strong>: Attention can directly connect any two words in a sequence, no matter how far apart, making it excellent at capturing long-range contextual information.</li> </ol> <p><strong>Pre-trained Models: The Age of Transfer Learning</strong> The Transformer architecture paved the way for massive <strong>pre-trained language models</strong>. These models are trained on gigantic text datasets (like the entire internet!) to learn general language understanding. Then, they can be “fine-tuned” for specific NLP tasks with relatively small amounts of task-specific data. This is akin to a student getting a broad education and then specializing in a particular field.</p> <ul> <li> <strong>BERT (Bidirectional Encoder Representations from Transformers)</strong>: Trained by Google, BERT learns context from both the left and right sides of a word simultaneously (bidirectionally). It does this by predicting masked words (like a fill-in-the-blank game) and predicting if two sentences logically follow each other. BERT became a benchmark for many downstream NLP tasks.</li> <li> <strong>GPT (Generative Pre-trained Transformer) series</strong>: Developed by OpenAI, these models are famous for their ability to generate incredibly coherent and contextually relevant text. From writing poetry to answering complex questions, GPT models (like GPT-3 and GPT-4) showcase the generative power of Transformers, often surprising us with their human-like outputs.</li> </ul> <p>These models have truly pushed the boundaries of what’s possible in NLP, achieving state-of-the-art results across a multitude of tasks.</p> <h3 id="nlp-in-action-everyday-marvels">NLP in Action: Everyday Marvels</h3> <p>The advancements in NLP have led to incredible applications that many of us interact with daily:</p> <ul> <li> <strong>Sentiment Analysis</strong>: Determining the emotional tone of text (positive, negative, neutral). Crucial for customer feedback analysis or social media monitoring.</li> <li> <strong>Named Entity Recognition (NER)</strong>: Identifying and classifying named entities in text, like people, organizations, locations, dates, etc. (“Tim Cook visited Apple Inc. headquarters in Cupertino yesterday.”)</li> <li> <strong>Machine Translation</strong>: Instant translation of text or speech, like Google Translate or DeepL.</li> <li> <strong>Text Summarization</strong>: Condensing long documents into shorter, coherent summaries.</li> <li> <strong>Question Answering Systems</strong>: Think search engines that directly answer your questions, or chatbots that provide information.</li> <li> <strong>Chatbots and Virtual Assistants</strong>: Powering conversational AI like Siri, Alexa, and customer service bots.</li> <li> <strong>Spam Detection &amp; Content Moderation</strong>: Filtering unwanted emails or identifying harmful content online.</li> </ul> <h3 id="navigating-the-nuances-challenges-and-ethical-considerations">Navigating the Nuances: Challenges and Ethical Considerations</h3> <p>Despite the incredible progress, NLP is not without its challenges and ethical dilemmas:</p> <ul> <li> <strong>Ambiguity</strong>: Human language is inherently ambiguous. “I saw the man with the telescope.” (Who has the telescope?). Machines struggle with this more than humans.</li> <li> <strong>Sarcasm and Irony</strong>: Detecting subtle nuances like sarcasm or irony is extremely difficult, as models often miss the intended meaning behind the literal words.</li> <li> <strong>Bias in Data</strong>: NLP models learn from the data they are trained on. If this data contains societal biases (e.g., gender stereotypes, racial prejudice), the models will reflect and even amplify those biases. This is a critical ethical challenge that researchers are actively working to address.</li> <li> <strong>Data Privacy</strong>: The large datasets used to train these models often contain sensitive information. Ensuring privacy and responsible data usage is paramount.</li> <li> <strong>Model Interpretability</strong>: Deep learning models, especially large Transformers, can often feel like “black boxes.” Understanding <em>why</em> a model made a particular decision can be challenging, which is crucial in high-stakes applications like medical diagnostics or legal analysis.</li> </ul> <h3 id="my-nlp-journey-and-whats-next">My NLP Journey and What’s Next</h3> <p>My journey into NLP began with a simple curiosity: how do computers make sense of the squiggly lines we call letters and the sounds we make? That curiosity led me down a rabbit hole of N-grams, RNNs, and finally, the incredible world of Transformers. The “aha!” moments, when a complex concept suddenly clicks, are what make this field so exhilarating.</p> <p>The pace of innovation in NLP is breathtaking. What was state-of-the-art just a few years ago might now be considered foundational. Looking ahead, I’m particularly excited about:</p> <ul> <li> <strong>Multimodal NLP</strong>: Combining text with other data types like images and audio to build more holistic understanding (e.g., describing an image accurately).</li> <li> <strong>Explainable AI (XAI) in NLP</strong>: Developing models that can not only make predictions but also explain <em>how</em> they arrived at those predictions, addressing the interpretability challenge.</li> <li> <strong>More Robust and Ethical Models</strong>: Creating models that are less susceptible to biases, more fair, and perform reliably across diverse languages and cultures.</li> <li> <strong>Personalized Language Models</strong>: Imagine models that adapt to your unique speaking or writing style.</li> </ul> <h3 id="conclusion">Conclusion</h3> <p>From the simple counting of N-grams to the intricate attention mechanisms of Transformers, Natural Language Processing has come an astonishingly long way. It’s a field that beautifully marries linguistics, computer science, and statistics, allowing us to build ever more intelligent systems that interact with us in the most natural way possible: through language.</p> <p>Whether you’re aiming to build the next generation of virtual assistants, analyze vast amounts of text data, or simply curious about how machines are learning to talk, NLP offers a rich and rewarding area of study. It’s a field where the future is literally being written, one word embedding, one attention head, one Transformer layer at a time. The possibilities are truly boundless, and I, for one, can’t wait to see (and build!) what comes next.</p> <hr> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>