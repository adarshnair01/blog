<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Gradient Descent: Our Guide Down the Mountain of Machine Learning | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/gradient-descent-our-guide-down-the-mountain-of-ma/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Gradient Descent: Our Guide Down the Mountain of Machine Learning</h1> <p class="post-meta"> Created on February 05, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/optimization"> <i class="fa-solid fa-hashtag fa-sm"></i> Optimization</a>   <a href="/blog/blog/tag/gradient-descent"> <i class="fa-solid fa-hashtag fa-sm"></i> Gradient Descent</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/algorithms"> <i class="fa-solid fa-hashtag fa-sm"></i> Algorithms</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>My journey into machine learning felt a bit like being dropped blindfolded onto a vast, undulating mountain range. Everywhere I looked, there were hills and valleys, and the ultimate goal was to find the lowest point – the global minimum. This “lowest point,” I quickly learned, represents the optimal state for our models, where they make the most accurate predictions with the least error. But how do you find that point when you can’t see the whole landscape?</p> <p>That’s where the magic of <strong>Gradient Descent</strong> came in. It’s not just an algorithm; it’s a fundamental concept, the very backbone of how many machine learning models, from simple linear regression to complex neural networks, learn and improve.</p> <p>Let’s unravel this mountain-climbing analogy and see how Gradient Descent actually works.</p> <h3 id="the-mountain-our-cost-function">The Mountain: Our Cost Function</h3> <p>Imagine you’re building a simple model, say, a linear regression model that predicts house prices based on their size. Your model makes an initial guess, but it’s probably not very good. There’s a difference between your model’s predictions and the actual house prices. This difference is what we call <strong>error</strong>.</p> <p>To make our model better, we need a way to quantify this error across <em>all</em> our predictions. This is where the <strong>cost function</strong> (or loss function) comes in. It’s a single value that tells us “how wrong” our model is overall. Our goal is to minimize this cost function.</p> <p>A common cost function is the <strong>Mean Squared Error (MSE)</strong>. If $h_w(x)$ is our model’s prediction for an input $x$, and $y$ is the actual value, and $m$ is the number of data points, then the MSE is:</p> <p>$J(w) = \frac{1}{2m} \sum_{i=1}^{m} (h_w(x^{(i)}) - y^{(i)})^2$</p> <p>Here, $w$ represents the <em>parameters</em> (or weights/coefficients) of our model. In our house price example, $w$ would include things like the slope and y-intercept of our regression line. Changing these parameters changes our predictions, and thus changes the cost function.</p> <p>Think of $J(w)$ as the altitude at any point on our mountain. Our goal is to find the specific values of $w$ that lead to the lowest possible $J(w)$.</p> <h3 id="the-compass-derivatives-and-slopes">The Compass: Derivatives and Slopes</h3> <p>Okay, so we’re on the mountain (defined by our cost function $J(w)$), and we want to go down. Which way is down? If you’re standing on a hill, you intuitively know which direction is downhill – it’s the direction of the steepest decline.</p> <p>In mathematics, the concept that tells us the “steepness” and “direction” of a function at any given point is the <strong>derivative</strong>.</p> <p>For a function with a single parameter, say $J(w)$, the derivative $\frac{dJ}{dw}$ tells us the slope of the tangent line to the function at that point.</p> <ul> <li>If the derivative is positive, the function is increasing (uphill).</li> <li>If the derivative is negative, the function is decreasing (downhill).</li> <li>If the derivative is zero, we’re at a peak, valley, or saddle point.</li> </ul> <p>So, if we want to go downhill, we need to move in the <em>opposite</em> direction of the slope. If the slope is positive, we subtract a value to move left. If the slope is negative, we subtract a negative value (i.e., add a value) to move right. In both cases, we’re moving towards the minimum.</p> <h3 id="the-gradient-our-multi-dimensional-guide">The Gradient: Our Multi-Dimensional Guide</h3> <p>Most machine learning models don’t just have one parameter; they have many. Our house price model might consider size, number of bedrooms, location, etc., each with its own weight. This means our “mountain” isn’t a simple 2D curve; it’s a multi-dimensional surface in an N-dimensional space.</p> <p>When we have multiple parameters, we can’t just use a single derivative. Instead, we use <strong>partial derivatives</strong>. A partial derivative tells us the slope of the function with respect to one parameter, assuming all other parameters are held constant.</p> <p>The collection of all these partial derivatives, organized into a vector, is called the <strong>gradient</strong> ($\nabla J(w)$).</p> <p>$\nabla J(w) = \begin{pmatrix} \frac{\partial J}{\partial w_0} \ \frac{\partial J}{\partial w_1} \ \vdots \ \frac{\partial J}{\partial w_n} \end{pmatrix}$</p> <p>This gradient vector points in the direction of the <em>steepest ascent</em> on our multi-dimensional mountain. Since we want to go <em>downhill</em>, we move in the exact opposite direction of the gradient.</p> <h3 id="the-algorithm-taking-steps-down">The Algorithm: Taking Steps Down</h3> <p>Now that we know which way is down, how do we actually move? We take small steps. Gradient Descent is an iterative algorithm. Here’s the core idea:</p> <ol> <li> <strong>Start Somewhere</strong>: Initialize your model’s parameters ($w$) with some random values (or zeros). This is like randomly dropping yourself onto the mountain.</li> <li> <strong>Look Downhill</strong>: Calculate the gradient of the cost function $J(w)$ with respect to each parameter. This tells you the direction of steepest ascent.</li> <li> <strong>Take a Step</strong>: Update your parameters by moving in the opposite direction of the gradient.</li> </ol> <p>The update rule looks like this for each parameter $w_j$:</p> <p>$w_j := w_j - \alpha \frac{\partial J}{\partial w_j}$</p> <p>Or, in vector form for all parameters:</p> <p>$w := w - \alpha \nabla J(w)$</p> <p>Let’s break down this crucial equation:</p> <ul> <li>$w$: Our current set of parameters.</li> <li>$\alpha$ (alpha): This is the <strong>learning rate</strong>, a critically important hyperparameter. It determines the size of each step we take down the mountain.</li> <li>$\nabla J(w)$: The gradient vector, pointing uphill.</li> <li>$w - \alpha \nabla J(w)$: Subtracting the scaled gradient moves us downhill.</li> </ul> <h4 id="the-learning-rate-alpha-our-step-size">The Learning Rate ($\alpha$): Our Step Size</h4> <p>The learning rate is paramount. It’s like deciding how big of a step you’ll take each time you move down the mountain.</p> <ul> <li> <strong>Too small $\alpha$</strong>: You’ll take tiny, slow steps. It might take ages to reach the bottom. You could even get stuck or give up before reaching the minimum.</li> <li> <strong>Too large $\alpha$</strong>: You might overshoot the minimum, bounce around erratically, or even climb up the other side of the valley, diverging completely! Imagine leaping off a cliff instead of carefully stepping down.</li> <li> <strong>Just right $\alpha$</strong>: You’ll descend efficiently, finding the minimum in a reasonable amount of time without overshooting.</li> </ul> <p>Choosing the right learning rate is often an art as much as a science, requiring experimentation and sometimes dynamic adjustments (learning rate schedules).</p> <ol> <li> <strong>Repeat</strong>: We repeat steps 2 and 3 many times, iteratively adjusting our parameters, until the cost function stops decreasing significantly, or until it reaches a very small value. This indicates we’ve likely found a minimum.</li> </ol> <h3 id="different-ways-to-descend-batch-stochastic-mini-batch">Different Ways to Descend: Batch, Stochastic, Mini-Batch</h3> <p>When calculating the gradient, we need to consider how much of our data we use for each step. This leads to three main flavors of Gradient Descent:</p> <ol> <li> <strong>Batch Gradient Descent (BGD)</strong>: <ul> <li> <strong>How it works</strong>: Calculates the gradient using <em>all</em> the training examples in each iteration.</li> <li> <strong>Pros</strong>: Provides a very accurate estimate of the true gradient, leading to a smooth descent towards the minimum.</li> <li> <strong>Cons</strong>: Can be very slow and computationally expensive for large datasets, as it needs to process the entire dataset before making a single parameter update.</li> </ul> </li> <li> <strong>Stochastic Gradient Descent (SGD)</strong>: <ul> <li> <strong>How it works</strong>: Calculates the gradient and updates parameters using <em>only one</em> randomly chosen training example at a time.</li> <li> <strong>Pros</strong>: Much faster than BGD, especially for large datasets. Its noisy updates can help escape shallow local minima.</li> <li> <strong>Cons</strong>: The cost function fluctuates a lot (it’s “noisy”) because of the frequent updates based on single examples. It might never truly converge to the exact minimum, but rather oscillate around it.</li> </ul> </li> <li> <strong>Mini-Batch Gradient Descent (MBGD)</strong>: <ul> <li> <strong>How it works</strong>: A compromise between BGD and SGD. It calculates the gradient and updates parameters using a small “mini-batch” of training examples (typically 32 to 512 examples).</li> <li> <strong>Pros</strong>: Combines the benefits of both: faster than BGD, less noisy than SGD. It’s the most common and practical choice for deep learning.</li> <li> <strong>Cons</strong>: Requires choosing the optimal mini-batch size, which is another hyperparameter.</li> </ul> </li> </ol> <h3 id="beyond-the-basics-a-glimpse-at-challenges">Beyond the Basics: A Glimpse at Challenges</h3> <p>While Gradient Descent is incredibly powerful, the “mountain landscape” isn’t always perfectly smooth and convex (like a single bowl). Sometimes, we face challenges:</p> <ul> <li> <strong>Local Minima</strong>: The algorithm might get stuck in a “local minimum” – a valley that’s lower than its immediate surroundings, but not the absolute lowest point (the “global minimum”) on the entire landscape.</li> <li> <strong>Saddle Points</strong>: These are points where the slope is zero, but it’s a minimum in one dimension and a maximum in another. Gradient Descent can get stuck here too.</li> <li> <strong>Vanishing/Exploding Gradients</strong>: Especially in deep neural networks, gradients can become extremely small (vanishing) or extremely large (exploding), making learning very difficult or unstable.</li> </ul> <p>Fortunately, researchers have developed advanced optimization techniques (like Adam, RMSprop, Adagrad) that build upon Gradient Descent to address these issues, allowing our models to navigate even the most treacherous landscapes.</p> <h3 id="why-gradient-descent-is-everywhere">Why Gradient Descent is Everywhere</h3> <p>Gradient Descent, in its various forms, is the workhorse behind countless machine learning algorithms:</p> <ul> <li> <strong>Linear Regression</strong>: Minimizing the MSE.</li> <li> <strong>Logistic Regression</strong>: Minimizing the cross-entropy loss.</li> <li> <strong>Neural Networks</strong>: Training the vast number of weights and biases to learn complex patterns in data.</li> </ul> <p>It’s truly the engine that drives the learning process in much of what we call Artificial Intelligence today.</p> <h3 id="our-descent-complete">Our Descent Complete</h3> <p>From being lost on a conceptual mountain to understanding how to navigate its complex terrain, Gradient Descent provides an elegant and effective solution. It’s a testament to the power of calculus and iterative refinement. By calculating the gradient of our cost function and taking carefully measured steps in the opposite direction, we empower our machines to learn, adapt, and ultimately make sense of the world around them.</p> <p>So next time you marvel at a machine’s ability to recognize a face or translate a language, remember the humble, yet incredibly powerful, journey of Gradient Descent tirelessly guiding it down the mountain of error to find its peak performance.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>