<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Alchemy of Data: Mastering Feature Engineering for Smarter Models | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/the-alchemy-of-data-mastering-feature-engineering/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Alchemy of Data: Mastering Feature Engineering for Smarter Models</h1> <p class="post-meta"> Created on June 11, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/feature-engineering"> <i class="fa-solid fa-hashtag fa-sm"></i> Feature Engineering</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/data-transformation"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Transformation</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>Welcome to another deep dive into the fascinating world of data science. Today, we’re talking about something that often feels like magic, a secret sauce that can catapult your machine learning models from “okay” to “absolutely phenomenal”: <strong>Feature Engineering</strong>.</p> <p>If you’re anything like I was when I first started, you might think that once you’ve collected your data, the hard part is over. Just feed it into an algorithm, right? Well, not quite. I vividly remember training my first predictive model. I threw all the raw data I had at it – numbers, dates, text – expecting groundbreaking results. The model performed… adequately. Good enough, I thought. But then, I stumbled upon the concept of Feature Engineering, and it was like unlocking a hidden superpower. My model’s performance soared, and suddenly, the “why” behind the data became much clearer. It was a true “aha!” moment, and it completely changed how I approach every data science project.</p> <h3 id="the-why-beyond-raw-ingredients">The “Why”: Beyond Raw Ingredients</h3> <p>Imagine you’re a chef. You have a pile of raw ingredients: flour, sugar, eggs, butter. You <em>could</em> just serve them as they are, but no one’s going to be raving about your “raw flour surprise.” To create something delicious like a cake, you need to transform these ingredients: mix them, bake them, perhaps add some frosting. This transformation process – combining, refining, creating new forms – is essentially what Feature Engineering is to data.</p> <p>In the world of machine learning, our “raw ingredients” are the columns in our dataset. Our “recipes” are the algorithms. But if the ingredients aren’t prepared correctly, even the best recipe will fall flat.</p> <p>Let’s take a simple example: predicting house prices. Your raw data might include <code class="language-plaintext highlighter-rouge">number_of_bedrooms</code>, <code class="language-plaintext highlighter-rouge">number_of_bathrooms</code>, and <code class="language-plaintext highlighter-rouge">square_footage</code>. A machine learning model can learn from these. But what if we created new features like:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">bathrooms_per_bedroom</code> = <code class="language-plaintext highlighter-rouge">number_of_bathrooms</code> / <code class="language-plaintext highlighter-rouge">number_of_bedrooms</code> </li> <li> <code class="language-plaintext highlighter-rouge">price_per_square_foot</code> (if we have target data) = <code class="language-plaintext highlighter-rouge">selling_price</code> / <code class="language-plaintext highlighter-rouge">square_footage</code> </li> </ul> <p>Suddenly, these new features might reveal relationships that the raw data couldn’t. A house with a high <code class="language-plaintext highlighter-rouge">bathrooms_per_bedroom</code> might indicate luxury, regardless of the absolute number of bathrooms. Or a low <code class="language-plaintext highlighter-rouge">price_per_square_foot</code> might signal a great deal. Models often struggle to infer these complex, derived relationships from raw data alone. Feature Engineering is about explicitly giving the model these more informative perspectives, helping it “see” the underlying patterns and relationships in the data much more clearly. It’s about teaching the model common sense and domain expertise, one feature at a time.</p> <h3 id="core-techniques-crafting-your-features">Core Techniques: Crafting Your Features</h3> <p>Feature Engineering is both an art and a science. It’s an art because it requires creativity and domain knowledge; it’s a science because it involves systematic experimentation and statistical understanding. Let’s explore some common techniques.</p> <h4 id="1-numerical-features-the-art-of-transformation">1. Numerical Features: The Art of Transformation</h4> <p>Numerical features are already numbers, but they often benefit from transformations to better expose their relationship with the target variable.</p> <ul> <li> <p><strong>Polynomial Features:</strong> Sometimes, the relationship between a feature and the target isn’t linear. For instance, house prices might increase quadratically with <code class="language-plaintext highlighter-rouge">square_footage</code> up to a certain point. We can create polynomial features by raising an existing feature to a power. If you have a feature $x$, you can create $x^2$, $x^3$, etc. For example, <code class="language-plaintext highlighter-rouge">square_footage_squared</code> = <code class="language-plaintext highlighter-rouge">square_footage</code> $\times$ <code class="language-plaintext highlighter-rouge">square_footage</code>. This helps models capture non-linear trends.</p> </li> <li> <p><strong>Interaction Terms:</strong> The effect of one feature on the target might depend on another feature. For instance, the impact of <code class="language-plaintext highlighter-rouge">age</code> on loan default might be different for someone with high <code class="language-plaintext highlighter-rouge">income</code> versus low <code class="language-plaintext highlighter-rouge">income</code>. We can create interaction features by multiplying two or more features together. An interaction term for features $x_1$ and $x_2$ would be $x_1 \times x_2$. Example: <code class="language-plaintext highlighter-rouge">age_x_income</code> = <code class="language-plaintext highlighter-rouge">age</code> $\times$ <code class="language-plaintext highlighter-rouge">income</code>.</p> </li> <li> <strong>Binning (Discretization):</strong> Converting continuous numerical features into categorical bins. For example, <code class="language-plaintext highlighter-rouge">age</code> can be binned into <code class="language-plaintext highlighter-rouge">child</code> (0-12), <code class="language-plaintext highlighter-rouge">teen</code> (13-19), <code class="language-plaintext highlighter-rouge">adult</code> (20-65), <code class="language-plaintext highlighter-rouge">senior</code> (65+). Why do this? <ol> <li> <strong>Robustness to Outliers:</strong> Outliers won’t skew the model as much within a bin.</li> <li> <strong>Capturing Non-Linearity:</strong> It can help capture non-linear relationships if the impact of <code class="language-plaintext highlighter-rouge">age</code> changes drastically across different life stages.</li> <li> <strong>Simplicity:</strong> Sometimes, simpler categories are easier for models to learn from.</li> </ol> </li> <li> <strong>Logarithmic and Other Mathematical Transformations:</strong> If your numerical data is heavily skewed (e.g., <code class="language-plaintext highlighter-rouge">income</code>, <code class="language-plaintext highlighter-rouge">website_visits</code>), a logarithmic transformation can help make its distribution more symmetrical, which often helps models perform better. A common transformation is $\log(x)$ or $\log(x+1)$ to handle zero values. Other transformations might include square root ($\sqrt{x}$), reciprocal ($1/x$), etc., depending on the data’s distribution and domain knowledge.</li> </ul> <h4 id="2-categorical-features-giving-labels-meaning">2. Categorical Features: Giving Labels Meaning</h4> <p>Categorical features represent distinct groups or labels (e.g., <code class="language-plaintext highlighter-rouge">city</code>, <code class="language-plaintext highlighter-rouge">gender</code>, <code class="language-plaintext highlighter-rouge">product_type</code>). Machine learning models, being fundamentally mathematical, usually need numbers.</p> <ul> <li> <p><strong>One-Hot Encoding:</strong> This is one of the most common and robust methods. For each unique category in a feature, we create a new binary (0 or 1) feature. Example: If you have a <code class="language-plaintext highlighter-rouge">color</code> feature with values <code class="language-plaintext highlighter-rouge">red</code>, <code class="language-plaintext highlighter-rouge">blue</code>, <code class="language-plaintext highlighter-rouge">green</code>: | color | is_red | is_blue | is_green | |——-|——–|———|———-| | red | 1 | 0 | 0 | | blue | 0 | 1 | 0 | | green | 0 | 0 | 1 | Why? It prevents the model from assuming an arbitrary ordinal relationship between categories (e.g., that ‘green’ is somehow “greater” than ‘red’).</p> </li> <li> <p><strong>Label Encoding:</strong> Assigns a unique integer to each category. Example: <code class="language-plaintext highlighter-rouge">small</code>=0, <code class="language-plaintext highlighter-rouge">medium</code>=1, <code class="language-plaintext highlighter-rouge">large</code>=2. Why? Useful when there <em>is</em> an inherent order (ordinality) in the categories. However, be cautious: if there’s no inherent order, tree-based models might handle this okay, but linear models might incorrectly interpret the assigned numbers as reflecting magnitude.</p> </li> <li> <p><strong>Target Encoding (Mean Encoding):</strong> Replaces a category with the mean of the target variable for that category. Example: If we’re predicting house price and have a <code class="language-plaintext highlighter-rouge">city</code> feature, we might replace ‘New York’ with the average house price in New York. Why? It captures information directly related to the target. However, it’s prone to <em>data leakage</em> if not done carefully (e.g., using the target mean calculated on the entire dataset, including the validation set, can artificially inflate performance). It’s best performed within a cross-validation loop to avoid this.</p> </li> </ul> <h4 id="3-date-and-time-features-unlocking-temporal-secrets">3. Date and Time Features: Unlocking Temporal Secrets</h4> <p>Dates and times are goldmines for feature engineering, especially for time-series data like sales forecasting or event prediction. Raw datetime stamps (e.g., <code class="language-plaintext highlighter-rouge">2023-10-27 14:30:00</code>) aren’t directly useful for most models.</p> <ul> <li> <strong>Extracting Components:</strong> Break down the datetime into its constituent parts: <ul> <li> <code class="language-plaintext highlighter-rouge">year</code>, <code class="language-plaintext highlighter-rouge">month</code>, <code class="language-plaintext highlighter-rouge">day</code>, <code class="language-plaintext highlighter-rouge">day_of_week</code> (Monday=0, Sunday=6), <code class="language-plaintext highlighter-rouge">hour</code>, <code class="language-plaintext highlighter-rouge">minute</code>, <code class="language-plaintext highlighter-rouge">second</code>.</li> <li> <code class="language-plaintext highlighter-rouge">day_of_year</code>, <code class="language-plaintext highlighter-rouge">week_of_year</code>, <code class="language-plaintext highlighter-rouge">quarter</code>.</li> <li> <code class="language-plaintext highlighter-rouge">is_weekend</code>, <code class="language-plaintext highlighter-rouge">is_holiday</code>. These features capture seasonality and periodicity.</li> </ul> </li> <li> <p><strong>Time Differences:</strong> Calculate the time elapsed <em>since</em> an important event or <em>until</em> a future event. For example, <code class="language-plaintext highlighter-rouge">days_since_last_purchase</code>.</p> </li> <li> <strong>Cyclical Features:</strong> For features like <code class="language-plaintext highlighter-rouge">month</code>, <code class="language-plaintext highlighter-rouge">day_of_week</code>, <code class="language-plaintext highlighter-rouge">hour</code>, where the start and end values are conceptually close (December is followed by January), simple numerical encoding can confuse models. January (1) is far from December (12), but they are adjacent in reality. We can use sine and cosine transformations to represent these cyclical relationships. For an angle (e.g., hour in a 24-hour cycle, or month in a 12-month cycle): $sin(\text{angle}) = \sin(2 \pi \times \frac{\text{value}}{\text{max_value}})$ $cos(\text{angle}) = \cos(2 \pi \times \frac{\text{value}}{\text{max_value}})$ This preserves the cyclical nature and ensures that the model understands that 23:00 is “close” to 00:00.</li> </ul> <h4 id="4-text-features-bridging-words-and-numbers-briefly">4. Text Features: Bridging Words and Numbers (Briefly)</h4> <p>While Natural Language Processing (NLP) is a vast field, basic feature engineering for text often involves converting words into numerical representations.</p> <ul> <li> <strong>Bag-of-Words (BoW):</strong> Counting the occurrences of each word in a document. This creates a vector where each dimension corresponds to a unique word in the vocabulary, and the value is its count.</li> <li> <strong>TF-IDF (Term Frequency-Inverse Document Frequency):</strong> This technique weighs words based on how frequently they appear in a document (TF) and how unique they are across all documents (IDF). It gives more importance to rare, significant words.</li> <li>More advanced techniques like <strong>Word Embeddings</strong> (e.g., Word2Vec, GloVe, BERT) represent words as dense vectors in a continuous space, capturing semantic relationships. These are powerful “features” in themselves, often pre-trained.</li> </ul> <h4 id="5-domain-specific-features-the-experts-touch">5. Domain-Specific Features: The Expert’s Touch</h4> <p>This is where human intelligence truly shines. Domain knowledge – understanding the specifics of your problem area – allows you to create highly relevant and powerful features that might be invisible to generic algorithms.</p> <ul> <li> <strong>Example: Healthcare:</strong> If you’re predicting disease risk, <code class="language-plaintext highlighter-rouge">BMI</code> (Body Mass Index) calculated from <code class="language-plaintext highlighter-rouge">height</code> and <code class="language-plaintext highlighter-rouge">weight</code> ($BMI = \frac{\text{weight (kg)}}{\text{height (m)}^2}$) is a crucial feature derived from raw data.</li> <li> <strong>Example: E-commerce:</strong> <code class="language-plaintext highlighter-rouge">number_of_items_in_cart</code>, <code class="language-plaintext highlighter-rouge">time_on_page</code>, <code class="language-plaintext highlighter-rouge">is_first_time_customer</code>, <code class="language-plaintext highlighter-rouge">average_item_value</code>. These are not directly present in raw transaction logs but can be engineered.</li> <li> <strong>Example: Sports Analytics:</strong> <code class="language-plaintext highlighter-rouge">points_per_game</code>, <code class="language-plaintext highlighter-rouge">assists_to_turnover_ratio</code>.</li> </ul> <p>These features, born from a deep understanding of the problem, often provide the biggest boosts to model performance.</p> <h3 id="best-practices-and-pitfalls">Best Practices and Pitfalls</h3> <p>Feature engineering is a journey, not a destination. Here are some lessons I’ve learned along the way:</p> <ol> <li> <strong>Exploratory Data Analysis (EDA) is King:</strong> Before you even think about creating features, spend time understanding your data. Plot distributions, look for correlations, identify outliers. EDA guides your feature engineering choices, highlighting areas where transformations might be beneficial or where new features could capture hidden relationships.</li> <li> <strong>Start Simple, Iterate:</strong> Don’t try to create dozens of complex features from the get-go. Start with a few promising ones, test their impact, and then iterate. It’s an iterative process of hypothesis, creation, testing, and refinement.</li> <li> <strong>Beware of Data Leakage:</strong> This is a crucial pitfall. Data leakage occurs when your training data includes information that would not be available at prediction time. For instance, if you’re predicting fraud and you engineer a feature <code class="language-plaintext highlighter-rouge">is_transaction_flagged_by_human</code> <em>after</em> the fraud has occurred, your model will look incredibly good on training data, but utterly fail in real-world deployment. Always ensure your features are derived <em>only</em> from information available at the time of prediction.</li> <li> <strong>Feature Scaling:</strong> While not strictly <em>feature engineering</em>, it’s a critical post-FE step. Many machine learning algorithms (like K-Nearest Neighbors, Support Vector Machines, neural networks) are sensitive to the scale of features. Scaling (e.g., using <code class="language-plaintext highlighter-rouge">StandardScaler</code> to have zero mean and unit variance, or <code class="language-plaintext highlighter-rouge">MinMaxScaler</code> to scale between 0 and 1) ensures that no single feature dominates the learning process due to its larger magnitude.</li> <li> <strong>Automation vs. Art:</strong> There are tools and libraries that can automate some aspects of feature engineering (e.g., <code class="language-plaintext highlighter-rouge">featuretools</code>). While these can be great for generating many candidates quickly, they often lack the domain-specific intuition that human experts bring. The best results usually come from a blend of automated approaches and creative, manual crafting.</li> </ol> <h3 id="conclusion-your-creative-superpower">Conclusion: Your Creative Superpower</h3> <p>Feature Engineering is truly one of the most impactful stages in the entire machine learning pipeline. It’s where you, the data scientist, infuse your understanding of the world, your intuition, and your creativity into the raw data, transforming it into a language that algorithms can truly understand and learn from. It’s not just about crunching numbers; it’s about crafting intelligence.</p> <p>My own journey has shown me time and again that a well-engineered set of features can often outperform complex models fed with raw data. It’s a testament to the idea that thoughtful preparation and understanding are just as important, if not more, than the algorithms themselves.</p> <p>So, next time you approach a new dataset, don’t just jump to model training. Pause. Explore. Ask yourself: “What stories are hidden in this data? How can I combine and transform these raw ingredients to tell a richer, more informative story to my model?”</p> <p>Happy engineering! What features will you craft next to make your models smarter?</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>