<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Unveiling the Future, One Noisy Measurement at a Time: A Deep Dive into Kalman Filters | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/unveiling-the-future-one-noisy-measurement-at-a-ti/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Unveiling the Future, One Noisy Measurement at a Time: A Deep Dive into Kalman Filters</h1> <p class="post-meta"> Created on April 24, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/kalman-filters"> <i class="fa-solid fa-hashtag fa-sm"></i> Kalman Filters</a>   <a href="/blog/blog/tag/state-estimation"> <i class="fa-solid fa-hashtag fa-sm"></i> State Estimation</a>   <a href="/blog/blog/tag/time-series"> <i class="fa-solid fa-hashtag fa-sm"></i> Time Series</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/control-systems"> <i class="fa-solid fa-hashtag fa-sm"></i> Control Systems</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hello fellow data adventurers!</p> <p>Today, I want to talk about something that, when I first encountered it, felt like pure magic. It’s a tool that quietly powers everything from the Apollo moon missions to your smartphone’s GPS, from self-driving cars to predicting stock prices. I’m talking about the <strong>Kalman Filter</strong>.</p> <p>As data scientists, we often grapple with imperfect data. Our measurements are rarely pristine; they’re riddled with noise, errors, and uncertainty. Imagine you’re trying to track a drone in the sky, but your radar gives slightly different readings each time, even if the drone is perfectly still. Or perhaps you’re building a model to predict a stock price, but daily fluctuations make it impossible to get a “true” reading. How do you find the signal amidst all that noise? How do you make the <em>best possible estimate</em> of what’s truly happening, and predict what will happen next, even with shaky information?</p> <p>This, my friends, is where the Kalman Filter shines. It’s an elegant, recursive algorithm that takes a series of noisy measurements and produces an estimate of the system’s true state that is more accurate than any single measurement alone. It does this by combining two things: a prediction based on the system’s dynamics, and a correction based on actual measurements. Think of it as a relentless optimist who makes a best guess, then humbly adjusts that guess based on new evidence, constantly refining its understanding of reality.</p> <h3 id="the-core-idea-predict-then-update">The Core Idea: Predict, then Update</h3> <p>At its heart, the Kalman Filter operates in a continuous loop, cycling through two main phases: <strong>Predict</strong> and <strong>Update</strong>.</p> <ol> <li> <strong>Predict:</strong> Based on our <em>previous best estimate</em> of the system’s state and our understanding of how the system <em>behaves</em> (its dynamics), we predict what the system’s state will be at the next time step. We also estimate how uncertain we are about this prediction.</li> <li> <strong>Update:</strong> When a new measurement arrives, we compare it to our prediction. We then use this new information to refine our state estimate, correcting any discrepancies. Crucially, the filter weighs how much to trust our prediction versus the new measurement based on their respective uncertainties.</li> </ol> <p>This dance between prediction and correction is what makes the Kalman Filter so robust and powerful. It’s like having a crystal ball that gets clearer with every new piece of information you feed it.</p> <h3 id="why-is-this-so-hard-the-problem-of-noise">Why is this so hard? The Problem of Noise</h3> <p>Before we dive into the math, let’s understand <em>why</em> this problem is tricky. When we talk about a “system’s state,” we mean all the variables that describe it at a given time. For a drone, this might be its position ($x, y, z$) and its velocity ($\dot{x}, \dot{y}, \dot{z}$).</p> <p>We have two main sources of uncertainty:</p> <ul> <li> <strong>Process Noise:</strong> This is the noise inherent in the system itself. Our model of how the drone moves might not be perfect (wind gusts, minor engine fluctuations, etc.). The drone might not follow our predicted trajectory perfectly. This introduces uncertainty into our <em>prediction</em>.</li> <li> <strong>Measurement Noise:</strong> This is the noise in our sensors or observation equipment. Our radar isn’t perfectly accurate; it adds random errors to its readings. This introduces uncertainty into our <em>measurements</em>.</li> </ul> <p>The Kalman Filter’s genius lies in how it systematically handles these uncertainties, represented by <strong>covariance matrices</strong>, to produce the optimal estimate.</p> <h3 id="the-mathematics-of-estimation-lets-get-our-hands-dirty">The Mathematics of Estimation: Let’s Get Our Hands Dirty!</h3> <p>Don’t worry, we’ll take it step by step. We’ll represent our system’s state as a vector $\mathbf{x}$. For instance, for our 1D car tracking example, $\mathbf{x}$ could be $[\text{position}, \text{velocity}]^T$. Our uncertainty about this state is captured by a <strong>covariance matrix</strong> $\mathbf{P}$. A larger $\mathbf{P}$ means more uncertainty.</p> <h4 id="phase-1-the-predict-step">Phase 1: The Predict Step</h4> <p>In this step, we project our current state estimate forward in time.</p> <ol> <li> <p><strong>Project the State Estimate:</strong> We use our system’s dynamic model to predict the next state.</p> <p>$\hat{\mathbf{x}}<em>k^{-} = \mathbf{F}_k \hat{\mathbf{x}}</em>{k-1} + \mathbf{B}_k \mathbf{u}_k$</p> <p>Let’s break this down:</p> <ul> <li>$\hat{\mathbf{x}}<em>k^{-}$: This is our _a priori</em> (predicted) state estimate at time step $k$. The “hat” means it’s an estimate, and the superscript “-“ means it’s <em>before</em> incorporating the measurement at time $k$.</li> <li>$\hat{\mathbf{x}}<em>{k-1}$: Our _a posteriori</em> (updated) state estimate from the previous time step, $k-1$.</li> <li>$\mathbf{F}<em>k$: The <strong>state transition matrix</strong>. This matrix describes how the state evolves from $k-1$ to $k$ in the _absence</em> of any external forces. If our state is just position and velocity, $\mathbf{F}_k$ would propagate position based on velocity.</li> <li>$\mathbf{B}_k$: The <strong>control input matrix</strong>. This matrix relates optional control inputs to the state.</li> <li>$\mathbf{u}_k$: The <strong>control input vector</strong>. This represents any known external forces acting on the system (e.g., if we actively accelerate the car).</li> </ul> <p>Essentially, this equation is saying: “My next estimated state is based on where I thought I was, plus how I expect the system to move, plus any known external influences.”</p> </li> <li> <p><strong>Project the Error Covariance:</strong> Just as our state estimate evolves, so does our uncertainty about it. Our prediction isn’t perfect; it introduces more uncertainty.</p> <p>$\mathbf{P}<em>k^{-} = \mathbf{F}_k \mathbf{P}</em>{k-1} \mathbf{F}_k^T + \mathbf{Q}_k$</p> <ul> <li>$\mathbf{P}<em>k^{-}$: The _a priori</em> error covariance matrix for the predicted state. It represents our uncertainty <em>before</em> we see the measurement.</li> <li>$\mathbf{P}<em>{k-1}$: The _a posteriori</em> error covariance matrix from the previous step.</li> <li>$\mathbf{Q}<em>k$: The <strong>process noise covariance matrix</strong>. This is a crucial term! It quantifies the uncertainty we introduce _in our prediction itself</em>. How much could the drone deviate from its predicted path due to wind? That’s process noise. A larger $\mathbf{Q}_k$ means we’re less confident in our model’s prediction.</li> </ul> <p>This equation says: “My uncertainty in the next state is based on how my previous uncertainty propagated through the system, plus the inherent uncertainty from my system’s dynamics.”</p> </li> </ol> <h4 id="phase-2-the-update-step">Phase 2: The Update Step</h4> <p>Now, a new measurement $\mathbf{z}_k$ arrives. This is our chance to correct our prediction.</p> <ol> <li> <p><strong>Calculate the Kalman Gain:</strong> This is the heart of the update step, and it’s where the “magic” really happens. The Kalman Gain ($\mathbf{K}_k$) determines how much we trust the new measurement versus our current prediction.</p> <p>$\mathbf{K}_k = \mathbf{P}_k^{-} \mathbf{H}_k^T (\mathbf{H}_k \mathbf{P}_k^{-} \mathbf{H}_k^T + \mathbf{R}_k)^{-1}$</p> <ul> <li>$\mathbf{K}_k$: The <strong>Kalman Gain matrix</strong>. Its value will be between 0 and 1 (conceptually, for each component), determining the “blend” between prediction and measurement.</li> <li>$\mathbf{H}_k$: The <strong>measurement matrix</strong>. This matrix transforms our state estimate into the measurement space. For example, if our state is position and velocity, but our sensor only measures position, $\mathbf{H}_k$ would extract just the position component.</li> <li>$\mathbf{R}_k$: The <strong>measurement noise covariance matrix</strong>. This describes the uncertainty in our actual measurements. How accurate is our radar? A larger $\mathbf{R}_k$ means our measurements are noisier, and we should trust them less.</li> </ul> <p>Think about the ratio in the Kalman Gain equation:</p> <ul> <li>If $\mathbf{R}_k$ is very small (accurate measurements), then $\mathbf{R}_k$ dominates the denominator, making the inverse term large. This leads to a larger $\mathbf{K}_k$, meaning we trust the measurement more.</li> <li>If $\mathbf{P}_k^{-}$ is very small (confident prediction), then the whole term $\mathbf{P}_k^{-} \mathbf{H}_k^T$ gets smaller, leading to a smaller $\mathbf{K}_k$, meaning we trust our prediction more.</li> </ul> <p>The Kalman Gain beautifully balances our confidence in our prediction with our confidence in the incoming measurement.</p> </li> <li> <p><strong>Update the State Estimate:</strong> Now we incorporate the actual measurement to refine our state estimate.</p> <p>$\hat{\mathbf{x}}_k = \hat{\mathbf{x}}_k^{-} + \mathbf{K}_k (\mathbf{z}_k - \mathbf{H}_k \hat{\mathbf{x}}_k^{-})$</p> <ul> <li>$\hat{\mathbf{x}}<em>k$: Our _a posteriori</em> (updated) state estimate at time $k$. This is our new “best guess” for the true state.</li> <li>$\mathbf{z}_k$: The actual measurement received at time $k$.</li> <li>$(\mathbf{z}<em>k - \mathbf{H}_k \hat{\mathbf{x}}_k^{-})$: This is the <strong>measurement residual</strong> or <strong>innovation</strong>. It’s the difference between what we actually measured ($\mathbf{z}_k$) and what we _predicted</em> we would measure ($\mathbf{H}_k \hat{\mathbf{x}}_k^{-}$). It’s the “surprise” factor.</li> </ul> <p>This equation says: “My new best estimate is my prediction, plus a fraction (determined by Kalman Gain) of the difference between what I measured and what I expected to measure.”</p> </li> <li> <p><strong>Update the Error Covariance:</strong> Finally, we update our uncertainty. Because we’ve incorporated a new measurement, our uncertainty should decrease (or at least not increase if the measurement was completely uninformative).</p> <p>$\mathbf{P}_k = (\mathbf{I} - \mathbf{K}_k \mathbf{H}_k) \mathbf{P}_k^{-}$</p> <ul> <li>$\mathbf{P}<em>k$: The _a posteriori</em> error covariance matrix. This is our updated uncertainty.</li> <li>$\mathbf{I}$: The identity matrix.</li> </ul> <p>This equation shows that by incorporating the measurement, we reduce our uncertainty in the state estimate. A larger Kalman Gain (meaning we trusted the measurement more) will lead to a greater reduction in uncertainty.</p> </li> </ol> <p>And that’s it! We now have our updated state estimate $\hat{\mathbf{x}}<em>k$ and its associated uncertainty $\mathbf{P}_k$. These will become $\hat{\mathbf{x}}</em>{k-1}$ and $\mathbf{P}_{k-1}$ for the next iteration, and the loop continues.</p> <h3 id="a-simple-analogy-tracking-your-weight">A Simple Analogy: Tracking Your Weight</h3> <p>Imagine you’re trying to track your “true” weight, but your scale is a bit finicky (measurement noise), and your weight fluctuates naturally day-to-day even with consistent habits (process noise).</p> <ul> <li> <strong>Predict:</strong> You weigh yourself today. You estimate you gain 0.1kg per week based on your diet and exercise. So, you predict your weight next week will be (current weight + 0.1kg). You also know your prediction isn’t perfect, so you have some uncertainty.</li> <li> <strong>Update:</strong> Next week, you step on the scale. It shows a new measurement. <ul> <li>If your scale is very accurate (low $\mathbf{R}$), and your prediction was quite uncertain (high $\mathbf{P}^-$), the Kalman Filter will lean heavily on the new scale reading.</li> <li>If your scale is notoriously inaccurate (high $\mathbf{R}$), but you’ve been very consistent with your routine and are confident in your prediction (low $\mathbf{P}^-$), the Kalman Filter will give less weight to the new scale reading and stick closer to your prediction.</li> </ul> </li> <li>The Kalman Gain ($\mathbf{K}_k$) essentially tells you how much to adjust your predicted weight based on the scale’s reading, considering both its reliability and your confidence in your prediction. The result is a more stable, “truer” weight trend than just looking at the wobbly daily scale readings.</li> </ul> <h3 id="why-is-it-so-powerful">Why is it so powerful?</h3> <ol> <li> <strong>Optimal Estimation:</strong> Under the assumptions of linearity and Gaussian noise, the Kalman Filter is the <strong>optimal linear estimator</strong>. No other linear estimator can produce a more accurate estimate.</li> <li> <strong>Handles Uncertainty Explicitly:</strong> It doesn’t just give you an estimate; it gives you the uncertainty of that estimate, which is invaluable for decision-making.</li> <li> <strong>Real-Time Processing:</strong> Its recursive nature means it only needs the previous state and the current measurement, making it ideal for real-time applications where data arrives continuously.</li> <li> <strong>Handles Missing Data:</strong> If a measurement is missed, you simply skip the update step and continue with your prediction.</li> </ol> <h3 id="where-youll-find-it">Where You’ll Find It</h3> <p>The Kalman Filter’s influence is vast:</p> <ul> <li> <strong>Aerospace:</strong> Guiding spacecraft, missiles, and aircraft (e.g., Apollo navigation, F-35 fighter jets).</li> <li> <strong>Robotics:</strong> For localization and mapping (SLAM - Simultaneous Localization and Mapping).</li> <li> <strong>Automotive:</strong> In self-driving cars, fusing data from radar, lidar, and cameras to understand the car’s position and the environment.</li> <li> <strong>GPS:</strong> Filtering noisy satellite signals to pinpoint your exact location.</li> <li> <strong>Finance &amp; Economics:</strong> State-space models using Kalman Filters to estimate latent variables like “true” inflation or market volatility.</li> <li> <strong>Signal Processing:</strong> Denoising audio or sensor data.</li> </ul> <h3 id="beyond-the-linear-extensions">Beyond the Linear: Extensions</h3> <p>The standard Kalman Filter assumes that the system dynamics ($\mathbf{F}, \mathbf{B}$) and measurement relationships ($\mathbf{H}$) are linear, and that the noise is Gaussian. What if they’re not? That’s where its relatives come in:</p> <ul> <li> <strong>Extended Kalman Filter (EKF):</strong> Linearizes the non-linear functions around the current state estimate using Taylor series expansion. It’s widely used but can suffer from linearization errors.</li> <li> <strong>Unscented Kalman Filter (UKF):</strong> Uses a deterministic sampling approach (unscented transform) to capture the true mean and covariance of a non-linear transformation more accurately, often performing better than EKF for highly non-linear systems.</li> <li> <strong>Particle Filters:</strong> For highly non-linear and non-Gaussian systems, using a set of “particles” to represent the probability distribution.</li> </ul> <p>These advanced filters continue the legacy of the original Kalman Filter, extending its power to even more complex real-world scenarios.</p> <h3 id="my-takeaway-and-your-next-step">My Takeaway and Your Next Step</h3> <p>Learning about Kalman Filters was a true “aha!” moment for me. It transformed my understanding of how we can extract meaningful insights from inherently noisy, imperfect data. It’s a testament to the power of mathematical modeling and Bayesian inference, showing how we can continually refine our understanding of the world with each new piece of information.</p> <p>If you’re intrigued, I highly recommend diving deeper. There are fantastic resources online, including interactive visualizations and Python libraries (like <code class="language-plaintext highlighter-rouge">filterpy</code> or <code class="language-plaintext highlighter-rouge">scipy.signal.lfilter</code> for basic filtering concepts). Try implementing a simple 1D Kalman Filter for tracking position and velocity – you’ll see its elegance come alive!</p> <p>The Kalman Filter isn’t just an algorithm; it’s a philosophy of embracing uncertainty, making the best possible guess, and constantly learning from reality. And in the world of data science, that’s a philosophy we can all live by.</p> <p>Happy estimating!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>