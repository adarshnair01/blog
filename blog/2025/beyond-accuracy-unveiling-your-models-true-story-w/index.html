<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Beyond Accuracy: Unveiling Your Model's True Story with ROC and AUC | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/beyond-accuracy-unveiling-your-models-true-story-w/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Beyond Accuracy: Unveiling Your Model's True Story with ROC and AUC</h1> <p class="post-meta"> Created on June 28, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/classification"> <i class="fa-solid fa-hashtag fa-sm"></i> Classification</a>   <a href="/blog/blog/tag/model-evaluation"> <i class="fa-solid fa-hashtag fa-sm"></i> Model Evaluation</a>   <a href="/blog/blog/tag/roc-curve"> <i class="fa-solid fa-hashtag fa-sm"></i> ROC Curve</a>   <a href="/blog/blog/tag/auc"> <i class="fa-solid fa-hashtag fa-sm"></i> AUC</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>As I reflect on my journey through the fascinating world of machine learning, I vividly remember a moment of profound confusion. I had just built my first binary classification model – something simple, like predicting whether a customer would churn or not. I proudly showed off an accuracy of 95% to a mentor, expecting praise. Instead, I got a quizzical look. “That’s great,” he said, “but what if only 1% of your customers actually churn?”</p> <p><em>Boom.</em> My world of simple accuracy shattered. My model could simply predict “no churn” for everyone and still achieve 99% accuracy! It was a critical lesson: a single metric, especially accuracy, often paints an incomplete, sometimes misleading, picture of a model’s true capabilities. This is where the magic of ROC curves and AUC scores stepped in, offering a much richer, more nuanced perspective.</p> <h3 id="the-foundation-understanding-binary-classification-outcomes">The Foundation: Understanding Binary Classification Outcomes</h3> <p>Before we dive into the elegance of ROC and AUC, let’s quickly lay the groundwork for binary classification. Imagine our model is trying to classify something into one of two categories – “positive” (e.g., customer churns, a patient has a disease, an email is spam) or “negative” (customer stays, patient is healthy, email is not spam).</p> <p>When our model makes a prediction, there are four possible outcomes, often summarized in a “confusion matrix”:</p> <ol> <li> <strong>True Positive (TP):</strong> The model correctly predicted a positive case. (e.g., predicted churn, actual churn)</li> <li> <strong>True Negative (TN):</strong> The model correctly predicted a negative case. (e.g., predicted no churn, actual no churn)</li> <li> <strong>False Positive (FP):</strong> The model incorrectly predicted a positive case when it was actually negative. (Type I error - e.g., predicted churn, but customer stayed)</li> <li> <strong>False Negative (FN):</strong> The model incorrectly predicted a negative case when it was actually positive. (Type II error - e.g., predicted no churn, but customer churned)</li> </ol> <p>Most classification models don’t just spit out “positive” or “negative.” Instead, they output a <em>probability</em> (e.g., “70% chance of churn”). We then apply a <strong>threshold</strong> (typically 0.5) to convert this probability into a final classification: if probability &gt; threshold, predict positive; otherwise, predict negative. This threshold is key to understanding ROC curves.</p> <h3 id="the-rates-that-matter-tpr-and-fpr">The Rates That Matter: TPR and FPR</h3> <p>With our four outcomes defined, we can now define two crucial rates that form the backbone of the ROC curve:</p> <ol> <li> <p><strong>True Positive Rate (TPR)</strong>, also known as <strong>Sensitivity</strong> or <strong>Recall</strong>: This metric tells us, “Out of all the <em>actual positive</em> cases, how many did our model correctly identify?” \(TPR = \frac{TP}{TP + FN}\) A high TPR means our model is good at catching positives. If we’re building a model to detect a rare disease, we want a high TPR so we don’t miss many patients who actually have it.</p> </li> <li> <p><strong>False Positive Rate (FPR)</strong>: This metric tells us, “Out of all the <em>actual negative</em> cases, how many did our model <em>incorrectly</em> identify as positive?” \(FPR = \frac{FP}{FP + TN}\) You might also see <strong>Specificity</strong>, which is $ Specificity = \frac{TN}{TN + FP} $. Notice that $ FPR = 1 - Specificity $. A low FPR is generally desirable. If our disease detection model has a high FPR, it means many healthy people are being incorrectly told they have the disease, leading to unnecessary stress and further testing.</p> </li> </ol> <p>Here’s the rub: TPR and FPR often have an inverse relationship. If you want to catch <em>all</em> positive cases (maximize TPR), you might have to lower your prediction threshold, which will inevitably lead to more false alarms (increase FPR). Conversely, if you want <em>no</em> false alarms (minimize FPR), you’ll likely miss some actual positive cases (decrease TPR). It’s a fundamental trade-off.</p> <h3 id="the-roc-curve-visualizing-the-trade-off">The ROC Curve: Visualizing the Trade-off</h3> <p>The <strong>Receiver Operating Characteristic (ROC) curve</strong> is a brilliant graphical representation that illustrates this TPR vs. FPR trade-off <em>across all possible classification thresholds</em>.</p> <p>Imagine our model gives probabilities for each prediction. Instead of picking one threshold (like 0.5), we can systematically test <em>every possible threshold</em> from 0 to 1. For each threshold, we calculate the corresponding TPR and FPR.</p> <ul> <li> <strong>How it’s built:</strong> <ul> <li>Start with a very high threshold (e.g., 1.0). At this threshold, the model predicts very few positives (only those with 100% probability). This will likely result in a very low TPR (missing most actual positives) and a very low FPR (almost no false alarms). This point will be near (0,0) on the graph.</li> <li>Gradually decrease the threshold (e.g., 0.9, 0.8, …, 0.1, 0.0). As the threshold drops, the model becomes more lenient, predicting more positives.</li> <li>With each decrease, both TPR (we catch more true positives) and FPR (we also create more false positives) generally increase.</li> <li>When the threshold reaches 0, the model predicts every case as positive. This gives a TPR of 1 (caught all positives) and an FPR of 1 (incorrectly labeled all negatives as positive). This point will be at (1,1) on the graph.</li> </ul> </li> </ul> <p>By plotting these (FPR, TPR) pairs for every threshold, we draw the ROC curve.</p> <p><strong>Interpreting the ROC Curve:</strong></p> <ul> <li> <strong>X-axis:</strong> False Positive Rate (FPR)</li> <li> <strong>Y-axis:</strong> True Positive Rate (TPR)</li> </ul> <p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/e/e0/Roc_curve.svg/450px-Roc_curve.svg.png" width="400"> <em>Image source: Wikipedia</em></p> <ul> <li> <strong>The Diagonal Line (Dashed Line):</strong> This represents a random classifier. If your model’s ROC curve follows this line, it’s no better than flipping a coin. For every 1% increase in TPR, you get a 1% increase in FPR.</li> <li> <strong>The Perfect Classifier (Top-Left Corner):</strong> A perfect model would have a TPR of 1 and an FPR of 0 for some threshold. Its curve would shoot straight up the y-axis to (0,1) and then across to (1,1). This is rarely achievable in the real world.</li> <li> <strong>Good vs. Bad Models:</strong> A good model’s ROC curve will hug the top-left corner as much as possible, meaning it achieves high TPR with low FPR. Curves that are closer to the top-left are better. If your curve falls below the diagonal line, your model is performing worse than random guessing – you might even be able to simply invert its predictions to make it useful!</li> </ul> <p>The ROC curve gives you a powerful visual tool to understand your model’s intrinsic ability to distinguish between positive and negative classes, regardless of the specific threshold chosen. It shows you the entire spectrum of its performance, allowing you to pick a threshold that aligns with your specific business needs (e.g., prioritizing high TPR even if it means higher FPR, or vice-versa).</p> <h3 id="auc-the-single-number-summary">AUC: The Single Number Summary</h3> <p>While the ROC curve is incredibly informative, sometimes we need a single number to compare different models or model versions quickly. This is where the <strong>Area Under the ROC Curve (AUC)</strong> comes into play.</p> <p><strong>AUC is quite literally the area underneath the ROC curve.</strong></p> \[\text{AUC} = \int\_{0}^{1} \text{TPR}(FPR) \, d(FPR)\] <p>It quantifies the entire 2D area underneath the curve from (0,0) to (1,1).</p> <p><strong>Interpreting AUC Score:</strong></p> <p>The AUC score has a very intuitive probabilistic interpretation: <strong>AUC represents the probability that a randomly chosen positive example will be ranked higher (assigned a higher probability of being positive) than a randomly chosen negative example.</strong></p> <ul> <li> <strong>AUC = 1.0:</strong> A perfect model. It can perfectly distinguish between positive and negative classes.</li> <li> <strong>AUC = 0.5:</strong> A random model. It’s no better than guessing. The model assigns similar probabilities to both positive and negative examples.</li> <li> <strong>AUC &lt; 0.5:</strong> Worse than random. This suggests the model is making systematic errors and could likely be improved by simply inverting its predictions (if it predicts P with 0.8, treat it as N with 0.2).</li> </ul> <p><strong>Why is AUC so important and widely used?</strong></p> <ol> <li> <strong>Threshold-Independent:</strong> Unlike accuracy, precision, or recall (which require picking a specific threshold), AUC evaluates the model’s performance across <em>all possible thresholds</em>. This gives you a holistic view of the model’s discriminative power, free from the arbitrary choice of a single decision boundary.</li> <li> <strong>Robust to Class Imbalance:</strong> This is HUGE. Remember my mentor’s question about 1% churn? AUC shines here. If 99% of cases are negative, a model that always predicts “negative” will have 99% accuracy. Its ROC curve would be close to the diagonal, and its AUC would be around 0.5. A good model, even if it makes a few mistakes on the common class, will have a high AUC if it successfully ranks the rare positive cases higher than the negative ones. It doesn’t get fooled by skewed class distributions.</li> <li> <strong>Scale-Invariant:</strong> AUC measures the model’s ability to rank predictions correctly, not their absolute probability values. If you re-calibrate your model’s output probabilities (e.g., make all predictions slightly higher or lower) but maintain their relative order, the AUC will remain the same.</li> </ol> <h3 id="a-practical-glimpse-python-example">A Practical Glimpse (Python Example)</h3> <p>In practice, calculating ROC and AUC is straightforward with libraries like scikit-learn. You typically need the true labels (<code class="language-plaintext highlighter-rouge">y_true</code>) and the predicted probabilities (<code class="language-plaintext highlighter-rouge">y_score</code>) for the positive class.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_curve</span><span class="p">,</span> <span class="n">roc_auc_score</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Assume y_true contains the actual labels (0 or 1)
# Assume y_score contains the predicted probabilities for the positive class
# Example:
# y_true = [0, 0, 1, 1, 0, 1, 0, 1]
# y_score = [0.1, 0.4, 0.35, 0.8, 0.2, 0.9, 0.5, 0.7]
</span>
<span class="c1"># Calculate FPR, TPR, and thresholds
</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> <span class="nf">roc_curve</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_score</span><span class="p">)</span>

<span class="c1"># Calculate the AUC score
</span><span class="n">auc_score</span> <span class="o">=</span> <span class="nf">roc_auc_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_score</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Model AUC: </span><span class="si">{</span><span class="n">auc_score</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Plotting the ROC curve
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">darkorange</span><span class="sh">'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">'</span><span class="s">ROC curve (AUC = </span><span class="si">{</span><span class="n">auc_score</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">navy</span><span class="sh">'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Random Classifier</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">False Positive Rate</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">True Positive Rate</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Receiver Operating Characteristic (ROC) Curve</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="sh">'</span><span class="s">lower right</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <p>This snippet demonstrates how easily you can obtain these metrics and visualize the curve, allowing for quick and insightful model comparisons.</p> <h3 id="when-to-use-rocauc-and-when-to-consider-alternatives">When to Use ROC/AUC (and when to consider alternatives)</h3> <p>ROC and AUC are excellent choices for evaluating classification models, especially when:</p> <ul> <li> <strong>You have imbalanced datasets:</strong> As discussed, they provide a reliable metric despite skewed class distributions.</li> <li> <strong>The costs of False Positives and False Negatives are unknown or vary:</strong> Since ROC shows all trade-offs, you can decide the optimal operating point later.</li> <li> <strong>You need a general measure of a model’s discriminative ability:</strong> AUC gives a single score of how well a model separates the classes.</li> </ul> <p>However, no metric is a silver bullet. If your dataset is <em>extremely</em> imbalanced (e.g., 1:1000 ratio) and you are <em>primarily</em> interested in the performance on the positive class (especially if False Positives are very costly), the <strong>Precision-Recall (PR) curve</strong> might be more informative. AUC for the PR curve (AP) focuses more on the trade-off between precision and recall, highlighting performance for the minority class more directly. But for most general classification tasks, ROC and AUC are powerful and robust tools.</p> <h3 id="conclusion">Conclusion</h3> <p>Understanding ROC curves and AUC scores has been one of the most transformative lessons in my data science journey. They push us beyond the simplistic view of “accuracy” and encourage a deeper, more comprehensive understanding of how our models truly perform.</p> <p>Next time you build a classification model, don’t just stop at accuracy. Plot that ROC curve, calculate that AUC, and truly appreciate the nuanced story your model has to tell. It’s a fundamental step towards building more robust, reliable, and interpretable machine learning systems. Happy modeling!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>