<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Support Vector Machines: Drawing the Ultimate Line in Your Data | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/support-vector-machines-drawing-the-ultimate-line/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Support Vector Machines: Drawing the Ultimate Line in Your Data</h1> <p class="post-meta"> Created on March 12, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/support-vector-machines"> <i class="fa-solid fa-hashtag fa-sm"></i> Support Vector Machines</a>   <a href="/blog/blog/tag/classification"> <i class="fa-solid fa-hashtag fa-sm"></i> Classification</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/hyperplane"> <i class="fa-solid fa-hashtag fa-sm"></i> Hyperplane</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Welcome back to my personal data science journal! Today, I want to talk about a machine learning algorithm that, for a long time, felt like pure magic to me: Support Vector Machines (SVMs). If you’ve ever had to sort a pile of diverse items into distinct categories, you’ve intuitively performed classification. SVMs do this, but with a mathematical elegance that’s truly captivating.</p> <p>Imagine you’re trying to separate two types of marbles – red and blue – scattered on a table. If they’re neatly clustered, you could draw a simple line to separate them. Easy, right? But what if they’re a bit mixed up? What if some red marbles are near the blue cluster, and vice-versa? This is where classification gets interesting, and where SVMs really shine.</p> <h3 id="the-intuition-more-than-just-any-line">The Intuition: More Than Just Any Line</h3> <p>At its core, an SVM is a discriminative classifier that aims to find an optimal “hyperplane” that best separates different classes in your dataset. Let’s break that down.</p> <p>In our marble example, if we’re separating red and blue marbles on a flat table (a 2D space), a “hyperplane” is simply a line. If we were separating types of fruit by weight, color, and size (a 3D space), the hyperplane would be a plane. In higher dimensions, it’s just called a hyperplane – a fancy term for a decision boundary that’s one dimension less than the space it occupies.</p> <p>The key isn’t just <em>any</em> line that separates the data. There could be infinitely many lines that do the trick. An SVM looks for the <em>best</em> line. And what makes a line “best” in this context? It’s the one that maximizes the “margin.”</p> <h4 id="the-margin-your-datas-personal-no-fly-zone">The Margin: Your Data’s Personal No-Fly Zone</h4> <p>Think of the margin as a street, or a “no-fly zone,” around our separating line (or hyperplane). The SVM’s goal is to find the hyperplane that has the largest possible street around it, such that no data points from <em>either</em> class fall within this street.</p> <p>Why is a wider street better? Because it means our separating boundary is as far as possible from the closest points of both classes. This makes our model more robust and less prone to misclassifying new, unseen data points that might be slightly different from our training data. It gives us a safety buffer.</p> <p>Imagine two lines parallel to the main hyperplane, one on each side, touching the closest data points of each class. The distance between these two parallel lines is our margin. The SVM’s objective is to maximize this distance. This is why SVMs are often called “maximum margin classifiers.”</p> <h3 id="support-vectors-the-pillars-of-your-decision">Support Vectors: The Pillars of Your Decision</h3> <p>This brings us to a crucial concept: <strong>Support Vectors</strong>. These are the data points that lie on the edges of our “street” – the ones closest to the separating hyperplane. They are literally “supporting” the hyperplane and defining the margin.</p> <p>Why are they so important? Because if you remove any other data point that is <em>not</em> a support vector, the optimal hyperplane and margin would not change. Only the support vectors influence the position and orientation of the hyperplane. This makes SVMs very memory-efficient in prediction, as you only need to store the support vectors, not the entire dataset. It’s like only needing the corner pillars to define the walls of a room.</p> <h3 id="the-math-behind-the-magic-simplified">The Math Behind the Magic (Simplified)</h3> <p>Let’s get a tiny bit mathematical, but I promise we’ll keep it intuitive.</p> <p>A hyperplane can be described by the equation: $w \cdot x + b = 0$</p> <p>Where:</p> <ul> <li>$w$ is a vector perpendicular to the hyperplane. It tells us the orientation of our separating boundary.</li> <li>$x$ is a data point in our feature space.</li> <li>$b$ is a scalar bias term. It helps us shift the hyperplane.</li> </ul> <p>For our two classes, let’s say one class ($y_i = +1$) should be on one side of the hyperplane and the other class ($y_i = -1$) on the other. The data points closest to the hyperplane (our support vectors) will satisfy: $w \cdot x_i + b = 1$ for class +1 $w \cdot x_i + b = -1$ for class -1</p> <p>Combining these with the class label $y_i$, we can write: $y_i (w \cdot x_i + b) \ge 1$ for all data points <em>outside</em> the margin.</p> <p>The distance between the two parallel hyperplanes ($w \cdot x + b = 1$ and $w \cdot x + b = -1$) is $2/||w||$, where $||w||$ is the Euclidean norm (length) of vector $w$. To maximize this distance, we need to minimize $||w||$. So, the core optimization problem for a linear SVM is: $\min_{w,b} \frac{1}{2} ||w||^2$ subject to $y_i (w \cdot x_i + b) \ge 1$ for all $i$.</p> <table> <tbody> <tr> <td>We use $</td> <td> </td> <td>w</td> <td> </td> <td>^2/2$ instead of $</td> <td> </td> <td>w</td> <td> </td> <td>$ because it simplifies the calculus, giving the same minimum point. This is a convex optimization problem, which means we’re guaranteed to find a global minimum. Pretty neat, right?</td> </tr> </tbody> </table> <h3 id="the-kernel-trick-when-a-straight-line-isnt-enough">The Kernel Trick: When a Straight Line Isn’t Enough</h3> <p>What if our data isn’t linearly separable? Imagine trying to separate red and blue marbles where the red ones form a circle in the middle, and the blue ones are scattered around the outside. No single straight line can separate them. This is where the <strong>Kernel Trick</strong> comes to our rescue, and it’s truly one of the most brilliant ideas in machine learning.</p> <p>The idea is to transform our data into a higher-dimensional space where it <em>becomes</em> linearly separable. For example, if you have data points in 2D ($x_1, x_2$) that form a circle, you might map them to 3D using a transformation like $\phi(x) = (x_1, x_2, x_1^2 + x_2^2)$. In this new 3D space, a simple plane might now perfectly separate your data.</p> <p>The “trick” part is that we often don’t actually need to compute these high-dimensional transformations explicitly. Instead, we use a “kernel function” $K(x_i, x_j)$ which calculates the dot product of the data points <em>as if</em> they were transformed into the higher-dimensional space: $K(x_i, x_j) = \phi(x_i) \cdot \phi(x_j)$</p> <p>This allows SVMs to operate efficiently in high-dimensional spaces without ever explicitly computing the coordinates in that space, saving immense computational cost. It’s like comparing the “similarity” of two data points in a very complex way without having to define all the complex features explicitly.</p> <p>Common kernel functions include:</p> <ul> <li> <strong>Polynomial Kernel:</strong> $K(x_i, x_j) = (\gamma x_i \cdot x_j + r)^d$</li> <li> <table> <tbody> <tr> <td> <strong>Radial Basis Function (RBF) / Gaussian Kernel:</strong> $K(x_i, x_j) = \exp(-\gamma</td> <td> </td> <td>x_i - x_j</td> <td> </td> <td>^2)$</td> </tr> <tr> <td>The RBF kernel is very popular. It basically measures the similarity between two points: points closer together (small $</td> <td> </td> <td>x_i - x_j</td> <td> </td> <td>^2$) will have a kernel value closer to 1, while points far apart will have a value closer to 0.</td> </tr> </tbody> </table> </li> </ul> <p>The choice of kernel can dramatically affect an SVM’s performance and is a crucial hyperparameter to tune.</p> <h3 id="soft-margins-embracing-real-world-messiness">Soft Margins: Embracing Real-World Messiness</h3> <p>In the real world, data is rarely perfectly separable. There might be noise, outliers, or simply overlapping classes. If we insist on a “hard margin” (no points allowed inside the street), our model might overfit to the noise or fail to find any separating hyperplane at all.</p> <p>This is where <strong>Soft Margins</strong> come in. Instead of strictly forbidding any points inside the margin or on the wrong side of the hyperplane, we allow for some misclassifications or violations of the margin constraint. We introduce “slack variables” ($\xi_i$) into our optimization problem. These variables measure how much each point $x_i$ violates the margin constraint.</p> <p>Our optimization problem then includes a penalty for these violations: $\min_{w,b,\xi} \frac{1}{2} ||w||^2 + C \sum_{i=1}^N \xi_i$ subject to $y_i (w \cdot x_i + b) \ge 1 - \xi_i$ and $\xi_i \ge 0$</p> <p>Here, $C$ is a regularization parameter. It’s a hyperparameter you tune:</p> <ul> <li> <strong>Small C:</strong> Allows for a larger margin and more misclassifications (higher bias, lower variance). The model prioritizes generalization.</li> <li> <strong>Large C:</strong> Insists on a smaller margin and fewer misclassifications (lower bias, higher variance). The model tries harder to fit the training data, potentially leading to overfitting.</li> </ul> <p>The $C$ parameter gives us a powerful way to control the trade-off between having a wider margin and correctly classifying training points. It makes SVMs incredibly flexible for messy, real-world datasets.</p> <h3 id="advantages-and-disadvantages-of-svms">Advantages and Disadvantages of SVMs</h3> <p>Like any powerful tool, SVMs have their strengths and weaknesses:</p> <p><strong>Advantages:</strong></p> <ol> <li> <strong>Effective in High Dimensions:</strong> Works well even when the number of features is greater than the number of samples.</li> <li> <strong>Memory Efficient:</strong> Only a subset of training points (the support vectors) are used in the decision function, making them efficient during prediction.</li> <li> <strong>Versatile with Kernels:</strong> Can handle non-linear relationships using various kernel functions.</li> <li> <strong>Robustness to Outliers:</strong> With soft margins, they can be less sensitive to outliers compared to some other models.</li> <li> <strong>Strong Theoretical Foundation:</strong> Based on statistical learning theory, which means they often generalize well to unseen data.</li> </ol> <p><strong>Disadvantages:</strong></p> <ol> <li> <strong>Computational Cost:</strong> Can be computationally expensive for very large datasets, especially without optimized implementations or appropriate kernel choices.</li> <li> <strong>Hyperparameter Tuning:</strong> Performance is highly dependent on the choice of C (and gamma for RBF kernel), requiring careful tuning.</li> <li> <strong>Lack of Probability Estimates:</strong> SVMs directly output class labels, not probabilities. While some extensions can provide probabilities, they are not inherent to the core algorithm.</li> <li> <strong>Interpretability:</strong> For complex kernels, understanding <em>why</em> a decision was made can be challenging, though less so than deep neural networks.</li> </ol> <h3 id="conclusion">Conclusion</h3> <p>Support Vector Machines are a testament to the elegance and power of machine learning. From their intuitive goal of finding the widest “street” between classes to the ingenious kernel trick that tackles non-linear data, SVMs offer a robust and efficient solution for a wide array of classification problems.</p> <p>Next time you encounter a classification challenge, remember the SVM. It’s a reminder that sometimes, the best way to separate things isn’t just to draw <em>a</em> line, but to draw the <em>ultimate</em> line, with a generous margin of safety. Keep exploring, keep learning, and keep drawing those ultimate lines!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>