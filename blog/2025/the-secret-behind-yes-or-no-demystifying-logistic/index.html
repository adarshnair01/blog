<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Secret Behind "Yes" or "No": Demystifying Logistic Regression | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/the-secret-behind-yes-or-no-demystifying-logistic/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Secret Behind "Yes" or "No": Demystifying Logistic Regression</h1> <p class="post-meta"> Created on March 15, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/classification"> <i class="fa-solid fa-hashtag fa-sm"></i> Classification</a>   <a href="/blog/blog/tag/logistic-regression"> <i class="fa-solid fa-hashtag fa-sm"></i> Logistic Regression</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/statistics"> <i class="fa-solid fa-hashtag fa-sm"></i> Statistics</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>As a budding data scientist, one of the first revelations for me was realizing that not all problems are about predicting a number. Sometimes, the world demands a definitive “yes” or “no,” a “spam” or “not spam,” a “churn” or “no churn.” This is where the fascinating realm of <em>classification</em> comes into play, and its undisputed patriarch is <strong>Logistic Regression</strong>.</p> <p>If you’ve ever dabbled in predicting continuous values like house prices or temperatures, you’re likely familiar with <em>Linear Regression</em>. It draws a straight line through data points, aiming to minimize the distance between the line and the points. But what if our target isn’t a continuous number, but rather a <em>category</em>? What if we want to predict if a student will <em>pass</em> or <em>fail</em> an exam?</p> <p>That’s where linear regression falls short, and brilliantly, Logistic Regression steps in. Join me as we unravel the elegant simplicity and profound power of this foundational algorithm.</p> <h3 id="why-linear-regression-fails-at-yes-or-no">Why Linear Regression Fails at “Yes” or “No”</h3> <p>Imagine you’re trying to predict if a student passes (1) or fails (0) an exam based on the hours they studied. If we try to use linear regression, our line might look something like this:</p> <p><img src="https://i.imgur.com/example_linear_fail.png" alt="Conceptual image of linear regression trying to classify binary data" title="Linear Regression trying to classify binary data"> <em>(Self-note: In a real blog, I’d generate or find an actual plot here)</em></p> <p>Problems immediately arise:</p> <ol> <li> <strong>Output Range</strong>: The linear regression line can output values outside the reasonable range of [0, 1]. What does a predicted value of -0.5 or 1.2 mean for a “pass/fail” scenario? It’s nonsensical for probabilities.</li> <li> <strong>Thresholding Issues</strong>: Even if we try to set a threshold (e.g., anything above 0.5 is a “pass”), a single outlier can severely skew the line, leading to poor classifications.</li> <li> <strong>Non-Linear Relationship</strong>: The relationship between study hours and passing probability isn’t likely linear in this way. It’s more of an “S-curve”—initially, more study hours lead to a sharp increase in passing probability, which then tapers off as you approach certainty.</li> </ol> <p>We need a function that naturally <em>squishes</em> our linear output into a probability-like range, always between 0 and 1.</p> <h3 id="enter-the-sigmoid-our-probability-s-curve">Enter the Sigmoid: Our Probability S-Curve</h3> <p>This is where the magic really begins. Logistic Regression doesn’t directly predict 0 or 1. Instead, it predicts the <em>probability</em> that an instance belongs to a certain class (e.g., the probability of passing the exam). To do this, it employs a special function called the <strong>Sigmoid function</strong>, also known as the <strong>Logistic function</strong>.</p> <p>The sigmoid function is defined as:</p> \[\sigma(z) = \frac{1}{1 + e^{-z}}\] <p>Where $e$ is Euler’s number (approximately 2.71828), and $z$ is the output of our familiar linear combination of features and weights:</p> \[z = w_0 + w_1x_1 + w_2x_2 + \dots + w_nx_n = \mathbf{w}^T\mathbf{x}\] <p>Let’s break down why this function is so perfect:</p> <ul> <li> <strong>S-Shape</strong>: Plotting $\sigma(z)$ reveals a beautiful S-shaped curve. As $z$ approaches negative infinity, $\sigma(z)$ approaches 0. As $z$ approaches positive infinity, $\sigma(z)$ approaches 1.</li> <li> <strong>Range [0, 1]</strong>: Crucially, the output of the sigmoid function is always between 0 and 1, making it ideal for interpreting as a probability.</li> <li> <strong>Gradient</strong>: The slope is steepest around $z=0$, meaning small changes in $z$ result in large changes in probability when we’re uncertain. This is intuitive – when you’re on the fence, a little extra effort can make a big difference.</li> </ul> <table> <tbody> <tr> <td>So, in Logistic Regression, we’re essentially taking our linear model’s output ($z$) and <em>feeding it into the sigmoid function</em> to get a probability $P(Y=1</td> <td>\mathbf{x}; \mathbf{w})$.</td> </tr> </tbody> </table> \[h_{\mathbf{w}}(\mathbf{x}) = P(Y=1|\mathbf{x}; \mathbf{w}) = \sigma(\mathbf{w}^T\mathbf{x}) = \frac{1}{1 + e^{-(\mathbf{w}^T\mathbf{x})}}\] <table> <tbody> <tr> <td>Here, $h_{\mathbf{w}}(\mathbf{x})$ represents our model’s predicted probability that the target variable $Y$ is 1, given the features $\mathbf{x}$ and the learned weights $\mathbf{w}$. If $P(Y=1</td> <td>\mathbf{x})$ is, say, 0.7, it means there’s a 70% chance of the event occurring.</td> </tr> </tbody> </table> <h3 id="making-a-decision-the-classification-threshold">Making a Decision: The Classification Threshold</h3> <p>Once we have a probability, how do we get back to a “yes” or “no”? We use a <strong>classification threshold</strong>. Typically, this threshold is 0.5.</p> <ul> <li>If $h_{\mathbf{w}}(\mathbf{x}) \geq 0.5$, we classify it as <strong>Class 1</strong> (e.g., “Pass”).</li> <li>If $h_{\mathbf{w}}(\mathbf{x}) &lt; 0.5$, we classify it as <strong>Class 0</strong> (e.g., “Fail”).</li> </ul> <p>This threshold can be adjusted based on the specific problem. For example, in a medical diagnosis where false negatives are very costly, we might lower the threshold to 0.3 to be more cautious and flag more potential cases, even if it means more false positives.</p> <h3 id="learning-the-parameters-the-cost-function">Learning the Parameters: The Cost Function</h3> <p>Now, the big question: how do we find the “best” values for our weights $\mathbf{w}$? Just like in linear regression, we need a <strong>cost function</strong> (or loss function) that tells us how “wrong” our predictions are. Our goal is to minimize this cost function.</p> <p>For classification problems, the mean squared error (used in linear regression) isn’t ideal because when combined with the sigmoid, it results in a non-convex cost function with many local minima. This makes it difficult for optimization algorithms like gradient descent to find the global minimum.</p> <p>Instead, Logistic Regression uses the <strong>Binary Cross-Entropy Loss</strong> (also known as Log Loss), which is derived from the principle of Maximum Likelihood Estimation. It’s perfectly convex, guaranteeing that gradient descent will find the optimal global minimum.</p> <p>Let’s look at the Binary Cross-Entropy Loss for a single training example $(\mathbf{x}^{(i)}, y^{(i)})$ where $y^{(i)}$ is the actual label (0 or 1):</p> <ul> <li>If $y^{(i)} = 1$: Loss is $-\log(h_{\mathbf{w}}(\mathbf{x}^{(i)}))$</li> <li>If $y^{(i)} = 0$: Loss is $-\log(1 - h_{\mathbf{w}}(\mathbf{x}^{(i)}))$</li> </ul> <p>We can combine these into a single elegant expression:</p> \[\text{Cost}(h_{\mathbf{w}}(\mathbf{x}^{(i)}), y^{(i)}) = -y^{(i)} \log(h_{\mathbf{w}}(\mathbf{x}^{(i)})) - (1 - y^{(i)}) \log(1 - h_{\mathbf{w}}(\mathbf{x}^{(i)}))\] <p>Let’s intuitively understand this:</p> <ul> <li> <strong>If $y^{(i)} = 1$ (actual class is 1):</strong> The term $(1 - y^{(i)}) \log(1 - h_{\mathbf{w}}(\mathbf{x}^{(i)}))$ becomes zero. The loss is then $-\log(h_{\mathbf{w}}(\mathbf{x}^{(i)}))$. <ul> <li>If $h_{\mathbf{w}}(\mathbf{x}^{(i)})$ is close to 1 (correct prediction), $-\log(\text{small number})$ will be a small positive number, meaning low cost.</li> <li>If $h_{\mathbf{w}}(\mathbf{x}^{(i)})$ is close to 0 (wrong prediction), $-\log(\text{number close to 0})$ will be a very large positive number, meaning high cost. The model is heavily penalized for being confidently wrong.</li> </ul> </li> <li> <strong>If $y^{(i)} = 0$ (actual class is 0):</strong> The term $-y^{(i)} \log(h_{\mathbf{w}}(\mathbf{x}^{(i)}))$ becomes zero. The loss is then $-\log(1 - h_{\mathbf{w}}(\mathbf{x}^{(i)}))$. <ul> <li>If $h_{\mathbf{w}}(\mathbf{x}^{(i)})$ is close to 0 (correct prediction), then $1 - h_{\mathbf{w}}(\mathbf{x}^{(i)})$ is close to 1. $-\log(\text{number close to 1})$ will be a small positive number, low cost.</li> <li>If $h_{\mathbf{w}}(\mathbf{x}^{(i)})$ is close to 1 (wrong prediction), then $1 - h_{\mathbf{w}}(\mathbf{x}^{(i)})$ is close to 0. $-\log(\text{number close to 0})$ will be a very large positive number, high cost. Again, heavily penalized for confidently being wrong.</li> </ul> </li> </ul> <p>To get the total cost for our entire dataset of $m$ examples, we average the cost over all examples:</p> \[J(\mathbf{w}) = -\frac{1}{m} \sum_{i=1}^m [y^{(i)} \log(h_{\mathbf{w}}(\mathbf{x}^{(i)})) + (1 - y^{(i)}) \log(1 - h_{\mathbf{w}}(\mathbf{x}^{(i)}))]\] <p>This is our objective function, the mathematical representation of what we want to minimize.</p> <h3 id="optimizing-the-weights-gradient-descent">Optimizing the Weights: Gradient Descent</h3> <p>With our cost function defined, we need an algorithm to find the weights $\mathbf{w}$ that minimize $J(\mathbf{w})$. The most common method is <strong>Gradient Descent</strong>.</p> <p>Imagine you’re standing on a mountain (the cost function landscape) blindfolded and trying to reach the lowest point (the minimum cost). Gradient descent works by repeatedly taking small steps in the direction of the steepest descent.</p> <p>For each weight $w_j$, we update it iteratively using the following rule:</p> \[w_j := w_j - \alpha \frac{\partial}{\partial w_j} J(\mathbf{w})\] <p>Where:</p> <ul> <li>$w_j$ is the weight we’re updating.</li> <li>$\alpha$ is the <strong>learning rate</strong>, a small positive number that controls the size of our steps. Too large, and we might overshoot the minimum; too small, and training might take forever.</li> <li>$\frac{\partial}{\partial w_j} J(\mathbf{w})$ is the <strong>partial derivative</strong> of the cost function with respect to $w_j$. This tells us the direction and steepness of the slope.</li> </ul> <p>Remarkably, the derivative for Logistic Regression’s cost function with respect to $w_j$ has a very elegant form (which often surprises students for its similarity to linear regression’s gradient):</p> \[\frac{\partial}{\partial w_j} J(\mathbf{w}) = \frac{1}{m} \sum_{i=1}^m (h_{\mathbf{w}}(\mathbf{x}^{(i)}) - y^{(i)}) x_j^{(i)}\] <p>So, the update rule becomes:</p> \[w_j := w_j - \alpha \frac{1}{m} \sum_{i=1}^m (h_{\mathbf{w}}(\mathbf{x}^{(i)}) - y^{(i)}) x_j^{(i)}\] <p>We repeat this process for all weights, over many iterations (epochs), until the weights converge and the cost function no longer significantly decreases.</p> <h3 id="interpreting-the-coefficients-a-glimpse-into-the-log-odds">Interpreting the Coefficients: A Glimpse into the Log-Odds</h3> <p>One of the most appealing aspects of Logistic Regression, particularly for its interpretability, is how we can understand the learned coefficients. Unlike linear regression where coefficients tell us about the direct change in the target variable, here, they describe changes in the <strong>log-odds</strong>.</p> <p>The term $\mathbf{w}^T\mathbf{x}$ is the “logit” or “log-odds.” It’s the natural logarithm of the odds of the event occurring:</p> \[\log\left(\frac{P(Y=1|\mathbf{x})}{1 - P(Y=1|\mathbf{x})}\right) = \mathbf{w}^T\mathbf{x}\] <p>If we exponentiate both sides, we get the odds:</p> \[\frac{P(Y=1|\mathbf{x})}{1 - P(Y=1|\mathbf{x})} = e^{\mathbf{w}^T\mathbf{x}} = e^{w_0 + w_1x_1 + \dots + w_nx_n}\] <p>This means that for a one-unit increase in a feature $x_j$, holding all other features constant, the odds of the event occurring are multiplied by $e^{w_j}$. For example, if $w_j = 0.5$, then $e^{0.5} \approx 1.65$. This means a one-unit increase in $x_j$ increases the odds of $Y=1$ by 65%. This interpretability is incredibly valuable in fields like healthcare or social sciences where understanding the <em>why</em> is as important as the <em>what</em>.</p> <h3 id="assumptions-and-considerations">Assumptions and Considerations</h3> <p>While robust, Logistic Regression relies on a few key assumptions:</p> <ol> <li> <strong>Binary Outcome</strong>: Naturally, the dependent variable must be binary (two classes). For multi-class classification, extensions like One-vs-Rest (OvR) or Softmax Regression (Multinomial Logistic Regression) are used.</li> <li> <strong>Linearity of Log-Odds</strong>: There should be a linear relationship between the independent variables and the <em>log-odds</em> of the dependent variable. This is crucial and often misunderstood. It’s not a linear relationship with the probability itself, but with its logarithmic transformation.</li> <li> <strong>Independence of Observations</strong>: Observations should be independent of each other.</li> <li> <strong>No Strong Multicollinearity</strong>: Independent variables should not be too highly correlated with each other, as this can lead to unstable and hard-to-interpret coefficients.</li> <li> <strong>Large Sample Size</strong>: Logistic regression tends to perform better with larger sample sizes.</li> </ol> <h3 id="strengths-and-weaknesses">Strengths and Weaknesses</h3> <p><strong>Strengths:</strong></p> <ul> <li> <strong>Interpretability</strong>: Coefficients can be interpreted in terms of odds ratios, providing clear insights into feature importance and direction.</li> <li> <strong>Probabilistic Output</strong>: Provides probabilities for predictions, which is useful for risk assessment or setting custom thresholds.</li> <li> <strong>Efficiency</strong>: Relatively fast to train and predict, even on large datasets.</li> <li> <strong>Good Baseline</strong>: Often serves as an excellent baseline model against which more complex models can be compared.</li> <li> <strong>Well-Understood</strong>: Its statistical foundations are well-established.</li> </ul> <p><strong>Weaknesses:</strong></p> <ul> <li> <strong>Assumes Linear Relationship in Log-Odds</strong>: Cannot capture complex non-linear relationships without explicit feature engineering (e.g., polynomial features).</li> <li> <strong>Sensitivity to Outliers</strong>: Extreme values can disproportionately influence the model.</li> <li> <strong>Does not handle categorical features with many levels well</strong>: Can lead to sparse data and overfitting without proper encoding.</li> <li> <strong>Less Powerful than Complex Models</strong>: For highly non-linear or intricate datasets, tree-based models (like Random Forest or XGBoost) or neural networks often outperform it.</li> </ul> <h3 id="beyond-binary-multiclass-classification">Beyond Binary: Multiclass Classification</h3> <p>While Logistic Regression is inherently binary, it can be extended to handle problems with more than two classes:</p> <ul> <li> <strong>One-vs-Rest (OvR) / One-vs-All (OvA)</strong>: Train a separate binary logistic regression classifier for each class. For $K$ classes, you train $K$ classifiers. To classify a new instance, you run all $K$ classifiers and pick the class whose classifier outputs the highest probability.</li> <li> <strong>Softmax Regression (Multinomial Logistic Regression)</strong>: This is a direct generalization of Logistic Regression to multiple classes. It calculates probabilities for each class and normalizes them, ensuring they sum to 1. This is often preferred over OvR for true multiclass problems as it directly models the relative probabilities between classes.</li> </ul> <h3 id="conclusion-a-cornerstone-of-machine-learning">Conclusion: A Cornerstone of Machine Learning</h3> <p>Logistic Regression might sound deceptively simple, but its elegant transformation of linear outputs into probabilities, coupled with a robust optimization strategy, makes it an indispensable tool in the data scientist’s arsenal. From predicting customer churn to diagnosing diseases, its versatility and interpretability have cemented its place as a foundational algorithm.</p> <p>As you venture deeper into the world of machine learning, you’ll encounter far more complex models. But always remember Logistic Regression: a powerful, efficient, and surprisingly insightful algorithm that brilliantly solves the riddle of “yes” or “no.” It’s not just a model; it’s a way of thinking about and dissecting binary choices in data. And for that, it deserves our deep appreciation.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>