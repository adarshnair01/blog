<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Unraveling the Magic of Computer Vision: My Journey into Convolutional Neural Networks | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/unraveling-the-magic-of-computer-vision-my-journey/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Unraveling the Magic of Computer Vision: My Journey into Convolutional Neural Networks</h1> <p class="post-meta"> Created on May 27, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/computer-vision"> <i class="fa-solid fa-hashtag fa-sm"></i> Computer Vision</a>   <a href="/blog/blog/tag/neural-networks"> <i class="fa-solid fa-hashtag fa-sm"></i> Neural Networks</a>   <a href="/blog/blog/tag/cnns"> <i class="fa-solid fa-hashtag fa-sm"></i> CNNs</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hello fellow explorers of the digital frontier!</p> <p>Have you ever stopped to think about how incredible our own vision system is? We instantly recognize faces, differentiate a cat from a dog, read text, and navigate complex environments, all without conscious effort. It’s truly astounding! For a long time, enabling computers to perform even a fraction of these tasks seemed like science fiction. Early attempts at “computer vision” were often clunky, rule-based systems that struggled with variations in lighting, angle, or even slight occlusions.</p> <p>Then came the revolution: Neural Networks. While standard neural networks showed promise, they faced a massive challenge when it came to images. Imagine trying to feed a 100x100 pixel grayscale image into a fully connected neural network. That’s 10,000 input neurons! For a color image (100x100x3), it’s 30,000! Now, if each of those input neurons connected to, say, 100 neurons in the first hidden layer, we’re talking about 3 million connections <em>just for that first layer</em>. The number of parameters quickly becomes astronomical, leading to slow training, huge memory requirements, and a high risk of overfitting. It was like trying to drink from a firehose.</p> <p>This is where Convolutional Neural Networks (CNNs), or ConvNets, burst onto the scene, fundamentally changing the game for computer vision. When I first encountered CNNs, they felt like magic. How could a computer learn to <em>see</em>? My journey into understanding them has been one of gradual enlightenment, breaking down this complex architecture into its elegant, powerful components. I invite you to join me as we peel back the layers and discover the brilliance behind these networks.</p> <h3 id="the-heart-of-the-matter-convolution">The Heart of the Matter: Convolution!</h3> <p>At the core of a CNN lies the <strong>convolutional layer</strong>. This is where the network truly starts to “look” at the image. Think of it like this: instead of looking at the entire image at once, we use a small magnifying glass (which we call a <strong>filter</strong> or <strong>kernel</strong>) to scan over small portions of the image, one at a time.</p> <p>Imagine you’re trying to find all the vertical edges in an image. You could design a small 3x3 filter, a tiny matrix of numbers, that “activates” when it sees a vertical line.</p> <p>Let’s say we have a tiny 5x5 pixel image (a portion of a much larger image):</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Image (I):
[[10, 20, 30, 40, 50],
 [10, 20, 30, 40, 50],
 [10, 20, 30, 40, 50],
 [10, 20, 30, 40, 50],
 [10, 20, 30, 40, 50]]
</code></pre></div></div> <p>And a 3x3 filter (F) designed to detect a vertical edge:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Filter (F):
[[-1, 0, 1],
 [-1, 0, 1],
 [-1, 0, 1]]
</code></pre></div></div> <p>The convolution operation works by sliding this filter across the image. At each position, we perform an element-wise multiplication between the filter and the underlying image patch, and then sum up all the results. This sum becomes a single pixel in our output, which we call a <strong>feature map</strong> or <strong>activation map</strong>.</p> <p>For example, if our filter is over the top-left 3x3 patch of the image:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Image Patch:
[[10, 20, 30],
 [10, 20, 30],
 [10, 20, 30]]
</code></pre></div></div> <p>The convolution would be: $ (10 \times -1) + (20 \times 0) + (30 \times 1) + $ $ (10 \times -1) + (20 \times 0) + (30 \times 1) + $ $ (10 \times -1) + (20 \times 0) + (30 \times 1) = $ $ (-10 + 0 + 30) + (-10 + 0 + 30) + (-10 + 0 + 30) = 20 + 20 + 20 = 60 $</p> <p>This “60” is the first pixel in our feature map. We then slide the filter by a certain number of pixels (called the <strong>stride</strong>) and repeat the process. If the stride is 1, we move one pixel at a time. If the stride is 2, we skip a pixel.</p> <p><strong>Why is this brilliant?</strong></p> <ol> <li> <strong>Locality:</strong> Filters only look at small, local regions of the image. This mirrors how our brain processes visual information – we focus on local features before combining them.</li> <li> <strong>Parameter Sharing:</strong> The <em>same</em> filter is applied across the <em>entire</em> image. This is incredibly powerful! If a vertical edge detector is useful in one part of the image (e.g., detecting a tree trunk), it’s likely useful in another part (e.g., detecting a building edge). This dramatically reduces the number of parameters the network needs to learn, making it much more efficient and less prone to overfitting than a fully connected layer.</li> <li> <strong>Multiple Filters:</strong> A convolutional layer doesn’t just have one filter; it has many! Each filter learns to detect a different feature – one might look for horizontal edges, another for corners, another for specific textures, and so on. Each filter produces its own feature map, and these feature maps are stacked together to form the output of the convolutional layer.</li> </ol> <h3 id="adding-a-spark-activation-functions">Adding a Spark: Activation Functions</h3> <p>After a convolutional operation, the output (the feature map) is typically passed through an <strong>activation function</strong>. Why? Because without them, stacking multiple layers of convolution would just result in another linear transformation, no matter how deep the network. We need non-linearity to learn complex patterns and relationships in the data.</p> <p>The most popular activation function in CNNs is the <strong>Rectified Linear Unit (ReLU)</strong>. It’s elegantly simple: $ ReLU(x) = max(0, x) $</p> <p>Essentially, if the input value $x$ is positive, ReLU outputs $x$. If $x$ is negative, it outputs 0. This simple operation introduces non-linearity, is computationally very efficient, and helps combat issues like vanishing gradients during training. It’s like giving the network a “light switch” – turning on neurons that detect strong features and turning off those that don’t.</p> <h3 id="downsizing-for-efficiency-pooling-layers">Downsizing for Efficiency: Pooling Layers</h3> <p>Following a convolutional layer and an activation function, we often find a <strong>pooling layer</strong>. The primary purpose of pooling layers is to reduce the spatial dimensions (width and height) of the feature maps, thereby reducing the number of parameters and computational load in subsequent layers. Think of it as summarizing the most important information in a region.</p> <p>The most common types of pooling are:</p> <ul> <li> <strong>Max Pooling:</strong> This is like a “spotlight” operator. It slides a small window (e.g., 2x2) over the feature map and selects the <em>maximum</em> value within that window.</li> <li> <strong>Average Pooling:</strong> Similar to Max Pooling, but it calculates the <em>average</em> value within the window.</li> </ul> <p>Let’s illustrate Max Pooling with a small 4x4 feature map and a 2x2 pooling window with a stride of 2:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Feature Map:
[[1, 1, 2, 4],
 [5, 6, 7, 8],
 [3, 2, 1, 0],
 [1, 2, 3, 4]]
</code></pre></div></div> <p>With a 2x2 window and stride 2, we take the max from each 2x2 block:</p> <ul> <li>Top-left block <code class="language-plaintext highlighter-rouge">[[1,1],[5,6]]</code> -&gt; max is 6</li> <li>Top-right block <code class="language-plaintext highlighter-rouge">[[2,4],[7,8]]</code> -&gt; max is 8</li> <li>Bottom-left block <code class="language-plaintext highlighter-rouge">[[3,2],[1,2]]</code> -&gt; max is 3</li> <li>Bottom-right block <code class="language-plaintext highlighter-rouge">[[1,0],[3,4]]</code> -&gt; max is 4</li> </ul> <p>The resulting pooled feature map would be:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Pooled Feature Map:
[[6, 8],
 [3, 4]]
</code></pre></div></div> <p><strong>Benefits of Pooling:</strong></p> <ul> <li> <strong>Dimensionality Reduction:</strong> Reduces the size of the representation, making computation faster.</li> <li> <strong>Translation Invariance:</strong> Makes the network more robust to small shifts or distortions in the input image. If an important feature moves a few pixels, its maximum value will likely still be captured within the pooling window.</li> <li> <strong>Reduces Overfitting:</strong> Fewer parameters to learn.</li> </ul> <h3 id="the-full-picture-a-cnns-architecture">The Full Picture: A CNN’s Architecture</h3> <p>A typical CNN architecture is a sequence of these building blocks:</p> <p><strong><code class="language-plaintext highlighter-rouge">Input Image -&gt; [CONVOLUTION -&gt; ReLU -&gt; POOLING] (repeated multiple times) -&gt; Fully Connected Layers -&gt; Output</code></strong></p> <ol> <li> <strong>Input Layer:</strong> This is your raw image data (e.g., 224x224x3 for height, width, color channels).</li> <li> <strong>Convolutional Layers (with ReLU):</strong> The network starts by learning simple features like edges and corners. As we stack more convolutional layers, the filters in deeper layers learn to combine these simpler features into more complex patterns – textures, eyes, wheels, specific shapes, etc. It’s a hierarchical learning process, much like how we build understanding from basic elements.</li> <li> <strong>Pooling Layers:</strong> Interspersed after convolutional layers to progressively reduce spatial dimensions and focus on the most salient features.</li> <li> <strong>Flattening:</strong> After several rounds of convolution and pooling, the 3D output (height x width x number of filters) is “flattened” into a single, long vector. This vector represents the high-level features extracted from the image.</li> <li> <strong>Fully Connected Layers:</strong> This flattened vector is then fed into one or more standard fully connected neural network layers. These layers act as the “brain” of the network, taking the extracted features and using them to make a final decision (e.g., classifying the image).</li> <li> <strong>Output Layer:</strong> The final fully connected layer, often with a <strong>softmax</strong> activation function for classification tasks, outputs the probabilities for each class (e.g., “90% chance this is a cat, 5% dog, 5% bird”).</li> </ol> <h3 id="teaching-the-network-to-see-training">Teaching the Network to See: Training</h3> <p>How do these filters magically learn to detect edges or faces? This is where the magic of <strong>training</strong> comes in. Just like traditional neural networks, CNNs are trained using techniques like <strong>backpropagation</strong> and <strong>gradient descent</strong>.</p> <p>During training, we feed the network millions of labeled images (e.g., “this image is a cat,” “this image is a car”). Initially, the filter values are random, and the network makes terrible predictions. A <strong>loss function</strong> measures how far off these predictions are from the true labels. Backpropagation then calculates the “gradient” (how much each parameter contributed to the error) and adjusts the filter values (and weights in the fully connected layers) slightly to reduce the error. This iterative process, guided by an <strong>optimizer</strong> like Adam or SGD, slowly refines the filters. Over time, the filters evolve to become excellent feature detectors, and the fully connected layers learn to combine these features for accurate classification.</p> <h3 id="why-cnns-are-so-powerful">Why CNNs are So Powerful</h3> <ul> <li> <strong>Hierarchical Feature Learning:</strong> They automatically learn a hierarchy of features, from simple (edges, textures) to complex (object parts, entire objects).</li> <li> <strong>Parameter Efficiency:</strong> Weight sharing in convolutional layers drastically reduces the number of parameters compared to fully connected networks for images.</li> <li> <strong>Translational Invariance:</strong> Because filters scan the entire image, a CNN can recognize a feature (like an eye) regardless of where it appears in the image. This is a huge advantage over traditional methods.</li> <li> <strong>Spatial Relationships:</strong> Convolution inherently respects the spatial relationships between pixels, which is crucial for understanding images.</li> </ul> <h3 id="beyond-classification-the-versatility-of-cnns">Beyond Classification: The Versatility of CNNs</h3> <p>While image classification (e.g., identifying objects in a photo) is a cornerstone application, CNNs are incredibly versatile and power countless other computer vision tasks:</p> <ul> <li> <strong>Object Detection:</strong> Not just <em>what</em> is in the image, but <em>where</em> it is (e.g., drawing bounding boxes around all cars in a street scene).</li> <li> <strong>Image Segmentation:</strong> Classifying every single pixel in an image (e.g., labeling which pixels belong to a person, a car, or the road).</li> <li> <strong>Facial Recognition:</strong> Identifying individuals from images or video streams.</li> <li> <strong>Medical Imaging:</strong> Detecting diseases from X-rays, MRIs, and CT scans.</li> <li> <strong>Autonomous Driving:</strong> Helping self-driving cars perceive their surroundings.</li> <li> <strong>Generative Models:</strong> Creating realistic fake images and videos (think deepfakes or AI art).</li> </ul> <h3 id="my-takeaway-a-glimpse-into-ais-future">My Takeaway: A Glimpse into AI’s Future</h3> <p>Delving into Convolutional Neural Networks has been one of the most rewarding parts of my data science journey. It’s a prime example of how carefully designed architectures, inspired by biological systems and clever mathematical operations, can unlock truly remarkable capabilities in artificial intelligence. From their elegant parameter-sharing mechanism to their ability to build complex understanding from simple patterns, CNNs are a testament to the power of deep learning.</p> <p>They are not just tools; they are the eyes of our AI systems, allowing them to perceive, interpret, and interact with the visual world in ways that were once confined to the realm of science fiction. If you’re passionate about making machines intelligent, understanding CNNs is not just beneficial, it’s essential. I encourage you to experiment, build your own CNNs, and see the world through their digital eyes. The possibilities are truly boundless!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>