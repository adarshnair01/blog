<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Art of Discipline: How Regularization Teaches Our Models to Think, Not Just Memorize | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/the-art-of-discipline-how-regularization-teaches-o/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Art of Discipline: How Regularization Teaches Our Models to Think, Not Just Memorize</h1> <p class="post-meta"> Created on July 04, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/regularization"> <i class="fa-solid fa-hashtag fa-sm"></i> Regularization</a>   <a href="/blog/blog/tag/overfitting"> <i class="fa-solid fa-hashtag fa-sm"></i> Overfitting</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/model-building"> <i class="fa-solid fa-hashtag fa-sm"></i> Model Building</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>As a budding data scientist, there’s a certain thrill in building your first predictive model. You feed it data, tweak parameters, and watch its performance metrics soar on your training set. “Yes!” you exclaim, feeling like a wizard. But then, you introduce it to new, unseen data, and suddenly, your wizardry turns into a magic trick gone wrong. The model, which seemed so brilliant moments ago, stumbles, performs poorly, and leaves you wondering what went amiss.</p> <p>Sound familiar? Welcome to the frustrating, yet fundamental, challenge of <strong>overfitting</strong>. And trust me, every single person who has ever trained a machine learning model has faced it. This isn’t just a glitch; it’s a deep-seated philosophical problem in machine learning: how do we build models that truly <em>learn</em> and <em>generalize</em> from data, rather than just <em>memorize</em> it?</p> <p>This, my friends, is where <strong>Regularization</strong> steps onto the stage. Think of it as the wise, disciplined mentor for our models, teaching them to focus on the signal, not the noise, to build robust understanding rather than fragile memorization.</p> <h3 id="the-problem-child-overfitting">The Problem Child: Overfitting</h3> <p>Imagine you’re studying for a history exam. One way to prepare is to truly understand the historical context, the cause-and-effect relationships, and the broader themes. This allows you to answer any question, even if it’s phrased slightly differently from what you’ve seen before. This is <strong>generalization</strong>.</p> <p>The other way is to memorize every single sentence, every date, every name from your textbook, verbatim. If the exam asks questions exactly as they appear in the book, you’ll ace it. But if a question is phrased even slightly differently, or asks for an interpretation you haven’t memorized, you’re lost. This is <strong>overfitting</strong>. You’ve learned the training data (your textbook) perfectly, but you can’t generalize to new, unseen questions (the actual exam).</p> <p>In machine learning, an overfit model is one that has learned the training data too well, capturing not just the underlying patterns but also the random noise and idiosyncrasies specific to <em>that</em> particular dataset. When presented with new data, these learned “noise patterns” become detrimental, leading to poor performance.</p> <p>Visually, imagine plotting some data points with a slightly curvy underlying relationship, but also some random scatter. A simple model might draw a straight line, missing some of the curve (underfitting). A “just right” model might draw a smooth curve that captures the main trend. An <em>overfit</em> model, however, would draw a wildly wiggly line that perfectly passes through every single data point, even the noisy ones. It’s essentially “connecting the dots” of the noise, not the underlying story.</p> <h3 id="the-solution-introducing-discipline-with-regularization">The Solution: Introducing Discipline with Regularization</h3> <p>How do we prevent our models from becoming overly complex, from learning the “noise” in addition to the “signal”? We introduce a penalty for complexity. This is the core idea behind regularization.</p> <p>When we train a model, we typically define a <strong>loss function</strong> (e.g., Mean Squared Error for regression, Cross-Entropy for classification). This function quantifies how “wrong” our model’s predictions are. Our goal is to minimize this loss.</p> <p>Regularization modifies this objective. Instead of just minimizing the prediction error, we minimize:</p> <p>$ \text{New Loss} = \text{Original Loss (Prediction Error)} + \text{Penalty Term (for Complexity)} $</p> <p>This “penalty term” is crucial. It discourages the model from assigning very large weights (coefficients) to features, which often leads to overly complex models that are highly sensitive to small changes in the input data. By keeping weights small, we essentially force the model to be simpler, smoother, and less prone to fitting noise.</p> <p>The strength of this penalty is controlled by a hyperparameter, typically denoted as $ \lambda $ (lambda).</p> <ul> <li>If $ \lambda = 0 $, there’s no penalty, and it’s just regular training.</li> <li>If $ \lambda $ is small, the penalty is weak, allowing for some complexity.</li> <li>If $ \lambda $ is large, the penalty is strong, forcing the model to be much simpler (potentially leading to underfitting if $ \lambda $ is too high).</li> </ul> <p>Finding the right $ \lambda $ is often an art, tuned through techniques like cross-validation.</p> <p>Let’s dive into the two most common types of regularization: L1 and L2.</p> <h4 id="1-l2-regularization-ridge-regression-the-team-player">1. L2 Regularization: Ridge Regression (The “Team Player”)</h4> <p>L2 regularization, often called <strong>Ridge Regression</strong> when used with linear models, adds a penalty proportional to the sum of the <em>squares</em> of the magnitude of the coefficients.</p> <p>The penalty term looks like this: $ \lambda \sum_{j=1}^p w_j^2 $</p> <p>Here, $ w_j $ represents the coefficient for the $ j $-th feature, and $ p $ is the total number of features.</p> <p><strong>What does this do?</strong></p> <ul> <li> <strong>Shrinks Coefficients:</strong> L2 regularization tends to shrink the coefficients towards zero, but it rarely makes them <em>exactly</em> zero. It encourages all features to contribute, but not too strongly.</li> <li> <strong>Handles Multicollinearity:</strong> If you have highly correlated features, L2 regularization distributes the impact among them, making the model more stable.</li> </ul> <p><strong>Analogy:</strong> Imagine a sports team where everyone tries to be the star player. L2 regularization is like a coach telling everyone, “Hey, contribute, but don’t try to hog all the glory. Play as a team, keep your individual contributions balanced.” No one gets completely benched (coefficients rarely zero), but everyone learns to play within their role.</p> <p><strong>Geometric Intuition (for the visually inclined):</strong> Imagine you’re trying to minimize your loss function in a 2D space of two coefficients ($w_1, w_2$). The loss function creates an elliptical contour. The L2 penalty imposes a circular constraint around the origin. The optimal solution is where the elliptical contours of the loss function first touch this circular constraint. Because the constraint is smooth and circular, it pushes coefficients towards zero but doesn’t easily force them exactly onto the axes (i.e., making them zero).</p> <h4 id="2-l1-regularization-lasso-regression-the-feature-selector">2. L1 Regularization: Lasso Regression (The “Feature Selector”)</h4> <p>L1 regularization, known as <strong>Lasso Regression</strong>, adds a penalty proportional to the sum of the <em>absolute values</em> of the coefficients.</p> <table> <tbody> <tr> <td>The penalty term looks like this: $ \lambda \sum_{j=1}^p</td> <td>w_j</td> <td>$</td> </tr> </tbody> </table> <p><strong>What does this do?</strong></p> <ul> <li> <strong>Shrinks Coefficients (and zeroes them out!):</strong> Unlike L2, L1 regularization has a property that makes it capable of shrinking some coefficients <em>exactly</em> to zero. This is incredibly powerful!</li> <li> <strong>Feature Selection:</strong> By zeroing out coefficients, L1 regularization effectively performs automatic feature selection. It identifies and discards irrelevant features, leading to simpler, more interpretable models.</li> </ul> <p><strong>Analogy:</strong> L1 regularization is like a strict editor. When you write something, you might include many words, some important, some less so. The editor comes in and says, “Cut the fluff! If a word isn’t absolutely necessary, get rid of it.” This results in a concise, impactful piece of writing (a model with only the most important features).</p> <p><strong>Geometric Intuition:</strong> In our 2D coefficient space, the L1 penalty imposes a diamond-shaped (square rotated by 45 degrees) constraint around the origin. When the elliptical contours of the loss function touch this diamond constraint, it’s very common for the intersection point to occur at one of the “corners” or edges of the diamond, which corresponds to one or more coefficients being exactly zero. This is why L1 is great for feature selection.</p> <h4 id="3-elastic-net-regularization-the-best-of-both-worlds">3. Elastic Net Regularization (The “Best of Both Worlds”)</h4> <p>Sometimes, you want the best of both worlds: the feature selection capability of Lasso and the group-effect handling (and stability) of Ridge. That’s where <strong>Elastic Net</strong> comes in. It combines both L1 and L2 penalties:</p> <table> <tbody> <tr> <td>$ \lambda<em>1 \sum</em>{j=1}^p</td> <td>w*j</td> <td>+ \lambda_2 \sum*{j=1}^p w_j^2 $</td> </tr> </tbody> </table> <p>Here, you have two regularization parameters, $ \lambda_1 $ and $ \lambda_2 $, allowing fine-grained control over the balance between L1 and L2 effects. Elastic Net is particularly useful when you have many features and some of them are highly correlated.</p> <h3 id="beyond-l1l2-regularization-in-neural-networks">Beyond L1/L2: Regularization in Neural Networks</h3> <p>While L1 and L2 regularization (often called “weight decay” in neural networks) are fundamental, other regularization techniques exist, especially crucial for complex models like neural networks:</p> <ul> <li> <strong>Dropout:</strong> During training, randomly “drops out” (sets to zero) a fraction of neurons and their connections. This forces the network to learn more robust features that don’t rely on any single neuron, preventing co-adaptation of features. Imagine it like training multiple smaller, slightly different networks and averaging their results – a powerful ensemble effect.</li> <li> <strong>Early Stopping:</strong> This is a surprisingly simple yet effective technique. You monitor the model’s performance not just on the training data, but also on a separate validation set. As the model trains, training loss usually goes down. Validation loss initially goes down too, but eventually, if the model starts overfitting, validation loss will start to <em>increase</em>. Early stopping simply says, “Stop training when the validation loss starts getting worse!” It saves computation and prevents overfitting.</li> </ul> <h3 id="when-to-use-regularization">When to Use Regularization?</h3> <p>My short answer: Almost always!</p> <p>In practice, regularization is a fundamental tool in the machine learning engineer’s toolkit. It’s especially critical when:</p> <ul> <li>You have a large number of features.</li> <li>Your model is powerful and prone to complexity (e.g., deep neural networks, complex decision trees).</li> <li>Your dataset is noisy or relatively small compared to the number of features.</li> <li>You suspect multicollinearity among your features.</li> </ul> <p>It’s a safeguard, a way to build more robust, more generalizable models that perform well on unseen data – which is the true test of a model’s worth.</p> <h3 id="the-final-lesson-a-model-that-truly-understands">The Final Lesson: A Model That Truly Understands</h3> <p>Building a machine learning model isn’t just about minimizing error on the data you have; it’s about building a system that can wisely navigate the data it <em>hasn’t</em> seen yet. Regularization is the elegant, mathematical solution to this challenge. It teaches our models discipline, encouraging them to find simpler explanations, to focus on the truly important patterns, and to resist the temptation of memorizing noise.</p> <p>Next time you train a model, don’t just aim for zero training error. Aim for generalization. Embrace regularization, and your models will not only perform better, but they’ll also truly understand the underlying story of your data, rather than just reciting a memorized script. It’s a key step from being a data wizard to becoming a genuine data scientist.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>