<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Invisible Hand: How Kalman Filters Unveil the True State of a Noisy World | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/the-invisible-hand-how-kalman-filters-unveil-the-t/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Invisible Hand: How Kalman Filters Unveil the True State of a Noisy World</h1> <p class="post-meta"> Created on April 14, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/kalman-filter"> <i class="fa-solid fa-hashtag fa-sm"></i> Kalman Filter</a>   <a href="/blog/blog/tag/state-estimation"> <i class="fa-solid fa-hashtag fa-sm"></i> State Estimation</a>   <a href="/blog/blog/tag/time-series"> <i class="fa-solid fa-hashtag fa-sm"></i> Time Series</a>   <a href="/blog/blog/tag/probabilistic-modeling"> <i class="fa-solid fa-hashtag fa-sm"></i> Probabilistic Modeling</a>   <a href="/blog/blog/tag/control-systems"> <i class="fa-solid fa-hashtag fa-sm"></i> Control Systems</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Have you ever tried to find your way in a dense fog, relying on a compass that occasionally spins wildly and a map that might be slightly outdated? It’s a frustrating dance between your best guess of where you are, and unreliable new information. Now, imagine a powerful, invisible assistant by your side, constantly whispering the <em>most probable</em> truth about your location, even as the world throws curveballs.</p> <p>That invisible assistant, my friend, is a <strong>Kalman Filter</strong>.</p> <p>As a data scientist, I’ve always been fascinated by algorithms that seem to pull signal out of sheer noise. The Kalman Filter, a probabilistic powerhouse developed in the late 1950s and early 60s by Rudolf Kálmán, is one such marvel. It’s an algorithm that takes a series of noisy measurements observed over time, along with a mathematical model of how a system behaves, and produces an estimate of the system’s unknown variables that is, in a specific statistical sense, <em>optimal</em>.</p> <p>It sounds like magic, doesn’t it? But like all good magic, there’s elegant science behind it.</p> <h2 id="the-core-problem-finding-truth-in-uncertainty">The Core Problem: Finding Truth in Uncertainty</h2> <p>At its heart, the Kalman Filter addresses a fundamental problem: how do we know the “true state” of something when our measurements are always imperfect, and our understanding of how it changes is also an approximation?</p> <p>Imagine you’re tracking a self-driving car. You want to know its precise position and velocity.</p> <ul> <li> <strong>Measurements:</strong> You have GPS readings (which can be off by meters), radar (can be noisy), and internal odometers (drift over time). Each measurement comes with its own uncertainty.</li> <li> <strong>System Model:</strong> You also know how cars generally move – they don’t instantly jump from one place to another; they accelerate, decelerate, and turn. This is your prediction of its motion.</li> </ul> <p>The Kalman Filter doesn’t just average the measurements. It performs a sophisticated dance, weighing the certainty of its own prediction against the certainty of each new measurement. It’s constantly saying, “Based on where I thought you were and how I thought you’d move, here’s my best guess. Now, a new sensor just told me something different, but how much should I trust it? Let me combine these two pieces of information to get an even better guess.”</p> <h2 id="a-walkthrough-prediction-and-update">A Walkthrough: Prediction and Update</h2> <p>The Kalman Filter operates in a continuous cycle, alternating between two main steps:</p> <ol> <li> <strong>Prediction (or “Time Update”):</strong> Based on the system’s previous state estimate and its known dynamics (how it’s supposed to move), it predicts the current state. This step invariably <em>increases</em> the uncertainty of our estimate, because models are never perfect, and unmodeled forces exist.</li> <li> <strong>Update (or “Measurement Update”):</strong> When a new measurement arrives, the filter combines this noisy observation with its prediction to refine the state estimate. This step <em>decreases</em> the uncertainty, because new information helps narrow down the possibilities.</li> </ol> <p>Let’s get a bit more concrete. Everything in a Kalman Filter is represented by <strong>probability distributions</strong>, specifically <strong>Gaussian (normal) distributions</strong>.</p> <h3 id="whats-a-gaussian">What’s a Gaussian?</h3> <p>Think of a bell curve. It describes the probability of a value occurring.</p> <ul> <li>The peak of the bell curve is the <strong>mean</strong> ($\mu$), which is our best estimate of the true value.</li> <li>The width of the bell curve is the <strong>variance</strong> ($\sigma^2$) or <strong>covariance</strong> ($P$), which quantifies our uncertainty. A wider curve means more uncertainty; a narrower curve means higher confidence.</li> </ul> <p>So, when we talk about the “state” of our car (position, velocity), we’re not just talking about a single number; we’re talking about a <em>probability distribution</em> around that number.</p> <h2 id="diving-deeper-the-equations-of-motion-and-estimation">Diving Deeper: The Equations of Motion (and Estimation!)</h2> <p>Now, let’s peek under the hood at the mathematical engine driving this process. Don’t worry if the symbols seem intimidating at first; we’ll break them down.</p> <p>Our state at time $k$ is represented by a vector $\mathbf{x}_k$. For our car, this might be its position ($p_x, p_y$) and velocity ($v_x, v_y$): $\mathbf{x}_k = [p_x, p_y, v_x, v_y]^T$. Our uncertainty about this state is captured by a <strong>covariance matrix</strong> $P_k$.</p> <h3 id="step-1-prediction-the-guessing-phase">Step 1: Prediction (The “Guessing” Phase)</h3> <p>First, we predict the next state $\hat{\mathbf{x}}_k^-$ (the hat denotes an estimate, and the minus superscript means <em>before</em> incorporating the current measurement) and its associated uncertainty $P_k^-$.</p> <ol> <li> <p><strong>Project the current state forward:</strong> $\hat{\mathbf{x}}<em>k^- = A \hat{\mathbf{x}}</em>{k-1} + B \mathbf{u}_{k-1}$</p> <ul> <li>$\hat{\mathbf{x}}_{k-1}$: Our previous best estimate of the state.</li> <li>$A$: The state transition matrix. This describes how the system’s state evolves from $k-1$ to $k$ <em>in the absence of external forces</em>. For our car, this matrix encodes constant velocity motion.</li> <li>$\mathbf{u}_{k-1}$: The control input vector (e.g., accelerator pedal, steering wheel angle).</li> <li>$B$: The control input matrix, mapping the control input to the state.</li> <li>$\hat{\mathbf{x}}_k^-$: Our <em>a priori</em> (predicted) estimate for the current state.</li> </ul> </li> <li> <p><strong>Project the error covariance forward:</strong> $P_k^- = A P_{k-1} A^T + Q$</p> <ul> <li>$P_{k-1}$: The previous state’s error covariance matrix.</li> <li>$P_k^-$: The <em>a priori</em> error covariance matrix for the current state. Notice it’s larger than $P_{k-1}$ because our prediction isn’t perfect.</li> <li>$Q$: The process noise covariance matrix. This accounts for the uncertainty in our system model (e.g., unexpected bumps in the road, unmodeled wind gusts). It explicitly models the fact that our prediction will introduce new uncertainty.</li> </ul> </li> </ol> <p>Think of it this way: We’ve made our best guess for where the car will be. We know this guess comes with more uncertainty than our last confirmed location because things can always go a little off-script.</p> <h3 id="step-2-update-the-refining-phase">Step 2: Update (The “Refining” Phase)</h3> <p>Now, a new measurement $\mathbf{z}_k$ arrives (e.g., a new GPS reading). This measurement is noisy, but it contains valuable information.</p> <ol> <li> <p><strong>Calculate the measurement residual (innovation):</strong> $\mathbf{y}_k = \mathbf{z}_k - H \hat{\mathbf{x}}_k^-$</p> <ul> <li>$\mathbf{z}_k$: The actual measurement received.</li> <li>$H$: The observation matrix. This matrix converts the state space representation into the measurement space. For instance, if your state includes velocity but your sensor only measures position, $H$ would select just the position components.</li> <li>$H \hat{\mathbf{x}}_k^-$: The predicted measurement based on our <em>a priori</em> state estimate.</li> <li>$\mathbf{y}_k$: The residual, which is the difference between what we <em>actually measured</em> and what we <em>expected to measure</em>. If this is large, it means our prediction was off, or the measurement is wildly wrong.</li> </ul> </li> <li> <p><strong>Calculate the residual covariance:</strong> $S_k = H P_k^- H^T + R$</p> <ul> <li>$S_k$: The covariance of the residual. It tells us how uncertain the difference $\mathbf{y}_k$ is. It combines the uncertainty from our prediction ($H P_k^- H^T$) and the uncertainty from the measurement itself ($R$).</li> <li>$R$: The measurement noise covariance matrix. This quantifies the uncertainty inherent in the sensor data itself (e.g., GPS noise, radar inaccuracies).</li> </ul> </li> <li> <p><strong>Calculate the optimal Kalman Gain:</strong> $K_k = P_k^- H^T S_k^{-1}$</p> <ul> <li>$K_k$: The <strong>Kalman Gain</strong> is the heart of the filter! It’s a weighting factor that tells us how much to “trust” the new measurement (the residual $\mathbf{y}_k$) versus our own prediction.</li> <li>If $S_k$ is large (meaning both our prediction and measurement are very uncertain), $K_k$ will be small, giving more weight to our existing prediction.</li> <li>If $S_k$ is small (meaning we’re confident in both), $K_k$ will be larger, giving more weight to the new measurement.</li> </ul> </li> <li> <p><strong>Update the state estimate:</strong> $\hat{\mathbf{x}}_k = \hat{\mathbf{x}}_k^- + K_k \mathbf{y}_k$</p> <ul> <li>$\hat{\mathbf{x}}_k$: Our <em>a posteriori</em> (updated) best estimate of the state at time $k$. We take our initial prediction and add a correction proportional to the residual, weighted by the Kalman Gain.</li> </ul> </li> <li> <p><strong>Update the error covariance:</strong> $P_k = (I - K_k H) P_k^-$</p> <ul> <li>$P_k$: Our <em>a posteriori</em> error covariance matrix. This matrix is <strong>smaller</strong> than $P_k^-$, reflecting the fact that our uncertainty has decreased because we’ve incorporated new, valuable information. $I$ is the identity matrix.</li> </ul> </li> </ol> <p>And then the cycle repeats! With the new, refined $\hat{\mathbf{x}}_k$ and $P_k$, we go back to the prediction step for the next time instant.</p> <p>This iterative process ensures that the Kalman Filter always maintains an optimal balance between its internal model and external measurements. The magic of “optimality” here means that it minimizes the mean squared error (MSE) of the estimate, assuming linear system dynamics and Gaussian noise.</p> <h2 id="where-kalman-filters-shine-real-world-applications">Where Kalman Filters Shine: Real-World Applications</h2> <p>The impact of Kalman filters is vast and often unseen, touching almost every piece of modern technology that relies on precise state estimation:</p> <ul> <li> <strong>Navigation:</strong> GPS receivers use Kalman filters to combine satellite signals with internal inertial sensors (accelerometers, gyroscopes) to provide smooth, accurate location data, even when GPS signals are weak.</li> <li> <strong>Aerospace:</strong> From missile guidance systems to spacecraft attitude control (like the Apollo program!), Kalman filters are critical for precise trajectory and orientation.</li> <li> <strong>Robotics:</strong> Autonomous vehicles, drones, and industrial robots use them to estimate their position, velocity, and orientation (SLAM - Simultaneous Localization and Mapping often leverages filter-based approaches).</li> <li> <strong>Finance:</strong> In quantitative finance, Kalman filters are used in state-space models for asset price prediction, portfolio optimization, and understanding hidden market factors.</li> <li> <strong>Weather Forecasting:</strong> They help assimilate vast amounts of noisy sensor data into complex atmospheric models to improve prediction accuracy.</li> <li> <strong>Computer Vision:</strong> Object tracking (e.g., tracking a person in a video stream) often uses Kalman filters to predict an object’s next position and smooth its trajectory.</li> </ul> <h2 id="limitations-and-the-path-forward">Limitations and the Path Forward</h2> <p>While powerful, the classic Kalman Filter has its assumptions:</p> <ol> <li> <strong>Linearity:</strong> It assumes the system dynamics ($A, B$) and measurement model ($H$) are linear.</li> <li> <strong>Gaussian Noise:</strong> It assumes both process noise ($Q$) and measurement noise ($R$) are Gaussian.</li> </ol> <p>When these assumptions are violated, the filter is no longer <em>optimal</em>, though it can still perform reasonably well in some cases. This led to the development of extensions:</p> <ul> <li> <strong>Extended Kalman Filter (EKF):</strong> Linearizes the non-linear functions using Taylor series expansions around the current estimate. It’s widely used but can be prone to divergence if the non-linearity is severe.</li> <li> <strong>Unscented Kalman Filter (UKF):</strong> Uses a deterministic sampling technique (unscented transform) to pick a set of “sigma points” that capture the mean and covariance of the state distribution, then propagates these points through the non-linear functions. It generally performs better than EKF for highly non-linear systems.</li> <li> <strong>Particle Filters:</strong> For highly non-linear and non-Gaussian systems, particle filters use a set of random “particles” to represent the probability distribution, offering a more flexible (but computationally more intensive) solution.</li> </ul> <h2 id="why-this-matters-for-data-science--machine-learning">Why This Matters for Data Science &amp; Machine Learning</h2> <p>For us in data science and machine learning, understanding Kalman Filters is more than just appreciating a cool algorithm; it’s about building foundational intuition:</p> <ul> <li> <strong>Probabilistic Thinking:</strong> It’s a beautiful example of Bayesian inference in action – constantly updating beliefs (priors) with new evidence (likelihoods) to get a refined belief (posteriori).</li> <li> <strong>Time Series Analysis:</strong> It provides a robust framework for handling sequential data, estimating hidden states, and forecasting in dynamic systems where traditional statistical methods might struggle with noise.</li> <li> <strong>Uncertainty Quantification:</strong> The explicit modeling of covariance matrices ($\text{P, Q, R, S}$) teaches us the critical importance of not just having an estimate, but also knowing <em>how confident</em> we are in that estimate.</li> <li> <strong>Model-Based vs. Data-Driven:</strong> While much of modern ML is data-driven, Kalman Filters show the power of combining a strong <em>model</em> of the system with incoming data. This hybrid approach is increasingly relevant.</li> </ul> <h2 id="conclusion">Conclusion</h2> <p>The Kalman Filter truly is an invisible hand, constantly working behind the scenes, sifting through the cacophony of noisy data to reveal a clearer, more accurate picture of reality. It’s a testament to the power of mathematical modeling combined with probabilistic reasoning.</p> <p>Next time your GPS tells you exactly where you are, or a drone floats steadily in the sky, take a moment to appreciate the elegant dance of prediction and correction that the Kalman Filter orchestrates. It’s not just an algorithm; it’s a profound way of understanding and navigating uncertainty in our complex, data-rich world. And mastering its principles will undoubtedly make you a more insightful data scientist and machine learning engineer.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>