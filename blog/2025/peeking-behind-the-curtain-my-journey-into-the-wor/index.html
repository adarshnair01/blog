<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Peeking Behind the Curtain: My Journey into the World of Neural Networks | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/peeking-behind-the-curtain-my-journey-into-the-wor/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Peeking Behind the Curtain: My Journey into the World of Neural Networks</h1> <p class="post-meta"> Created on August 09, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/neural-networks"> <i class="fa-solid fa-hashtag fa-sm"></i> Neural Networks</a>   <a href="/blog/blog/tag/artificial-intelligence"> <i class="fa-solid fa-hashtag fa-sm"></i> Artificial Intelligence</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>From my earliest days tinkering with code, the idea of “intelligence” in machines felt like science fiction. Yet, here we are, witnessing AI perform feats that were once confined to movies. At the heart of much of this revolution lies a concept inspired by biology: the Neural Network.</p> <p>When I first encountered neural networks, the term itself felt intimidating. “Neural” implies brains, and “Network” suggests complex connections. But as I peeled back the layers (pun intended!), I discovered an elegant system built from surprisingly simple components. This isn’t just theory; it’s the engine behind image recognition, natural language processing, and personalized recommendations. In this post, I want to take you on the same journey of discovery I undertook, breaking down these incredible systems piece by piece.</p> <h3 id="the-neuron-the-fundamental-building-block">The Neuron: The Fundamental Building Block</h3> <p>Imagine the human brain. It’s a supercomputer made of billions of tiny cells called neurons. These biological neurons receive signals, process them, and then send new signals onwards. Neural networks, in their artificial form, draw direct inspiration from this biological marvel.</p> <p>Our artificial neuron, often called a <strong>perceptron</strong>, is much simpler, but its function mirrors its biological counterpart. Here’s how it works:</p> <ol> <li> <strong>Inputs ($x_i$):</strong> Just like a biological neuron receives signals from dendrites, an artificial neuron takes in numerical inputs. These could be pixel values from an image, word embeddings from text, or any other numerical data.</li> <li> <strong>Weights ($w_i$):</strong> Each input $x_i$ is multiplied by a corresponding weight $w_i$. Think of weights as the “strength” or “importance” of each input. A higher weight means that input has a greater influence on the neuron’s output. Initially, these weights are random, but they get fine-tuned during learning.</li> <li> <strong>Summation:</strong> All the weighted inputs are summed up: $ \sum_{i=1}^n (w_i x_i) $.</li> <li> <strong>Bias ($b$):</strong> To this sum, we add a single value called a bias. The bias can be thought of as an adjustable threshold or an intercept term, allowing the neuron to activate even if all inputs are zero, or conversely, making it harder to activate. So, the total sum becomes $ z = \sum_{i=1}^n (w_i x_i) + b $.</li> <li> <strong>Activation Function ($f$):</strong> Finally, this sum $z$ is passed through an <strong>activation function</strong>. This function decides whether the neuron should “fire” or not, and what its output should be. It introduces non-linearity, which is crucial for the network to learn complex patterns. The output of our single neuron is $ a = f(z) $.</li> </ol> <p>To put this into perspective, imagine a neuron deciding if you should bring an umbrella today.</p> <ul> <li>$x_1$: Cloudiness (0-100%)</li> <li>$x_2$: Wind Speed (mph)</li> <li>$x_3$: Temperature (Fahrenheit)</li> <li>$w_1$: Might be high (clouds are a strong indicator of rain)</li> <li>$w_2$: Might be moderate (wind sometimes brings rain)</li> <li>$w_3$: Might be low (temperature isn’t a primary indicator for rain)</li> <li>$b$: A general tendency to be prepared.</li> <li>$f$: A function that says, if the weighted sum crosses a certain threshold, “Yes, bring an umbrella!”</li> </ul> <p>This single neuron, or perceptron, is a simple decision-maker. But the magic truly begins when we connect many of them.</p> <h3 id="from-perceptron-to-neural-network-building-layers-of-intelligence">From Perceptron to Neural Network: Building Layers of Intelligence</h3> <p>A single perceptron is quite limited. It can only solve linearly separable problems (think drawing a single straight line to separate two categories of data points). This is where the “Network” part comes in. By connecting multiple neurons in layers, we create a powerful, interconnected system capable of far more complex decision-making.</p> <p>A typical neural network structure looks like this:</p> <ol> <li> <strong>Input Layer:</strong> This layer simply receives the initial data. No computations happen here; it just distributes the inputs to the next layer. If you’re classifying images, each neuron in the input layer might represent a single pixel’s intensity.</li> <li> <strong>Hidden Layers:</strong> These are the “brains” of the operation. Neurons in a hidden layer take inputs from the previous layer, perform their weighted sum and activation function, and then pass their outputs to the next layer. A network can have one, two, or even hundreds of hidden layers. When a network has many hidden layers, we call it a <strong>Deep Neural Network</strong>, which is where the “Deep” in Deep Learning comes from.</li> <li> <strong>Output Layer:</strong> This is the final layer that produces the network’s prediction or decision. For a binary classification (e.g., “cat” or “dog”), it might have one neuron outputting a probability. For multi-class classification (e.g., identifying digits 0-9), it would have one neuron for each class.</li> </ol> <p>Each neuron in a given layer is typically connected to <em>every</em> neuron in the next layer – this is known as a <strong>fully connected</strong> or <strong>dense</strong> layer. Information flows only in one direction, from input to output, which is why these are often called <strong>Feedforward Neural Networks</strong>.</p> <p>Imagine our “umbrella” decision expanded:</p> <ul> <li>The <strong>Input Layer</strong> gets data like cloudiness, wind, temperature.</li> <li>The first <strong>Hidden Layer</strong> might learn abstract concepts like “storm likelihood” or “comfort level.”</li> <li>A second <strong>Hidden Layer</strong> might combine these to form “potential impact on plans.”</li> <li>Finally, the <strong>Output Layer</strong> gives a refined decision: “definitely bring umbrella,” “maybe,” “no need.”</li> </ul> <p>The genius here is that the hidden layers automatically learn hierarchical features. The first layer might detect edges in an image, the second might combine edges into shapes, the third might combine shapes into objects (like eyes or ears), and finally, the output layer recognizes a full face. This automatic feature learning is a major advantage over traditional machine learning.</p> <h3 id="how-neural-networks-learn-the-magic-of-backpropagation">How Neural Networks Learn: The Magic of Backpropagation</h3> <p>So, we have this network of neurons, but how does it learn? How do those initial random weights become so precisely tuned that the network can perform astonishing feats? The answer lies in a powerful algorithm called <strong>Backpropagation</strong>.</p> <p>It’s an iterative process that works in two main steps:</p> <ol> <li> <strong>The Forward Pass (Prediction):</strong> <ul> <li>We feed a training example (e.g., an image of a cat) through the network, from the input layer, through all the hidden layers, to the output layer.</li> <li>The network makes a prediction (e.g., “90% dog, 10% cat”).</li> <li>We then compare this prediction ($\hat{y}$) to the actual truth (the <strong>label</strong>, $y$, which is “100% cat” in our example).</li> </ul> </li> <li> <strong>Calculating the Error (Loss Function):</strong> <ul> <li>We quantify how “wrong” the network’s prediction was using a <strong>loss function</strong> (or cost function). A common one is the Mean Squared Error (MSE), defined as: $ L(\mathbf{y}, \hat{\mathbf{y}}) = \frac{1}{m} \sum_{i=1}^m (y_i - \hat{y}_i)^2 $ Here, $y_i$ is the true value, $\hat{y}_i$ is the predicted value, and $m$ is the number of samples. This function gives us a single number representing the “penalty” for incorrect predictions. A high loss means the network is far off; a low loss means it’s doing well.</li> </ul> </li> <li> <strong>The Backward Pass (Backpropagation - Learning):</strong> <ul> <li>This is the core of learning. The goal is to adjust all the weights and biases in the network just enough to reduce the loss. But how do we know <em>which</em> weights to adjust and by <em>how much</em>?</li> <li>Think of it like tuning a complex instrument. When you play a wrong note, you know which string is off, and you know whether to tighten or loosen it. Backpropagation does something similar for all the thousands or millions of weights.</li> <li>It uses calculus, specifically the chain rule of differentiation, to calculate the <strong>gradient</strong> of the loss function with respect to <em>every single weight and bias</em> in the network. The gradient tells us two crucial things for each weight: <ul> <li> <strong>Direction:</strong> Should we increase or decrease this weight to reduce the loss?</li> <li> <strong>Magnitude:</strong> How sensitive is the loss to changes in this specific weight? (i.e., how big of a step should we take?).</li> </ul> </li> <li>This calculation starts from the output layer (where the error is visible) and propagates backward through the network, layer by layer, attributing responsibility for the error to each neuron and its connections. This is why it’s called “backpropagation.”</li> <li>Once we have these gradients, we update the weights and biases using an optimization algorithm called <strong>Gradient Descent</strong>. The update rule for a weight $w$ is: $ w<em>{new} = w</em>{old} - \alpha \frac{\partial L}{\partial w} $ Where $\frac{\partial L}{\partial w}$ is the partial derivative (gradient) of the loss with respect to that weight.</li> <li>The $\alpha$ (alpha) term is called the <strong>learning rate</strong>. It’s a crucial hyperparameter that determines how big a step we take in the direction of the steepest descent. A high learning rate might make us overshoot the minimum, while a very low one might make learning too slow.</li> </ul> </li> </ol> <p>This process of forward pass, loss calculation, and backward pass (weight adjustment) is repeated thousands or millions of times over many training examples. Each full pass through the entire dataset is called an <strong>epoch</strong>. With each epoch, the weights and biases get better and better, and the network’s predictions become more accurate.</p> <h3 id="activation-functions-the-key-to-non-linearity">Activation Functions: The Key to Non-Linearity</h3> <p>We briefly mentioned activation functions earlier, but they deserve a closer look. Without them, a neural network, no matter how many layers it has, would essentially just be performing a series of linear transformations. This means it could only learn linear relationships, which are insufficient for most real-world problems.</p> <p>Activation functions introduce non-linearity, allowing the network to model complex, non-linear relationships in the data. Think of it like bending and shaping the decision boundaries, rather than just drawing straight lines.</p> <p>Some common activation functions:</p> <ol> <li> <strong>Sigmoid:</strong> $ \sigma(z) = \frac{1}{1 + e^{-z}} $ <ul> <li>Outputs values between 0 and 1, making it useful for binary classification where the output can be interpreted as a probability.</li> <li>It compresses the input, which historically led to issues like the “vanishing gradient problem” during backpropagation, where gradients become extremely small, making learning very slow or even stopping it.</li> </ul> </li> <li> <strong>Rectified Linear Unit (ReLU):</strong> $ ReLU(z) = \max(0, z) $ <ul> <li>This is currently the most popular choice for hidden layers.</li> <li>It’s simple: if the input is positive, it outputs the input directly; otherwise, it outputs zero.</li> <li>Its simplicity makes it computationally efficient, and it helps mitigate the vanishing gradient problem. However, it can suffer from the “dying ReLU” problem, where neurons can get stuck outputting zero.</li> </ul> </li> <li> <strong>Tanh (Hyperbolic Tangent):</strong> Similar to Sigmoid but outputs values between -1 and 1.</li> <li> <strong>Softmax:</strong> Often used in the output layer for multi-class classification, as it converts a vector of numbers into a probability distribution, where all outputs sum to 1.</li> </ol> <p>The choice of activation function can significantly impact the network’s ability to learn and its training speed.</p> <h3 id="the-power-and-promise-of-neural-networks">The Power and Promise of Neural Networks</h3> <p>Why have neural networks exploded in popularity and capability in recent years?</p> <ul> <li> <strong>Universal Approximation Theorem:</strong> This fascinating theorem states that a feedforward neural network with just <em>one</em> hidden layer and a non-linear activation function can approximate any continuous function to an arbitrary degree of accuracy. This means, theoretically, NNs can learn any mapping from input to output, given enough data and neurons.</li> <li> <strong>Automatic Feature Learning:</strong> Unlike traditional machine learning algorithms that often require extensive manual feature engineering (telling the model what aspects of the data are important), deep neural networks can learn relevant features directly from raw data. This is particularly powerful for complex data types like images and raw text.</li> <li> <strong>Scalability:</strong> With the advent of massive datasets (Big Data) and powerful computational resources (GPUs), neural networks truly shine. More data and more compute generally lead to better performance for deep learning models.</li> <li> <strong>Versatility:</strong> From recognizing faces in photos (Convolutional Neural Networks) to understanding natural language (Recurrent Neural Networks, Transformers) and generating realistic images, NNs are incredibly adaptable to a wide range of tasks.</li> </ul> <h3 id="challenges-and-the-road-ahead">Challenges and the Road Ahead</h3> <p>Despite their immense power, neural networks aren’t a silver bullet. They come with their own set of challenges:</p> <ul> <li> <strong>Data Hunger:</strong> They typically require vast amounts of labeled data to train effectively, which can be expensive and time-consuming to acquire.</li> <li> <strong>Computational Cost:</strong> Training deep models can be incredibly resource-intensive, requiring powerful GPUs or TPUs and significant energy.</li> <li> <strong>Interpretability (The “Black Box”):</strong> Understanding <em>why</em> a neural network makes a particular decision can be incredibly difficult. This lack of transparency is a significant concern in critical applications like medicine or autonomous driving, leading to research in Explainable AI (XAI).</li> <li> <strong>Ethical Concerns:</strong> Bias present in training data can be amplified by neural networks, leading to unfair or discriminatory outcomes.</li> </ul> <p>However, research in neural networks is advancing at an astonishing pace. We’re seeing innovations in model architectures, optimization techniques, and methods to address interpretability and bias. The future promises even more sophisticated, efficient, and ethical AI systems.</p> <h3 id="conclusion-a-journey-just-beginning">Conclusion: A Journey Just Beginning</h3> <p>My journey into neural networks has been one of continuous awe and discovery. From the humble perceptron, inspired by a biological neuron, to complex deep learning architectures solving problems once thought insurmountable, the elegance and power of these systems are truly captivating. They represent a fundamental shift in how we approach problem-solving with machines, moving from explicit programming to learning from data.</p> <p>If you’ve followed along, I hope you now have a clearer picture of what neural networks are, how they’re built, and how they learn. This is just the beginning. The field of AI, powered by these incredible networks, is evolving daily. So, don’t just observe; build, experiment, and shape the future! The tools are more accessible than ever, and the possibilities are limitless.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>