<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Beyond the Snapshot: How Recurrent Neural Networks Teach Machines to Remember the Past | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/beyond-the-snapshot-how-recurrent-neural-networks/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Beyond the Snapshot: How Recurrent Neural Networks Teach Machines to Remember the Past</h1> <p class="post-meta"> Created on October 15, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/recurrent-neural-networks"> <i class="fa-solid fa-hashtag fa-sm"></i> Recurrent Neural Networks</a>   <a href="/blog/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> NLP</a>   <a href="/blog/blog/tag/sequence-modeling"> <i class="fa-solid fa-hashtag fa-sm"></i> Sequence Modeling</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Remember that moment in a conversation when someone brings up something you said five minutes ago, and it completely changes the meaning of what they’re saying <em>now</em>? Our human brains are incredible at this. We don’t just process information in isolated snapshots; we build context, link ideas, and crucially, remember the past to understand the present.</p> <p>But what about machines? For the longest time, the workhorse of deep learning – the Feedforward Neural Network – was a bit like a goldfish. It was brilliant at tasks like classifying images, where each input (an image) is independent of the last. Show it a cat, it says “cat.” Show it a dog, it says “dog.” It doesn’t remember the cat it saw a moment ago, nor does it care.</p> <p>This “goldfish memory” became a problem when we wanted AI to tackle sequential data: things like spoken language, text, music, or time series. In these domains, the order of information matters immensely. “I did not buy it” means something entirely different from “I did buy it.” The single word “not” flips the entire meaning, relying on context from the preceding words. A traditional feedforward network would struggle here, as it processes each word in isolation.</p> <p>This is where the magic of <strong>Recurrent Neural Networks (RNNs)</strong> comes in. They are deep learning architectures specifically designed to process sequential data by incorporating a form of “memory.” Imagine a neural network that doesn’t just look at the current input, but also whispers to itself about what it saw <em>just before</em>. That’s the essence of an RNN.</p> <h3 id="the-core-idea-a-loop-in-time">The Core Idea: A Loop in Time</h3> <p>At its heart, an RNN is very similar to a feedforward network, but with a twist: it has a loop. This loop allows information to persist from one step of the network to the next. Think of it like this: when an RNN processes an input at a particular time step, it doesn’t just produce an output; it also updates an internal “hidden state.” This hidden state then acts as memory, influencing the processing of the <em>next</em> input in the sequence.</p> <p>Let’s try to visualize this. Imagine you’re reading a book. You read a word. To understand that word, you also remember the context from the previous words in the sentence. An RNN works similarly:</p> <ol> <li> <strong>Input ($x_t$):</strong> At time $t$, the network receives a new piece of data from the sequence (e.g., a word in a sentence).</li> <li> <strong>Hidden State ($h_t$):</strong> It combines this new input with its “memory” from the previous time step ($h_{t-1}$). This combination results in a new hidden state, which encapsulates the current understanding of the sequence up to time $t$.</li> <li> <strong>Output ($y_t$):</strong> Based on this new hidden state, the network can produce an output (e.g., predicting the next word, or classifying the sentiment of the sentence so far).</li> <li> <strong>Recurrence:</strong> The crucial part is that $h_t$ is then passed along as $h_{t-1}$ for the <em>next</em> time step, $t+1$. This creates the loop, enabling the network to learn patterns and dependencies across time.</li> </ol> <p>To make this clearer, we often “unroll” the RNN over time. Instead of a single neuron with a loop, we picture multiple copies of the same network, each passing its hidden state to the next.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>x_0 --&gt; RNN_cell_0 --&gt; h_0 --&gt; RNN_cell_1 --&gt; h_1 --&gt; RNN_cell_2 --&gt; h_2 ...
        |       ^             |       ^             |       ^
        v       |             v       |             v       |
        y_0     |             y_1     |             y_2     |
        (from previous h)     (from previous h)     (from previous h)
</code></pre></div></div> <p><em>A simplified unrolled RNN, where $x_t$ is the input at time $t$, $h_t$ is the hidden state (memory) at time $t$, and $y_t$ is the output at time $t$. Notice how $h_t$ feeds into the next cell.</em></p> <p>Crucially, <strong>the same set of weights and biases are used at every time step.</strong> This is what allows RNNs to learn sequential patterns and generalize across different parts of a sequence, regardless of its length.</p> <h3 id="the-math-behind-the-memory">The Math Behind the Memory</h3> <p>Let’s peek under the hood at the core equations that govern a simple vanilla RNN cell. Don’t worry, it’s not as scary as it looks!</p> <p>At each time step $t$:</p> <p>The new hidden state $h_t$ is calculated using the current input $x_t$ and the previous hidden state $h_{t-1}$:</p> <p>$h_t = \tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)$</p> <p>And the output $y_t$ (if we’re making an output at every step) is calculated from the current hidden state $h_t$:</p> <p>$y_t = W_{hy} h_t + b_y$</p> <p>Let’s break down these terms:</p> <ul> <li>$x_t$: This is your input vector at time step $t$. For text, this might be a word embedding (a numerical representation of a word).</li> <li>$h_t$: This is the hidden state vector at time step $t$. It’s the network’s “memory” or understanding of the sequence up to this point.</li> <li>$h_{t-1}$: This is the hidden state from the previous time step. It’s the memory passed from the past.</li> <li>$W_{hh}$: These are the weights for the recurrent connection (hidden-to-hidden). They determine how much influence the previous hidden state has on the current one.</li> <li>$W_{xh}$: These are the weights for the input-to-hidden connection. They determine how much influence the current input has on the current hidden state.</li> <li>$W_{hy}$: These are the weights for the hidden-to-output connection. They map the hidden state to the desired output.</li> <li>$b_h$ and $b_y$: These are bias vectors, which allow the network to shift the activation function.</li> <li>$\tanh$: This is the hyperbolic tangent activation function. It squashes the values between -1 and 1, helping to stabilize the hidden state values and prevent them from exploding. Other activation functions like ReLU or sigmoid can also be used, though $\tanh$ is common for $h_t$.</li> </ul> <p>The beauty here is that $W_{hh}$, $W_{xh}$, $W_{hy}$, $b_h$, and $b_y$ are learned during training and are <strong>shared across all time steps</strong>. This parameter sharing is fundamental to RNNs’ ability to recognize patterns no matter where they appear in a sequence.</p> <h3 id="training-rnns-backpropagation-through-time-bptt">Training RNNs: Backpropagation Through Time (BPTT)</h3> <p>Training an RNN is similar to training a regular neural network, but with a twist. We use an algorithm called <strong>Backpropagation Through Time (BPTT)</strong>. Essentially, we unroll the network for the entire sequence, and then apply standard backpropagation to this unrolled computation graph. The gradients are computed by summing them up across all time steps.</p> <p>However, this process isn’t without its challenges.</p> <h4 id="the-vanishing-and-exploding-gradient-problem">The Vanishing and Exploding Gradient Problem</h4> <p>Imagine you’re playing a game of “telephone” down a very long line. If each person slightly mumbles (vanishing gradient) or wildly exaggerates (exploding gradient) the message, by the time it reaches the end, the original message is completely lost or unrecognizable.</p> <p>RNNs suffer from this problem because gradients are propagated back through many time steps.</p> <ul> <li> <strong>Vanishing Gradients:</strong> As gradients are multiplied by weight matrices at each time step during backpropagation, if these weights are small, the gradients can shrink exponentially, becoming effectively zero. This means the network struggles to learn long-term dependencies; it “forgets” information from earlier in the sequence. It’s like trying to remember the very first word of a long book while reading the last chapter – the signal has faded too much.</li> <li> <strong>Exploding Gradients:</strong> Conversely, if the weights are large, gradients can grow exponentially, leading to extremely large updates to the network weights. This causes instability, making the network unable to learn effectively (think NaN values and random guesses).</li> </ul> <p>These issues made training vanilla RNNs for sequences longer than a few dozen steps very difficult. And this is precisely why researchers developed more sophisticated recurrent architectures.</p> <h3 id="the-gradient-problem-solvers-lstms-and-grus">The Gradient Problem Solvers: LSTMs and GRUs</h3> <p>To combat the vanishing and exploding gradient problems and better capture long-term dependencies, two ingenious variations of RNNs were introduced: <strong>Long Short-Term Memory (LSTM)</strong> networks and <strong>Gated Recurrent Units (GRU)</strong>. These models are essentially RNNs with more complex “memory cells” that have internal mechanisms (called “gates”) to regulate the flow of information.</p> <h4 id="long-short-term-memory-lstm-networks">Long Short-Term Memory (LSTM) Networks</h4> <p>LSTMs, introduced by Hochreiter &amp; Schmidhuber in 1997, are the undisputed champions of sequential data processing for a long time. They effectively solved the vanishing gradient problem, allowing RNNs to learn dependencies over hundreds, or even thousands, of time steps.</p> <p>The key innovation in an LSTM is the <strong>cell state ($C_t$)</strong>, which acts like a conveyor belt running through the entire chain. It’s designed to carry information across many time steps, preserving it or updating it as needed. Three main “gates” regulate this cell state:</p> <ol> <li> <p><strong>Forget Gate ($f_t$):</strong> Decides what information to throw away from the cell state. It looks at the previous hidden state $h_{t-1}$ and the current input $x_t$, and outputs a number between 0 and 1 for each number in the cell state $C_{t-1}$. A 0 means “completely forget,” while a 1 means “completely keep.” $f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$</p> </li> <li> <strong>Input Gate ($i_t$) and Candidate Cell State ($\tilde{C}_t$):</strong> Decides what new information to store in the cell state. <ul> <li>The input gate $i_t$ decides which values to update.</li> <li>The candidate cell state $\tilde{C}<em>t$ is a new potential memory that could be added. $i_t = \sigma(W_i \cdot [h</em>{t-1}, x_t] + b_i)$ $\tilde{C}<em>t = \tanh(W_C \cdot [h</em>{t-1}, x_t] + b_C)$</li> </ul> </li> <li> <p><strong>Update Cell State:</strong> The old cell state $C_{t-1}$ is updated to the new cell state $C_t$ by first forgetting what the forget gate decided, and then adding the new candidate memory, scaled by the input gate: $C_t = f_t * C_{t-1} + i_t * \tilde{C}_t$</p> </li> <li> <strong>Output Gate ($o_t$):</strong> Decides what parts of the cell state to output as the new hidden state ($h_t$). This output gate filters the cell state based on the current input and previous hidden state. $o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$ $h_t = o_t * \tanh(C_t)$</li> </ol> <p>These gates, typically implemented with sigmoid activation functions ($\sigma$) which output values between 0 and 1, allow LSTMs to selectively remember or forget information, effectively managing long-term dependencies. It’s like having a sophisticated memory manager in your AI’s brain!</p> <h4 id="gated-recurrent-units-grus">Gated Recurrent Units (GRUs)</h4> <p>GRUs, introduced by Cho et al. in 2014, are a slightly simpler variation of LSTMs. They combine the cell state and hidden state into a single “hidden state” and use fewer gates. This makes them computationally less expensive and faster to train, while often achieving comparable performance to LSTMs.</p> <p>A GRU has two main gates:</p> <ol> <li> <p><strong>Update Gate ($z_t$):</strong> This gate determines how much of the past information (from $h_{t-1}$) should be carried over to the current time step and how much of the new information (from $x_t$) should be used. It essentially combines the forget and input gates of an LSTM. $z_t = \sigma(W_z \cdot [h_{t-1}, x_t] + b_z)$</p> </li> <li> <p><strong>Reset Gate ($r_t$):</strong> This gate decides how much of the past hidden state to “forget” when calculating the new candidate hidden state. If the reset gate is close to 0, it means the network effectively ignores the past hidden state and focuses only on the current input. $r_t = \sigma(W_r \cdot [h_{t-1}, x_t] + b_r)$</p> </li> </ol> <p>The new candidate hidden state $\tilde{h}<em>t$ is then calculated by combining the current input with the *reset* version of the previous hidden state: $\tilde{h}_t = \tanh(W \cdot [r_t * h</em>{t-1}, x_t] + b)$</p> <p>Finally, the new hidden state $h_t$ is a linear combination of the previous hidden state and the new candidate hidden state, controlled by the update gate: $h_t = (1 - z_t) * h_{t-1} + z_t * \tilde{h}_t$</p> <p>GRUs offer a good balance between complexity and performance, making them a popular choice in many sequence modeling tasks.</p> <h3 id="applications-of-rnns-and-lstmsgrus">Applications of RNNs (and LSTMs/GRUs)</h3> <p>The ability of RNNs (especially LSTMs and GRUs) to process and remember sequential information has unlocked a vast array of applications across various fields:</p> <ul> <li> <strong>Natural Language Processing (NLP):</strong> <ul> <li> <strong>Machine Translation:</strong> Understanding a sentence in one language and generating it in another (e.g., Google Translate).</li> <li> <strong>Text Generation:</strong> Writing coherent sentences or paragraphs, like predictive text or creative writing AI.</li> <li> <strong>Sentiment Analysis:</strong> Determining if a piece of text expresses positive, negative, or neutral sentiment.</li> <li> <strong>Speech Recognition:</strong> Converting spoken language into text.</li> </ul> </li> <li> <strong>Time Series Prediction:</strong> Forecasting future values based on past observations, such as stock prices, weather patterns, or energy consumption.</li> <li> <strong>Music Generation:</strong> Creating new melodies or harmonies.</li> <li> <strong>Video Processing:</strong> Understanding actions and events in video clips by processing frames sequentially.</li> <li> <strong>Recommendation Systems:</strong> Suggesting items based on a user’s sequence of past interactions.</li> </ul> <h3 id="limitations-and-the-rise-of-transformers">Limitations and the Rise of Transformers</h3> <p>While LSTMs and GRUs were a monumental leap forward, they aren’t without their limitations. The fundamental sequential nature of their processing means:</p> <ol> <li> <strong>Slow Parallelization:</strong> Each step relies on the output of the previous step. This makes it difficult to process different parts of a sequence simultaneously, which is a bottleneck for very long sequences and slows down training on modern parallel hardware (like GPUs).</li> <li> <strong>Still Limited Long-Term Memory:</strong> Although LSTMs/GRUs are much better, they can still struggle with extremely long dependencies, as the information has to pass through many gates and calculations, potentially leading to information loss.</li> </ol> <p>These limitations paved the way for a new architecture that has dominated the field, particularly in NLP: <strong>Transformers</strong>. Transformers ditch the recurrence entirely, relying instead on a mechanism called “attention” to weigh the importance of different parts of the input sequence. This allows them to process all parts of a sequence in parallel and capture very long-range dependencies effectively.</p> <p>However, this doesn’t mean RNNs are obsolete! They still excel in specific scenarios:</p> <ul> <li> <strong>Streaming Data:</strong> For tasks where data arrives sequentially and real-time processing is crucial (e.g., live speech processing), RNNs can be more suitable than Transformers which typically require the entire sequence upfront.</li> <li> <strong>Shorter Sequences:</strong> For sequences that aren’t excessively long, GRUs and LSTMs can be simpler, faster to train, and require fewer parameters than large Transformer models.</li> <li> <strong>Specific Architectures:</strong> They still form components in hybrid models or certain encoder-decoder architectures.</li> </ul> <h3 id="conclusion-the-enduring-legacy-of-memory">Conclusion: The Enduring Legacy of Memory</h3> <p>Recurrent Neural Networks represent a crucial turning point in artificial intelligence. By giving machines the ability to “remember” and incorporate past information into current decisions, they enabled AI to move beyond static, independent inputs and delve into the rich, dynamic world of sequences.</p> <p>From the simple loop of a vanilla RNN to the sophisticated gating mechanisms of LSTMs and GRUs, these architectures have profoundly shaped how we build intelligent systems that can understand and generate language, predict future trends, and interact with the world in a more context-aware manner. While new architectures like Transformers push the boundaries further, the fundamental concept of recurrence and sequential memory, pioneered by RNNs, remains a cornerstone of deep learning and a testament to the power of giving machines a past.</p> <p>So, the next time an AI understands your nuanced query or finishes your sentence perfectly, remember the recurrent networks whispering to themselves, diligently recalling the sequence of thoughts that led to the present moment. They taught machines how to remember, and that, my friends, is a powerful form of intelligence.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>