<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Your Digital Sidekick: Unpacking the Magic of Recommender Systems | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/your-digital-sidekick-unpacking-the-magic-of-recom/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Your Digital Sidekick: Unpacking the Magic of Recommender Systems</h1> <p class="post-meta"> Created on November 11, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/recommender-systems"> <i class="fa-solid fa-hashtag fa-sm"></i> Recommender Systems</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/collaborative-filtering"> <i class="fa-solid fa-hashtag fa-sm"></i> Collaborative Filtering</a>   <a href="/blog/blog/tag/content-based"> <i class="fa-solid fa-hashtag fa-sm"></i> Content-Based</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>Have you ever paused to think about how much of your digital life is shaped by recommendations? From the movies Netflix suggests to the products Amazon pushes your way, or even the news articles you see on your feed – it’s all powered by an incredibly clever piece of technology: the Recommender System.</p> <p>For a long time, I took these suggestions for granted. They just <em>appeared</em>. But as I delved deeper into the world of data science and machine learning, I realized that behind every “you might also like” lies a sophisticated algorithm, working tirelessly to understand our preferences and predict our desires. It’s like having a digital friend who just <em>gets</em> your taste.</p> <p>In this post, I want to take you on a journey through the heart of recommender systems. We’ll peel back the layers, understand how they work, explore their different flavors, and even touch upon some of the fascinating challenges they face. Whether you’re a high school student curious about AI or a fellow data science enthusiast, I hope this makes the magic a little more tangible.</p> <p>Ready? Let’s dive in!</p> <h3 id="whats-a-recommender-system-anyway">What’s a Recommender System, Anyway?</h3> <p>At its core, a recommender system is an information filtering system that predicts what a user might like. It aims to solve the “information overload” problem by sifting through a vast ocean of items (movies, books, products, songs) and presenting only those most relevant and appealing to <em>you</em>.</p> <p>Think about it: back in the day, you’d ask a friend for a movie recommendation. Now, Netflix does it automatically, and often, it does a pretty good job! This isn’t magic; it’s data.</p> <p>The goal? Enhance user experience, increase engagement, and drive business value by connecting users with items they’re likely to enjoy, discover, or purchase.</p> <h3 id="the-big-two-content-based-vs-collaborative-filtering">The Big Two: Content-Based vs. Collaborative Filtering</h3> <p>Most recommender systems fall into one of two main categories, or sometimes, a clever combination of both.</p> <h4 id="1-content-based-filtering-the-if-you-like-that-youll-like-this-approach">1. Content-Based Filtering: The “If You Like That, You’ll Like This” Approach</h4> <p>Imagine you love sci-fi movies, specifically ones with space exploration and philosophical themes. A content-based recommender system would learn <em>your preferences</em> from these movies and then suggest other movies that share similar characteristics (i.e., also have “space exploration” and “philosophical themes” tags).</p> <p><strong>How it Works:</strong></p> <ol> <li> <strong>Item Representation:</strong> Each item is described by a set of features (its “content”). For a movie, these could be genre, actors, director, keywords from the plot summary, release year, etc. We can represent these features as a vector.</li> <li> <strong>User Profile:</strong> The system builds a profile for you based on the items you’ve <em>already liked</em>. This profile is often an aggregation (e.g., average, sum) of the feature vectors of all the items you’ve interacted positively with.</li> <li> <strong>Similarity Matching:</strong> When it’s time to recommend, the system compares your user profile vector to the feature vectors of un-watched/un-rated items. Items with high similarity to your profile are recommended.</li> </ol> <p>One common way to measure similarity between these feature vectors is using <strong>Cosine Similarity</strong>. If you have two vectors, A and B, representing items or a user profile, the cosine similarity is:</p> <table> <tbody> <tr> <td>$\cos(\theta) = \frac{\mathbf{A} \cdot \mathbf{B}}{</td> <td> </td> <td>\mathbf{A}</td> <td> </td> <td>\cdot</td> <td> </td> <td>\mathbf{B}</td> <td> </td> <td>}$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>Here, $\mathbf{A} \cdot \mathbf{B}$ is the dot product of the vectors, and $</td> <td> </td> <td>\mathbf{A}</td> <td> </td> <td>$ and $</td> <td> </td> <td>\mathbf{B}</td> <td> </td> <td>$ are their magnitudes. This value ranges from -1 (completely dissimilar) to 1 (completely similar), with 0 indicating no relationship.</td> </tr> </tbody> </table> <p><strong>Pros:</strong></p> <ul> <li> <strong>User Independence:</strong> It doesn’t need data from other users. If you’re the only person on the platform, it can still recommend based on your past actions.</li> <li> <strong>Niche Items:</strong> Can recommend niche items if they align with a user’s specific interests.</li> <li> <strong>Transparency:</strong> Recommendations can be easily explained (e.g., “because you watched other sci-fi movies”).</li> </ul> <p><strong>Cons:</strong></p> <ul> <li> <strong>Limited Serendipity:</strong> Tends to recommend items very similar to what you already liked, limiting discovery of new interests. It can get you stuck in a “filter bubble.”</li> <li> <strong>Cold Start for Items:</strong> New items without detailed content descriptions can’t be recommended effectively.</li> <li> <strong>Feature Engineering:</strong> Requires careful and often manual extraction of item features, which can be time-consuming and challenging.</li> </ul> <h4 id="2-collaborative-filtering-the-birds-of-a-feather-approach">2. Collaborative Filtering: The “Birds of a Feather” Approach</h4> <p>This is often considered the most powerful and widely used approach. Collaborative filtering works on the principle that if two users have similar tastes in the past, they will likely have similar tastes in the future. Or, conversely, if two items are often liked by the same group of users, they are probably similar.</p> <p>“People who bought X also bought Y” is the classic example of collaborative filtering in action.</p> <p>There are two main sub-types here:</p> <h5 id="a-user-based-collaborative-filtering">a) User-Based Collaborative Filtering</h5> <p>“Find users similar to me, and recommend items <em>they</em> liked that <em>I haven’t</em> seen yet.”</p> <p>Imagine you and your friend, Sarah, have watched and rated many of the same movies, and you both gave high ratings to the same obscure indie films. A user-based system would identify Sarah as similar to you. Then, it would look at movies Sarah loved that you haven’t seen, and recommend those to you.</p> <p><strong>How it Works:</strong></p> <ol> <li> <strong>User Similarity:</strong> Identify users with similar taste patterns. Similarity can be calculated using metrics like Pearson Correlation or Cosine Similarity on their rating vectors.</li> <li> <strong>Recommendation Generation:</strong> For a target user, take a weighted average of the ratings given by similar users to unrated items. The weighting comes from the similarity scores.</li> </ol> <p><strong>Challenges:</strong></p> <ul> <li> <strong>Scalability:</strong> Finding similar users among millions can be computationally very expensive, especially in real-time.</li> <li> <strong>Sparsity:</strong> Many users rate only a tiny fraction of available items, making it hard to find truly similar users based on overlapping ratings.</li> <li> <strong>Cold Start for Users:</strong> New users have no rating history, so it’s impossible to find similar users.</li> </ul> <h5 id="b-item-based-collaborative-filtering">b) Item-Based Collaborative Filtering</h5> <p>“Find items similar to <em>this item I liked</em>, and recommend those.”</p> <p>This approach is generally more robust and scalable than user-based CF. Instead of finding similar <em>users</em>, it finds similar <em>items</em>. If you loved “The Matrix,” the system finds other movies that users who liked “The Matrix” also enjoyed, and recommends those.</p> <p><strong>How it Works:</strong></p> <ol> <li> <strong>Item Similarity:</strong> Calculate the similarity between every pair of items. This similarity is based on how users rated them. For example, two movies are similar if many users rated them similarly. Cosine Similarity on item-rating vectors is common here.</li> <li> <strong>Recommendation Generation:</strong> When recommending for a user, look at the items they have already liked. Then, for each liked item, find its most similar items (that the user hasn’t seen) and recommend them.</li> </ol> <p><strong>Pros:</strong></p> <ul> <li> <strong>Scalability:</strong> Item-item similarities are often pre-computed offline and are more stable than user-user similarities (user preferences change faster than item characteristics).</li> <li> <strong>Real-time Recommendations:</strong> Fast to serve recommendations online.</li> <li> <strong>Serendipity:</strong> Can recommend items that are “far” from the user’s explicit content profile but loved by similar users.</li> </ul> <p><strong>Cons:</strong></p> <ul> <li> <strong>Cold Start for Items:</strong> New items have no user ratings, so their similarity to other items cannot be computed.</li> <li> <strong>Limited Serendipity (compared to user-based):</strong> Can still fall into a trap of recommending popular items.</li> </ul> <h5 id="c-model-based-collaborative-filtering-matrix-factorization">c) Model-Based Collaborative Filtering: Matrix Factorization</h5> <p>This is where things get a bit more mathematical and truly powerful. Instead of directly using user-item similarities, model-based methods try to <em>learn</em> underlying patterns from the data.</p> <p><strong>The Core Idea:</strong> Imagine you have a huge table (a matrix) where rows are users, columns are items, and the cells contain the user’s rating for that item (many cells would be empty, representing unrated items). This is called the user-item interaction matrix, $R$.</p> <p>Matrix factorization aims to decompose this large, sparse matrix $R$ into two smaller, lower-dimensional matrices:</p> <ol> <li>A “User Factor” matrix $P$, where each row represents a user and their relationship to a set of <em>latent factors</em>.</li> <li>An “Item Factor” matrix $Q$, where each row represents an item and its relationship to the same set of <em>latent factors</em>.</li> </ol> <p>The idea is that each user and each item can be characterized by a small number of “latent factors” – hidden features or preferences that we don’t explicitly define (like “love for epic fantasy” or “preference for indie documentaries”).</p> <p>So, for any user $u$ and item $i$, the predicted rating $\hat{r}_{ui}$ can be approximated by the dot product of their respective latent factor vectors, $p_u$ and $q_i$:</p> <p>$\hat{r}_{ui} \approx p_u^T q_i$</p> <p>The process involves learning the values in matrices $P$ and $Q$ by minimizing the difference between the predicted ratings and the actual known ratings, often using techniques like Singular Value Decomposition (SVD) or Alternating Least Squares (ALS). A typical loss function might look something like this:</p> <table> <tbody> <tr> <td>$\min_{P, Q} \sum_{(u,i) \in K} (r_{ui} - p_u^T q_i)^2 + \lambda (</td> <td> </td> <td>p_u</td> <td> </td> <td>^2 +</td> <td> </td> <td>q_i</td> <td> </td> <td>^2)$</td> </tr> </tbody> </table> <p>Here, $K$ is the set of known ratings, $r_{ui}$ is the actual rating, $p_u^T q_i$ is the predicted rating, and $\lambda$ is a regularization parameter to prevent overfitting.</p> <p><strong>Pros:</strong></p> <ul> <li> <strong>Handles Sparsity:</strong> Can make good predictions even when the user-item matrix is very sparse.</li> <li> <strong>Scalability:</strong> More scalable than traditional user-based CF for large datasets.</li> <li> <strong>Discovers Latent Features:</strong> Uncovers hidden patterns and relationships between users and items.</li> <li> <strong>Better Accuracy:</strong> Often provides more accurate recommendations than simpler CF methods.</li> </ul> <p><strong>Cons:</strong></p> <ul> <li> <strong>Interpretability:</strong> The “latent factors” are often abstract and hard to explain.</li> <li> <strong>Cold Start Problem:</strong> Still struggles with new users or new items (they have no existing ratings to learn from).</li> </ul> <h3 id="hybrid-recommender-systems-the-best-of-both-worlds">Hybrid Recommender Systems: The Best of Both Worlds</h3> <p>Given the strengths and weaknesses of content-based and collaborative filtering, it’s not surprising that many real-world recommender systems combine them. Hybrid approaches aim to leverage the advantages of each to overcome individual limitations.</p> <p>Imagine a new movie is released. A content-based system could immediately recommend it based on its genre and actors. Once a few users rate it, collaborative filtering can kick in, using these ratings to refine its recommendations. This directly addresses the “cold start for items” problem.</p> <p>Common hybrid strategies include:</p> <ul> <li> <strong>Weighted Hybrid:</strong> Combining the scores from multiple recommenders (e.g., $Score = w_1 \cdot Score_{CB} + w_2 \cdot Score_{CF}$).</li> <li> <strong>Switching Hybrid:</strong> Switching between recommenders based on the situation (e.g., content-based for new users/items, collaborative for established ones).</li> <li> <strong>Feature Combination:</strong> Integrating content features directly into collaborative filtering models (e.g., using item features as part of the input to a matrix factorization model).</li> </ul> <h3 id="challenges-in-building-recommender-systems">Challenges in Building Recommender Systems</h3> <p>Building these systems is complex and comes with its own set of hurdles:</p> <ol> <li> <strong>Cold Start Problem:</strong> How do you recommend to a brand new user with no history, or recommend a brand new item with no ratings? This is a major challenge, often addressed by popularity-based recommendations, asking users for initial preferences, or using content-based methods.</li> <li> <strong>Sparsity:</strong> In most datasets, users only interact with a tiny fraction of available items. The user-item matrix is largely empty, making it hard to find strong patterns.</li> <li> <strong>Scalability:</strong> Real-world systems like Netflix or Amazon deal with millions of users and items, requiring efficient algorithms and distributed computing infrastructure.</li> <li> <strong>Diversity and Serendipity:</strong> Recommending only what’s popular or extremely similar can lead to a “filter bubble.” A good system balances relevance with suggesting diverse, unexpected, yet enjoyable items (serendipity).</li> <li> <strong>Bias:</strong> Data reflects existing biases. If certain demographics are underrepresented in the data, the system might not recommend well for them. Popularity bias (popular items get recommended more, reinforcing their popularity) is also common.</li> <li> <strong>Explainability:</strong> Users often want to know <em>why</em> an item was recommended. “Because you watched other action movies” is simple, but “because latent factor 7, 12, and 23 align with your profile” is not very helpful.</li> </ol> <h3 id="evaluating-recommender-systems">Evaluating Recommender Systems</h3> <p>How do we know if a recommender system is “good”? We need metrics!</p> <ul> <li> <strong>Offline Metrics:</strong> <ul> <li> <strong>RMSE (Root Mean Squared Error) / MAE (Mean Absolute Error):</strong> Used when predicting explicit ratings. They measure how close our predicted ratings are to the actual ratings.</li> <li> <strong>Precision, Recall, F1-score:</strong> Used for implicit feedback (e.g., clicks, purchases) or when making a ranked list of recommendations. They measure how many of the top-N recommendations were relevant.</li> <li> <strong>NDCG (Normalized Discounted Cumulative Gain):</strong> A ranking metric that gives higher scores to relevant items appearing earlier in the recommendation list.</li> <li> <strong>Coverage:</strong> Measures the percentage of items or users for which the system can make recommendations.</li> <li> <strong>Diversity / Serendipity:</strong> Harder to quantify but crucial for a healthy ecosystem.</li> </ul> </li> <li> <strong>Online Metrics (A/B Testing):</strong> The ultimate test. We deploy different versions of the recommender system to different user groups and measure real-world impact on metrics like click-through rates, conversion rates, user engagement, and retention.</li> </ul> <h3 id="conclusion-your-digital-sidekick-is-learning">Conclusion: Your Digital Sidekick is Learning</h3> <p>Recommender systems are more than just fancy algorithms; they are the digital architects of our personalized world. They’ve transformed how we discover new music, choose our next show, and even shop for groceries. As data grows and machine learning techniques become more sophisticated, these systems will only get smarter, more nuanced, and hopefully, even better at surprising us with delightful discoveries.</p> <p>I hope this deep dive has demystified some of the magic behind your digital sidekicks. The next time you see a recommendation pop up, take a moment to appreciate the incredible engineering and data science that made it possible.</p> <p>The field is constantly evolving, with new research in areas like deep learning for recommendations, fairness, and causal inference. It’s a fantastic area to explore if you’re passionate about building intelligent systems that truly understand and enhance human experience.</p> <p>What’s your favorite recommendation you’ve ever received? Let me know in the comments!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>