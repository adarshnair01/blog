<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Silent Maestro of Uncertainty: Demystifying the Kalman Filter | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/the-silent-maestro-of-uncertainty-demystifying-the/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Silent Maestro of Uncertainty: Demystifying the Kalman Filter</h1> <p class="post-meta"> Created on January 09, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/kalman-filter"> <i class="fa-solid fa-hashtag fa-sm"></i> Kalman Filter</a>   <a href="/blog/blog/tag/state-estimation"> <i class="fa-solid fa-hashtag fa-sm"></i> State Estimation</a>   <a href="/blog/blog/tag/control-theory"> <i class="fa-solid fa-hashtag fa-sm"></i> Control Theory</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/robotics"> <i class="fa-solid fa-hashtag fa-sm"></i> Robotics</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hello fellow explorers of the data universe!</p> <p>Today, I want to pull back the curtain on a truly remarkable piece of engineering and mathematics that, despite its immense power, often lurks in the shadows for those outside specialized fields. It’s called the <strong>Kalman Filter</strong>. If you’ve ever marveled at a drone smoothly holding its position, an autonomous vehicle navigating city streets, or even your phone’s GPS pin-pointing your location with uncanny accuracy, you’ve witnessed the Kalman Filter in action.</p> <p>For a long time, the Kalman Filter felt like this intimidating, arcane beast of equations. But as I dove deeper, I realized its core idea is beautifully intuitive – a masterpiece of common sense, formalized. And trust me, once you grasp its essence, you’ll start seeing its potential everywhere.</p> <h3 id="the-problem-life-in-a-noisy-world">The Problem: Life in a Noisy World</h3> <p>Imagine you’re trying to track something. Let’s say, a tiny robot scurrying across your living room floor. You want to know its exact position and velocity at all times. How do you do it?</p> <p>You could use sensors! Maybe a camera to “see” it, or wheel encoders to “feel” its movement. But here’s the catch:</p> <ol> <li> <strong>Sensors are noisy:</strong> Your camera might give slightly different coordinates each time, even if the robot is perfectly still. Your wheel encoders might slip, or tiny debris could throw off their readings.</li> <li> <strong>Our models are imperfect:</strong> Even if you know the robot’s motors are running, you can only <em>predict</em> its next position. Wind resistance, friction, or tiny manufacturing defects mean your prediction won’t be 100% accurate.</li> <li> <strong>The “true” state is hidden:</strong> You never actually <em>know</em> the robot’s <em>exact</em> position and velocity. All you have are noisy observations and imperfect predictions.</li> </ol> <p>This is the fundamental problem the Kalman Filter addresses: <strong>How do we get the best possible estimate of a system’s true state (its position, velocity, temperature, stock price, etc.) when all our information is noisy and uncertain?</strong></p> <h3 id="the-intuition-a-dance-between-prediction-and-observation">The Intuition: A Dance Between Prediction and Observation</h3> <p>Think about how <em>you</em> deal with uncertainty. If you’re driving to a new restaurant:</p> <ol> <li>You <strong>predict</strong> where you’ll be based on your last known location, speed, and direction. (You expect to be closer to the restaurant.)</li> <li>Then, you <strong>observe</strong> the environment – maybe you see a street sign or check your GPS.</li> <li>You <strong>update</strong> your belief about your current location by combining your prediction with your observation.</li> </ol> <p>If your GPS is usually spot-on, you’ll trust it more than your “dead reckoning” prediction. If your GPS signal is weak, you might trust your prediction more. This <em>balancing act</em> is the core idea of the Kalman Filter! It’s a continuous, two-step cycle: <strong>predict</strong> where you think the system is going, then <strong>update</strong> that prediction with new measurements.</p> <p>The magic? It tells you <em>how much</em> to trust your prediction versus your observation.</p> <h3 id="the-mathematical-framework-a-state-space-model">The Mathematical Framework: A State-Space Model</h3> <p>The Kalman Filter works with systems that can be described by a “state.” The state is a collection of variables that fully describe the system at any given time. For our robot, the state might be:</p> <p>$ \mathbf{x} = \begin{bmatrix} \text{position}_x \ \text{position}_y \ \text{velocity}_x \ \text{velocity}_y \end{bmatrix} $</p> <p>This is our <strong>state vector</strong>, $\mathbf{x}$. The Kalman Filter doesn’t just estimate $\mathbf{x}$; it also estimates our <em>uncertainty</em> about $\mathbf{x}$. This uncertainty is captured by the <strong>state covariance matrix</strong>, $\mathbf{P}$. A small $\mathbf{P}$ means we’re very confident; a large $\mathbf{P}$ means we’re less certain.</p> <h3 id="the-two-step-cycle-predict-and-update">The Two-Step Cycle: Predict and Update</h3> <p>Let’s break down the filter’s elegant, iterative process. At each time step $k$, we perform these two phases:</p> <h4 id="1-the-prediction-time-update-phase">1. The Prediction (Time Update) Phase</h4> <p>In this phase, we project our current state estimate forward in time, predicting what the next state will be.</p> <ul> <li> <p><strong>Predicting the State:</strong> We use a mathematical model of how our system evolves. $ \hat{\mathbf{x}}<em>k^- = \mathbf{A} \hat{\mathbf{x}}</em>{k-1}^+ + \mathbf{B} \mathbf{u}_k $</p> <p>Let’s unpack this:</p> <ul> <li>$\hat{\mathbf{x}}<em>k^-$: Our _a priori</em> (predicted) estimate of the state at time $k$. The <code class="language-plaintext highlighter-rouge">^-</code> denotes “before incorporating measurements.”</li> <li>$\hat{\mathbf{x}}<em>{k-1}^+$: Our _a posteriori</em> (updated) estimate from the previous step ($k-1$). The <code class="language-plaintext highlighter-rouge">^+</code> denotes “after incorporating measurements.”</li> <li>$\mathbf{A}$: The <strong>state transition matrix</strong>. It describes how the state evolves from $k-1$ to $k$ <em>if there were no external forces</em>. For our robot, this might describe constant velocity motion.</li> <li>$\mathbf{B}$: The <strong>control input matrix</strong>. It tells us how external control inputs affect the state.</li> <li>$\mathbf{u}_k$: The <strong>control vector</strong>. This is our known input, like the motor commands sent to the robot.</li> </ul> </li> <li> <p><strong>Predicting the Uncertainty:</strong> As we predict, our uncertainty generally grows. $ \mathbf{P}<em>k^- = \mathbf{A} \mathbf{P}</em>{k-1}^+ \mathbf{A}^T + \mathbf{Q} $</p> <ul> <li>$\mathbf{P}<em>k^-$: The _a priori</em> estimate of the covariance matrix.</li> <li>$\mathbf{P}<em>{k-1}^+$: The _a posteriori</em> estimate of the covariance matrix from the previous step.</li> <li>$\mathbf{Q}$: The <strong>process noise covariance matrix</strong>. This accounts for the uncertainty in our system model itself – things like unexpected disturbances, friction, or minor inaccuracies in our physics. It’s a measure of how much our prediction model might deviate from reality.</li> </ul> </li> </ul> <p>At the end of the prediction step, we have a new predicted state ($\hat{\mathbf{x}}_k^-$) and an associated predicted uncertainty ($\mathbf{P}_k^-$). We’ve made our best guess based on where we were and where we expected to go.</p> <h4 id="2-the-update-measurement-update-phase">2. The Update (Measurement Update) Phase</h4> <p>Now, we get a new measurement from our sensors. This is where the magic really happens – we fuse our noisy sensor data with our prediction to get a more accurate estimate.</p> <ul> <li> <strong>Measurement:</strong> Our sensors give us an observation, $\mathbf{z}<em>k$. This observation isn’t directly the state; it’s a measurement _related</em> to the state. $ \mathbf{z}_k = \mathbf{H} \mathbf{x}_k + \mathbf{R} $ <ul> <li>$\mathbf{z}_k$: The actual sensor measurement at time $k$.</li> <li>$\mathbf{H}$: The <strong>measurement matrix</strong>. It maps our state space into the measurement space. For example, if our state includes position and velocity, but our sensor only measures position, $\mathbf{H}$ would select only the position components.</li> <li>$\mathbf{R}$: The <strong>measurement noise covariance matrix</strong>. This quantifies the uncertainty inherent in our sensor readings – how noisy our camera or wheel encoders are.</li> </ul> </li> <li> <strong>The Innovation (Measurement Residual):</strong> First, we figure out how “wrong” our prediction was compared to the actual measurement. $ \tilde{\mathbf{y}}_k = \mathbf{z}_k - \mathbf{H} \hat{\mathbf{x}}_k^- $ <ul> <li>$\tilde{\mathbf{y}}<em>k$: The innovation, or residual. This is the difference between the actual measurement $\mathbf{z}_k$ and what we _expected</em> to measure ($\mathbf{H} \hat{\mathbf{x}}_k^-$) based on our prediction.</li> </ul> </li> <li> <strong>The Innovation (Residual) Covariance:</strong> We also need to know the uncertainty associated with this difference. $ \mathbf{S}_k = \mathbf{H} \mathbf{P}_k^- \mathbf{H}^T + \mathbf{R} $ <ul> <li>$\mathbf{S}_k$: The innovation covariance. It represents the total uncertainty in the innovation, combining the uncertainty from our prediction ($\mathbf{P}_k^-$) and our measurement ($\mathbf{R}$).</li> </ul> </li> <li> <p><strong>The Kalman Gain:</strong> This is the heart of the filter! The Kalman Gain, $\mathbf{K}_k$, is a weighting factor. It determines how much we trust the new measurement versus our prediction. $ \mathbf{K}_k = \mathbf{P}_k^- \mathbf{H}^T \mathbf{S}_k^{-1} $</p> <p>Think of $\mathbf{K}_k$ as a dial. If our measurement noise $\mathbf{R}$ is very low (meaning precise sensors), $\mathbf{K}_k$ will be large, and we’ll trust the measurement more. If our prediction uncertainty $\mathbf{P}_k^-$ is very low (meaning we’re very confident in our prediction), $\mathbf{K}_k$ will be small, and we’ll trust the prediction more. It’s an optimal blend!</p> </li> <li> <p><strong>Updating the State:</strong> Now, we use the Kalman Gain to adjust our predicted state, incorporating the new measurement. $ \hat{\mathbf{x}}_k^+ = \hat{\mathbf{x}}_k^- + \mathbf{K}_k \tilde{\mathbf{y}}_k $</p> <p>We take our predicted state ($\hat{\mathbf{x}}_k^-$) and add a correction term. The correction is the innovation ($\tilde{\mathbf{y}}_k$) scaled by the Kalman Gain ($\mathbf{K}_k$). This is where the magic fusion happens!</p> </li> <li> <strong>Updating the Uncertainty:</strong> Finally, we reduce our uncertainty because we’ve incorporated new information. Our estimate is now more precise. $ \mathbf{P}_k^+ = (\mathbf{I} - \mathbf{K}_k \mathbf{H}) \mathbf{P}_k^- $ <ul> <li>$\mathbf{I}$: The identity matrix.</li> <li>$\mathbf{P}<em>k^+$: The _a posteriori</em> (updated) covariance matrix. Our uncertainty has decreased!</li> </ul> </li> </ul> <p>And just like that, we have our best estimate of the system’s state ($\hat{\mathbf{x}}<em>k^+$) and its associated uncertainty ($\mathbf{P}_k^+$). This new, refined estimate then becomes the starting point ($\hat{\mathbf{x}}</em>{k-1}^+$) for the next prediction cycle.</p> <h3 id="why-is-it-so-powerful">Why is it so powerful?</h3> <p>The Kalman Filter is <strong>optimal</strong> for linear systems with Gaussian (normally distributed) noise. This means that under these assumptions, no other linear filter can produce a more accurate estimate. Even when these assumptions aren’t perfectly met, the Kalman Filter often performs remarkably well.</p> <p>Its strength lies in its ability to:</p> <ul> <li> <strong>Handle noisy data:</strong> It doesn’t just average measurements; it intelligently weights them.</li> <li> <strong>Provide a best estimate of hidden states:</strong> It allows us to infer things we can’t directly measure.</li> <li> <strong>Quantify uncertainty:</strong> The covariance matrix $\mathbf{P}$ is crucial for understanding the reliability of our estimates.</li> <li> <strong>Operate in real-time:</strong> It’s recursive, meaning it only needs the previous state and covariance, not the entire history of data. This makes it incredibly efficient for live applications.</li> </ul> <h3 id="beyond-the-basics-nonlinear-worlds">Beyond the Basics: Nonlinear Worlds</h3> <p>The standard Kalman Filter works wonders for linear systems. But what if our system’s evolution or measurement functions are nonlinear? Think of a drone flying through complex aerodynamics or a robot moving with highly non-linear kinematics.</p> <p>For these scenarios, we have extensions:</p> <ul> <li> <strong>Extended Kalman Filter (EKF):</strong> It linearizes the non-linear functions around the current state estimate using Taylor series expansion. It’s a widely used approximation but can struggle with highly non-linear systems.</li> <li> <strong>Unscented Kalman Filter (UKF):</strong> This is a more advanced technique that uses a deterministic sampling approach (sigma points) to approximate the probability distribution more accurately, often outperforming the EKF for highly non-linear problems.</li> </ul> <h3 id="real-world-applications-where-youve-seen-it">Real-World Applications (Where You’ve Seen It)</h3> <ul> <li> <strong>GPS Navigation:</strong> Combining noisy satellite signals with dead reckoning (your car’s speed and direction) to give you a smooth, accurate position on the map.</li> <li> <strong>Robotics:</strong> Estimating a robot’s position, velocity, and orientation (SLAM - Simultaneous Localization and Mapping).</li> <li> <strong>Autonomous Vehicles:</strong> Fusing data from LiDAR, radar, cameras, and IMUs to create a robust understanding of the vehicle’s own state and its environment.</li> <li> <strong>Aerospace Engineering:</strong> Guidance, navigation, and control systems for spacecraft, missiles, and aircraft. The Apollo moon missions famously used Kalman Filters!</li> <li> <strong>Finance:</strong> Estimating underlying trends in stock prices or economic indicators.</li> <li> <strong>Weather Forecasting:</strong> Combining atmospheric models with sensor data.</li> </ul> <h3 id="my-takeaway">My Takeaway</h3> <p>Learning about the Kalman Filter was one of those “aha!” moments that profoundly changed how I think about data and uncertainty. It’s a testament to the elegance of mathematics applied to real-world problems. It teaches us that uncertainty isn’t a problem to be avoided, but a factor to be embraced and managed intelligently.</p> <p>If you’re passionate about data science, machine learning, or building intelligent systems, understanding the Kalman Filter is like learning a secret language that unlocks a new dimension of control and estimation. It shows how a robust mathematical framework can turn noisy, imperfect data into confident, reliable insights.</p> <p>So, next time your GPS calmly guides you through a tunnel, remember the silent maestro performing its continuous, elegant dance of prediction and update. It’s a beautiful algorithm, and it’s everywhere.</p> <p>Happy filtering!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>