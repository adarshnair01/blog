<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Cracking the Code: Unpacking the GPT Architecture | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/cracking-the-code-unpacking-the-gpt-architecture/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Cracking the Code: Unpacking the GPT Architecture</h1> <p class="post-meta"> Created on August 04, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> NLP</a>   <a href="/blog/blog/tag/transformers"> <i class="fa-solid fa-hashtag fa-sm"></i> Transformers</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/ai-architecture"> <i class="fa-solid fa-hashtag fa-sm"></i> AI Architecture</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>It feels like just yesterday we were marveling at simple rule-based chatbots, and now, we’re interacting with AI that can write essays, code, and even compose poetry. The shift has been monumental, and much of this revolution is thanks to one particular family of models: the Generative Pre-trained Transformers, or GPTs.</p> <p>As a fellow enthusiast in the world of data science and machine learning, I remember the first time I really dove into the architecture behind GPT. It felt like trying to understand a magical spell – powerful, yet seemingly inscrutable. But trust me, once you break it down, it’s a testament to elegant engineering. And today, I want to demystify it for you, peeling back the layers of this fascinating system, even if you’re just starting your journey into AI.</p> <p>So, let’s embark on this adventure and uncover the “T” in GPT: The Transformer!</p> <h3 id="the-genesis-why-transformers">The Genesis: Why Transformers?</h3> <p>Before we dive into GPT itself, let’s set the stage. For a long time, models like Recurrent Neural Networks (RNNs) and their cousins, LSTMs, were the go-to for processing sequences like text. They worked by processing one word at a time, maintaining a “memory” of previous words. This was okay, but they had a couple of big issues:</p> <ol> <li> <strong>Long-range dependencies:</strong> Remembering information from many words ago was tough.</li> <li> <strong>Parallelization:</strong> Because they processed word by word, they were slow to train on large datasets.</li> </ol> <p>Enter the Transformer in 2017, with its groundbreaking paper “Attention Is All You Need.” It completely changed the game by ditching sequential processing in favor of something called <strong>attention</strong>. And GPT? It’s a special kind of Transformer – a <em>decoder-only</em> Transformer, specifically designed for generation.</p> <h3 id="the-t-in-gpt-the-transformers-core-ideas">The “T” in GPT: The Transformer’s Core Ideas</h3> <p>Imagine you’re trying to write a sentence. You don’t just consider the very last word you wrote; you think about all the words you’ve written so far, and how they relate to each other, to decide the next one. This holistic view is what attention tries to capture.</p> <p>At its heart, a Transformer is built from layers of “blocks,” each performing a specific task. For GPT, these are <em>decoder blocks</em>. Let’s break down what goes into each block.</p> <h4 id="1-input-embedding--positional-encoding-words-to-numbers-with-a-sense-of-order">1. Input Embedding &amp; Positional Encoding: Words to Numbers, with a Sense of Order</h4> <p>Computers don’t understand words; they understand numbers. So, our first step is to convert each word (or sub-word, called a “token”) in our input sentence into a numerical vector. This is done by an <strong>embedding layer</strong>. Each token gets a unique, dense vector representation, where words with similar meanings tend to have similar vectors.</p> <p>But here’s a catch: the Transformer processes all words simultaneously. Unlike RNNs, there’s no inherent order. If you shuffle the words, the embeddings alone wouldn’t tell you the original sequence. This is where <strong>Positional Encoding</strong> comes in.</p> <p>For each token embedding, we add another vector that uniquely identifies its position in the sequence. It’s like adding a small “tag” to each word that says, “I’m word #1,” “I’m word #2,” and so on.</p> <p>Mathematically, a common approach for positional encoding uses sine and cosine functions:</p> <p>$PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{model}})$ $PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d_{model}})$</p> <p>Where $pos$ is the position of the token, $i$ is the dimension within the embedding vector, and $d_{model}$ is the dimension of the embedding space. This clever trick ensures that each position has a unique encoding, and critically, it allows the model to easily learn relative positions. So now, our input to the main blocks is a rich vector that knows both <em>what</em> the word is and <em>where</em> it is.</p> <h4 id="2-the-decoder-block-where-the-magic-happens">2. The Decoder Block: Where the Magic Happens</h4> <p>GPT models are essentially a stack of many identical <strong>decoder blocks</strong>. Each block refines the understanding of the input sequence. Within each decoder block, there are two main sub-layers:</p> <h5 id="a-masked-multi-head-self-attention-looking-back-intelligently">a. Masked Multi-Head Self-Attention: Looking Back, Intelligently</h5> <p>This is the absolute core of the Transformer, and the “masked” part is crucial for GPT.</p> <p><strong>Self-Attention Intuition:</strong> Imagine you’re reading a sentence: “The animal didn’t cross the street because <em>it</em> was too tired.” When you read “it,” you know “it” refers to “the animal.” Self-attention mimics this by allowing each word in the sequence to “look” at all other words and figure out how relevant they are to itself.</p> <p>How does it work? Each word vector is transformed into three different vectors:</p> <ul> <li> <strong>Query (Q):</strong> “What am I looking for?” (like a search query)</li> <li> <strong>Key (K):</strong> “What do I offer?” (like a tag on information)</li> <li> <strong>Value (V):</strong> “Here’s the information I carry.” (the actual data)</li> </ul> <p>For each word’s Query, we compare it against all other words’ Keys using a dot product. A higher dot product means higher similarity. These scores are then scaled and passed through a <code class="language-plaintext highlighter-rouge">softmax</code> function to get probability-like weights. Finally, we multiply these weights by the Value vectors to get a weighted sum of information from all relevant words.</p> <p>The formula for scaled dot-product attention is: $Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$</p> <p>Where $d_k$ is the dimension of the key vectors, used for scaling to prevent very large dot products that push <code class="language-plaintext highlighter-rouge">softmax</code> into saturated regions.</p> <p><strong>The “Masked” Part (The GPT Secret Sauce):</strong> Since GPT’s job is to <em>generate</em> text, it’s designed to predict the <em>next</em> word. This means when it’s processing a word, it should <em>only</em> be able to look at the words that came <em>before</em> it, not the words yet to be generated. If it could see future words, it would be cheating!</p> <p>This is enforced by <strong>masking</strong>. During training, we apply a “look-ahead mask” to the attention scores. This mask effectively blocks the attention mechanism from attending to subsequent tokens. Conceptually, it makes the similarity score for any future tokens effectively negative infinity, so their <code class="language-plaintext highlighter-rouge">softmax</code> probability becomes zero. So, when generating the third word, it can only attend to the first and second words. This is fundamental to GPT’s auto-regressive (one word after another) generation capability.</p> <p><strong>Multi-Head Attention:</strong> Instead of just one set of Q, K, V transformations, we perform this attention process multiple times in parallel, using different linear transformations for each “head.” Each head learns to focus on different aspects of relationships between words (e.g., one head might focus on grammatical dependencies, another on semantic relatedness). The outputs from all these heads are then concatenated and linearly transformed back into a single vector. This allows the model to jointly attend to information from different representation subspaces at different positions.</p> <h5 id="b-feed-forward-network-ffn-processing-the-attended-information">b. Feed-Forward Network (FFN): Processing the Attended Information</h5> <p>After the attention mechanism has gathered relevant information from the entire sequence (or rather, the preceding part of it), a simple, fully connected <strong>Feed-Forward Network</strong> (also known as a Position-wise FFN) is applied independently to each position. It’s usually two linear transformations with a ReLU activation in between:</p> <p>$FFN(x) = \max(0, xW_1 + b_1)W_2 + b_2$</p> <p>This network allows the model to perform further, more complex transformations on the information aggregated by the attention layer.</p> <h5 id="c-layer-normalization-and-residual-connections-stabilizing-and-boosting-flow">c. Layer Normalization and Residual Connections: Stabilizing and Boosting Flow</h5> <p>Throughout the Transformer block, you’ll also find <strong>Residual Connections</strong> and <strong>Layer Normalization</strong>.</p> <ul> <li> <strong>Residual Connections (Skip Connections):</strong> Each sub-layer (attention and FFN) has a residual connection around it, followed by layer normalization. This means the input to the sub-layer is added to its output. It’s like a shortcut that allows gradients to flow more easily through the network, preventing issues like vanishing gradients in deep networks: $Output = x + Sublayer(x)$.</li> <li> <strong>Layer Normalization:</strong> This normalizes the activations across the features for each sample. It helps stabilize training, especially in deep networks, by ensuring that the inputs to subsequent layers have a consistent mean and variance.</li> </ul> <p>This entire sequence – Masked Multi-Head Self-Attention, then a Feed-Forward Network, all wrapped with residual connections and layer normalization – constitutes a single GPT decoder block. And GPT models stack many, many of these blocks!</p> <h4 id="3-output-layer-from-vectors-to-words">3. Output Layer: From Vectors to Words</h4> <p>After passing through all the decoder blocks, the final output vector for each position (representing the model’s understanding of that word <em>in context</em>) is passed through a linear layer, followed by a <code class="language-plaintext highlighter-rouge">softmax</code> function. This <code class="language-plaintext highlighter-rouge">softmax</code> layer converts the numerical output into a probability distribution over the entire vocabulary (all possible words the model knows).</p> <p>$\hat{y} = softmax(W_{vocab}h + b_{vocab})$</p> <p>Where $h$ is the final hidden state vector for a token, and $W_{vocab}, b_{vocab}$ are the weights and biases of the linear layer projecting to the vocabulary size. The word with the highest probability is then chosen as the next word in the sequence.</p> <h3 id="the-p-in-gpt-pre-training">The “P” in GPT: Pre-training</h3> <p>The “Pre-trained” aspect is just as crucial as the “Transformer” architecture. GPT models are trained on absolutely massive datasets of text and code (think billions upon billions of words from books, articles, websites, etc.).</p> <p>The pre-training objective is simple but powerful: <strong>causal language modeling</strong>. This means the model is given a sequence of words and asked to predict the <em>next</em> word. For example, if it sees “The cat sat on the”, it tries to predict “mat” (or “couch”, “rug”, etc.). By doing this over and over again on vast amounts of text, the model learns not just grammar and syntax, but also an incredible amount of factual knowledge, reasoning abilities, and common sense embedded within human language. It learns how language works and what makes sense.</p> <h3 id="the-g-in-gpt-generative-power">The “G” in GPT: Generative Power</h3> <p>Once pre-trained, the “Generative” capability kicks in. To generate new text, we give GPT a “prompt” – a starting piece of text.</p> <ol> <li>GPT processes this prompt through its layers.</li> <li>The output layer predicts the probability distribution for the <em>next</em> word.</li> <li>A word is chosen from this distribution (often by simply picking the most probable word, or more sophisticated sampling techniques like “nucleus sampling” for more creative outputs).</li> <li>This chosen word is then appended to the prompt, and the whole process repeats.</li> </ol> <p>This autoregressive (self-regressing) loop continues, building sentence by sentence, paragraph by paragraph, until a desired length is reached or a specific “stop token” is generated.</p> <h3 id="putting-it-all-together-a-simple-walkthrough">Putting It All Together: A Simple Walkthrough</h3> <p>Imagine you give GPT the prompt: “The quick brown”</p> <ol> <li>“The quick brown” tokens are embedded and positional encodings are added.</li> <li>These vectors go through the first decoder block. <ul> <li>The “brown” token’s masked self-attention looks at “The” and “quick” to understand its context.</li> <li>The FFN processes this refined information.</li> </ul> </li> <li>This repeats through all subsequent decoder blocks.</li> <li>The final output for “brown” is fed to the output layer, which predicts the <em>next</em> word (e.g., “fox”) with a high probability.</li> <li>“fox” is added to the sequence: “The quick brown fox”.</li> <li>Now, the model takes “The quick brown fox” as input and predicts the next word (e.g., “jumps”).</li> <li>This continues until you have a full, coherent sentence or more!</li> </ol> <h3 id="beyond-the-basics-the-road-ahead">Beyond the Basics: The Road Ahead</h3> <p>While this breakdown covers the fundamental architecture, modern GPT models are incredibly complex, involving:</p> <ul> <li> <strong>Massive Scaling:</strong> Increasing the number of parameters (weights and biases), layers, and training data leads to increasingly capable models (e.g., from GPT-1 with 117 million parameters to GPT-3 with 175 billion!).</li> <li> <strong>Fine-tuning &amp; Prompt Engineering:</strong> After pre-training, models can be further fine-tuned on smaller, task-specific datasets, or guided by carefully crafted prompts to perform specific tasks without additional training.</li> <li> <strong>Instruction Tuning:</strong> Training the model to follow instructions given in natural language, which is key to models like ChatGPT.</li> </ul> <h3 id="conclusion">Conclusion</h3> <p>So, there you have it! The GPT architecture, at its core, is an ingenious stack of Transformer decoder blocks, leveraging <strong>masked multi-head self-attention</strong> to understand context and <strong>positional encodings</strong> to maintain order. Its power is unleashed through <strong>massive pre-training</strong> on diverse text data, learning the intricate patterns of human language.</p> <p>It’s a beautiful example of how thoughtful architectural design, combined with vast computational resources and data, can lead to truly transformative AI. Understanding these building blocks isn’t just an academic exercise; it’s empowering. It allows you to peer behind the curtain of what often feels like magic, appreciate the engineering, and perhaps even inspire you to build the next generation of intelligent systems.</p> <p>Keep exploring, keep building, and remember: the future of AI is being written, one token at a time!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>