<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> From Neurons to Networks: Demystifying AI's Building Blocks | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/from-neurons-to-networks-demystifying-ais-building/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">From Neurons to Networks: Demystifying AI's Building Blocks</h1> <p class="post-meta"> Created on December 02, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/neural-networks"> <i class="fa-solid fa-hashtag fa-sm"></i> Neural Networks</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hello there, fellow explorers of the digital frontier!</p> <p>I remember the first time I truly wrapped my head around the concept of a Neural Network. It felt like unlocking a secret chamber in the grand castle of Artificial Intelligence. Before that, AI seemed like pure magic, an inscrutable force. But then I saw the elegant simplicity of its core building blocks, and suddenly, the magic became engineering.</p> <p>Today, I want to invite you on a journey to demystify these incredible systems. Whether you’re a high school student just starting your dive into tech or a data science enthusiast looking to solidify your foundations, understanding Neural Networks is like getting a backstage pass to the future.</p> <p>Think about it: Your phone can recognize your face, Netflix recommends your next binge-watch, and self-driving cars navigate complex roads. These aren’t just clever tricks; they’re manifestations of sophisticated algorithms, with Neural Networks often at their very heart. So, let’s pull back the curtain and see what makes these digital brains tick.</p> <h3 id="the-biological-blueprint-our-brains-as-inspiration">The Biological Blueprint: Our Brains as Inspiration</h3> <p>Before we dive into the artificial, let’s take a quick peek at the natural. Our own brains are incredibly complex networks of biological neurons. Each neuron is a tiny, living processor that receives signals from other neurons through its <strong>dendrites</strong>. If the combined input signals are strong enough, the neuron “fires,” sending its own signal down its <strong>axon</strong> to other neurons via <strong>synapses</strong>. This firing is an “all-or-nothing” event – it either activates or it doesn’t.</p> <p>This constant communication, activation, and inhibition is how we think, learn, and experience the world. It’s a beautifully intricate system, and surprisingly, the core idea behind it isn’t too far from what we replicate in code.</p> <h3 id="the-artificial-neuron-a-digital-decision-maker">The Artificial Neuron: A Digital Decision-Maker</h3> <p>Inspired by our biological counterparts, computer scientists in the 1940s and 50s began to conceptualize the “artificial neuron,” also known as a <strong>perceptron</strong>. At its core, an artificial neuron is a remarkably simple decision-making unit.</p> <p>Imagine you’re trying to decide if you should study for an exam. You consider several factors:</p> <ol> <li> <strong>How difficult is the subject?</strong> (High importance)</li> <li> <strong>How much time do you have?</strong> (Medium importance)</li> <li> <strong>Are your friends studying?</strong> (Low importance, maybe)</li> </ol> <p>Each of these factors is an <strong>input</strong> to your “decision neuron.” Each input has a certain <strong>weight</strong> associated with it, representing its importance. So, the difficulty of the subject might have a high weight, while what your friends are doing might have a low weight.</p> <p>Let’s put this into mathematical terms. Suppose we have inputs $x_1, x_2, …, x_n$. Each input $x_i$ is multiplied by its corresponding weight $w_i$. So, we calculate a “weighted sum”: $Z = (x_1 \cdot w_1) + (x_2 \cdot w_2) + \dots + (x_n \cdot w_n)$</p> <p>To make it even more flexible, we add a <strong>bias</strong> term, $b$. Think of the bias as an adjustable threshold or a predisposition for the neuron to activate, regardless of the inputs.</p> <p>So, the total input to our artificial neuron becomes: $Z = \sum_{i=1}^{n} (x_i w_i) + b$</p> <p>Now, just like a biological neuron needs to reach a certain threshold to “fire,” our artificial neuron applies an <strong>activation function</strong> to this sum $Z$. This function decides whether the neuron should “activate” and pass on a signal, and if so, how strong that signal should be.</p> <p>One common activation function you might encounter is the <strong>Sigmoid function</strong>: $\sigma(Z) = \frac{1}{1 + e^{-Z}}$</p> <p>The Sigmoid function squashes any input value between 0 and 1, making it useful for probabilities or binary classifications. Another popular one is the <strong>Rectified Linear Unit (ReLU)</strong>: $\text{ReLU}(Z) = \max(0, Z)$ ReLU simply outputs the input if it’s positive, otherwise, it outputs zero. This introduces non-linearity, which is crucial for Neural Networks to learn complex patterns.</p> <p>The output $A$ of our neuron is then: $A = \text{activation}(Z)$</p> <p>This output $A$ then becomes an input to other neurons in the network. Simple, right? But the magic truly begins when we connect many of these simple units.</p> <h3 id="building-layers-the-network-comes-alive">Building Layers: The Network Comes Alive</h3> <p>A single artificial neuron is limited. It can only solve linearly separable problems (think of drawing a single straight line to separate two categories of data). But our world isn’t always that neat. This is where the “network” part of Neural Networks comes in.</p> <p>We arrange these artificial neurons into <strong>layers</strong>:</p> <ol> <li> <strong>Input Layer</strong>: This is where your data (e.g., pixel values of an image, features of a house) enters the network. These aren’t “neurons” in the computational sense, but rather placeholders for your data.</li> <li> <strong>Hidden Layers</strong>: These are the computational powerhouses. Each neuron in a hidden layer receives inputs from the previous layer, performs its weighted sum and activation, and then passes its output to the next layer. Networks with more than one hidden layer are called <strong>Deep Neural Networks</strong> – hence the term “Deep Learning”!</li> <li> <strong>Output Layer</strong>: This layer provides the final result. For classifying an image as a “cat” or “dog,” it might have two neurons, each giving a probability. For predicting a house price, it might have a single neuron outputting a continuous value.</li> </ol> <p>Imagine a chain reaction: inputs flow into the first hidden layer, which processes them and passes its outputs to the next hidden layer, and so on, until the final output layer produces a prediction. This process is called <strong>forward propagation</strong>.</p> <h3 id="the-learning-curve-how-neural-networks-get-smart">The Learning Curve: How Neural Networks Get Smart</h3> <p>This is the truly fascinating part: how do these networks <em>learn</em>? Initially, all the weights and biases in a Neural Network are set randomly. So, its first predictions will be utterly terrible, like a baby guessing answers on a complex exam. The goal is to adjust these weights and biases iteratively until the network’s predictions are as accurate as possible.</p> <p>This learning process involves three key steps, repeated many, many times:</p> <ol> <li> <p><strong>Forward Propagation (as discussed):</strong> The network makes a prediction based on the current inputs, weights, and biases. Let’s say for a given input $x$, our network predicts $\hat{y}$.</p> </li> <li> <p><strong>Calculating the Loss:</strong> We compare the network’s prediction ($\hat{y}$) with the actual correct answer ($y$). The difference between these two is the <strong>error</strong> or <strong>loss</strong>. A common way to measure loss for regression tasks (predicting continuous values) is the <strong>Mean Squared Error (MSE)</strong>: $L = \frac{1}{2m} \sum_{j=1}^{m} (y_j - \hat{y}_j)^2$ Here, $m$ is the number of training examples, $y_j$ is the true value, and $\hat{y}_j$ is the network’s prediction. The goal is to minimize this loss.</p> </li> <li> <p><strong>Backpropagation: The “Aha!” Moment:</strong> This is the clever algorithm that tells the network <em>how</em> to adjust its weights and biases to reduce the loss. It’s like a teacher giving feedback to a student. If the student answers a question incorrectly, the teacher doesn’t just say “wrong,” but explains <em>why</em> it’s wrong and <em>how</em> to improve for next time.</p> <p>Backpropagation uses a concept from calculus called the <strong>gradient</strong>. Think of the loss function as a landscape with hills and valleys. We want to find the lowest point (minimum loss) in this landscape. The gradient points in the direction of the steepest ascent. We want to go <em>down</em> the hill, so we move in the opposite direction of the gradient. This optimization process is called <strong>Gradient Descent</strong>.</p> <p>Mathematically, we calculate the <strong>partial derivatives</strong> of the loss function with respect to each weight and bias in the network. A partial derivative tells us how much the loss would change if we slightly tweaked a specific weight or bias. Using the <strong>chain rule</strong> (another fundamental calculus concept), backpropagation efficiently computes these gradients by working backward from the output layer to the input layer.</p> <p>Once we know how much each weight and bias contributes to the total error, we update them using a simple rule: $w_{new} = w_{old} - \alpha \frac{\partial L}{\partial w_{old}}$ $b_{new} = b_{old} - \alpha \frac{\partial L}{\partial b_{old}}$</p> <p>Here, $\alpha$ is the <strong>learning rate</strong>, a crucial hyperparameter. It controls how big a step we take down the gradient “hill.” A small learning rate makes learning slow, while a large one might overshoot the minimum loss.</p> </li> </ol> <p>This entire process – forward propagation, loss calculation, and backpropagation – is repeated thousands or millions of times, over many <strong>epochs</strong> (a full pass through the entire training dataset). With each iteration, the network gets a little bit smarter, its weights and biases finely tuned to make increasingly accurate predictions.</p> <h3 id="why-are-neural-networks-so-powerful">Why Are Neural Networks So Powerful?</h3> <ol> <li> <strong>Universal Approximation Theorem:</strong> This amazing theorem states that a Neural Network with at least one hidden layer can approximate <em>any</em> continuous function to arbitrary accuracy, given enough neurons and the right weights. This means they are incredibly versatile.</li> <li> <strong>Feature Learning:</strong> Unlike traditional machine learning algorithms where you might need to manually extract “features” from your data (e.g., edges in an image), Deep Neural Networks can learn relevant features <em>on their own</em> from raw data. The early layers might learn simple features like edges or corners, while deeper layers combine these to learn more complex features like eyes, noses, or entire objects.</li> <li> <strong>Scalability:</strong> With more data and more computational power (thanks to GPUs!), Neural Networks can often achieve better performance than other algorithms, especially on complex tasks like image recognition and natural language processing.</li> </ol> <h3 id="challenges-and-the-road-ahead">Challenges and the Road Ahead</h3> <p>While powerful, Neural Networks aren’t without their quirks. They require vast amounts of data to train effectively, and training can be computationally expensive. They are also often seen as “black boxes” – it can be challenging to interpret <em>why</em> a network made a particular decision, which is a significant area of ongoing research.</p> <p>However, the field is rapidly evolving. We’re seeing specialized architectures like <strong>Convolutional Neural Networks (CNNs)</strong> for image processing and <strong>Recurrent Neural Networks (RNNs)</strong> for sequential data like text and speech. The journey of Neural Networks, from simple perceptrons to complex deep learning models, is a testament to human ingenuity and the power of iterative refinement.</p> <h3 id="your-journey-begins">Your Journey Begins</h3> <p>If you’ve followed along, you now have a fundamental understanding of how Neural Networks work. You’ve peeked behind the curtain of modern AI, realizing that it’s not magic, but a beautiful symphony of simple mathematical operations, executed at scale.</p> <p>This is just the beginning. The world of AI is vast and exciting, and your curiosity is your best guide. I encourage you to experiment, play with libraries like TensorFlow or PyTorch, build your own simple networks, and see the power for yourself.</p> <p>Keep learning, keep building, and who knows what incredible things you’ll unravel next!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>