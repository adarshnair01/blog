<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> From Mess to Mastery: Practical Data Cleaning Strategies for Aspiring Data Scientists | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/2025/from-mess-to-mastery-practical-data-cleaning-strat/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">From Mess to Mastery: Practical Data Cleaning Strategies for Aspiring Data Scientists</h1> <p class="post-meta"> Created on June 12, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/data-cleaning"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Cleaning</a>   <a href="/blog/blog/tag/data-preprocessing"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Preprocessing</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/python"> <i class="fa-solid fa-hashtag fa-sm"></i> Python</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hello fellow data enthusiasts! Have you ever started a new data science project, brimming with excitement, only to find your shiny new dataset is… well, a bit of a disaster? Rows with missing values, columns with inconsistent entries, bizarre outliers skewing your statistics, or even duplicate records silently bloating your analysis. If you’ve nodded along, welcome to the club! This isn’t just <em>a</em> part of data science; it’s arguably the <em>most crucial</em> part.</p> <p>As the old adage goes in the world of data: “Garbage In, Garbage Out” (GIGO). You can have the most sophisticated machine learning model in the world, a truly cutting-edge algorithm, but if you feed it poor quality data, the insights it spits out will be, at best, misleading, and at worst, catastrophically wrong. This is why data scientists often report spending 60-80% of their time on data cleaning and preparation. It’s the unsung hero, the backstage crew ensuring the star performers (our models) shine.</p> <p>In my own journey, I quickly learned that understanding data cleaning isn’t just about applying a few functions; it’s a strategic mindset. It’s about becoming a detective, a forensic expert sifting through digital evidence, trying to understand <em>why</em> the data is messy and <em>how</em> to best rehabilitate it. And that’s what I want to share with you today: not just the ‘what’ but the ‘how’ and ‘why’ behind effective data cleaning strategies, making this often-daunting task accessible and even, dare I say, enjoyable!</p> <h3 id="the-data-cleaning-mindset-more-than-just-code">The Data Cleaning Mindset: More Than Just Code</h3> <p>Before we dive into specific techniques, let’s cultivate the right mindset. Data cleaning isn’t a one-and-done chore; it’s an iterative, investigative, and deeply domain-specific process.</p> <ol> <li> <strong>Be a Detective:</strong> Always ask “Why?” Why is this value missing? Why is this entry inconsistent? Often, understanding the source of the mess helps you choose the best cleaning strategy.</li> <li> <strong>It’s Iterative:</strong> You won’t get it perfect on the first pass. You clean a bit, explore the data again, find new issues, clean more. It’s a dance between cleaning and exploration.</li> <li> <strong>Domain Knowledge is Gold:</strong> Your understanding of the real-world context of your data is invaluable. A price of $1 might be an outlier for a house, but perfectly normal for a candy bar. Without context, you’re just guessing.</li> <li> <strong>Document Everything:</strong> Keep a log of every cleaning step. This makes your work reproducible and understandable for others (and your future self!).</li> </ol> <p>With that mindset in place, let’s roll up our sleeves and explore some fundamental strategies.</p> <h3 id="1-handling-the-dreaded-missing-values">1. Handling the Dreaded Missing Values</h3> <p>Missing data is perhaps the most common headache. It occurs for various reasons: data entry errors, respondents skipping questions, sensor malfunctions, or simply data that wasn’t collected. Whatever the cause, it creates gaps in our understanding and can cause many machine learning algorithms to simply crash or produce inaccurate results.</p> <h4 id="detection">Detection:</h4> <p>The first step is always to identify <em>where</em> and <em>how much</em> data is missing. In Python, <code class="language-plaintext highlighter-rouge">pandas</code> is your best friend:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="c1"># Assuming df is your DataFrame
</span><span class="nf">print</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="nf">isnull</span><span class="p">().</span><span class="nf">sum</span><span class="p">())</span> <span class="c1"># Shows count of missing values per column
</span><span class="nf">print</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="nf">isnull</span><span class="p">().</span><span class="nf">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span><span class="p">)</span> <span class="c1"># Shows percentage of missing values
</span></code></pre></div></div> <p>Visualizations like heatmaps can also be incredibly insightful to spot patterns of missingness:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">heatmap</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="nf">isnull</span><span class="p">(),</span> <span class="n">cbar</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <h4 id="strategies-for-imputation-filling-missing-values">Strategies for Imputation (Filling Missing Values):</h4> <p>Once identified, you have a few options:</p> <ul> <li> <strong>Deletion:</strong> <ul> <li> <strong>Row-wise Deletion:</strong> Drop entire rows with any missing values. <code class="language-plaintext highlighter-rouge">df.dropna(how='any')</code>.</li> <li> <strong>Column-wise Deletion:</strong> Drop entire columns if they have too many missing values. <code class="language-plaintext highlighter-rouge">df.dropna(axis=1, thresh=len(df)*0.7)</code> (Keeps columns with at least 70% non-null values).</li> <li> <strong>When to use:</strong> Use sparingly. Deletion can lead to significant data loss, especially in smaller datasets or if missingness is not random. If a column is almost entirely empty (e.g., &gt;70-80% missing), dropping it might be the most sensible approach.</li> </ul> </li> <li> <p><strong>Imputation (Filling in):</strong> This is generally preferred as it preserves more data.</p> <ol> <li> <strong>Mean/Median/Mode Imputation:</strong> <ul> <li> <strong>Mean:</strong> Replace missing numerical values with the column’s mean. Good for normally distributed data. <code class="language-plaintext highlighter-rouge">df['column'].fillna(df['column'].mean())</code>.</li> <li> <strong>Median:</strong> Replace missing numerical values with the column’s median. More robust to outliers and skewed data. <code class="language-plaintext highlighter-rouge">df['column'].fillna(df['column'].median())</code>.</li> <li> <strong>Mode:</strong> Replace missing categorical (or numerical) values with the most frequent value. <code class="language-plaintext highlighter-rouge">df['column'].fillna(df['column'].mode()[0])</code>.</li> <li> <strong>Caveat:</strong> These methods don’t account for the relationships between features and can reduce variance, potentially underestimating variability.</li> </ul> </li> <li> <strong>Forward/Backward Fill (for Time Series):</strong> <ul> <li> <code class="language-plaintext highlighter-rouge">ffill()</code> (forward fill) propagates the last valid observation forward.</li> <li> <code class="language-plaintext highlighter-rouge">bfill()</code> (backward fill) propagates the next valid observation backward.</li> <li>Excellent for time-series data where values are often correlated with their predecessors/successors. <code class="language-plaintext highlighter-rouge">df['column'].fillna(method='ffill')</code>.</li> </ul> </li> <li> <strong>Advanced Imputation Techniques:</strong> <ul> <li> <strong>K-Nearest Neighbors (KNN) Imputation:</strong> This method finds the ‘k’ nearest neighbors to a data point with a missing value and imputes the missing value based on the values of those neighbors. For numerical data, it might use the mean of neighbors; for categorical, the mode. It’s more sophisticated as it considers the structure of the data.</li> <li> <strong>Regression Imputation:</strong> You can build a predictive model (e.g., a linear regression) to predict missing values in one feature using other features in the dataset. This can be quite powerful but adds complexity.</li> </ul> </li> </ol> </li> </ul> <p>The choice of imputation strategy depends heavily on the nature of your data, the extent of missingness, and the problem you’re trying to solve. Always evaluate the impact of your imputation on the overall data distribution.</p> <h3 id="2-taming-the-wild-dealing-with-outliers">2. Taming the Wild: Dealing with Outliers</h3> <p>Outliers are data points that significantly deviate from other observations. They can be genuine extreme values, or they can be errors (e.g., a typo in data entry where someone accidentally types 1000 instead of 100). Outliers can disproportionately affect statistical analyses (like the mean) and lead to poor model performance, especially for algorithms sensitive to distance metrics (e.g., K-Means, SVM, Linear Regression).</p> <h4 id="detection-1">Detection:</h4> <ol> <li> <strong>Visualizations:</strong> <ul> <li> <strong>Box Plots:</strong> Excellent for spotting outliers visually. Points beyond the ‘whiskers’ are typically considered outliers.</li> <li> <strong>Scatter Plots:</strong> Useful for multivariate outliers (outliers in the relationship between two variables).</li> <li> <strong>Histograms/Distribution Plots:</strong> Can show extreme values at the tails of the distribution.</li> </ul> </li> <li> <p><strong>Statistical Methods:</strong></p> <ul> <li> <p><strong>Z-score:</strong> For normally distributed data, the Z-score measures how many standard deviations a data point is from the mean. The formula is: $Z = \frac{x - \mu}{\sigma}$ where $x$ is the data point, $\mu$ is the mean, and $\sigma$ is the standard deviation. A common threshold for identifying outliers is $|Z| &gt; 3$ (meaning values more than 3 standard deviations from the mean).</p> </li> <li> <p><strong>Interquartile Range (IQR):</strong> A robust method for skewed data, not relying on the mean or standard deviation.</p> <ol> <li>Calculate $Q_1$ (25th percentile) and $Q_3$ (75th percentile).</li> <li>Calculate the $IQR = Q_3 - Q_1$.</li> <li>Outliers are typically defined as values below $Q_1 - 1.5 \times IQR$ or above $Q_3 + 1.5 \times IQR$.</li> </ol> </li> </ul> </li> </ol> <h4 id="strategies-for-handling-outliers">Strategies for Handling Outliers:</h4> <ul> <li> <strong>Removal:</strong> If you’re confident an outlier is due to a data entry error or measurement error and constitutes a small fraction of your data, removing it might be appropriate. However, be cautious; removing too many outliers can bias your dataset.</li> <li> <strong>Transformation:</strong> Non-linear transformations can reduce the impact of outliers by compressing the range of values. <ul> <li> <strong>Log Transformation:</strong> <code class="language-plaintext highlighter-rouge">np.log(df['column'])</code>. Useful for positively skewed data.</li> <li> <strong>Box-Cox Transformation:</strong> A more general transformation that can handle various types of distributions. <code class="language-plaintext highlighter-rouge">scipy.stats.boxcox(df['column'])</code>. The goal is to make the data more “normal-like.”</li> </ul> </li> <li> <strong>Capping (Winsorization):</strong> Instead of removing outliers, you can cap them at a certain percentile. For example, replace all values above the 95th percentile with the value at the 95th percentile, and all values below the 5th percentile with the value at the 5th percentile. This reduces their extreme influence without removing the data points entirely.</li> <li> <strong>Treat as Separate Cases:</strong> In some scenarios (e.g., fraud detection), outliers are the very events you’re trying to predict. In such cases, they shouldn’t be removed but rather meticulously studied.</li> </ul> <p>Always remember: an outlier might be an error, but it could also be a critical piece of information. Domain knowledge is paramount here.</p> <h3 id="3-consistency-is-key-inconsistent-data--duplicates">3. Consistency is Key: Inconsistent Data &amp; Duplicates</h3> <p>Imagine your dataset having “USA,” “U.S.A.,” and “United States” all referring to the same country. Or dates entered in “MM/DD/YYYY” in one column and “YYYY-MM-DD” in another. These inconsistencies are common and can wreak havoc on your analysis, treating identical entities as distinct ones.</p> <h4 id="inconsistent-data-formats">Inconsistent Data Formats:</h4> <ul> <li> <strong>Categorical Data:</strong> <ul> <li> <strong>Standardization:</strong> Convert all text to lowercase or uppercase (<code class="language-plaintext highlighter-rouge">.str.lower()</code>, <code class="language-plaintext highlighter-rouge">.str.upper()</code>).</li> <li> <strong>Stripping Whitespace:</strong> Remove leading/trailing spaces (<code class="language-plaintext highlighter-rouge">.str.strip()</code>).</li> <li> <strong>Replacement:</strong> Use <code class="language-plaintext highlighter-rouge">str.replace()</code> or regular expressions (<code class="language-plaintext highlighter-rouge">re</code> module) to fix variations. E.g., <code class="language-plaintext highlighter-rouge">df['country'].str.replace('U.S.A.', 'USA')</code>.</li> </ul> </li> <li> <strong>Date/Time Data:</strong> <ul> <li>Ensure all date columns are in a consistent format and data type. Pandas’ <code class="language-plaintext highlighter-rouge">pd.to_datetime()</code> is incredibly powerful for parsing various date string formats into datetime objects.</li> <li> <code class="language-plaintext highlighter-rouge">df['date_column'] = pd.to_datetime(df['date_column'], errors='coerce')</code> (the <code class="language-plaintext highlighter-rouge">errors='coerce'</code> will turn unparseable dates into <code class="language-plaintext highlighter-rouge">NaT</code> - Not a Time, which you can then handle as missing values).</li> </ul> </li> </ul> <h4 id="duplicate-records">Duplicate Records:</h4> <p>Duplicates occur when the same information is recorded multiple times. This can happen during data merges, re-entries, or simply flawed collection processes.</p> <ul> <li> <strong>Detection:</strong> <ul> <li> <code class="language-plaintext highlighter-rouge">df.duplicated().sum()</code> will tell you how many duplicate rows exist based on all columns.</li> <li>You can also check for duplicates based on a subset of columns: <code class="language-plaintext highlighter-rouge">df.duplicated(subset=['column1', 'column2']).sum()</code>. This is useful if a unique identifier might exist, but other fields are duplicates.</li> </ul> </li> <li> <strong>Strategy:</strong> <ul> <li> <strong>Removal:</strong> <code class="language-plaintext highlighter-rouge">df.drop_duplicates(inplace=True)</code> removes all but the first occurrence of duplicate rows. You can specify <code class="language-plaintext highlighter-rouge">keep='last'</code> or <code class="language-plaintext highlighter-rouge">keep=False</code> to remove all duplicates.</li> <li> <strong>Caution:</strong> Think carefully if a record <em>could</em> legitimately appear multiple times (e.g., multiple purchases by the same customer, or multiple transactions from the same account). In such cases, removing duplicates might be incorrect.</li> </ul> </li> </ul> <h3 id="4-feature-scaling-preparing-for-the-model-post-cleaning-but-related">4. Feature Scaling: Preparing for the Model (Post-Cleaning but Related)</h3> <p>While not strictly “cleaning” in the sense of fixing errors, feature scaling is a crucial preprocessing step that often follows cleaning and ensures your data is uniformly prepared for machine learning models. Many algorithms (like K-Nearest Neighbors, Support Vector Machines, neural networks using gradient descent) are sensitive to the scale of features. A feature with values ranging from 0 to 1000 will dominate a feature ranging from 0 to 1.</p> <p>The two most common scaling techniques are:</p> <ul> <li> <p><strong>Min-Max Scaling (Normalization):</strong> Scales features to a fixed range, usually 0 to 1. $X_{scaled} = \frac{X - X_{min}}{X_{max} - X_{min}}$ This is useful when you want to bound your values within a specific range.</p> </li> <li> <p><strong>Standardization (Z-score Scaling):</strong> Transforms data to have a mean ($\mu$) of 0 and a standard deviation ($\sigma$) of 1. $X_{scaled} = \frac{X - \mu}{\sigma}$ Standardization is generally preferred for algorithms that assume normally distributed data (like Linear Regression) or that use gradient descent, as it helps optimize faster.</p> </li> </ul> <p>Scikit-learn’s <code class="language-plaintext highlighter-rouge">MinMaxScaler</code> and <code class="language-plaintext highlighter-rouge">StandardScaler</code> are excellent tools for this. Always fit the scaler on your training data <em>only</em> and then transform both training and test data using that fitted scaler to prevent data leakage.</p> <h3 id="the-iterative-dance-with-data">The Iterative Dance with Data</h3> <p>Data cleaning is rarely linear. You’ll often find yourself going back and forth:</p> <ol> <li> <strong>Initial Exploration:</strong> Get a feel for the data, identify obvious issues.</li> <li> <strong>Apply a Cleaning Strategy:</strong> Address missing values, or outliers.</li> <li> <strong>Re-explore:</strong> Did your cleaning introduce new issues? Did you miss something?</li> <li> <strong>Repeat:</strong> Continue until your data is robust and ready.</li> </ol> <p>Leverage powerful Python libraries like <strong>Pandas</strong> for data manipulation, <strong>NumPy</strong> for numerical operations, <strong>Matplotlib</strong> and <strong>Seaborn</strong> for visualizations (which are essential for detecting issues!), and <strong>Scikit-learn</strong> for more advanced preprocessing steps and scaling.</p> <h3 id="wrapping-up-the-art-and-science-of-clean-data">Wrapping Up: The Art and Science of Clean Data</h3> <p>Data cleaning, while often perceived as tedious, is where the real magic happens. It’s where you build trust in your data, laying a rock-solid foundation for any subsequent analysis or model building. It’s a blend of technical skill, domain expertise, and a healthy dose of investigative curiosity.</p> <p>As you build your data science portfolio, demonstrating your ability to meticulously clean and prepare data is a powerful statement. It shows you understand the fundamentals, appreciate the nuances of real-world datasets, and are committed to producing reliable, high-quality results.</p> <p>So, the next time you encounter a messy dataset, don’t despair! Embrace the challenge. Put on your detective hat, apply these strategies, and transform that chaos into clarity. Your future models (and employers!) will thank you for it.</p> <p>Happy cleaning!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>