<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Q-Learning: How Machines Learn to Master Any Game (and Real Life!) | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/q-learning-how-machines-learn-to-master-any-game-a/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Q-Learning: How Machines Learn to Master Any Game (and Real Life!)</h1> <p class="post-meta"> Created on January 04, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/reinforcement-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Reinforcement Learning</a>   <a href="/blog/blog/tag/q-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Q-Learning</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Have you ever tried to teach a new trick to a pet, learn a new sport, or figure out the best route through a bustling city? What’s common in all these scenarios is that we learn by <em>doing</em>. We try something, observe the outcome, and adjust our strategy for next time. If a particular action leads to a good result (like a treat for your dog, or winning a point in tennis), we’re more likely to repeat it. If it leads to a bad outcome (a scolding, or hitting the ball out), we learn to avoid it.</p> <p>This intuitive, trial-and-error learning is precisely what <strong>Reinforcement Learning (RL)</strong> is all about in the world of Artificial Intelligence. And at its heart, for many foundational applications, lies a deceptively simple yet incredibly powerful algorithm called <strong>Q-Learning</strong>.</p> <p>Today, we’re going to pull back the curtain on Q-Learning. We’ll explore how it allows machines to make intelligent decisions in uncertain environments, effectively teaching them to “think” for themselves, one reward at a time.</p> <h3 id="the-world-of-reinforcement-learning-agents-environments-and-rewards">The World of Reinforcement Learning: Agents, Environments, and Rewards</h3> <p>Before we dive into Q-Learning itself, let’s quickly set the stage with the core components of any Reinforcement Learning problem:</p> <ol> <li> <strong>The Agent:</strong> This is our “learner” – the AI program or robot that’s trying to figure things out.</li> <li> <strong>The Environment:</strong> This is the world the agent interacts with. It could be a chess board, a virtual maze, a stock market, or even the controls of a robotic arm.</li> <li> <strong>States ($s$):</strong> A specific configuration or snapshot of the environment at a given time. For a chess game, a state would be the arrangement of all pieces on the board. For a robot, it might be its current location and battery level.</li> <li> <strong>Actions ($a$):</strong> The moves or choices the agent can make within a given state. Moving a chess piece, accelerating a car, or moving a robot arm are all actions.</li> <li> <strong>Rewards ($R$):</strong> A feedback signal from the environment after an action is taken. This is the crucial part! A positive reward encourages the agent to repeat an action, while a negative reward (often called a penalty) discourages it. The ultimate goal of the agent is to maximize its <em>cumulative reward</em> over time.</li> </ol> <p>Think of it like training a dog: The dog is the agent. Its world (your living room, the park) is the environment. “Sit,” “stay,” “fetch” are actions. And the reward? A tasty treat or a pat on the head!</p> <h3 id="enter-q-learning-the-quality-of-action">Enter Q-Learning: The Quality of Action</h3> <p>Q-Learning is a <em>model-free</em> reinforcement learning algorithm. “Model-free” means the agent doesn’t need to know the internal mechanics or rules of the environment beforehand. It learns purely by interacting and observing rewards, just like a human learning a new skill.</p> <p>The “Q” in Q-Learning stands for “Quality.” Specifically, it aims to learn the <em>quality</em> or <em>value</em> of taking a particular <strong>action</strong> (<code class="language-plaintext highlighter-rouge">$a$</code>) when the agent is in a particular <strong>state</strong> (<code class="language-plaintext highlighter-rouge">$s$</code>). This quality is represented by a value called the <strong>Q-value</strong>, denoted as <code class="language-plaintext highlighter-rouge">$Q(s, a)$</code>.</p> <p><strong>What does a Q-value tell us?</strong> A higher <code class="language-plaintext highlighter-rouge">$Q(s, a)$</code> means that taking action <code class="language-plaintext highlighter-rouge">$a$</code> in state <code class="language-plaintext highlighter-rouge">$s$</code> is likely to lead to greater cumulative future rewards. The agent’s goal then becomes straightforward: in any given state, choose the action that has the highest Q-value!</p> <h3 id="the-q-table-our-agents-scorecard">The Q-Table: Our Agent’s Scorecard</h3> <p>How does the agent store all these Q-values? For environments with a finite and manageable number of states and actions, Q-Learning typically uses a <strong>Q-table</strong>.</p> <p>Imagine a giant spreadsheet where:</p> <ul> <li>Each <strong>row</strong> represents a possible <strong>state</strong> the agent can be in.</li> <li>Each <strong>column</strong> represents a possible <strong>action</strong> the agent can take.</li> <li>Each <strong>cell</strong> at <code class="language-plaintext highlighter-rouge">(state, action)</code> contains the current <strong>Q-value</strong> for taking that specific action in that specific state.</li> </ul> <p>Initially, all Q-values in the table are usually set to zero (or some small random value). As the agent explores the environment, interacts, and receives rewards, these Q-values are updated, gradually converging towards the optimal values.</p> <h3 id="the-learning-process-exploration-exploitation-and-update">The Learning Process: Exploration, Exploitation, and Update!</h3> <p>The core of Q-Learning lies in its iterative update process. The agent repeatedly performs cycles of:</p> <ol> <li><strong>Observing the current state ($s$).</strong></li> <li><strong>Choosing an action ($a$).</strong></li> <li><strong>Performing the action, observing the immediate reward ($R$), and the new state ($s’$).</strong></li> <li><strong>Updating the Q-value for the state-action pair <code class="language-plaintext highlighter-rouge">$(s, a)$</code> using a special formula.</strong></li> </ol> <p>Let’s break down the critical concepts within this loop:</p> <h4 id="1-exploration-vs-exploitation-the-innovator-vs-the-expert">1. Exploration vs. Exploitation: The Innovator vs. The Expert</h4> <p>This is a fundamental dilemma in RL.</p> <ul> <li> <strong>Exploitation:</strong> The agent uses its current knowledge (the Q-table) to choose the action with the highest known Q-value for the current state. This is like sticking to what <em>works best</em> based on past experience.</li> <li> <strong>Exploration:</strong> The agent tries a random action, even if it doesn’t currently seem like the best choice. This is crucial for discovering new, potentially better paths or rewards that it hasn’t encountered yet. Without exploration, the agent might get stuck in a suboptimal strategy.</li> </ul> <p>To balance these, Q-Learning often employs an <strong>$\epsilon$-greedy strategy</strong>:</p> <ul> <li>With a small probability <code class="language-plaintext highlighter-rouge">$\epsilon$</code> (epsilon, e.g., 10%), the agent chooses a random action (exploration).</li> <li>With probability <code class="language-plaintext highlighter-rouge">$(1 - \epsilon)$</code> (e.g., 90%), the agent chooses the action with the highest Q-value for the current state (exploitation).</li> </ul> <p>Typically, <code class="language-plaintext highlighter-rouge">$\epsilon$</code> starts high (more exploration) and gradually decreases over time (more exploitation) as the agent learns more about the environment.</p> <h4 id="2-the-q-value-update-rule-the-core-intelligence">2. The Q-Value Update Rule: The Core Intelligence</h4> <p>This is where the magic happens. After the agent takes an action <code class="language-plaintext highlighter-rouge">$a$</code> in state <code class="language-plaintext highlighter-rouge">$s$</code>, receives reward <code class="language-plaintext highlighter-rouge">$R$</code>, and lands in a new state <code class="language-plaintext highlighter-rouge">$s'$</code>, it updates the Q-value for <code class="language-plaintext highlighter-rouge">$Q(s, a)$</code> using the following formula:</p> <p>`$Q(s, a) \leftarrow Q(s, a) + \alpha [R + \gamma \max_{a’} Q(s’, a’) - Q(s, a)]$</p> <p>Let’s dissect this powerful equation:</p> <ul> <li> <strong><code class="language-plaintext highlighter-rouge">$Q(s, a)$</code> (Old Value):</strong> This is the current Q-value for the state-action pair we just experienced.</li> <li> <strong><code class="language-plaintext highlighter-rouge">$\alpha$</code> (Alpha - Learning Rate):</strong> This value (between 0 and 1) determines how much new information overrides old information. A high <code class="language-plaintext highlighter-rouge">$\alpha$</code> means the agent learns quickly from new experiences, potentially forgetting old knowledge. A low <code class="language-plaintext highlighter-rouge">$\alpha$</code> means it learns slowly but steadily.</li> <li> <strong><code class="language-plaintext highlighter-rouge">$R$</code> (Reward):</strong> This is the immediate reward received after taking action <code class="language-plaintext highlighter-rouge">$a$</code> in state <code class="language-plaintext highlighter-rouge">$s$</code>.</li> <li> <strong><code class="language-plaintext highlighter-rouge">$\gamma$</code> (Gamma - Discount Factor):</strong> This value (between 0 and 1) determines the importance of future rewards. <ul> <li>If <code class="language-plaintext highlighter-rouge">$\gamma$</code> is close to 0, the agent focuses only on immediate rewards.</li> <li>If <code class="language-plaintext highlighter-rouge">$\gamma$</code> is close to 1, the agent considers future rewards almost as important as immediate ones, encouraging long-term planning.</li> </ul> </li> <li> <strong><code class="language-plaintext highlighter-rouge">$\max_{a'} Q(s', a')$</code> (Maximum Future Q-Value):</strong> This is the <em>estimate</em> of the best possible future reward the agent can get from the <em>new state</em> <code class="language-plaintext highlighter-rouge">$s'$</code>. It looks at all possible actions <code class="language-plaintext highlighter-rouge">$a'$</code> from <code class="language-plaintext highlighter-rouge">$s'$</code> and picks the one with the highest current Q-value. This term is crucial because it incorporates the “Bellman Equation” idea: the value of a state-action pair is based on the immediate reward plus the discounted maximum future reward.</li> <li> <strong><code class="language-plaintext highlighter-rouge">$[R + \gamma \max_{a'} Q(s', a') - Q(s, a)]$</code> (Temporal Difference Error):</strong> This entire term represents the “surprise” or the difference between what the agent <em>expected</em> to get (<code class="language-plaintext highlighter-rouge">$Q(s, a)$</code>) and what it <em>actually</em> learned from the immediate reward and the best possible future reward from the next state. The agent adjusts its <code class="language-plaintext highlighter-rouge">$Q(s, a)$</code> based on this error.</li> </ul> <p>In essence, the Q-learning update rule says: <em>Adjust your current estimate of <code class="language-plaintext highlighter-rouge">Q(s,a)</code> by a small fraction of the difference between what you expected and what you observed (immediate reward + best possible future reward from the next state).</em></p> <h3 id="a-simple-example-the-robot-in-a-maze">A Simple Example: The Robot in a Maze</h3> <p>Imagine a tiny robot in a 5x5 grid maze.</p> <ul> <li> <strong>States:</strong> Each cell in the grid (25 states).</li> <li> <strong>Actions:</strong> Move Up, Down, Left, Right (4 actions).</li> <li> <strong>Rewards:</strong> <ul> <li>Moving to an empty cell: -1 (small penalty for time/energy).</li> <li>Reaching the “Goal” cell: +100.</li> <li>Falling into a “Trap” cell: -100.</li> </ul> </li> </ul> <p>Initially, the robot knows nothing. It wanders randomly (<code class="language-plaintext highlighter-rouge">$\epsilon$</code> is high). It might hit a trap (-100 reward), or stumble upon the goal (+100 reward). When it gets a reward, its Q-table starts to get updated.</p> <p>If the robot reaches the goal, the <code class="language-plaintext highlighter-rouge">$Q(s, a)$</code> that led it there will get a significant positive update. The next time it’s in that previous state <code class="language-plaintext highlighter-rouge">$s$</code>, it will be more likely to choose action <code class="language-plaintext highlighter-rouge">$a$</code>. Over many, many episodes (runs through the maze), the robot will gradually refine its Q-table. The cells leading directly to the goal will have high positive Q-values, and those leading to traps will have low negative ones. Slowly, the “optimal path” (the sequence of actions that leads to the goal with maximum reward) emerges in the Q-table.</p> <p>After sufficient training, when <code class="language-plaintext highlighter-rouge">$\epsilon$</code> is very low, the robot will mostly exploit its knowledge, taking the path with the highest Q-values, effectively navigating the maze perfectly!</p> <h3 id="the-power-and-the-curse-of-dimensionality">The Power and the “Curse of Dimensionality”</h3> <p>Q-Learning is elegant and powerful for several reasons:</p> <ul> <li> <strong>Model-Free:</strong> It doesn’t need a mathematical model of the environment. It learns solely through experience.</li> <li> <strong>Guaranteed Convergence:</strong> Under certain conditions (finite states/actions, sufficient exploration, appropriate learning rate decay), Q-Learning is guaranteed to find the optimal policy.</li> <li> <strong>Simplicity:</strong> The core idea and update rule are relatively straightforward to understand and implement.</li> </ul> <p>However, its reliance on a Q-table is also its biggest limitation: the <strong>“Curse of Dimensionality.”</strong></p> <p>What if our environment isn’t a simple 5x5 grid, but a complex video game with millions of possible screen pixels (states) or a robotic arm with continuous joint angles and velocities (infinite states)? A Q-table would become astronomically large, impossible to store or populate with enough experience.</p> <p>This is where more advanced techniques come into play!</p> <h3 id="beyond-basic-q-learning-the-dawn-of-deep-q-networks-dqn">Beyond Basic Q-Learning: The Dawn of Deep Q-Networks (DQN)</h3> <p>To overcome the curse of dimensionality, researchers had a brilliant idea: instead of explicitly storing Q-values in a table, what if we could <em>approximate</em> them using a function? And what’s a fantastic function approximator in Machine Learning? <strong>Neural Networks!</strong></p> <p>This led to the development of <strong>Deep Q-Networks (DQN)</strong>, where a neural network takes the state as input and outputs the Q-values for all possible actions. The network learns to predict these Q-values, effectively generalizing across states and making Q-Learning scalable to incredibly complex environments, like playing Atari games from raw pixel data.</p> <h3 id="conclusion-your-first-step-into-a-smarter-future">Conclusion: Your First Step into a Smarter Future</h3> <p>Q-Learning is a cornerstone of Reinforcement Learning. It’s a beautiful demonstration of how simple iterative updates, guided by reward signals, can lead to incredibly sophisticated behaviors. From a robot learning to navigate a maze to the foundations of complex AI agents in games and real-world applications, the principles of Q-Learning are pervasive.</p> <p>Understanding Q-Learning isn’t just about mastering an algorithm; it’s about grasping a fundamental paradigm of learning. It shows us that intelligence, in many forms, can emerge from the continuous process of trial, error, reward, and adjustment. So, next time you see an AI making a surprisingly smart decision, remember the humble Q-table and its powerful update rule working diligently behind the scenes. This is just the beginning of your journey into the exciting world of intelligent agents!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>