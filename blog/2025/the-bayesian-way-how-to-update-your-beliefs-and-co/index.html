<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Bayesian Way: How to Update Your Beliefs and Conquer Uncertainty with Data | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/the-bayesian-way-how-to-update-your-beliefs-and-co/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Bayesian Way: How to Update Your Beliefs and Conquer Uncertainty with Data</h1> <p class="post-meta"> Created on April 05, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/bayesian-statistics"> <i class="fa-solid fa-hashtag fa-sm"></i> Bayesian Statistics</a>   <a href="/blog/blog/tag/probability"> <i class="fa-solid fa-hashtag fa-sm"></i> Probability</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/statistical-inference"> <i class="fa-solid fa-hashtag fa-sm"></i> Statistical Inference</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>Have you ever had a gut feeling about something, only to have it strengthened or completely overturned once you got more information? Maybe you were convinced your favorite sports team would win, but then you saw their star player was benched. Or perhaps you thought a particular movie would be terrible, but after watching the trailer, you changed your mind.</p> <p>This process of forming initial beliefs, observing new evidence, and then updating your stance is a fundamental part of being human. And guess what? There’s a powerful branch of statistics that formalizes this exact process: <strong>Bayesian Statistics</strong>.</p> <p>For a long time, in my early days of learning statistics, I was taught mostly what’s called “Frequentist Statistics.” It’s the kind where you hear about p-values, confidence intervals, and null hypothesis testing. It’s incredibly useful, don’t get me wrong! But there was always something that felt a little <em>off</em> to me, like it was missing a piece of the puzzle. It felt like it forced the world into a rigid “yes/no” box without fully embracing the shades of grey.</p> <p>Then I stumbled upon Bayesian statistics, and it felt like a lightbulb went off. It changed the way I thought about probability, data, and even how I approach problems in data science.</p> <h3 id="frequentist-vs-bayesian-a-philosophical-showdown">Frequentist vs. Bayesian: A Philosophical Showdown</h3> <p>Before we dive into the math, let’s briefly set the stage by understanding the core difference between the two main schools of thought in statistics.</p> <p><strong>Frequentist Statistics</strong> treats probability as the <em>long-run frequency</em> of an event. If you flip a fair coin an infinite number of times, the proportion of heads will approach 0.5. From this perspective, parameters (like the true bias of a coin, or the true average height of a population) are fixed, but unknown constants. Data, on the other hand, is random. We use the observed data to estimate these fixed parameters.</p> <p>Think of it like this: A frequentist asks, “Given that the coin <em>is</em> fair, how likely is it that I’d observe this specific sequence of flips?” They’re focused on the data’s randomness around a fixed, true parameter.</p> <p><strong>Bayesian Statistics</strong> takes a different approach. Here, probability is seen as a <em>measure of belief</em> or plausibility. Parameters are <em>not</em> fixed constants; instead, they are treated as random variables themselves. We start with an initial belief about these parameters (our “prior”), then collect data, and use that data to <em>update</em> our belief, resulting in a new, refined belief (our “posterior”).</p> <p>A Bayesian asks, “Given this sequence of flips, what’s my updated belief about how fair this coin is?” They’re focused on updating their beliefs about the parameter, given the observed data.</p> <p>This distinction is crucial. Bayesian statistics allows us to incorporate prior knowledge or existing beliefs directly into our analysis, making it incredibly intuitive and powerful for real-world problems where we often <em>do</em> have some initial information.</p> <h3 id="the-heartbeat-of-bayesianism-bayes-theorem">The Heartbeat of Bayesianism: Bayes’ Theorem</h3> <p>The entire edifice of Bayesian statistics stands on the shoulders of one beautiful, simple formula: <strong>Bayes’ Theorem</strong>.</p> <table> <tbody> <tr> <td>Let’s imagine we have a hypothesis $H$ (e.g., “This coin is biased towards heads”) and we observe some evidence $E$ (e.g., “I flipped the coin 10 times and got 8 heads”). What we want to know is the probability of our hypothesis being true <em>given</em> the evidence we’ve seen. This is written as $P(H</td> <td>E)$.</td> </tr> </tbody> </table> <p>Bayes’ Theorem gives us a way to calculate this:</p> \[P(H|E) = \frac{P(E|H)P(H)}{P(E)}\] <p>Let’s break down each term, because understanding them is key to unlocking the magic:</p> <ol> <li> <p><strong>$P(H|E)$ - The Posterior Probability:</strong> This is what we’re after! It’s our <strong>updated belief</strong> in the hypothesis $H$ <em>after</em> we’ve seen the evidence $E$. It represents our refined understanding.</p> </li> <li> <p><strong>$P(E|H)$ - The Likelihood:</strong> This tells us “how likely is it to observe the evidence $E$ if our hypothesis $H$ were true?” It’s the data speaking. If getting 8 heads in 10 flips is very likely if the coin is biased towards heads, then this term will be high.</p> </li> <li> <p><strong>$P(H)$ - The Prior Probability:</strong> This is our <strong>initial belief</strong> in the hypothesis $H$ <em>before</em> we’ve seen any evidence $E$. It’s your gut feeling, your common sense, or perhaps knowledge from previous studies. If you generally believe most coins are fair, your prior for “this coin is biased” might be low.</p> </li> <li> <p><strong>$P(E)$ - The Evidence (or Marginal Likelihood):</strong> This term is the overall probability of observing the evidence $E$, regardless of whether our hypothesis $H$ is true or not. It acts as a normalization constant, ensuring that our posterior probabilities sum to 1. For practical purposes, when comparing a few hypotheses, you can often think of it as “the sum of (likelihood * prior) for all possible hypotheses.”</p> </li> </ol> <p>So, in essence, Bayes’ Theorem states:</p> <p><strong>“Our updated belief in a hypothesis is proportional to how well the evidence supports the hypothesis, scaled by our initial belief in that hypothesis.”</strong></p> <p>It’s a continuous learning loop!</p> <h3 id="an-illustrative-example-the-rare-disease-test">An Illustrative Example: The Rare Disease Test</h3> <p>Let’s put Bayes’ Theorem into action with a classic example: a medical diagnostic test.</p> <p>Imagine a very rare disease that affects <strong>1 in 10,000 people</strong>. (This will be our <strong>Prior</strong> for the disease itself). There’s a diagnostic test for this disease. It’s pretty good, but not perfect:</p> <ul> <li> <table> <tbody> <tr> <td>It correctly identifies the disease <strong>99%</strong> of the time when someone <em>does</em> have it (True Positive Rate, $P(\text{Positive Test</td> <td>Disease})$).</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>It incorrectly gives a positive result <strong>0.5%</strong> of the time when someone <em>doesn’t</em> have the disease (False Positive Rate, $P(\text{Positive Test</td> <td>No Disease})$).</td> </tr> </tbody> </table> </li> </ul> <table> <tbody> <tr> <td>Now, imagine you get a positive test result. What is the probability that you <em>actually</em> have the disease? This is $P(\text{Disease</td> <td>Positive Test})$.</td> </tr> </tbody> </table> <p>Let’s define our terms:</p> <ul> <li>$H$: Having the Disease</li> <li>$E$: Positive Test Result</li> </ul> <table> <tbody> <tr> <td>Our goal is to find $P(H</td> <td>E)$, or $P(\text{Disease</td> <td>Positive Test})$.</td> </tr> </tbody> </table> <p>From the problem description:</p> <ul> <li> <strong>Prior $P(H)$</strong>: $P(\text{Disease}) = 1/10,000 = 0.0001$</li> <li> <table> <tbody> <tr> <td>**Likelihood $P(E</td> <td>H)$**: $P(\text{Positive Test</td> <td>Disease}) = 0.99$</td> </tr> </tbody> </table> </li> </ul> <p>We also need $P(E)$, the probability of getting a positive test result overall. A positive test can happen in two ways:</p> <ol> <li> <table> <tbody> <tr> <td>You have the disease AND the test is positive: $P(\text{Positive Test AND Disease}) = P(\text{Positive Test</td> <td>Disease}) \times P(\text{Disease})$</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>You DON’T have the disease AND the test is positive: $P(\text{Positive Test</td> <td>No Disease}) \times P(\text{No Disease})$</td> </tr> </tbody> </table> </li> </ol> <p>First, let’s find $P(\text{No Disease}) = 1 - P(\text{Disease}) = 1 - 0.0001 = 0.9999$. And we know $P(\text{Positive Test | No Disease}) = 0.005$.</p> <p>So, $P(E)$ (overall positive test) is: $P(E) = [P(\text{Positive Test | Disease}) \times P(\text{Disease})] + [P(\text{Positive Test | No Disease}) \times P(\text{No Disease})]$ $P(E) = (0.99 \times 0.0001) + (0.005 \times 0.9999)$ $P(E) = 0.000099 + 0.0049995$ $P(E) = 0.0050985$</p> <p>Now, plug these values into Bayes’ Theorem:</p> <table> <tbody> <tr> <td>$$ P(\text{Disease</td> <td>Positive Test}) = \frac{P(\text{Positive Test</td> <td>Disease}) \times P(\text{Disease})}{P(\text{Positive Test})} $$</td> </tr> <tr> <td>$$ P(\text{Disease</td> <td>Positive Test}) = \frac{0.99 \times 0.0001}{0.0050985} $$</td> <td> </td> </tr> <tr> <td>$$ P(\text{Disease</td> <td>Positive Test}) = \frac{0.000099}{0.0050985} $$</td> <td> </td> </tr> <tr> <td>$$ P(\text{Disease</td> <td>Positive Test}) \approx 0.0194 $$</td> <td> </td> </tr> </tbody> </table> <p>Wait, what?! If you get a positive test result, there’s only about a <strong>1.94% chance</strong> you actually have the disease.</p> <p>This often surprises people! Why is it so low if the test is 99% accurate? The key lies in the <strong>prior probability</strong> – the disease is extremely rare. Even a small false positive rate, when applied to a huge population of healthy people, can generate many more false positives than true positives for a rare condition.</p> <p>This example dramatically illustrates the power of incorporating prior knowledge. Without it, you might panic unnecessarily. With it, you get a much more realistic assessment of your situation.</p> <h3 id="why-bayesian-statistics-is-a-superpower-for-data-science-and-machine-learning">Why Bayesian Statistics is a Superpower for Data Science and Machine Learning</h3> <p>The elegance and flexibility of Bayesian thinking make it incredibly powerful for modern data science and machine learning tasks.</p> <ol> <li> <p><strong>Incorporating Prior Knowledge:</strong> As shown with the disease example, our existing knowledge (even if it’s just an informed guess) is valuable. Bayesian methods allow us to formally include this information. In ML, this can mean using domain expertise to set reasonable bounds on parameters or guide model training, especially when data is scarce. Think about building a recommendation system: you might have prior beliefs about user preferences or item popularity, which can be folded into the model before a user makes many purchases.</p> </li> <li> <p><strong>Uncertainty Quantification:</strong> Frequentist methods often give you a single “best estimate” (a point estimate) for a parameter. Bayesian methods, however, provide an entire <em>probability distribution</em> for the parameters – the posterior distribution. This means you don’t just get an average predicted value; you get a sense of <em>how certain</em> you are about that prediction. For example, instead of saying “the average click-through rate is 5%”, a Bayesian might say, “the click-through rate is between 4.5% and 5.5% with 95% probability.” This uncertainty estimate is crucial for decision-making, especially in high-stakes applications.</p> </li> <li> <p><strong>Small Data Problems:</strong> When you have very little data, frequentist methods can struggle to produce reliable estimates. The prior in Bayesian statistics acts as a form of regularization, helping to stabilize models and prevent overfitting when data is scarce. This is invaluable in fields like medical research or A/B testing with low traffic, where collecting vast amounts of data isn’t always feasible.</p> </li> <li> <p><strong>Hierarchical Models:</strong> Bayesian methods shine when dealing with complex, multi-level data structures (e.g., student performance across different schools, or product sales across different regions). Hierarchical Bayesian models allow information to be shared across groups, leading to more robust estimates, especially for groups with less data.</p> </li> <li> <p><strong>Direct Interpretability:</strong> Bayesian credible intervals (the Bayesian equivalent of confidence intervals) have a much more intuitive interpretation. A 95% credible interval means “there is a 95% probability that the true parameter lies within this interval,” which is often what people <em>think</em> a frequentist confidence interval means (but it doesn’t quite).</p> </li> </ol> <h3 id="challenges-to-consider">Challenges to Consider</h3> <p>While incredibly powerful, Bayesian statistics isn’t without its challenges:</p> <ul> <li> <strong>Choosing Priors:</strong> Sometimes, deciding on an appropriate prior can feel subjective. However, various strategies exist, from “uninformative” priors (that express minimal initial bias) to “weakly informative” priors (that inject a small amount of reasonable knowledge without being overly prescriptive).</li> <li> <strong>Computational Complexity:</strong> For complex models with many parameters, calculating the posterior distribution directly can be mathematically intractable. This often necessitates advanced computational techniques like Markov Chain Monte Carlo (MCMC) methods, which are algorithms that draw samples from the posterior distribution. While these tools (like PyMC or Stan) have made Bayesian computation much more accessible, understanding and effectively using them can still have a learning curve.</li> </ul> <h3 id="my-bayesian-journey-a-new-lens">My Bayesian Journey: A New Lens</h3> <p>Embracing Bayesian statistics was a pivotal moment in my data science journey. It gave me a new lens through which to view problems, moving beyond rigid statistical tests to a more fluid, adaptive way of thinking about uncertainty. It resonates deeply with how we, as humans, intuitively learn and adapt our worldview based on new experiences.</p> <p>Whether you’re building predictive models, conducting A/B tests, or just trying to make sense of the world, understanding the Bayesian approach can significantly enhance your ability to draw meaningful, actionable insights from data. It encourages a continuous dialogue between what you believe and what the data tells you, leading to smarter decisions and a richer understanding of the world around us.</p> <p>So, next time you’re faced with uncertainty, remember Bayes’ Theorem. It’s not just a formula; it’s a framework for learning, adapting, and always getting a little bit smarter with every piece of evidence you encounter. Dive in, and start updating your beliefs!</p> <p>Happy learning!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>