<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Decoding Decisions: My Journey into the Heart of Logistic Regression | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/decoding-decisions-my-journey-into-the-heart-of-lo/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Decoding Decisions: My Journey into the Heart of Logistic Regression</h1> <p class="post-meta"> Created on November 06, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/logistic-regression"> <i class="fa-solid fa-hashtag fa-sm"></i> Logistic Regression</a>   <a href="/blog/blog/tag/classification"> <i class="fa-solid fa-hashtag fa-sm"></i> Classification</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/supervised-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Supervised Learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><strong>(My Data Science Journal - Entry 10)</strong></p> <p>Hello fellow data adventurers!</p> <p>Today, I want to share a discovery that fundamentally shifted my understanding of predictive modeling. We’ve all heard of “Machine Learning,” and maybe you’ve even dabbled with linear regression – drawing straight lines through data points to predict things like house prices or temperatures. It’s powerful, sure, but what happens when the answer isn’t a number, but a <em>choice</em>?</p> <p>What if you’re trying to predict if an email is spam or not spam? If a customer will churn or stay? If a loan applicant will default or repay? These aren’t continuous values; they’re discrete categories, often just two: yes/no, 0/1, true/false. This is where the magic of <strong>Logistic Regression</strong> steps in, transforming our understanding of prediction from “how much?” to “which one?”</p> <p>Join me on a little mental journey as we peel back the layers of this fascinating algorithm. I promise, by the end, you’ll have a solid grasp of how this seemingly simple model underpins so much of the AI we interact with daily.</p> <h3 id="the-problem-with-straight-lines-when-linear-regression-falls-short">The Problem with Straight Lines: When Linear Regression Falls Short</h3> <p>Imagine we’re trying to predict if a student passes an exam based on the hours they studied. If we use linear regression, we might draw a line like this:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  ^ Grade (0-100)
  |
  |     *   *
  |   *   *
  | *       *
  +------------------&gt; Study Hours
</code></pre></div></div> <p>Now, what if we want to predict <em>pass/fail</em> (a binary outcome, say 0 for fail, 1 for pass)? If we just try to fit a line to 0s and 1s, we run into trouble.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  ^ Pass/Fail (1=Pass, 0=Fail)
  |
1 +           *   *   *
  |         *
  |       *
0 + - - * - - - - - - - &gt; Study Hours
  |
  | (Linear Regression tries to draw a line like this)
  |
  | - - - - - - - - - - - - - Line goes below 0 here
  |           Line goes above 1 here
</code></pre></div></div> <p>See the issue? A linear regression model can output values like -0.5 or 1.2. How can you have a “pass probability” of -0.5 or 1.2? It just doesn’t make sense for a probability, which <em>must</em> be between 0 and 1. This is our first “Aha!” moment: we need a function that constrains our output to this probability range.</p> <h3 id="the-s-curve-to-the-rescue-enter-the-sigmoid-function">The S-Curve to the Rescue: Enter the Sigmoid Function</h3> <p>This is where the <strong>Sigmoid function</strong> (also known as the Logistic function) makes its grand entrance. It’s an elegant mathematical function that takes any real-valued number and squashes it into a value between 0 and 1. Perfect for probabilities!</p> <p>Its formula looks a bit intimidating at first, but let’s break it down:</p> \[\sigma(z) = \frac{1}{1 + e^{-z}}\] <p>Where:</p> <ul> <li>$ \sigma(z) $ (pronounced “sigma of z”) is the output, a value between 0 and 1.</li> <li>$ e $ is Euler’s number, the base of the natural logarithm (approx 2.718).</li> <li>$ z $ is any real number, which in our case, will be the output of our familiar linear equation.</li> </ul> <p>Let’s visualize what this function does:</p> <ul> <li>If $ z $ is a very large positive number (e.g., 100), $ e^{-z} $ becomes extremely small (close to 0). So $ \sigma(100) \approx \frac{1}{1 + 0} = 1 $.</li> <li>If $ z $ is 0, $ e^{-0} = 1 $. So $ \sigma(0) = \frac{1}{1 + 1} = 0.5 $.</li> <li>If $ z $ is a very large negative number (e.g., -100), $ e^{-z} $ becomes extremely large. So $ \sigma(-100) \approx \frac{1}{\text{very large number}} = \text{very small number (close to 0)} $.</li> </ul> <p>The result is a beautiful S-shaped curve that smoothly transitions from 0 to 1. This curve is the heart of logistic regression, giving us that probability interpretation we desperately need.</p> <h3 id="building-the-logistic-regression-model-from-line-to-probability">Building the Logistic Regression Model: From Line to Probability</h3> <p>Now, how do we combine this sigmoid magic with our input data? Remember how linear regression computed an output $ y = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + … + \theta_n x_n $? In logistic regression, we take this exact linear combination of features and <em>feed it into the sigmoid function</em>.</p> <p>Let’s represent our linear combination as $ z $: \(z = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + ... + \theta_n x_n\) Or, in more compact vector notation: \(z = \theta^T X\) Where $ \theta $ is our vector of weights (coefficients) and $ X $ is our vector of input features.</p> <p>Then, the predicted probability $ h*\theta(x) $ that our output $ Y $ is 1 (e.g., “Pass”) given features $ X $ is: \(P(Y=1|X; \theta) = h*\theta(x) = \sigma(\theta^T X) = \frac{1}{1 + e^{-\theta^T X}}\)</p> <p>This $ h<em>\theta(x) $ gives us a probability between 0 and 1. For example, if $ h</em>\theta(x) = 0.8 $, it means there’s an 80% chance that the student will pass the exam.</p> <h3 id="making-the-call-the-decision-boundary">Making the Call: The Decision Boundary</h3> <p>So, we have a probability. But we need a “yes” or “no” answer. How do we convert, say, an 80% pass probability into a concrete prediction? We use a <strong>decision boundary</strong>.</p> <p>The most common threshold is 0.5.</p> <ul> <li>If $ h_\theta(x) \geq 0.5 $, we predict “1” (e.g., “Pass”).</li> <li>If $ h_\theta(x) &lt; 0.5 $, we predict “0” (e.g., “Fail”).</li> </ul> <p>Looking back at our sigmoid function, $ \sigma(z) $ is equal to 0.5 when $ z = 0 $. This means our decision boundary where we switch from predicting 0 to predicting 1 occurs when $ \theta^T X = 0 $.</p> <p>What does $ \theta^T X = 0 $ represent? It’s a linear equation! In a 2D space, it’s a line. In 3D, it’s a plane. In higher dimensions, it’s a hyperplane. This is why logistic regression is considered a <strong>linear classifier</strong>: it separates the classes with a straight line (or plane/hyperplane).</p> <h3 id="the-learning-phase-how-does-the-model-find-the-best-theta">The Learning Phase: How Does the Model Find the Best $\theta$?</h3> <p>This is where the “machine learning” truly happens. Our model needs to figure out the best values for $ \theta $ (our coefficients) that make the most accurate predictions. How do we define “accurate”? Through a <strong>Cost Function</strong>.</p> <p>In linear regression, we used Mean Squared Error (MSE). But for logistic regression, MSE doesn’t work well because the sigmoid function makes the cost function non-convex, meaning it would have many local minima, making it hard for our optimization algorithms to find the global best $ \theta $.</p> <p>Instead, for logistic regression, we use the <strong>Log-Loss</strong> (also known as Binary Cross-Entropy) cost function. It’s designed specifically for classification problems and has a beautiful intuition:</p> <p>For a single training example $(x, y)$:</p> <ul> <li>If the actual class $ y = 1 $: We want $h_\theta(x)$ (our predicted probability of 1) to be as close to 1 as possible. The cost is $ - \log(h_\theta(x)) $. <ul> <li>If $h_\theta(x)$ is close to 1, $ \log(h_\theta(x)) $ is close to 0, so cost is small.</li> <li>If $h_\theta(x)$ is close to 0, $ \log(h_\theta(x)) $ is a large negative number, so cost is large positive.</li> </ul> </li> <li>If the actual class $ y = 0 $: We want $h_\theta(x)$ (our predicted probability of 1) to be as close to 0 as possible (meaning $1 - h_\theta(x)$ is close to 1). The cost is $ - \log(1 - h_\theta(x)) $. <ul> <li>If $h_\theta(x)$ is close to 0, $ \log(1 - h_\theta(x)) $ is close to 0, so cost is small.</li> <li>If $h_\theta(x)$ is close to 1, $ \log(1 - h_\theta(x)) $ is a large negative number, so cost is large positive.</li> </ul> </li> </ul> <p>We can combine these two cases into one elegant formula for the cost of a single training example $(x^{(i)}, y^{(i)})$ where $y^{(i)}$ is either 0 or 1:</p> \[J(\theta)^{(i)} = - [y^{(i)} \log(h_\theta(x^{(i)})) + (1 - y^{(i)}) \log(1 - h_\theta(x^{(i)}))]\] <p>To get the total cost for all $ m $ training examples, we average the individual costs:</p> \[J(\theta) = - \frac{1}{m} \sum*{i=1}^{m} [y^{(i)} \log(h*\theta(x^{(i)})) + (1 - y^{(i)}) \log(1 - h\_\theta(x^{(i)}))]\] <p>Our goal? To find the values of $ \theta $ that <strong>minimize this total cost function $J(\theta)$</strong>.</p> <h3 id="minimizing-the-cost-gradient-descent-again">Minimizing the Cost: Gradient Descent (Again!)</h3> <p>Just like with linear regression, we use <strong>Gradient Descent</strong> to find the optimal $ \theta $ values. Gradient Descent is an iterative optimization algorithm that works by repeatedly adjusting $ \theta $ in the direction that most rapidly decreases the cost function.</p> <p>For each parameter $ \theta_j $, we update it simultaneously using the rule:</p> \[\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta)\] <p>Where:</p> <ul> <li>$ \alpha $ is the <strong>learning rate</strong>, controlling how big a step we take down the cost function’s “slope.”</li> <li>$ \frac{\partial}{\partial \theta_j} J(\theta) $ is the partial derivative of the cost function with respect to $ \theta_j $, which tells us the slope (or gradient) at our current position.</li> </ul> <p>The amazing thing is that the derivative for logistic regression’s cost function has a similar form to that of linear regression, making the implementation quite elegant! The core idea remains: move downhill on the cost surface until you reach the lowest point.</p> <h3 id="interpreting-the-coefficients-beyond-just-yes-or-no">Interpreting the Coefficients: Beyond Just “Yes” or “No”</h3> <p>One of the great strengths of logistic regression, similar to linear regression, is its interpretability. The coefficients $ \theta_j $ don’t directly tell us how much the probability changes (because of the sigmoid squeeze), but they tell us about the change in the <em>log-odds</em>.</p> <p>Specifically, $ e^{\theta_j} $ gives us the <strong>odds ratio</strong>. If $ e^{\theta_j} = 2 $, it means that for a one-unit increase in feature $ x_j $ (holding other features constant), the odds of the positive outcome (Y=1) double. This makes logistic regression very valuable in fields like medicine and social sciences where understanding the <em>impact</em> of a feature is crucial, not just the prediction itself.</p> <h3 id="strengths-and-limitations-every-tool-has-its-niche">Strengths and Limitations: Every Tool Has Its Niche</h3> <p><strong>Strengths:</strong></p> <ol> <li> <strong>Simplicity &amp; Interpretability:</strong> Easy to understand, implement, and interpret the coefficients (via odds ratios).</li> <li> <strong>Probabilistic Output:</strong> Provides probabilities, which are useful for ranking predictions or setting custom thresholds.</li> <li> <strong>Efficiency:</strong> Computationally inexpensive to train, especially for large datasets.</li> <li> <strong>Robust:</strong> Less prone to overfitting than more complex models if features are well-behaved.</li> <li> <strong>Good Baseline:</strong> Often a strong baseline model for classification tasks.</li> </ol> <p><strong>Limitations:</strong></p> <ol> <li> <strong>Linear Decision Boundary:</strong> Assumes that classes can be separated by a linear boundary. If the relationship is complex and non-linear, logistic regression might struggle.</li> <li> <strong>Feature Engineering Dependent:</strong> Performance heavily relies on well-engineered features.</li> <li> <strong>Sensitive to Outliers:</strong> Like linear regression, it can be sensitive to outliers, especially with small datasets.</li> <li> <strong>Assumes Independence:</strong> Assumes features are independent, though it can still perform well even if this assumption is violated.</li> </ol> <h3 id="real-world-applications-where-does-it-shine">Real-World Applications: Where Does It Shine?</h3> <p>Logistic regression is a workhorse in various industries:</p> <ul> <li> <strong>Healthcare:</strong> Predicting the likelihood of a disease (e.g., heart disease, diabetes) based on patient symptoms and test results.</li> <li> <strong>Finance:</strong> Credit scoring (will a customer default on a loan?), fraud detection.</li> <li> <strong>Marketing:</strong> Churn prediction (will a customer cancel their subscription?), click-through rate prediction for ads.</li> <li> <strong>Spam Detection:</strong> Classifying emails as “spam” or “not spam.”</li> <li> <strong>Recommendation Systems:</strong> Predicting if a user will like an item.</li> </ul> <h3 id="my-takeaway-a-foundational-pillar">My Takeaway: A Foundational Pillar</h3> <p>Learning about logistic regression was a profound experience for me. It wasn’t just another algorithm; it was the bridge that connected my understanding of continuous prediction (linear regression) to the world of discrete, categorical choices. It highlighted the importance of choosing the right tool for the job and introduced me to the elegance of transforming linear relationships into probabilities.</p> <p>It’s a foundational algorithm that every aspiring data scientist and ML engineer <em>must</em> understand. While more complex models exist today, logistic regression remains incredibly relevant for its interpretability, speed, and surprisingly robust performance on many real-world problems.</p> <p>So, the next time you get a “not spam” email, or a loan application is approved, remember the humble yet powerful sigmoid function and the cost-minimizing journey of logistic regression. It’s truly a testament to how elegant mathematics can solve complex, real-world problems!</p> <p>Until next time, keep exploring the data frontier!</p> <p>Cheers, [Your Name/Alias]</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>