<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Cracking the Code: How Logistic Regression Turns Probabilities into Decisions | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/2025/cracking-the-code-how-logistic-regression-turns-pr/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Cracking the Code: How Logistic Regression Turns Probabilities into Decisions</h1> <p class="post-meta"> Created on September 06, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/logistic-regression"> <i class="fa-solid fa-hashtag fa-sm"></i> Logistic Regression</a>   <a href="/blog/blog/tag/classification"> <i class="fa-solid fa-hashtag fa-sm"></i> Classification</a>   <a href="/blog/blog/tag/statistics"> <i class="fa-solid fa-hashtag fa-sm"></i> Statistics</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hello, fellow data explorers!</p> <p>Today, I want to share a foundational concept that truly clicked for me when I first started my journey in machine learning: <strong>Logistic Regression</strong>. Don’t let the word “regression” fool you; while it shares some lineage with its linear cousin, Logistic Regression is our go-to algorithm for <strong>classification</strong> problems – those moments when we need a crisp “yes” or “no,” a “spam” or “not spam,” a “malignant” or “benign.”</p> <p>It’s one of those algorithms that looks simple on the surface but holds a surprising depth once you peel back the layers. And trust me, understanding it deeply will unlock doors to many more complex models!</p> <h3 id="the-problem-with-yes-or-no-with-linear-regression">The Problem with “Yes” or “No” with Linear Regression</h3> <p>Imagine you’re trying to predict whether a student will pass an exam based on the hours they studied. A simple problem, right? You might think of using <strong>Linear Regression</strong>.</p> <p>Let’s say a student passes (Y=1) or fails (Y=0). If we plot “hours studied” vs. “pass/fail,” our data points would look something like this:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  Y (Pass/Fail)
  1 |      . . .
  0 | . . .
    +-------------------&gt;
           X (Hours Studied)
</code></pre></div></div> <p>If we try to fit a linear regression line through this, we’d get something like:</p> <p>$ h_\theta(x) = \theta_0 + \theta_1 x $</p> <p>The problem quickly becomes apparent:</p> <ol> <li> <strong>Output Range:</strong> The output of a linear regression model can be any real number ($ -\infty $ to $ +\infty $). But our target variable is strictly 0 or 1. How do we interpret a prediction of 0.7, or -0.2, or even 1.5? It doesn’t make sense in a binary context.</li> <li> <strong>Thresholding Issues:</strong> We could try to set a threshold, say, if $ h_\theta(x) \ge 0.5 $, predict 1, otherwise 0. But this is arbitrary.</li> <li> <strong>Sensitivity to Outliers:</strong> If we get a few more data points (e.g., a student who studied very little but somehow passed, or vice-versa), the linear regression line can significantly tilt, skewing our decision boundary and making it unstable.</li> </ol> <p>Linear Regression is designed to predict continuous values. We need a model that inherently understands and outputs probabilities, constrained between 0 and 1.</p> <h3 id="enter-the-sigmoid-squashing-our-predictions">Enter the Sigmoid: Squashing Our Predictions</h3> <p>This is where Logistic Regression truly shines. It takes the output of a linear equation and ‘squashes’ it into a probability using a special function called the <strong>Sigmoid function</strong> (also known as the Logistic function).</p> <p>The Sigmoid function looks like this:</p> <p>$ \sigma(z) = \frac{1}{1 + e^{-z}} $</p> <p>Where $e$ is Euler’s number (approximately 2.71828).</p> <p>Let’s break down why this function is so perfect for our task:</p> <ul> <li> <strong>S-Shape:</strong> When you plot the Sigmoid function, you get a beautiful ‘S’-shaped curve.</li> <li> <strong>Output Range:</strong> As $z$ approaches positive infinity, $e^{-z}$ approaches 0, so $ \sigma(z) $ approaches $ 1/1 = 1 $. As $z$ approaches negative infinity, $e^{-z}$ approaches infinity, so $ \sigma(z) $ approaches $ 1/\infty = 0 $. <ul> <li>This means the output of the Sigmoid function is always strictly between 0 and 1, precisely what we need for probabilities!</li> </ul> </li> <li> <strong>Decision Point:</strong> When $z=0$, $e^{-z} = e^0 = 1$, so $ \sigma(0) = \frac{1}{1+1} = 0.5 $. This point, where the probability is 0.5, will become our natural decision boundary.</li> </ul> <p>Now, how do we connect this to our features (like “hours studied”)? We simply plug our linear combination of features ($ \theta^T x $) into the Sigmoid function:</p> <table> <tbody> <tr> <td>$ h_\theta(x) = P(Y=1</td> <td>X;\theta) = \sigma(\theta^T x) = \frac{1}{1 + e^{-\theta^T x}} $</td> </tr> </tbody> </table> <p>Here, $ \theta^T x $ is the shorthand for $ \theta_0 + \theta_1 x_1 + \dots + \theta_n x_n $, where $ \theta $ represents our model’s parameters (weights) and $ x $ represents our input features.</p> <p>So, $ h_\theta(x) $ now gives us the <strong>estimated probability that our output Y is 1</strong>, given the input features $X$ and our current parameters $ \theta $.</p> <h3 id="interpreting-the-output-and-making-decisions">Interpreting the Output and Making Decisions</h3> <p>With $ h_\theta(x) $ providing a probability, making a decision becomes straightforward:</p> <ul> <li>If $ h_\theta(x) \ge 0.5 $, we predict $Y=1$.</li> <li>If $ h_\theta(x) &lt; 0.5 $, we predict $Y=0$.</li> </ul> <p>Thinking back to the Sigmoid function, we know that $ \sigma(z) \ge 0.5 $ when $ z \ge 0 $. Therefore, our decision rule is equivalent to:</p> <ul> <li>Predict $Y=1$ if $ \theta^T x \ge 0 $.</li> <li>Predict $Y=0$ if $ \theta^T x &lt; 0 $.</li> </ul> <p>The equation $ \theta^T x = 0 $ defines our <strong>decision boundary</strong>. For simple 2D cases, this is a line; in higher dimensions, it’s a hyperplane. This boundary separates the instances where we predict Y=1 from those where we predict Y=0.</p> <h3 id="training-logistic-regression-the-cost-function">Training Logistic Regression: The Cost Function</h3> <p>Okay, so we have our model ($ h_\theta(x) $). How do we find the best $ \theta $ values (our parameters/weights) that make our predictions as accurate as possible? This is where the <strong>cost function</strong> comes in. It measures how “wrong” our model is. Our goal is to find the $ \theta $ that minimizes this cost.</p> <p>For linear regression, we used Mean Squared Error (MSE). However, if we were to use MSE with the Sigmoid function, our cost function would become non-convex, meaning it would have many local minima. Gradient Descent (our optimization algorithm) might get stuck in one of these local minima and never find the global best solution. That’s a big no-no!</p> <p>Instead, for Logistic Regression, we use the <strong>Log Loss</strong> or <strong>Cross-Entropy Loss</strong>. This cost function is specifically designed for probability-based classification and is delightfully convex for Logistic Regression, guaranteeing that Gradient Descent will find the global minimum.</p> <p>The cost for a single training example is defined as:</p> <ul> <li>If the actual output $y=1$: $ \text{Cost}(h_\theta(x), y) = -\log(h_\theta(x)) $</li> <li>If the actual output $y=0$: $ \text{Cost}(h_\theta(x), y) = -\log(1-h_\theta(x)) $</li> </ul> <p>Let’s understand the intuition behind this:</p> <ul> <li>If $y=1$ and our model predicts $ h_\theta(x) $ close to 1 (e.g., 0.99), then $ -\log(0.99) $ will be a very small number, meaning low cost.</li> <li>If $y=1$ but our model predicts $ h_\theta(x) $ close to 0 (e.g., 0.01), then $ -\log(0.01) $ will be a very large positive number, meaning high cost (we’re heavily penalized for being confidently wrong!).</li> <li>The same logic applies when $y=0$, but using $ 1-h_\theta(x) $. If $y=0$ and our model predicts $ h_\theta(x) $ close to 0 (meaning $ 1-h_\theta(x) $ is close to 1), the cost is low. If it predicts $ h_\theta(x) $ close to 1, the cost is high.</li> </ul> <p>We can combine these two cases into a single, elegant formula for the cost of a single training example:</p> <p>$ \text{Cost}(h_\theta(x), y) = -y \log(h_\theta(x)) - (1-y) \log(1-h_\theta(x)) $</p> <p>And for the entire dataset of $m$ training examples, our total cost function $J(\theta)$ to minimize is:</p> <p>$ J(\theta) = -\frac{1}{m} \sum_{i=1}^m [y^{(i)} \log(h_\theta(x^{(i)})) + (1-y^{(i)}) \log(1-h_\theta(x^{(i)}))] $</p> <h3 id="optimization-gradient-descent">Optimization: Gradient Descent</h3> <p>With our cost function defined, we use an optimization algorithm like <strong>Gradient Descent</strong> to find the values of $ \theta $ that minimize $J(\theta)$.</p> <p>Gradient Descent iteratively updates each parameter $ \theta_j $ by moving in the direction opposite to the gradient of the cost function, multiplied by a learning rate $ \alpha $:</p> <p>$ \theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta) $</p> <p>The partial derivative of the cost function with respect to $ \theta_j $ (for a single training example) turns out to be surprisingly simple: $ (h_\theta(x) - y)x_j $.</p> <p>So, the update rule for each parameter $ \theta_j $ becomes:</p> <p>$ \theta_j := \theta_j - \alpha \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} $</p> <p>Notice the striking similarity to the Gradient Descent update rule for Linear Regression! This is one of those beautiful mathematical coincidences (or rather, elegant design choices) that makes ML so fascinating. The difference lies in the definition of $ h_\theta(x) $ – Sigmoid for Logistic Regression, linear for Linear Regression.</p> <h3 id="beyond-binary-multiclass-logistic-regression-softmax-regression">Beyond Binary: Multiclass Logistic Regression (Softmax Regression)</h3> <p>What if we have more than two classes? Say, classifying images of cats, dogs, and birds? Logistic Regression can be extended for multiclass classification using the <strong>Softmax function</strong>.</p> <p>Instead of a single probability for Y=1, Softmax Regression predicts a probability for <em>each</em> class. The Softmax function takes a vector of arbitrary real values and transforms it into a probability distribution, where each value is between 0 and 1, and all values sum up to 1. It essentially generalizes the Sigmoid function to multiple classes.</p> <h3 id="strengths-and-limitations">Strengths and Limitations</h3> <p>Every model has its sweet spot. Logistic Regression is no exception:</p> <p><strong>Strengths:</strong></p> <ul> <li> <strong>Simplicity and Interpretability:</strong> It’s relatively easy to understand and implement. The weights ($ \theta $) can tell you how important each feature is in predicting the outcome.</li> <li> <strong>Good Baseline:</strong> Often used as a robust baseline for classification problems. If a more complex model doesn’t significantly outperform Logistic Regression, it might be overfitting or unnecessarily complex.</li> <li> <strong>Probabilistic Output:</strong> Provides probabilities, which can be useful for ranking predictions or when the confidence of a prediction is important.</li> <li> <strong>Efficient:</strong> Computationally inexpensive to train, especially on large datasets.</li> </ul> <p><strong>Limitations:</strong></p> <ul> <li> <strong>Assumes Linearity:</strong> It models a linear relationship between the input features and the log-odds of the outcome. If the relationship is non-linear, it might not perform well unless you manually add non-linear feature transformations (e.g., polynomial features).</li> <li> <strong>Sensitivity to Outliers:</strong> While better than linear regression with MSE, it can still be sensitive to outliers, especially with small datasets.</li> <li> <strong>Feature Engineering:</strong> Often requires good feature engineering to capture complex relationships.</li> <li> <strong>Not for Highly Complex Data:</strong> For highly complex, non-linear relationships (like image recognition), deep learning models usually significantly outperform Logistic Regression.</li> </ul> <h3 id="conclusion">Conclusion</h3> <p>Logistic Regression, despite its humble name, is a powerhouse in the world of machine learning. It elegantly solves binary classification problems by transforming linear predictions into meaningful probabilities via the Sigmoid function and optimizing these probabilities using the Cross-Entropy loss.</p> <p>It’s a testament to how simple mathematical transformations can lead to powerful and interpretable models. Understanding Logistic Regression isn’t just about memorizing formulas; it’s about grasping the intuition behind turning a continuous output into a confident yes/no decision.</p> <p>So next time you encounter a problem that screams for a “yes” or “no,” remember our friend, Logistic Regression. It’s often the first, and sometimes the best, tool to reach for in your data science toolkit. Keep learning, keep exploring, and keep cracking those data codes!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>