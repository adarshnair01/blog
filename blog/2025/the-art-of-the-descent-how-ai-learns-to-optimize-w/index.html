<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Art of the Descent: How AI Learns to Optimize with Gradient Descent | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/2025/the-art-of-the-descent-how-ai-learns-to-optimize-w/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Art of the Descent: How AI Learns to Optimize with Gradient Descent</h1> <p class="post-meta"> Created on January 13, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/gradient-descent"> <i class="fa-solid fa-hashtag fa-sm"></i> Gradient Descent</a>   <a href="/blog/blog/tag/optimization"> <i class="fa-solid fa-hashtag fa-sm"></i> Optimization</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone! Have you ever looked at a perfectly trained AI model – whether it’s recognizing faces, recommending movies, or driving a car – and wondered, “How did it <em>learn</em> to do that?” It seems almost magical, right? For a long time, I found myself in awe, thinking there must be some incredibly complex, mystical process at play. But then I delved deeper, and discovered the elegant simplicity of its core mechanism: an algorithm called <strong>Gradient Descent</strong>.</p> <p>Think of it like this: if AI has a brain, Gradient Descent is one of the most critical processes running inside it, teaching it how to get better, step by careful step. Today, I want to take you on a journey to demystify this powerful algorithm, showing you not just <em>what</em> it is, but <em>how</em> it works, and why it’s so fundamental to the world of data science and machine learning.</p> <h2 id="the-valley-of-errors-our-ais-quest">The Valley of Errors: Our AI’s Quest</h2> <p>Imagine you’re blindfolded and dropped onto a vast, undulating mountain range. Your mission, should you choose to accept it, is to find the absolute lowest point in this entire landscape – a deep valley. You can’t see, so how do you proceed?</p> <p>You’d probably start by feeling around, taking a small step, and seeing if you’ve gone up or down. If you went up, you’d know that was the wrong direction. If you went down, you’d repeat the process, always trying to descend. You’d continuously take small steps in the steepest <em>downhill</em> direction, hoping to eventually reach the bottom.</p> <p>This, my friends, is the essence of Gradient Descent.</p> <p>In machine learning, our “mountain range” isn’t made of rock and soil; it’s a conceptual landscape of <strong>errors</strong>. We call this the <strong>cost function</strong> (or loss function). Each point on this landscape represents a different set of parameters (or “settings”) for our AI model, and the “height” at that point represents how much error our model makes with those parameters. Our goal? To find the set of parameters that correspond to the lowest point in the valley – where our model’s errors are minimized.</p> <h2 id="formalizing-the-error-the-cost-function">Formalizing the “Error”: The Cost Function</h2> <p>Let’s ground this with a simple example: <strong>Linear Regression</strong>. Our goal in linear regression is to find the best-fitting straight line ($y = mx + b$) through a scatter plot of data points. Here, $m$ is the slope and $b$ is the y-intercept. These are our model’s “parameters” or “settings.”</p> <p>How do we define “best-fitting”? We need a way to measure the “error” or “cost” for any given line ($m, b$). A common choice is the <strong>Mean Squared Error (MSE)</strong>.</p> <p>For each data point $(x_i, y_i)$, our line predicts a value $\hat{y}_i = mx_i + b$. The error for that point is $(y_i - \hat{y}_i)$. We square this error to ensure positive values and penalize larger errors more, then average it over all $N$ data points.</p> <p>So, our cost function $J(m, b)$ looks like this:</p> <p>$J(m, b) = \frac{1}{N} \sum_{i=1}^{N} (y_i - (mx_i + b))^2$</p> <p>Our blindfolded AI’s job is to find the values of $m$ and $b$ that make $J(m, b)$ as small as possible. This is the “lowest point in the valley.”</p> <h2 id="the-compass-what-is-a-gradient">The Compass: What is a Gradient?</h2> <p>How do we know which way is “downhill”? This is where calculus comes to our rescue!</p> <p>In our mountain analogy, you’d feel the slope around you. Mathematically, the “slope” on a multi-dimensional surface (like our cost function) is given by its <strong>gradient</strong>.</p> <p>A <strong>gradient</strong> is essentially a vector of all the partial derivatives of a function with respect to its variables. For our cost function $J(m, b)$, the gradient will tell us how much $J$ changes if we tweak $m$ a little, and how much $J$ changes if we tweak $b$ a little.</p> <ul> <li>A <strong>partial derivative</strong> $\frac{\partial J}{\partial m}$ tells us the slope of the cost function with respect to $m$, assuming $b$ is held constant.</li> <li>Similarly, $\frac{\partial J}{\partial b}$ tells us the slope with respect to $b$, assuming $m$ is held constant.</li> </ul> <p>Crucially, the gradient always points in the direction of the <strong>steepest ascent</strong> (uphill). So, if we want to go downhill, we need to move in the <em>opposite</em> direction of the gradient.</p> <h2 id="the-descent-the-gradient-descent-algorithm">The Descent: The Gradient Descent Algorithm</h2> <p>Now we have all the pieces! Here’s the step-by-step algorithm:</p> <ol> <li> <strong>Initialize Parameters:</strong> Start with some random values for our parameters, $m$ and $b$. Think of this as being dropped randomly on our error landscape.</li> <li> <strong>Calculate the Gradient:</strong> At our current $(m, b)$ position, calculate the partial derivatives of the cost function $J(m, b)$ with respect to $m$ and $b$. <ul> <li>$\frac{\partial J}{\partial m} = \frac{2}{N} \sum_{i=1}^{N} (y_i - (mx_i + b))(-x_i)$</li> <li>$\frac{\partial J}{\partial b} = \frac{2}{N} \sum_{i=1}^{N} (y_i - (mx_i + b))(-1)$ <em>(Don’t worry too much about the exact derivation here; the key is understanding that these formulas tell us the direction of steepest “uphill” for our cost.)</em> </li> </ul> </li> <li> <p><strong>Update Parameters:</strong> Move the parameters in the <em>opposite</em> direction of the gradient. We also need a crucial hyperparameter here: the <strong>learning rate</strong>, denoted by $\alpha$ (alpha). The learning rate controls the size of our steps.</p> <p>$m_{new} = m_{old} - \alpha \frac{\partial J}{\partial m}$ $b_{new} = b_{old} - \alpha \frac{\partial J}{\partial b}$</p> <p>Notice the minus sign! That’s how we ensure we’re always going downhill.</p> </li> <li> <strong>Repeat:</strong> Go back to Step 2 and repeat the process until convergence (i.e., until our parameters stop changing significantly, or until we’ve run a set number of iterations).</li> </ol> <h3 id="the-learning-rate-alpha-a-crucial-step-size">The Learning Rate ($\alpha$): A Crucial Step Size</h3> <p>The learning rate is perhaps the most important hyperparameter in Gradient Descent.</p> <ul> <li>If $\alpha$ is too <strong>large</strong>, we might take huge steps, overshooting the minimum, oscillating wildly, or even diverging entirely (climbing out of the valley!).</li> <li>If $\alpha$ is too <strong>small</strong>, we’ll take tiny, slow steps, and it might take an incredibly long time to reach the minimum, if ever.</li> </ul> <p>Finding the right learning rate often involves a bit of experimentation and clever techniques (like learning rate schedules, which change $\alpha$ over time).</p> <h2 id="flavors-of-descent-batch-stochastic-and-mini-batch">Flavors of Descent: Batch, Stochastic, and Mini-Batch</h2> <p>While the core idea remains the same, how we calculate the gradient can vary, leading to different flavors of Gradient Descent:</p> <ol> <li> <strong>Batch Gradient Descent (BGD):</strong> <ul> <li>Calculates the gradient using <strong>all</strong> data points in the training set for <em>each</em> parameter update.</li> <li> <strong>Pros:</strong> Smoother, more stable convergence directly to the minimum.</li> <li> <strong>Cons:</strong> Very slow for large datasets because it processes the entire dataset for every single step.</li> </ul> </li> <li> <strong>Stochastic Gradient Descent (SGD):</strong> <ul> <li>Calculates the gradient and updates parameters using only <strong>one randomly chosen data point</strong> at a time.</li> <li> <strong>Pros:</strong> Much faster per update, as it doesn’t process the entire dataset. Can sometimes escape shallow local minima due to its noisy updates.</li> <li> <strong>Cons:</strong> The updates are very noisy, causing the cost function to fluctuate and not always converge smoothly. It might bounce around the minimum rather than settling precisely.</li> </ul> </li> <li> <strong>Mini-Batch Gradient Descent:</strong> <ul> <li>The most popular approach in modern deep learning! It’s a compromise between BGD and SGD.</li> <li>Calculates the gradient and updates parameters using a <strong>small, randomly selected “batch”</strong> of data points (e.g., 32, 64, 128 data points).</li> <li> <strong>Pros:</strong> Offers a good balance of speed and stability. The batch size is small enough for faster updates than BGD, but large enough to provide a more stable estimate of the gradient than pure SGD, leading to smoother convergence.</li> <li> <strong>Cons:</strong> Requires choosing an optimal mini-batch size.</li> </ul> </li> </ol> <h2 id="challenges-and-considerations">Challenges and Considerations</h2> <p>While Gradient Descent is incredibly powerful, it’s not without its challenges:</p> <ul> <li> <strong>Local Minima:</strong> In complex, non-convex cost functions (like those in deep neural networks), there might be multiple “valleys” or <strong>local minima</strong>. Gradient Descent might get stuck in one of these local minima, even if a lower “global minimum” exists elsewhere. SGD and Mini-Batch GD, with their inherent “noise,” can sometimes help jiggle out of these.</li> <li> <strong>Plateaus and Saddle Points:</strong> Flat regions where the gradient is near zero can also slow down or stall convergence.</li> <li> <strong>Feature Scaling:</strong> If your features have very different scales, the cost function landscape can become elongated, making it harder for Gradient Descent to find the minimum efficiently. Scaling your data (e.g., normalization) often helps.</li> </ul> <p>Advanced optimization algorithms like Adam, RMSprop, and Adagrad are essentially more sophisticated versions of Gradient Descent that address many of these challenges by adaptively adjusting the learning rate for each parameter, providing momentum, and more. But at their core, they still rely on the fundamental principle of taking steps in the direction of the negative gradient.</p> <h2 id="why-gradient-descent-matters">Why Gradient Descent Matters</h2> <p>Gradient Descent is the beating heart of countless machine learning algorithms. From simple linear regression to complex neural networks with millions of parameters, the ability to iteratively find the optimal parameters by minimizing a cost function is what allows these models to “learn” from data.</p> <p>It allows us to build models that can:</p> <ul> <li> <strong>Predict</strong> house prices, stock movements, or weather patterns.</li> <li> <strong>Classify</strong> images, emails (spam/not spam), or medical diagnoses.</li> <li> <strong>Generate</strong> realistic text, images, or music.</li> </ul> <p>Without Gradient Descent, the modern AI revolution wouldn’t be possible. It’s an elegant mathematical tool that empowers machines to navigate complex data landscapes, discover patterns, and ultimately, learn to make sense of the world.</p> <h2 id="wrapping-up">Wrapping Up</h2> <p>So, the next time you hear about an AI accomplishing an impressive feat, remember the humble yet powerful Gradient Descent. It’s not a magical black box, but a methodical, iterative process of feeling its way down an error landscape, guided by the compass of the gradient, one carefully chosen step at a time. It’s a beautiful testament to how mathematics forms the bedrock of our technological future.</p> <p>Keep exploring, keep learning, and perhaps try implementing a simple Gradient Descent yourself! It’s a truly rewarding experience to see this concept come to life in code. Happy descending!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>