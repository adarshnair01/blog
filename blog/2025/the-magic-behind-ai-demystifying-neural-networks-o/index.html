<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Magic Behind AI: Demystifying Neural Networks, One Neuron at a Time | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/the-magic-behind-ai-demystifying-neural-networks-o/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Magic Behind AI: Demystifying Neural Networks, One Neuron at a Time</h1> <p class="post-meta"> Created on March 23, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/neural-networks"> <i class="fa-solid fa-hashtag fa-sm"></i> Neural Networks</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/artificial-intelligence"> <i class="fa-solid fa-hashtag fa-sm"></i> Artificial Intelligence</a>   <a href="/blog/blog/tag/backpropagation"> <i class="fa-solid fa-hashtag fa-sm"></i> Backpropagation</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>As a budding data scientist, I remember my early days staring at headlines about AI achieving superhuman feats in games, translating languages flawlessly, and even generating realistic art. My mind immediately jumped to scenes from sci-fi movies – sentient robots and complex algorithms operating on pure, unfathomable magic. But then I started digging, and what I found wasn’t magic, but rather an incredibly elegant and powerful framework: <strong>Neural Networks</strong>.</p> <p>This isn’t just a technical deep dive; it’s an invitation to explore the very architecture that underpins so much of modern AI. Think of it as a personal journal entry, a journey into understanding how these “digital brains” learn, adapt, and make sense of the world. If you’ve ever felt intimidated by the jargon, or simply curious about the “how” behind the AI hype, then let’s unravel this mystery together.</p> <h3 id="what-are-neural-networks-anyway">What <em>Are</em> Neural Networks Anyway?</h3> <p>At their core, Artificial Neural Networks (ANNs) are computational models inspired by the structure and function of the human brain. Our brains are astounding networks of billions of interconnected neurons, constantly processing information. ANNs attempt to mimic this by creating layers of interconnected “artificial neurons” that can learn complex patterns from data.</p> <p>When I first heard this, I imagined tiny computer brains. While that’s a fun image, the reality is more about mathematical functions and clever optimization.</p> <h3 id="the-humble-neuron-the-building-block">The Humble Neuron: The Building Block</h3> <p>Just like the biological neuron, the artificial neuron is the fundamental processing unit. Let’s break it down:</p> <p>Imagine a biological neuron receiving signals (inputs) through its dendrites. If these signals are strong enough, they trigger an electrical impulse (output) that travels down the axon to other neurons.</p> <p>An artificial neuron works similarly, but with numbers:</p> <ol> <li> <strong>Inputs ($x_i$):</strong> These are the features from your data, like pixel values in an image or words in a sentence.</li> <li> <strong>Weights ($w_i$):</strong> Each input has an associated weight. Think of weights as the neuron’s “attention” to that specific input. A higher weight means that input is more important.</li> <li> <strong>Bias ($b$):</strong> This is an additional value added to the weighted sum. It allows the neuron to activate even if all inputs are zero, or to shift the activation function. It’s like a neuron’s inherent “activation threshold.”</li> <li> <strong>Summation:</strong> The neuron calculates the weighted sum of its inputs plus the bias: $\sum_{i=1}^{n} w_i x_i + b$.</li> <li> <strong>Activation Function ($f$):</strong> This is the non-linear “decision maker.” It takes the weighted sum and transforms it into the neuron’s output. It introduces non-linearity, which is crucial for learning complex patterns.</li> </ol> <p>So, the output of a single artificial neuron can be represented as:</p> \[\text{Output} = f\left(\sum_{i=1}^{n} w_i x_i + b\right)\] <p>This simple formula is the heartbeat of every neural network!</p> <h3 id="layers-of-complexity-from-one-to-many">Layers of Complexity: From One to Many</h3> <p>A single neuron can do simple tasks, like a basic linear classifier. But real-world problems are rarely that simple. This is where multiple neurons, arranged in layers, come into play.</p> <p>A typical neural network structure looks like this:</p> <ul> <li> <strong>Input Layer:</strong> This layer simply receives the raw data. It doesn’t perform any computation, just passes the inputs ($x_i$) to the next layer.</li> <li> <strong>Hidden Layers:</strong> These are the “thinking” layers. Each neuron in a hidden layer takes inputs from the previous layer, performs its weighted sum and activation, and passes its output to the next layer. Networks can have one, two, or even hundreds of hidden layers (that’s where “deep” learning comes from!). These layers are where the network learns to extract complex features from the data.</li> <li> <strong>Output Layer:</strong> This layer produces the network’s final prediction. The number of neurons and the choice of activation function here depend on the task. For binary classification (e.g., “cat” or “dog”), you might have one neuron with a sigmoid activation. For multi-class classification (e.g., “cat”, “dog”, “bird”), you might have multiple neurons with a softmax activation.</li> </ul> <p>Information flows strictly in one direction, from the input layer through the hidden layers to the output layer. This is called a <strong>feedforward</strong> network.</p> <h3 id="activation-functions-giving-neurons-personality">Activation Functions: Giving Neurons Personality</h3> <p>Why do we need activation functions? If we didn’t have them, stacking layers would just result in another linear transformation, no matter how many layers we added. It would be like trying to model a curve with only straight lines – impossible! Activation functions introduce <strong>non-linearity</strong>, allowing neural networks to learn and represent virtually any complex function.</p> <p>Let’s look at a couple of popular ones:</p> <ol> <li> <p><strong>Sigmoid:</strong></p> \[f(x) = \frac{1}{1 + e^{-x}}\] <p>This function squashes any input value between 0 and 1. Historically popular for output layers in binary classification (representing probabilities). However, it suffers from the “vanishing gradient” problem for very large or very small inputs, making learning slow in deep networks.</p> </li> <li> <p><strong>Rectified Linear Unit (ReLU):</strong> \(f(x) = \max(0, x)\) This one is deceptively simple but incredibly powerful. It outputs the input directly if it’s positive, otherwise, it outputs zero. ReLU’s popularity comes from its computational efficiency and its ability to mitigate the vanishing gradient problem, making it a go-to for hidden layers in deep networks.</p> </li> </ol> <h3 id="the-loss-function-how-badly-did-we-do">The Loss Function: How Badly Did We Do?</h3> <p>After the network makes a prediction, we need to know how good that prediction is. This is where the <strong>loss function</strong> (or cost function) comes in. It quantifies the discrepancy between the network’s predicted output ($\hat{y}$) and the actual true output ($y$).</p> <p>Our goal during training is always to minimize this loss.</p> <ul> <li> <p><strong>Mean Squared Error (MSE):</strong> A common choice for regression problems (predicting continuous values).</p> \[L = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2\] <p>It calculates the average of the squared differences between predictions and actual values. The squaring penalizes larger errors more heavily.</p> </li> <li> <p><strong>Cross-Entropy Loss:</strong> Often used for classification problems. It measures how “different” two probability distributions are. For example, if your network predicts a high probability for “cat” but the true label was “dog,” the cross-entropy loss will be very high. If it correctly predicted “cat” with high confidence, the loss would be low.</p> </li> </ul> <h3 id="optimization-learning-from-mistakes-gradient-descent">Optimization: Learning from Mistakes (Gradient Descent)</h3> <p>Now that we know how good (or bad) our predictions are, how do we make the network better? This is the job of an <strong>optimizer</strong>. The most common optimization algorithm is <strong>Gradient Descent</strong>.</p> <p>Imagine you’re blindfolded on a mountainous terrain, trying to find the lowest point (the minimum loss). You can only feel the slope directly under your feet. What do you do? You take a step in the direction of the steepest descent. This is precisely what gradient descent does!</p> <p>The “slope” in our analogy is the <strong>gradient</strong> of the loss function with respect to each weight and bias in the network. The gradient tells us two things:</p> <ol> <li>The <strong>direction</strong> in which the loss function is increasing most rapidly.</li> <li>The <strong>magnitude</strong> of that increase (the steepness).</li> </ol> <p>Since we want to <em>minimize</em> the loss, we move in the <em>opposite</em> direction of the gradient. This involves updating each weight ($w$) and bias ($b$) using a simple rule:</p> \[w_{new} = w_{old} - \alpha \frac{\partial L}{\partial w}\] \[b_{new} = b_{old} - \alpha \frac{\partial L}{\partial b}\] <ul> <li>$\frac{\partial L}{\partial w}$ is the partial derivative of the loss function with respect to weight $w$. It tells us how much a small change in $w$ affects the loss.</li> <li>$\alpha$ is the <strong>learning rate</strong>. This is a crucial hyperparameter that determines the size of the steps we take down the loss landscape. Too large, and we might overshoot the minimum; too small, and training will take forever.</li> </ul> <h3 id="backpropagation-the-secret-sauce">Backpropagation: The Secret Sauce</h3> <p>Gradient descent is the strategy, but how do we <em>calculate</em> all those gradients efficiently for millions of weights and biases in a deep network? Enter <strong>Backpropagation</strong>, the algorithm that truly made training deep neural networks feasible.</p> <p>When I first heard about backpropagation, it sounded like pure wizardry. The idea is simple in concept but complex in execution:</p> <ol> <li> <strong>Forward Pass:</strong> Data flows from input to output, making a prediction ($\hat{y}$).</li> <li> <strong>Calculate Loss:</strong> We compare $\hat{y}$ with the true label $y$ to get the total loss $L$.</li> <li> <strong>Backward Pass (Backpropagation):</strong> The error signal (gradient of the loss) propagates backward through the network, from the output layer to the input layer. Using the <strong>chain rule of calculus</strong>, each weight and bias gets to “know” how much it contributed to the overall error. This allows us to calculate $\frac{\partial L}{\partial w}$ for every single parameter in the network.</li> <li> <strong>Update Weights:</strong> Once all gradients are calculated, the optimizer (like gradient descent) uses them to update the weights and biases.</li> </ol> <p>This iterative process of forward pass, error calculation, backward pass, and weight update is repeated thousands or millions of times over vast datasets. Each cycle refines the weights and biases, gradually minimizing the loss function, and making the network’s predictions more accurate. It’s truly how neural networks <em>learn</em>.</p> <h3 id="a-simple-analogy-learning-to-ride-a-bike">A Simple Analogy: Learning to Ride a Bike</h3> <p>Think about learning to ride a bike:</p> <ul> <li> <strong>Initial state:</strong> You hop on, wobbly (random initial weights).</li> <li> <strong>Attempt (Forward Pass):</strong> You push off, trying to balance.</li> <li> <strong>Feedback (Loss):</strong> You fall! Ouch. That’s your “loss.”</li> <li> <strong>Correction (Backpropagation &amp; Gradient Descent):</strong> Your brain analyzes <em>why</em> you fell (error signal). Maybe you leaned too far right, didn’t pedal enough, or steered too sharply. You adjust your balance, pedaling strength, and steering <em>in reverse</em> of what caused the fall.</li> <li> <strong>Repeat:</strong> You try again, incorporating those adjustments. Each fall makes you slightly better until you eventually learn to ride smoothly (minimal loss).</li> </ul> <h3 id="why-are-neural-networks-so-powerful">Why Are Neural Networks So Powerful?</h3> <ol> <li> <strong>Feature Learning:</strong> Unlike traditional machine learning algorithms where you manually engineer features, NNs can automatically learn hierarchical features directly from raw data. In image recognition, for instance, early layers might learn edges and corners, while deeper layers combine these to recognize complex shapes like eyes or wheels.</li> <li> <strong>Universal Approximation Theorem:</strong> This remarkable theorem states that a feedforward network with at least one hidden layer and a non-linear activation function can approximate any continuous function to an arbitrary degree of accuracy. This means, theoretically, NNs can learn incredibly complex relationships in data.</li> <li> <strong>Handling Non-Linearity:</strong> Thanks to activation functions, NNs excel at modeling non-linear relationships that are prevalent in real-world data, something simpler linear models struggle with.</li> <li> <strong>Scalability:</strong> With enough data and computational power, deep neural networks can scale to solve extremely complex problems like natural language understanding and large-scale image recognition.</li> </ol> <h3 id="my-ongoing-journey-and-your-next-steps">My Ongoing Journey and Your Next Steps</h3> <p>Understanding the fundamental mechanics of neural networks—the neuron, activation functions, loss, gradient descent, and backpropagation—was a lightbulb moment for me. It transformed “magic” into a beautiful, logical system.</p> <p>While we’ve only scratched the surface of basic feedforward networks, these principles are the bedrock for more advanced architectures like Convolutional Neural Networks (CNNs) for image processing, Recurrent Neural Networks (RNNs) for sequential data like text, and the groundbreaking Transformers dominating large language models.</p> <p>If this sparked your curiosity, I highly encourage you to:</p> <ul> <li> <strong>Experiment with code:</strong> Libraries like TensorFlow and PyTorch make building and training NNs surprisingly accessible. Start with a simple “Hello World” example like classifying handwritten digits (MNIST dataset).</li> <li> <strong>Dive deeper:</strong> Explore different activation functions, optimizers (Adam, RMSprop), and regularization techniques (dropout).</li> <li> <strong>Read more:</strong> There are incredible resources online, from academic papers to interactive visualizations.</li> </ul> <p>The world of Neural Networks is vast and constantly evolving, offering endless opportunities for innovation and discovery. My journey into understanding them has been incredibly rewarding, and I hope this exploration has made yours a little clearer and more exciting too!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>