<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> How Computers Learn to See: My Deep Dive into Convolutional Neural Networks | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/how-computers-learn-to-see-my-deep-dive-into-convo/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">How Computers Learn to See: My Deep Dive into Convolutional Neural Networks</h1> <p class="post-meta"> Created on October 27, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/convolutional-neural-networks"> <i class="fa-solid fa-hashtag fa-sm"></i> Convolutional Neural Networks</a>   <a href="/blog/blog/tag/cnns"> <i class="fa-solid fa-hashtag fa-sm"></i> CNNs</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/computer-vision"> <i class="fa-solid fa-hashtag fa-sm"></i> Computer Vision</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h3 id="the-magic-of-sight-deconstructed">The Magic of Sight, Deconstructed</h3> <p>It wasn’t that long ago that the idea of a computer truly “seeing” the world like we do seemed like pure science fiction. Sure, they could process pixels, identify colors, or maybe even detect simple shapes if you programmed them with incredibly specific rules. But recognizing a cat versus a dog, distinguishing different human faces, or even understanding the context of an entire scene? That felt like a uniquely human capability, a complex feat of our biological vision system honed over millions of years.</p> <p>Then came the explosion of Deep Learning, and with it, a particular architecture that utterly revolutionized how computers perceive images: <strong>Convolutional Neural Networks (CNNs)</strong>. The first time I saw a demo of a CNN classifying images with uncanny accuracy, it felt like watching magic unfold. I knew I had to understand what was going on under the hood, to peel back the layers of this digital marvel. This post is my attempt to share that journey, to break down the core ideas of CNNs into something accessible yet deep, just like I wished someone had explained it to me.</p> <h3 id="the-problem-with-seeing-for-traditional-neural-networks">The Problem with “Seeing” for Traditional Neural Networks</h3> <p>Before we jump into CNNs, let’s briefly consider why traditional neural networks struggle with images. Imagine a simple grayscale image, say, 28x28 pixels. That’s 784 individual numbers (pixel intensities). If you feed this into a regular neural network, each pixel would be an input feature. For a larger image, say 200x200, you’re looking at 40,000 input features!</p> <p>Now, consider what happens if you want to detect a cat. A cat’s ear might appear in the top-left corner in one image, and the bottom-right in another. A traditional neural network would have to learn an entirely new set of weights for each possible position of that ear, even though it’s the <em>same</em> ear. This leads to:</p> <ol> <li> <strong>Too many parameters:</strong> A massive number of weights and biases to learn, making training incredibly slow and prone to overfitting.</li> <li> <strong>Loss of spatial information:</strong> The network treats each pixel as an independent feature, losing the crucial information about which pixels are adjacent to each other.</li> <li> <strong>Lack of translation invariance:</strong> It struggles to recognize the same feature if it appears in a different location in the image.</li> </ol> <p>This is where CNNs step in, taking inspiration from our own biological visual cortex.</p> <h3 id="the-inspiration-our-own-eyes-and-brain">The Inspiration: Our Own Eyes and Brain</h3> <p>Think about how <em>we</em> see. When you look at an image, your brain isn’t processing every single pixel individually across your entire field of vision. Instead, your visual system has specialized cells that respond to very specific things: edges at certain orientations, corners, textures, or even more complex shapes. These detectors are localized; they only “look” at a small part of your visual field. As information moves deeper into your brain, these simple features are combined to form more complex ones, eventually leading to the recognition of entire objects.</p> <p>This hierarchical, localized, and feature-driven approach is precisely what CNNs mimic.</p> <h3 id="layer-by-layer-deconstructing-the-cnn">Layer by Layer: Deconstructing the CNN</h3> <p>A typical CNN architecture is a sequence of layers, each performing a specific transformation on the input image. Let’s break down the most important ones.</p> <h4 id="1-the-convolutional-layer-the-heart-of-the-cnn">1. The Convolutional Layer: The Heart of the CNN</h4> <p>This is where the “convolutional” part comes from, and it’s the real game-changer. Imagine you have a small “magnifying glass” or a “feature detector” that you slide across your entire image. This magnifying glass isn’t just showing you what’s there; it’s performing a specific operation.</p> <p>In CNNs, this “magnifying glass” is called a <strong>filter</strong> or <strong>kernel</strong>. It’s a small matrix of numbers (e.g., 3x3 or 5x5). When this filter slides over a section of the input image, it performs an element-wise multiplication with the corresponding pixels in that section and then sums up the results. This single sum becomes one pixel in a new output image, which we call a <strong>feature map</strong> or <strong>activation map</strong>.</p> <p>Let’s visualize it:</p> <p>Imagine a small part of your input image (grayscale values from 0-255):</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[10, 20, 30]
[40, 50, 60]
[70, 80, 90]
</code></pre></div></div> <p>And a 3x3 filter (kernel) designed to detect a vertical edge:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[-1, 0, 1]
[-1, 0, 1]
[-1, 0, 1]
</code></pre></div></div> <p>The convolution operation would be: $Output = (10 \cdot -1) + (20 \cdot 0) + (30 \cdot 1) + \ (40 \cdot -1) + (50 \cdot 0) + (60 \cdot 1) + \ (70 \cdot -1) + (80 \cdot 0) + (90 \cdot 1)$ $Output = -10 + 0 + 30 - 40 + 0 + 60 - 70 + 0 + 90 = 60$</p> <p>This calculated value, 60, becomes one pixel in our new feature map. The filter then slides (moves by a certain number of pixels, called <strong>stride</strong>) to the next section of the image, repeating the process until it has covered the entire input.</p> <p><strong>Why is this powerful?</strong></p> <ul> <li> <strong>Feature Detection:</strong> Different filters learn to detect different features. One might light up for horizontal edges, another for vertical edges, another for specific textures, and so on.</li> <li> <strong>Parameter Sharing:</strong> The <em>same</em> filter (set of weights) is applied across the entire image. This drastically reduces the number of parameters the network needs to learn, and it means the network can detect a feature no matter where it appears in the image (translation invariance).</li> <li> <strong>Local Connectivity:</strong> Each neuron in a convolutional layer is only connected to a small region of the input, mimicking the local processing in our visual cortex.</li> </ul> <p>In reality, a convolutional layer typically has <em>many</em> filters (e.g., 32, 64, 128 filters). Each filter creates its own feature map, and these feature maps are then stacked together, forming a multi-channel output for the next layer.</p> <h4 id="2-activation-functions-adding-the-spice">2. Activation Functions: Adding the “Spice”</h4> <p>After a convolution operation, the resulting values in the feature map can be positive or negative. To introduce non-linearity – which is crucial for a neural network to learn complex patterns and not just simple linear relationships – we apply an <strong>activation function</strong> to each value in the feature map.</p> <p>The most common activation function in CNNs is the <strong>Rectified Linear Unit (ReLU)</strong>. It’s elegantly simple: $f(x) = \max(0, x)$</p> <p>If the input value $x$ is positive, ReLU outputs $x$. If $x$ is negative, ReLU outputs 0. This makes the network learn faster and helps with the vanishing gradient problem in deep networks.</p> <p>So, a typical sequence is: <strong>Convolution -&gt; ReLU</strong>.</p> <h4 id="3-pooling-layers-downsizing-and-robustness">3. Pooling Layers: Downsizing and Robustness</h4> <p>After convolutional and ReLU layers, we often add a <strong>Pooling Layer</strong>. The main purpose of pooling is to reduce the spatial dimensions (width and height) of the feature maps, thereby reducing the number of parameters and computation in the network, and helping to control overfitting. It also makes the network more robust to small shifts or distortions in the input image (a form of translation invariance).</p> <p>The most common type is <strong>Max Pooling</strong>. Imagine a 2x2 window sliding across your feature map, usually with a stride of 2. For each window, Max Pooling simply takes the maximum value within that window and uses it as the single output for that region.</p> <p>Original Feature Map (e.g., 4x4):</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1, 1, 2, 4]
[5, 6, 7, 8]
[3, 2, 1, 0]
[1, 2, 3, 4]
</code></pre></div></div> <p>Applying 2x2 Max Pooling with stride 2:</p> <ul> <li>First 2x2 window: <code class="language-plaintext highlighter-rouge">[1, 1, 5, 6]</code> -&gt; Max is <code class="language-plaintext highlighter-rouge">6</code> </li> <li>Second 2x2 window: <code class="language-plaintext highlighter-rouge">[2, 4, 7, 8]</code> -&gt; Max is <code class="language-plaintext highlighter-rouge">8</code> </li> <li>Third 2x2 window: <code class="language-plaintext highlighter-rouge">[3, 2, 1, 2]</code> -&gt; Max is <code class="language-plaintext highlighter-rouge">3</code> </li> <li>Fourth 2x2 window: <code class="language-plaintext highlighter-rouge">[1, 0, 3, 4]</code> -&gt; Max is <code class="language-plaintext highlighter-rouge">4</code> </li> </ul> <p>Resulting Pooled Feature Map (2x2):</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[6, 8]
[3, 4]
</code></pre></div></div> <p>Other pooling types exist (like Average Pooling), but Max Pooling generally performs better in practice as it tends to extract the most prominent features.</p> <h3 id="assembling-the-cnn-a-hierarchical-feature-extractor">Assembling the CNN: A Hierarchical Feature Extractor</h3> <p>A typical CNN architecture is built by stacking these layers together:</p> <p><code class="language-plaintext highlighter-rouge">Input Image -&gt; CONV -&gt; ReLU -&gt; POOL -&gt; CONV -&gt; ReLU -&gt; POOL -&gt; ...</code></p> <p>As the image data passes through these layers:</p> <ul> <li> <strong>Early layers</strong> learn to detect very simple, low-level features like edges, corners, and basic textures.</li> <li> <strong>Middle layers</strong> combine these simple features to form more complex patterns, like circles, squares, or parts of objects (e.g., eyes, wheels).</li> <li> <strong>Deeper layers</strong> integrate these more complex patterns to recognize entire objects or object parts (e.g., a full face, a car body).</li> </ul> <p>This hierarchical learning process is incredibly powerful because it allows the network to automatically discover and represent complex visual patterns without explicit human programming.</p> <h3 id="the-final-stretch-fully-connected-layers-for-classification">The Final Stretch: Fully Connected Layers for Classification</h3> <p>After several blocks of <code class="language-plaintext highlighter-rouge">CONV -&gt; ReLU -&gt; POOL</code> layers, we’ve successfully extracted a rich set of high-level features from our input image. At this point, the output of the last pooling layer is typically “flattened” (transformed into a 1D vector).</p> <p>This flattened vector of features is then fed into one or more <strong>Fully Connected (FC) layers</strong>, which are just like the layers in a traditional neural network. Each neuron in an FC layer is connected to every neuron in the previous layer.</p> <p>The final FC layer usually has an activation function like <strong>Softmax</strong> (for multi-class classification). Softmax outputs a probability distribution over the possible classes. For example, if you’re classifying images of cats, dogs, and birds, the output layer might give you: <code class="language-plaintext highlighter-rouge">[0.05 (cat), 0.90 (dog), 0.05 (bird)]</code>, indicating a high probability that the image is a dog.</p> <h3 id="training-the-beast-learning-through-experience">Training the Beast: Learning Through Experience</h3> <p>So, how do these filters (kernels) actually learn to detect edges or eyes? This is where the magic of <strong>training</strong> comes in.</p> <p>Like other neural networks, CNNs are trained using a process called <strong>backpropagation</strong> and an optimization algorithm like <strong>gradient descent</strong>.</p> <ol> <li> <strong>Forward Pass:</strong> An input image is fed through the network, layer by layer, until it produces an output (e.g., predicted class probabilities).</li> <li> <strong>Loss Calculation:</strong> This predicted output is compared to the true label of the image (e.g., “cat”). A <strong>loss function</strong> calculates how “wrong” the prediction was.</li> <li> <strong>Backpropagation:</strong> The loss is then propagated backward through the network. This process calculates the <strong>gradients</strong> – essentially, how much each weight (including the values in our convolutional filters) contributed to the error.</li> <li> <strong>Weight Update:</strong> An optimizer uses these gradients to slightly adjust the weights in the direction that would reduce the loss in future predictions.</li> </ol> <p>This entire process is repeated millions of times with thousands of images. Slowly but surely, the convolutional filters “learn” to detect meaningful features, the activation functions learn to emphasize important information, and the fully connected layers learn to combine these features into accurate classifications.</p> <h3 id="the-impact-where-cnns-shine">The Impact: Where CNNs Shine</h3> <p>CNNs have truly revolutionized computer vision and beyond. Their applications are widespread and impactful:</p> <ul> <li> <strong>Image Classification:</strong> Identifying objects in photos (e.g., ImageNet Challenge, Google Photos).</li> <li> <strong>Object Detection:</strong> Locating and identifying multiple objects within an image, often with bounding boxes (e.g., self-driving cars recognizing pedestrians and other vehicles, security systems).</li> <li> <strong>Medical Imaging:</strong> Diagnosing diseases by analyzing X-rays, MRIs, and CT scans with superhuman accuracy.</li> <li> <strong>Facial Recognition:</strong> Unlocking phones, verifying identities.</li> <li> <strong>Image Segmentation:</strong> Assigning a class to <em>each pixel</em> in an image (e.g., separating foreground from background).</li> <li> <strong>Style Transfer &amp; Generative Models:</strong> Creating art or generating new images (e.g., deepfakes, DALL-E, Midjourney).</li> </ul> <p>It’s astonishing to think that these complex capabilities stem from the simple, elegant operations we discussed: convolution, activation, and pooling.</p> <h3 id="my-continuing-journey">My Continuing Journey</h3> <p>Understanding CNNs was a pivotal moment in my data science journey. It transformed my perception of what machines could achieve in the realm of vision. From what initially seemed like black box magic, it became a logical, albeit sophisticated, extension of fundamental mathematical and computational principles.</p> <p>The beauty of CNNs lies not just in their incredible performance, but in their elegant design, mirroring the hierarchical processing of our own brains. They are a testament to how combining simple, repeatable operations can lead to emergent intelligence that can truly change the world.</p> <p>This is just the beginning. The field of deep learning, and CNNs within it, is constantly evolving. But by grasping these core concepts, you’ve taken a significant step toward understanding the engines behind much of modern AI. Keep exploring, keep questioning, and maybe, just maybe, you’ll be the one to build the next groundbreaking visual intelligence.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>