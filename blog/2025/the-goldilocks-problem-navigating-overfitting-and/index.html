<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Goldilocks Problem: Navigating Overfitting and Underfitting in Machine Learning | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/the-goldilocks-problem-navigating-overfitting-and/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Goldilocks Problem: Navigating Overfitting and Underfitting in Machine Learning</h1> <p class="post-meta"> Created on June 18, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/overfitting"> <i class="fa-solid fa-hashtag fa-sm"></i> Overfitting</a>   <a href="/blog/blog/tag/underfitting"> <i class="fa-solid fa-hashtag fa-sm"></i> Underfitting</a>   <a href="/blog/blog/tag/model-evaluation"> <i class="fa-solid fa-hashtag fa-sm"></i> Model Evaluation</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>Welcome back to my journal, where we demystify the fascinating world of data science and machine learning. Today, we’re diving into a topic that’s not just fundamental but truly essential for anyone looking to build robust and reliable AI models: <strong>Overfitting vs. Underfitting</strong>. Think of it as the “Goldilocks Problem” of machine learning – finding the model that’s “just right.”</p> <p>You see, building a machine learning model isn’t just about feeding it data and pressing “go.” The real magic, and the real challenge, lies in ensuring your model learns the underlying patterns in your data <em>without</em> memorizing every single detail, noise included. It needs to be smart enough to understand the core concepts but not so rigid that it can’t adapt to new, unseen situations. This delicate balancing act is precisely what we’ll explore today.</p> <h3 id="the-ultimate-goal-generalization">The Ultimate Goal: Generalization</h3> <p>Before we jump into our two main antagonists, let’s nail down what we’re <em>trying</em> to achieve. In machine learning, our primary goal isn’t just for a model to perform well on the data it was trained on. That’s easy! The real test is its ability to perform well on <em>new, unseen data</em>. This ability is called <strong>generalization</strong>. A model that generalizes well has truly learned the essence of the problem, not just memorized the answers to the training questions.</p> <p>To understand this better, let’s use a simple analogy: imagine you’re studying for an exam.</p> <hr> <h3 id="antagonist-1-underfitting-the-lazy-student">Antagonist #1: Underfitting (The Lazy Student)</h3> <p>Let’s start with our first hurdle: <strong>underfitting</strong>.</p> <p><strong>What is it?</strong> Imagine you have a big exam coming up, but you barely open your textbook. You glance at a few chapter titles, maybe read a summary or two, but you don’t really dig into the material. When the exam comes, you’re likely to struggle with most questions because your understanding is too superficial.</p> <p>In machine learning terms, an <strong>underfit model</strong> is like that lazy student. It’s too simple, too rigid, or hasn’t been trained enough to capture the underlying patterns and relationships in the training data. It fails to learn even the basic structure of the data.</p> <p><strong>Symptoms:</strong></p> <ul> <li> <strong>Poor performance on training data:</strong> The model can’t even get the answers right for the questions it <em>has</em> seen.</li> <li> <strong>Poor performance on new, unseen data (test data):</strong> Naturally, if it can’t handle the training data, it won’t stand a chance with new data.</li> <li> <strong>High Bias:</strong> This is a technical term indicating that the model has made strong, incorrect assumptions about the data’s relationship. For example, assuming a linear relationship when the data is clearly non-linear.</li> </ul> <p><strong>A Visual Example:</strong> Consider trying to predict house prices based on their size. Let’s say the true relationship is somewhat curved (larger houses get disproportionately more expensive). An underfit model might try to fit a simple straight line ($y = \beta_0 + \beta_1 x$) through data that clearly needs a curve:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        .        .
    .          .
  .          .
.          .
--------------------  &lt;-- Underfit (straight line through curved data)
  .          .
    .      .
</code></pre></div></div> <p>The straight line just doesn’t capture the trend effectively. It misses the nuances.</p> <p><strong>Causes of Underfitting:</strong></p> <ol> <li> <strong>Too simple a model:</strong> Using a linear regression model for highly non-linear data.</li> <li> <strong>Insufficient features:</strong> Not providing enough relevant information to the model. Maybe house size isn’t enough; we also need the number of bedrooms, location, etc.</li> <li> <strong>Not enough training time/iterations:</strong> For iterative models (like neural networks), stopping training too early.</li> <li> <strong>Too much regularization:</strong> (We’ll get to regularization later, but for now, know it can make a model too simple if overdone).</li> </ol> <p><strong>How to Fix Underfitting:</strong></p> <ol> <li> <strong>Increase model complexity:</strong> Use a more powerful model (e.g., polynomial regression, decision trees, neural networks instead of linear regression).</li> <li> <strong>Add more relevant features:</strong> Feature engineering is crucial here – create or select new variables that might help the model.</li> <li> <strong>Reduce regularization:</strong> If regularization was applied, lessen its strength.</li> <li> <strong>Train for longer:</strong> Allow iterative models more time to learn.</li> </ol> <hr> <h3 id="antagonist-2-overfitting-the-memorizing-student">Antagonist #2: Overfitting (The Memorizing Student)</h3> <p>Now, for our second major problem: <strong>overfitting</strong>. This is often trickier because your model might <em>look</em> great initially!</p> <p><strong>What is it?</strong> Think back to our exam analogy. This time, you’re the student who memorizes <em>every single practice question</em> by heart, including the exact wording, the layout, and even any typos. You spend so much time memorizing specifics that you miss the underlying concepts. On the real exam, if a question is phrased even slightly differently, or if there’s a new question based on the same concept, you’re stumped. You’ve memorized, not learned.</p> <p>In machine learning, an <strong>overfit model</strong> is like that memorizing student. It has learned the training data <em>too well</em>, including the random noise and irrelevant details specific to that dataset. It’s excellent at predicting outcomes for the training data but performs poorly on new, unseen data because it can’t generalize. It’s essentially “mistaking the noise for the signal.”</p> <p><strong>Symptoms:</strong></p> <ul> <li> <strong>Excellent performance on training data:</strong> The model achieves near-perfect scores on the data it was trained on.</li> <li> <strong>Poor performance on new, unseen data (test data):</strong> This is the tell-tale sign! A huge drop in performance when introduced to fresh information.</li> <li> <strong>High Variance:</strong> This means the model is extremely sensitive to the specific training data it saw. Small changes in the training data would lead to a vastly different model.</li> </ul> <p><strong>A Visual Example:</strong> Let’s revisit our house price prediction. An overfit model might try to hit <em>every single data point</em> perfectly, even the outliers that might be due to measurement errors (noise). It might fit a highly complex, wiggly line (like a high-degree polynomial, $y = \sum_{i=0}^N \beta_i x^i$ where N is very large):</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        .  .
      .     .
    .         .
  .             .
/                 \   &lt;-- Overfit (squiggly line trying to hit every point)
\                 /
  .             .
    .         .
      .     .
        . .
</code></pre></div></div> <p>While it touches every training point, this wiggly line is clearly not the true underlying relationship. It’s just tracing the noise. If we got a new house price point that falls between these “wiggles,” the prediction would be way off!</p> <p><strong>Causes of Overfitting:</strong></p> <ol> <li> <strong>Too complex a model:</strong> Using a very powerful model (e.g., deep neural network with many layers, high-degree polynomial) on simple data.</li> <li> <strong>Too many features:</strong> Including too many irrelevant or redundant features can cause the model to pick up noise associated with them.</li> <li> <strong>Insufficient training data:</strong> If you don’t have enough examples, the model has fewer “true” patterns to learn from and might start memorizing the few examples it has.</li> <li> <strong>Training for too long:</strong> For iterative models, continuing to train after the model has already learned the optimal patterns can lead to it starting to memorize noise.</li> </ol> <p><strong>How to Fix Overfitting:</strong></p> <ol> <li> <strong>Simplify the model:</strong> Reduce complexity (e.g., fewer layers in a neural network, lower degree polynomial, prune a decision tree).</li> <li> <strong>Feature selection/engineering:</strong> Remove irrelevant features or combine them smartly.</li> <li> <strong>Get more training data:</strong> More data provides more true patterns and less relative noise, making memorization harder.</li> <li> <strong>Regularization:</strong> This is a crucial technique! Regularization methods (like L1 or L2) add a penalty term to the model’s loss function. This penalty discourages the model from assigning extremely large weights to features, thereby simplifying the model and making it less sensitive to individual data points. <ul> <li>For example, a common loss function $L(\theta)$ might be the sum of squared errors: $L(\theta) = \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2$.</li> <li>With L2 regularization (also called Ridge Regression), we add a penalty: $L_{reg}(\theta) = \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda \sum_{j=1}^n \theta_j^2$. Here, $\lambda$ controls the strength of the penalty, and $\theta_j$ are the model’s weights.</li> </ul> </li> <li> <strong>Cross-validation:</strong> A robust technique to evaluate model performance on unseen data by splitting your training data into multiple folds. This helps in tuning hyperparameters and identifying overfitting early.</li> <li> <strong>Early stopping:</strong> For iterative models, monitor the model’s performance on a separate validation set during training. Stop training when performance on the validation set starts to degrade, even if training set performance is still improving.</li> <li> <strong>Dropout (for Neural Networks):</strong> Randomly “turns off” some neurons during training, preventing individual neurons from becoming too co-dependent and forcing the network to learn more robust features.</li> </ol> <hr> <h3 id="the-sweet-spot-the-bias-variance-trade-off">The Sweet Spot: The Bias-Variance Trade-off</h3> <p>You might have noticed the terms “high bias” and “high variance” popped up. These are key components of the <strong>Bias-Variance Trade-off</strong>, one of the most fundamental concepts in machine learning.</p> <ul> <li> <strong>Bias:</strong> The error introduced by approximating a real-world problem (which may be complex) with a simplified model. High bias implies that the model makes strong assumptions about the data and fails to capture the true relationship (underfitting).</li> <li> <strong>Variance:</strong> The error introduced by the model’s sensitivity to small fluctuations in the training data. High variance implies that the model learns the training data too well, including the noise, and performs poorly on unseen data (overfitting).</li> </ul> <p>The trade-off is this: typically, as you decrease bias (make your model more complex to capture more patterns), you increase variance (make it more sensitive to specific training data). And vice-versa. Our goal is to find the optimal balance – the sweet spot – where both bias and variance are acceptably low, leading to the best generalization performance.</p> <p>Graphically, imagine plotting the error of your model against its complexity:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Error
^
|    \                                / Test Error
|     \                              /
|      \                            /
|       \        Min Error         /
|        --------------------------
|          /                 \ Training Error
|         /                   \
|        /                     \
+-----------------------------------&gt; Model Complexity
        (Underfit)             (Overfit)
</code></pre></div></div> <p>As model complexity increases:</p> <ul> <li> <strong>Training Error (blue line)</strong> typically decreases. The model gets better and better at explaining the data it has seen.</li> <li> <strong>Test Error (red line)</strong> initially decreases as the model learns useful patterns, but then it starts to <em>increase</em> as the model becomes too complex and begins to overfit to the training data’s noise.</li> </ul> <p>The “sweet spot” is where the test error is at its minimum. That’s our Goldilocks zone!</p> <hr> <h3 id="practical-tools-for-diagnosis">Practical Tools for Diagnosis</h3> <p>How do we actually <em>see</em> if our model is underfitting or overfitting?</p> <ol> <li> <strong>Train-Test Split:</strong> Always split your data into a training set (for the model to learn) and a separate test set (for evaluating how well it generalizes). Never train on your test set!</li> <li> <strong>Validation Set/Cross-Validation:</strong> For hyperparameter tuning and model selection, it’s common to use a <em>validation set</em> (a subset of the training data never seen during training) or <strong>k-fold cross-validation</strong>, which repeatedly splits the training data into different train/validation folds. This gives a more robust estimate of performance.</li> <li> <strong>Learning Curves:</strong> Plotting your model’s performance (e.g., accuracy or loss) on both the training set and a validation set as a function of training iterations or the size of the training data. <ul> <li> <strong>Underfitting:</strong> Both training and validation errors are high and plateau. The model just isn’t learning enough.</li> <li> <strong>Overfitting:</strong> Training error is low and decreasing, while validation error is high and potentially increasing (or has plateaued at a high value). There’s a significant gap between the two.</li> <li> <strong>Good Fit:</strong> Both errors are low and converge to a similar, acceptable level.</li> </ul> </li> </ol> <hr> <h3 id="conclusion-the-art-of-balance">Conclusion: The Art of Balance</h3> <p>Understanding overfitting and underfitting isn’t just theoretical knowledge; it’s a practical skill that you’ll use constantly in your machine learning journey. It’s the art of finding that perfect balance – a model that’s neither too simple nor too complex, a student who truly understands the subject matter rather than just memorizing facts.</p> <p>As you build your own models, always keep an eye on your training and validation performance. Don’t be fooled by a perfect score on your training data; the real test lies in how well your model performs on data it has never encountered before. By applying the techniques we’ve discussed – from careful feature engineering to strategic regularization and robust validation – you’ll be well on your way to building models that not only work but generalize beautifully.</p> <p>Keep experimenting, keep learning, and keep building! Until next time.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>