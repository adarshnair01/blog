<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> A/B Testing: Your Data-Powered Superpower for Smart Decisions (A Deep Dive) | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/2025/ab-testing-your-data-powered-superpower-for-smart/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">A/B Testing: Your Data-Powered Superpower for Smart Decisions (A Deep Dive)</h1> <p class="post-meta"> Created on April 23, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/a-b-testing"> <i class="fa-solid fa-hashtag fa-sm"></i> A/B Testing</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/statistics"> <i class="fa-solid fa-hashtag fa-sm"></i> Statistics</a>   <a href="/blog/blog/tag/experimentation"> <i class="fa-solid fa-hashtag fa-sm"></i> Experimentation</a>   <a href="/blog/blog/tag/product-management"> <i class="fa-solid fa-hashtag fa-sm"></i> Product Management</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey there, fellow explorers of the digital realm!</p> <p>Have you ever visited a website or used an app and thought, “Hmm, I wonder why they designed it <em>this</em> way?” Or perhaps, “What if that button were red instead of blue?” It’s easy to assume that product managers and designers just ‘know’ what works best, but the truth is, behind almost every successful digital product lies a relentless pursuit of improvement, backed by data. And one of the most powerful tools in that pursuit? You guessed it: A/B Testing.</p> <p>For a long time, I was fascinated by how companies like Netflix, Google, and Amazon seemed to always know what I wanted. As I delved deeper into data science, I realized it wasn’t a crystal ball, but rather a systematic approach to experimentation. A/B testing quickly became one of my favorite topics because it perfectly blends the scientific method with the fast-paced world of technology. It’s accessible enough for anyone to grasp, yet deep enough to challenge even seasoned statisticians. So, let’s pull back the curtain and peek into the fascinating world of A/B testing!</p> <h3 id="what-is-ab-testing-the-core-idea">What is A/B Testing? The Core Idea</h3> <p>At its heart, A/B testing is a controlled experiment. Imagine you have two versions of something – let’s call them “A” and “B”.</p> <ul> <li> <strong>Version A (Control):</strong> This is your original or current version. Think of it as the ‘status quo’.</li> <li> <strong>Version B (Variant):</strong> This is your modified version, where you’ve changed <em>one</em> specific element you want to test.</li> </ul> <p>The goal is simple: You show version A to one group of users and version B to another, equally sized and randomly selected group. Then, you measure how each group responds to its respective version. By comparing their behaviors, you can determine if your change (version B) is truly better, worse, or makes no significant difference.</p> <p>Think of it like this: You’re a chef with a popular chocolate chip cookie recipe (Version A). You wonder if adding a pinch of sea salt (Version B) would make it even better. You bake two batches – one with salt, one without. You then give them to two separate, random groups of taste testers and ask them to rate the cookies. Whichever batch gets higher ratings (your chosen metric) tells you which recipe is preferred. That’s A/B testing in a nutshell!</p> <h3 id="why-bother-the-power-of-data-driven-decisions">Why Bother? The Power of Data-Driven Decisions</h3> <p>In the past, decisions were often based on intuition, expert opinion, or the highest-paid person’s opinion (HiPPO). While these can be valuable, they carry significant risks. What if the expert is wrong? What if your intuition leads you astray?</p> <p>A/B testing removes guesswork. It allows companies to:</p> <ol> <li> <strong>Validate Hypotheses:</strong> Instead of <em>assuming</em> a change will improve a metric, you <em>prove</em> it with data.</li> <li> <strong>Minimize Risk:</strong> Before rolling out a major change to all users, you can test it on a small segment to ensure it doesn’t negatively impact your business.</li> <li> <strong>Optimize User Experience:</strong> Small, iterative improvements, accumulated over time, can lead to massive gains in user satisfaction and business metrics.</li> <li> <strong>Foster a Culture of Experimentation:</strong> It encourages teams to constantly question, test, and learn, leading to innovation.</li> </ol> <p>Consider a simple example: changing the color of a “Sign Up” button on a landing page. Intuitively, one might think a bright red button would grab more attention. But what if testing reveals a subtle green button actually leads to 10% more sign-ups? That 10% could translate into millions of dollars in revenue for a large company! This is the power of A/B testing.</p> <h3 id="the-ab-testing-playbook-a-step-by-step-guide">The A/B Testing Playbook: A Step-by-Step Guide</h3> <p>Ready to get technical? Let’s walk through the scientific method applied to product development.</p> <h4 id="step-1-define-your-goal-and-formulate-a-hypothesis">Step 1: Define Your Goal and Formulate a Hypothesis</h4> <p>Every good experiment starts with a clear question and an educated guess.</p> <ul> <li> <strong>Goal:</strong> What specific problem are you trying to solve or what metric are you trying to improve? (e.g., Increase Click-Through Rate (CTR) on the “Sign Up” button).</li> <li> <strong>Hypothesis:</strong> This is your testable prediction. It’s usually stated in two parts: <ul> <li> <strong>Null Hypothesis ($H_0$):</strong> This assumes there is <em>no significant difference</em> between your control (A) and variant (B). For our button example: $H_0$: “Changing the button color from blue to green will have no effect on the CTR.”</li> <li> <strong>Alternative Hypothesis ($H_1$):</strong> This is what you’re trying to prove – that there <em>is</em> a significant difference, or that your variant is better. $H_1$: “Changing the button color from blue to green <em>will</em> increase the CTR.”</li> </ul> </li> </ul> <h4 id="step-2-choose-your-metrics">Step 2: Choose Your Metric(s)</h4> <p>What will you measure to see if your change worked?</p> <ul> <li> <strong>Primary Metric:</strong> This is the one key metric directly tied to your hypothesis. For our button example, it would be the <strong>Click-Through Rate (CTR)</strong>. If 1000 users see the button and 100 click it, your CTR is: $CTR = \frac{\text{Number of Clicks}}{\text{Number of Impressions (Users who saw button)}} \times 100\%$</li> <li> <strong>Guardrail Metrics:</strong> These are secondary metrics you monitor to ensure your change doesn’t negatively impact other important areas. For instance, if the green button increases sign-ups but also dramatically increases user complaints (a guardrail metric), then the change might not be good overall.</li> </ul> <h4 id="step-3-determine-your-sample-size">Step 3: Determine Your Sample Size</h4> <p>This is where statistics start to shine! You can’t just run the test for an hour and declare a winner. You need enough data to be confident that any observed difference isn’t just due to random chance.</p> <p>To calculate the required sample size ($n$) for each group (Control and Variant), you need a few key pieces of information:</p> <ol> <li> <strong>Baseline Conversion Rate ($p_A$):</strong> Your current CTR for the control (e.g., 10%).</li> <li> <strong>Minimum Detectable Effect (MDE):</strong> The smallest difference you’d consider <em>practically significant</em> if it existed. For example, if you only care if the CTR increases by at least 1 percentage point (from 10% to 11%). So, $p_B - p_A = 0.01$.</li> <li> <strong>Significance Level ($\alpha$):</strong> This is the probability of making a Type I error (false positive) – rejecting the null hypothesis when it’s actually true. Commonly set at 0.05 (5%). This means you’re willing to accept a 5% chance of incorrectly concluding that your variant is better when it’s not.</li> <li> <strong>Statistical Power ($1-\beta$):</strong> This is the probability of correctly rejecting the null hypothesis when it’s false (i.e., detecting a real effect if it exists). Commonly set at 0.80 (80%). This means you want an 80% chance of finding a significant improvement if one truly exists.</li> </ol> <p>Calculating sample size can be complex, involving Z-scores for your chosen $\alpha$ and $\beta$ levels. While many online calculators exist, a common formula for proportions looks something like this (simplified for explanation, not derivation):</p> <p>$n = \frac{2 \times (Z_{1-\alpha/2} + Z_{1-\beta})^2 \times p \times (1-p)}{MDE^2}$</p> <p>Where $p$ is the average of $p_A$ and $p_B$. The $Z$ values come from standard normal distribution tables (e.g., $Z_{0.975} \approx 1.96$ for $\alpha=0.05$, $Z_{0.80} \approx 0.84$ for $\beta=0.20$).</p> <p>This calculation ensures you collect enough data to confidently detect your MDE with your desired level of certainty. Running a test with too small a sample size is like trying to hear a whisper in a hurricane – you’re unlikely to detect a real signal.</p> <h4 id="step-4-randomization-and-data-collection">Step 4: Randomization and Data Collection</h4> <p>This is perhaps the most critical step for a valid experiment.</p> <ul> <li> <strong>Random Assignment:</strong> Users must be randomly assigned to either group A or group B. This ensures that the two groups are as similar as possible in all characteristics (age, location, behavior, etc.), so any observed difference can be attributed solely to your variant, not pre-existing differences between the groups.</li> <li> <strong>Simultaneous Exposure:</strong> Both versions should run concurrently to avoid external factors (like holidays, news events, or marketing campaigns) from skewing results.</li> <li> <strong>Consistent Environment:</strong> Ensure the only difference between the groups is your tested change.</li> </ul> <p>You deploy your variant (green button) to 50% of your randomly selected users, while the other 50% continue to see the control (blue button). Your analytics systems then meticulously track clicks for both groups.</p> <h4 id="step-5-statistical-analysis">Step 5: Statistical Analysis</h4> <p>Once you’ve collected enough data (as determined by your sample size calculation), it’s time to crunch the numbers. The goal here is to determine if the observed difference between group A and group B is statistically significant, or if it could have happened purely by random chance.</p> <p>For comparing proportions (like CTR), you often use a Z-test or a Chi-squared test. The output you’re most interested in is the <strong>p-value</strong>.</p> <ul> <li> <strong>P-value:</strong> This is the probability of observing a difference <em>as extreme as, or more extreme than</em> what you actually saw, <em>assuming the null hypothesis is true</em> (i.e., assuming there’s no real difference). <ul> <li>If your p-value is less than your significance level ($\alpha$, typically 0.05), you <strong>reject the null hypothesis</strong>. This means the observed difference is statistically significant, and it’s unlikely to have occurred by chance.</li> <li>If your p-value is greater than $\alpha$, you <strong>fail to reject the null hypothesis</strong>. This means you don’t have enough evidence to conclude that there’s a significant difference. It does <em>not</em> mean there’s <em>no</em> difference, just that you couldn’t prove one.</li> </ul> </li> </ul> <p>Another important concept is the <strong>Confidence Interval (CI)</strong>. For our CTR example, if the variant’s CTR is $\hat{p}_B$, its confidence interval might be calculated as:</p> <p>$CI = \hat{p}<em>B \pm Z</em>{1-\alpha/2} \times \sqrt{\frac{\hat{p}_B(1-\hat{p}_B)}{n_B}}$</p> <p>This gives you a range (e.g., 95% CI: [10.5%, 12.5%]) within which the <em>true</em> CTR for the variant is likely to fall. If the confidence intervals for your control and variant do not overlap, it further supports the idea of a statistically significant difference.</p> <h4 id="step-6-interpretation-and-decision">Step 6: Interpretation and Decision</h4> <p>Congratulations, you’ve reached the finish line! Now, what do your results mean?</p> <ul> <li> <strong>If you reject $H_0$:</strong> The variant (green button) performed significantly better (or worse) than the control (blue button) based on your chosen metric. You now have data to support rolling out the green button to all users, or iterating further if it performed worse.</li> <li> <strong>If you fail to reject $H_0$:</strong> The variant did not show a statistically significant difference. This could mean: <ol> <li>There truly is no difference.</li> <li>There is a difference, but it’s smaller than your MDE, and your test wasn’t powered enough to detect it.</li> <li>The test itself had flaws.</li> </ol> </li> </ul> <p>Always consider both statistical significance and practical significance. A 0.0001% increase in CTR might be statistically significant with a huge sample size, but it’s likely not practically useful.</p> <h3 id="common-pitfalls-and-best-practices">Common Pitfalls and Best Practices</h3> <p>A/B testing isn’t just about running code; it’s about robust experimental design. Here are some traps to avoid:</p> <ol> <li> <strong>“P-Hacking” / Peeking:</strong> Do NOT check your results repeatedly before reaching your predetermined sample size. Every time you peek, you increase your chances of finding a false positive (Type I error). Run the experiment for the full duration / sample size, then check the results once.</li> <li> <strong>Insufficient Sample Size:</strong> As discussed, too little data leads to underpowered tests where you might miss real effects (Type II error).</li> <li> <strong>Seasonality and External Factors:</strong> Running a test during a major holiday or a viral news event can skew results. Ensure your test runs during typical operating conditions.</li> <li> <strong>Novelty Effect:</strong> Sometimes, any new change (even a bad one) can initially boost engagement simply because it’s new. Make sure your experiment runs long enough to account for this initial “novelty bounce.”</li> <li> <strong>Multiple Testing Problem:</strong> If you test many variants or many metrics simultaneously without correcting for it, your chances of a false positive skyrocket. Imagine flipping a coin 100 times; eventually, you’ll see a streak of heads purely by chance. Statistical corrections (like Bonferroni correction) are needed.</li> <li> <strong>Ignoring Practical Significance:</strong> A statistically significant result isn’t always a practically important one. A 0.01% increase in conversion might be statistically sound but not worth the development effort.</li> </ol> <h3 id="beyond-ab-abn-and-multivariate-tests">Beyond A/B: A/B/n and Multivariate Tests</h3> <p>Once you’re comfortable with A/B testing, you can explore more advanced techniques:</p> <ul> <li> <strong>A/B/n Testing:</strong> Testing more than two variants (e.g., button colors: blue, green, red).</li> <li> <strong>Multivariate Testing (MVT):</strong> Testing multiple changes on a single page simultaneously (e.g., button color, headline text, and image). MVT can quickly become complex, requiring much larger sample sizes.</li> </ul> <h3 id="conclusion-your-superpower-unlocked">Conclusion: Your Superpower Unlocked</h3> <p>A/B testing is more than just a statistical technique; it’s a mindset. It embodies the scientific method, urging us to question assumptions, formulate hypotheses, collect evidence, and draw conclusions based on data. For anyone interested in data science, product management, marketing, or indeed, making smarter decisions in any field, understanding A/B testing is a critical superpower.</p> <p>My journey into data science has shown me how powerful these tools are. It’s not about being a human crystal ball, but about being a diligent scientist, running controlled experiments, and letting the data speak for itself. So, next time you see a new feature on your favorite app, remember the hidden experiments that likely shaped its existence. And perhaps, you’ll be inspired to run your own experiments to make your corner of the digital world a little bit better, one data-driven decision at a time!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>