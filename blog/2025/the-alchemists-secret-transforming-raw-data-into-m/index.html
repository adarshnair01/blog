<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Alchemist's Secret: Transforming Raw Data into ML Gold with Feature Engineering | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/the-alchemists-secret-transforming-raw-data-into-m/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Alchemist's Secret: Transforming Raw Data into ML Gold with Feature Engineering</h1> <p class="post-meta"> Created on September 18, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/feature-engineering"> <i class="fa-solid fa-hashtag fa-sm"></i> Feature Engineering</a>   <a href="/blog/blog/tag/preprocessing"> <i class="fa-solid fa-hashtag fa-sm"></i> Preprocessing</a>   <a href="/blog/blog/tag/python"> <i class="fa-solid fa-hashtag fa-sm"></i> Python</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>Welcome back to my portfolio journal. Today, I want to pull back the curtain on something that, early in my data science journey, felt like a mystical art form, but has since become one of my favorite and most impactful aspects of building machine learning models: <strong>Feature Engineering</strong>.</p> <p>If you’ve spent any time exploring machine learning, you’ve probably heard the phrase “Garbage In, Garbage Out.” It’s a cliché for a reason. No matter how sophisticated your neural network or how finely tuned your gradient boosting machine, if the data you feed it is poor, irrelevant, or simply not in a format that helps the model understand the underlying patterns, your results will be disappointing. This is where Feature Engineering swoops in, cape flowing dramatically, to save the day.</p> <h3 id="what-even-are-features-and-why-should-we-engineer-them">What Even <em>Are</em> Features? And Why Should We Engineer Them?</h3> <p>Imagine you’re trying to predict if a student will pass an exam. What information would you look at? Their study hours, previous test scores, maybe attendance, whether they completed homework, or even how many times they asked questions in class. These individual pieces of information are your “features.” They are the independent variables that you believe influence the outcome (the dependent variable, “passed exam” in this case).</p> <p>Raw data, straight from a database or a CSV file, rarely arrives in a pristine, model-ready state. It’s like having all the individual ingredients for a gourmet meal – the vegetables, spices, meats – but they’re still in their raw form. You can’t just throw them all in a blender and expect a delicious dish. You need to chop, sauté, marinate, and combine them thoughtfully.</p> <p><strong>Feature Engineering is this process of transforming raw data into features that better represent the underlying problem to the predictive models, improving model accuracy and understanding.</strong> It’s about coaxing more information out of your existing data, or creating entirely new variables that didn’t exist before, but hold immense predictive power.</p> <p>My “aha!” moment with Feature Engineering came during a project where I was trying to predict customer churn for a telecom company. My initial models, using raw features like <code class="language-plaintext highlighter-rouge">call_duration</code>, <code class="language-plaintext highlighter-rouge">data_usage</code>, and <code class="language-plaintext highlighter-rouge">contract_type</code>, were performing okay, but nothing spectacular. I was getting frustrated, thinking I needed a more complex algorithm. Then, on a whim, I started combining and transforming these features. I calculated <code class="language-plaintext highlighter-rouge">average_call_duration_per_month</code>, <code class="language-plaintext highlighter-rouge">data_usage_to_contract_limit_ratio</code>, and even <code class="language-plaintext highlighter-rouge">days_since_last_customer_service_interaction</code>. Suddenly, my model’s performance leaped! It wasn’t the algorithm; it was how I was <em>presenting</em> the information to it.</p> <p>It taught me a crucial lesson: <strong>Feature engineering often contributes more to model performance than choosing a fancy algorithm or hyperparameter tuning.</strong></p> <p>Let’s dive into some common types of features and the powerful techniques we use to engineer them.</p> <h3 id="the-feature-engineers-toolkit-transforming-raw-data">The Feature Engineer’s Toolkit: Transforming Raw Data</h3> <p>We encounter various types of data in the wild, and each requires its own set of tools for transformation.</p> <h4 id="1-numerical-features-the-quantitative-storytellers">1. Numerical Features: The Quantitative Storytellers</h4> <p>Numerical data is often the most straightforward, but there are still plenty of ways to enhance it.</p> <ul> <li> <p><strong>Scaling and Normalization:</strong> Imagine you have two features: <code class="language-plaintext highlighter-rouge">age</code> (ranging from 18-80) and <code class="language-plaintext highlighter-rouge">income</code> (ranging from $20,000 to $200,000). Many machine learning algorithms, especially those that calculate distances (like K-Nearest Neighbors) or use gradient descent (like neural networks or linear regression), can get confused or biased by these vastly different scales. A feature with a large range might disproportionately influence the model.</p> <ul> <li> <strong>Min-Max Scaling:</strong> This squishes all your values into a fixed range, usually [0, 1]. $X_{\text{scaled}} = \frac{X - X_{\text{min}}}{X_{\text{max}} - X_{\text{min}}}$ This makes all features contribute equally to the distance calculation.</li> <li> <strong>Standardization (Z-score Normalization):</strong> This transforms your data to have a mean of 0 and a standard deviation of 1. It assumes your data is normally distributed (or close enough). $X_{\text{scaled}} = \frac{X - \mu}{\sigma}$ Where $\mu$ is the mean and $\sigma$ is the standard deviation. This is robust to outliers and works well for many algorithms.</li> </ul> </li> <li> <p><strong>Binning (Discretization):</strong> Sometimes, a precise numerical value isn’t as useful as the <em>category</em> it falls into. For instance, <code class="language-plaintext highlighter-rouge">age</code> can be binned into <code class="language-plaintext highlighter-rouge">teenager</code>, <code class="language-plaintext highlighter-rouge">young_adult</code>, <code class="language-plaintext highlighter-rouge">middle_aged</code>, <code class="language-plaintext highlighter-rouge">senior</code>. This can help capture non-linear relationships or make the model more robust to small variations and outliers.</p> </li> <li> <strong>Mathematical Transformations (Log, Square Root, Exponential):</strong> Data like <code class="language-plaintext highlighter-rouge">income</code>, <code class="language-plaintext highlighter-rouge">transaction_value</code>, or <code class="language-plaintext highlighter-rouge">population</code> often have a skewed distribution (e.g., many small values, a few very large ones). Such skewness can negatively impact models that assume normal distributions. <ul> <li> <strong>Log Transformation:</strong> $log(X)$. This is fantastic for reducing skewness and handling power-law distributions. It compresses large values and expands small values. For example, $log(100)$ is 2, $log(1000)$ is 3, $log(10000)$ is 4 (base 10). The differences between 100 and 1000 and between 1000 and 10000 are vast in raw terms, but in log terms, they are consistent.</li> <li>Square root ($ \sqrt{X} $) or reciprocal ($ \frac{1}{X} $) transformations serve similar purposes in specific scenarios.</li> </ul> </li> <li> <p><strong>Polynomial Features:</strong> Sometimes, the relationship between your feature and the target isn’t linear ($y = mx + b$). It might be curved. You can create new features by raising existing ones to a power, like $X^2$, $X^3$. For example, if predicting house prices, maybe adding $ (\text{square_footage})^2 $ captures an increasing marginal value for larger homes more effectively than just <code class="language-plaintext highlighter-rouge">square_footage</code>.</p> </li> <li> <strong>Interaction Features:</strong> The effect of one feature might depend on another. For example, a discount (<code class="language-plaintext highlighter-rouge">discount_percentage</code>) might have a much bigger impact on a high-value product (<code class="language-plaintext highlighter-rouge">product_price</code>) than a low-value one. You could create an interaction feature: $ \text{discount_amount} = \text{discount_percentage} \times \text{product_price} $. This tells the model that these two features are not independent in their impact.</li> </ul> <h4 id="2-categorical-features-giving-labels-a-voice">2. Categorical Features: Giving Labels a Voice</h4> <p>Categorical data represents categories or labels (e.g., <code class="language-plaintext highlighter-rouge">color</code>: ‘Red’, ‘Blue’, ‘Green’; <code class="language-plaintext highlighter-rouge">city</code>: ‘New York’, ‘London’, ‘Tokyo’). Algorithms don’t understand text directly, so we need to encode them numerically.</p> <ul> <li> <strong>One-Hot Encoding:</strong> This is the most common and generally safest method. For each unique category, we create a new binary (0 or 1) feature. If you have <code class="language-plaintext highlighter-rouge">color</code>: ‘Red’, ‘Blue’, ‘Green’: <ul> <li> <code class="language-plaintext highlighter-rouge">color_Red</code>: 1 if Red, 0 otherwise</li> <li> <code class="language-plaintext highlighter-rouge">color_Blue</code>: 1 if Blue, 0 otherwise</li> <li> <code class="language-plaintext highlighter-rouge">color_Green</code>: 1 if Green, 0 otherwise This avoids accidentally implying an ordinal relationship (e.g., ‘Red’ is “greater” than ‘Blue’ if you just assigned 1, 2, 3).</li> </ul> </li> <li> <p><strong>Label Encoding:</strong> Assign a unique integer to each category (e.g., ‘Red’: 0, ‘Blue’: 1, ‘Green’: 2). This is suitable for <em>ordinal</em> data where the order matters (e.g., <code class="language-plaintext highlighter-rouge">size</code>: ‘Small’, ‘Medium’, ‘Large’ could be 0, 1, 2). For nominal (unordered) data, it can mislead algorithms into thinking there’s an inherent order or magnitude. Tree-based models can sometimes handle this without issue, but it’s generally riskier for others.</p> </li> <li> <strong>Target Encoding (Mean Encoding):</strong> For categories with very high cardinality (many unique values, like <code class="language-plaintext highlighter-rouge">zip_code</code> or <code class="language-plaintext highlighter-rouge">user_id</code>), One-Hot Encoding can create thousands of new features, leading to the “curse of dimensionality.” Target encoding replaces each category with the mean of the target variable for that category. Example: Replace ‘New York’ with the average house price in New York, ‘London’ with the average house price in London. This can be very powerful but requires careful handling to avoid data leakage (using target information from the validation set).</li> </ul> <h4 id="3-date-and-time-features-unlocking-temporal-patterns">3. Date and Time Features: Unlocking Temporal Patterns</h4> <p>Date and time information is a treasure trove, but its raw format (e.g., ‘2023-10-26 14:30:00’) is useless to most models. We can extract rich features:</p> <ul> <li> <strong>Components:</strong> <ul> <li> <code class="language-plaintext highlighter-rouge">year</code>, <code class="language-plaintext highlighter-rouge">month</code>, <code class="language-plaintext highlighter-rouge">day</code>, <code class="language-plaintext highlighter-rouge">day_of_week</code>, <code class="language-plaintext highlighter-rouge">hour</code>, <code class="language-plaintext highlighter-rouge">minute</code>, <code class="language-plaintext highlighter-rouge">second</code>.</li> <li> <code class="language-plaintext highlighter-rouge">is_weekend</code>, <code class="language-plaintext highlighter-rouge">is_holiday</code>.</li> </ul> </li> <li> <strong>Time since/until:</strong> <ul> <li> <code class="language-plaintext highlighter-rouge">days_since_last_purchase</code>.</li> <li> <code class="language-plaintext highlighter-rouge">time_until_event</code>.</li> </ul> </li> <li> <strong>Cyclical Features:</strong> Many temporal features are cyclical (e.g., hour of day, month of year). If you just encode <code class="language-plaintext highlighter-rouge">month</code> as 1-12, the model sees a large jump from 12 back to 1, even though December and January are close. We can use sine and cosine transformations to capture this cyclical nature smoothly: <ul> <li>For <code class="language-plaintext highlighter-rouge">month</code> (1-12): $ \text{month_sin} = \sin\left(\frac{2\pi \cdot \text{month}}{12}\right) $ $ \text{month_cos} = \cos\left(\frac{2\pi \cdot \text{month}}{12}\right) $ This creates a continuous, circular representation.</li> </ul> </li> </ul> <h4 id="4-text-features-making-words-count-briefly">4. Text Features: Making Words Count (Briefly)</h4> <p>While a whole field in itself (Natural Language Processing or NLP), Feature Engineering for text is critical.</p> <ul> <li> <strong>Bag of Words (BoW):</strong> Counts the occurrences of each word in a document. The order of words is lost, but the frequency matters.</li> <li> <strong>TF-IDF (Term Frequency-Inverse Document Frequency):</strong> This is a sophisticated way to reflect how important a word is to a document in a corpus. It assigns higher values to words that appear frequently in a specific document but rarely across all documents.</li> <li> <strong>Word Embeddings:</strong> More advanced, these represent words as dense vectors in a continuous space, capturing semantic relationships.</li> </ul> <h3 id="the-iterative-dance-domain-expertise-and-experimentation">The Iterative Dance: Domain Expertise and Experimentation</h3> <p>Feature engineering isn’t a one-and-done step. It’s an iterative process, a dialogue between you, your data, and your model.</p> <ol> <li> <strong>Understand Your Data (and the Problem!):</strong> This is paramount. What does each column mean? What are the business implications? <strong>Domain expertise is your secret weapon.</strong> If you’re predicting house prices, knowing that “number of bathrooms” is important is basic data understanding. Knowing that “distance to the nearest highly-rated school” might be <em>even more</em> important is domain expertise that can lead to a powerful engineered feature.</li> <li> <strong>Brainstorm &amp; Create:</strong> Based on your understanding, hypothesize new features. “What if I combine X and Y? What if I look at the ratio of A to B?”</li> <li> <strong>Implement &amp; Evaluate:</strong> Add your new features and retrain your model. Does performance improve? By how much? Are the new features important (e.g., check feature importances from tree-based models)?</li> <li> <strong>Refine &amp; Iterate:</strong> If a feature helps, can you make it even better? If not, discard it. Go back to step 1.</li> </ol> <p>Don’t be afraid to try seemingly “crazy” ideas. Sometimes, the most unexpected feature can unlock significant improvements.</p> <h3 id="where-to-practice-this-art">Where to Practice this Art?</h3> <ul> <li> <strong>Pandas:</strong> Your go-to Python library for data manipulation. Creating new columns, applying functions, grouping data – it’s all there.</li> <li> <strong>Scikit-learn:</strong> Provides excellent preprocessing tools (<code class="language-plaintext highlighter-rouge">StandardScaler</code>, <code class="language-plaintext highlighter-rouge">MinMaxScaler</code>, <code class="language-plaintext highlighter-rouge">OneHotEncoder</code>, <code class="language-plaintext highlighter-rouge">PolynomialFeatures</code>, etc.) to streamline many of these transformations.</li> <li> <strong>Kaggle Competitions:</strong> A fantastic playground! Many top solutions in Kaggle competitions credit sophisticated feature engineering as their winning edge.</li> </ul> <h3 id="conclusion-youre-an-alchemist-not-just-a-coder">Conclusion: You’re an Alchemist, Not Just a Coder</h3> <p>Feature Engineering is truly where the “art” meets the “science” in data science. It’s not just about writing code; it’s about critical thinking, creativity, and a deep understanding of your data and the problem you’re trying to solve.</p> <p>By mastering these techniques, you’re not just feeding raw numbers to an algorithm; you’re <em>speaking the model’s language</em>, highlighting the most important patterns, and transforming your dataset from raw ingredients into machine learning gold.</p> <p>So, next time you’re building a model, challenge yourself: before tweaking that learning rate or adding another layer, ask, “Can I engineer a better feature?” The answer is often a resounding “Yes!”</p> <p>Happy engineering!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>