<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> BERT: Unpacking the Language Revolution that Changed NLP Forever | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/bert-unpacking-the-language-revolution-that-change/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">BERT: Unpacking the Language Revolution that Changed NLP Forever</h1> <p class="post-meta"> Created on March 22, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> NLP</a>   <a href="/blog/blog/tag/bert"> <i class="fa-solid fa-hashtag fa-sm"></i> BERT</a>   <a href="/blog/blog/tag/transformers"> <i class="fa-solid fa-hashtag fa-sm"></i> Transformers</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>As a budding data scientist, there are those “aha!” moments that fundamentally shift your understanding of a field. For me, one such moment came when I truly started to grasp BERT. Before BERT, Natural Language Processing (NLP) felt like a complex puzzle with many missing pieces. After BERT, it was as if someone had handed us a universal key.</p> <p>It’s easy to throw around acronyms like BERT, GPT, and Transformer, but what do they <em>really</em> mean? How do they work, and why were they such a monumental leap forward? That’s what I want to explore with you today. Whether you’re a seasoned ML practitioner or a curious high school student thinking about a career in AI, understanding BERT is a fantastic gateway into modern NLP.</p> <h3 id="the-problem-when-computers-dont-get-it">The Problem: When Computers Don’t “Get It”</h3> <p>Imagine trying to teach a computer to read a story and understand its nuances. This is the core challenge of NLP. For decades, we’ve been trying to bridge the gap between human language—rich, ambiguous, and context-dependent—and the rigid logic of machines.</p> <p>Early approaches relied on rule-based systems or simple statistical models. Then came the era of neural networks, bringing advancements like <strong>Word Embeddings</strong>. Models like Word2Vec and GloVe learned to represent words as dense vectors (lists of numbers) in a high-dimensional space. Words with similar meanings would have similar vectors. This was a massive step! For instance, “king” and “queen” would be close to each other, and “man” - “woman” + “queen” could even point near “king”.</p> <p>But there was a crucial limitation: these embeddings were static. The word “bank” would always have the same vector, regardless of whether you meant a “river bank” or a “financial bank.” Our intelligent machines were still missing the critical element of <em>context</em>. This is called <strong>polysemy</strong>, where a single word can have multiple meanings.</p> <p>Then came Recurrent Neural Networks (RNNs) and their more sophisticated cousins, Long Short-Term Memory networks (LSTMs). These models processed words sequentially, one after another, which helped them understand some context. However, they struggled with very long sentences and were primarily unidirectional (reading left-to-right, or sometimes right-to-left, but rarely both simultaneously with equal weight). This made capturing deep, bidirectional context incredibly difficult and inefficient.</p> <h3 id="enter-bert-the-game-changer">Enter BERT: The Game Changer</h3> <p>In 2018, Google AI introduced <strong>BERT: Bidirectional Encoder Representations from Transformers</strong>. This wasn’t just an incremental improvement; it was a paradigm shift. Let’s break down that formidable name:</p> <ul> <li> <strong>Bidirectional:</strong> This is <em>critical</em>. Unlike previous models that mainly looked left-to-right (or right-to-left), BERT processes text in both directions at once. It considers the entire context of a word—all the words surrounding it—to determine its meaning. Think of it like reading a sentence and truly understanding each word by seeing what comes before and after it, simultaneously.</li> <li> <strong>Encoder Representations:</strong> BERT’s job is to create rich, contextualized numerical representations (embeddings) for each word in a sentence. These representations are what other NLP models can then use for specific tasks.</li> <li> <strong>from Transformers:</strong> This tells us about BERT’s underlying architecture. The Transformer model, introduced in 2017, was a revolutionary neural network architecture that moved away from recurrent layers, relying instead entirely on a mechanism called “Self-Attention.”</li> </ul> <h3 id="the-magic-behind-bert-the-transformer-architecture">The Magic Behind BERT: The Transformer Architecture</h3> <p>To truly appreciate BERT, we need to understand the Transformer, its powerhouse. The original paper that introduced the Transformer was famously titled “Attention Is All You Need.” And indeed, attention is at its heart.</p> <h4 id="self-attention-understanding-context-with-a-glance">Self-Attention: Understanding Context with a Glance</h4> <p>Imagine reading the sentence: “The animal didn’t cross the street because <strong>it</strong> was too tired.” As humans, we instantly know “it” refers to “the animal.” How do we do that? Our brains “pay attention” to other words in the sentence to resolve the ambiguity of “it.”</p> <p>Self-attention does exactly this for machines. For each word in a sentence, it calculates how much “attention” that word should pay to every other word in the same sentence. This allows the model to weigh the importance of different words when determining the contextual meaning of the current word.</p> <p>Mathematically, for each word, self-attention uses three vectors: a <strong>Query</strong> ($Q$), a <strong>Key</strong> ($K$), and a <strong>Value</strong> ($V$).</p> <ul> <li>The <strong>Query</strong> vector is like asking, “What am I looking for?”</li> <li>The <strong>Key</strong> vector is like, “What do I have to offer?”</li> <li>The <strong>Value</strong> vector is the actual content or information.</li> </ul> <p>The attention score is calculated by taking the dot product of the Query with all other Key vectors, then scaling and applying a softmax function to get probabilities (attention weights). These weights are then used to create a weighted sum of the Value vectors.</p> <p>The core attention formula for a single head looks something like this: \(\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\) Where $d_k$ is the dimension of the key vectors, used for scaling.</p> <p>The Transformer also uses <strong>Multi-Head Attention</strong>, which means it performs this attention mechanism multiple times in parallel, each with different learned Query, Key, and Value projections. This allows the model to attend to different parts of the sequence and different types of relationships simultaneously, like looking at the sentence from multiple “perspectives” or focusing on different aspects of meaning.</p> <h4 id="positional-encoding-keeping-order-in-a-stateless-world">Positional Encoding: Keeping Order in a Stateless World</h4> <p>A crucial aspect of Transformers is that they process all words in a sentence <em>in parallel</em>. This is a huge advantage over RNNs for speed and handling long dependencies, but it introduces a problem: how does the model know the order of words? Without recurrence, there’s no inherent sense of sequence.</p> <p>The solution is <strong>Positional Encoding</strong>. Before feeding the word embeddings into the Transformer, a unique “positional vector” is added to each word’s embedding. These positional vectors don’t contain any learned parameters; they are generated using specific mathematical functions (sines and cosines) that provide a unique “coordinate” for each position in the sequence.</p> <p>For example, the positional encoding for position $pos$ and dimension $i$ (of the embedding vector $d_{model}$) can be calculated as: \(PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{model}})\) \(PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d_{model}})\) These unique, fixed patterns allow the model to infer the relative and absolute positions of words, even while processing them in parallel.</p> <h3 id="how-bert-learns-pre-training-tasks">How BERT Learns: Pre-training Tasks</h3> <p>The true brilliance of BERT lies in its pre-training strategy. Instead of training on a specific task like sentiment analysis, BERT is pre-trained on two ingenious, self-supervised tasks using massive amounts of raw text (like all of Wikipedia and BooksCorpus). “Self-supervised” means the data itself provides the labels, requiring no human annotation.</p> <h4 id="1-masked-language-model-mlm">1. Masked Language Model (MLM)</h4> <p>This is like a “fill-in-the-blanks” game for computers. BERT randomly masks (hides) about 15% of the words in a sentence and then tries to predict the original masked words based on the context provided by <em>all</em> the other unmasked words.</p> <p>For example, given the sentence: “The man went to the store to buy milk.” BERT might see: “The man went to the [MASK] to buy [MASK].” And it has to predict “store” and “milk.”</p> <p>This is where the “Bidirectional” aspect is crucial. To predict “[MASK]” (store), BERT uses the context “The man went to the…” <em>and</em> “…to buy [MASK]”. This forces the model to learn deep contextual relationships between words from both sides, which traditional unidirectional models couldn’t do effectively.</p> <h4 id="2-next-sentence-prediction-nsp">2. Next Sentence Prediction (NSP)</h4> <p>BERT is also trained to understand relationships <em>between</em> sentences. Given two sentences (A and B), it predicts whether sentence B is the actual next sentence that follows A, or if it’s a random sentence.</p> <p>Example:</p> <ul> <li> <strong>Input:</strong> <code class="language-plaintext highlighter-rouge">[CLS]</code> The man went to the store. <code class="language-plaintext highlighter-rouge">[SEP]</code> He bought a gallon of milk. <code class="language-plaintext highlighter-rouge">[SEP]</code> </li> <li> <p><strong>Label:</strong> IsNext</p> </li> <li> <strong>Input:</strong> <code class="language-plaintext highlighter-rouge">[CLS]</code> The man went to the store. <code class="language-plaintext highlighter-rouge">[SEP]</code> The sun is shining today. <code class="language-plaintext highlighter-rouge">[SEP]</code> </li> <li> <strong>Label:</strong> NotNext</li> </ul> <p>This task helps BERT learn to understand discourse coherence and relationships, which is vital for tasks like question answering and natural language inference.</p> <p>By pre-training on these two tasks with billions of words, BERT builds a profound understanding of language, grammar, and context, without needing any explicit labels for its initial training.</p> <h3 id="fine-tuning-bert-applying-its-knowledge">Fine-tuning BERT: Applying its Knowledge</h3> <p>After this extensive pre-training, BERT becomes a powerful general-purpose language understanding model. But how do we use it for specific tasks like spam detection, customer review sentiment analysis, or medical text summarization?</p> <p>This is where <strong>fine-tuning</strong> comes in. You take the pre-trained BERT model, which has already learned an incredible amount about language, and then add a small, task-specific output layer on top of it. You then train this entire (pre-trained BERT + new output layer) model on a much smaller dataset specific to your task.</p> <p>This process is known as <strong>transfer learning</strong>. It’s like sending a highly educated expert (pre-trained BERT) to a specialized workshop (fine-tuning) to learn a new, specific skill. The expert already has a vast foundation of knowledge, so they can quickly pick up the new skill with less data and training time than someone starting from scratch.</p> <h3 id="why-bert-was-a-game-changer">Why BERT Was a Game Changer</h3> <ol> <li> <strong>Truly Contextual Embeddings:</strong> BERT finally solved the “bank” problem. “River bank” and “financial bank” now get distinct, context-dependent embeddings. This means vastly improved understanding for ambiguous words.</li> <li> <strong>Bidirectional Power:</strong> By looking at context from both sides simultaneously, BERT grasps the deeper meaning of words and sentences.</li> <li> <strong>Transfer Learning in NLP:</strong> BERT popularized the pre-train/fine-tune paradigm in NLP. We no longer need massive, labeled datasets for <em>every</em> new task. We can leverage a powerful, pre-trained model and fine-tune it with much less data, saving immense computational resources and time.</li> <li> <strong>State-of-the-Art Performance:</strong> Upon its release, BERT achieved state-of-the-art results on 11 major NLP benchmarks, from question answering to natural language inference, completely redefining performance expectations.</li> </ol> <h3 id="my-experience-and-the-future">My Experience and the Future</h3> <p>For me, understanding BERT was a turning point in my NLP journey. It demystified how models could move beyond surface-level pattern matching to a more profound understanding of human language. Suddenly, complex tasks like summarizing documents or building intelligent chatbots felt within reach.</p> <p>BERT opened the floodgates for a new era of language models. It inspired a flurry of successors and variants like RoBERTa, ALBERT, ELECTRA, and the GPT series (though GPT models are decoder-only and excel at text generation, BERT’s encoder-only architecture is still paramount for understanding tasks). Each builds upon the Transformer and attention mechanisms, pushing the boundaries of what AI can do with language.</p> <p>If you’re interested in NLP, playing with BERT (or its successors) is an essential step. Libraries like Hugging Face’s Transformers make it incredibly easy to load pre-trained BERT models and fine-tune them for your own projects. Dive in, experiment, and you’ll quickly see the immense power of this foundational model.</p> <p>BERT isn’t just an acronym; it’s a testament to the incredible progress in AI, showing us how we can teach machines to “read between the lines” and truly understand the rich, complex tapestry of human language. And this, my friends, is just the beginning.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>