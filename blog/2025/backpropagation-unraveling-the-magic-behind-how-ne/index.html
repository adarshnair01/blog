<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Backpropagation: Unraveling the Magic Behind How Neural Networks Learn | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/backpropagation-unraveling-the-magic-behind-how-ne/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Backpropagation: Unraveling the Magic Behind How Neural Networks Learn</h1> <p class="post-meta"> Created on January 23, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/neural-networks"> <i class="fa-solid fa-hashtag fa-sm"></i> Neural Networks</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/backpropagation"> <i class="fa-solid fa-hashtag fa-sm"></i> Backpropagation</a>   <a href="/blog/blog/tag/gradient-descent"> <i class="fa-solid fa-hashtag fa-sm"></i> Gradient Descent</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey there, fellow explorers of the AI universe!</p> <p>Remember that moment when you first heard about neural networks? Machines that can “learn” and “think” like us (well, sort of)? It sounds like something straight out of science fiction! We talk about artificial neurons, layers, and making predictions, but then the inevitable question hits: <em>how do they actually learn?</em> How does a network, after making a wildly wrong guess, figure out <em>exactly</em> how to tweak its internal knobs and dials to do better next time?</p> <p>I remember scratching my head, trying to wrap my mind around it. It felt like trying to teach a baby to walk by just telling it, “walk better!” Without giving it feedback on <em>which</em> muscles to adjust, how can it improve? My “aha!” moment came when I finally understood <strong>Backpropagation</strong>. It’s not just an algorithm; it’s the fundamental secret sauce, the silent engine that allows neural networks to evolve from random guesses to highly sophisticated prediction machines.</p> <p>Today, I want to take you on a journey to truly understand Backpropagation. We’ll strip away the jargon, look at the elegant math (don’t worry, it’s mostly calculus you already know, or can quickly grasp!), and uncover why it’s such a cornerstone of modern AI.</p> <h3 id="the-networks-first-guess-the-forward-pass">The Network’s First Guess: The Forward Pass</h3> <p>Before we can correct mistakes, we first have to <em>make</em> a mistake. That’s where the <strong>forward pass</strong> comes in. Imagine our neural network as a series of interconnected processing units, or “neurons,” organized in layers:</p> <ol> <li> <strong>Input Layer:</strong> This is where our data (e.g., pixels of an image, features of a dataset) enters the network.</li> <li> <strong>Hidden Layers:</strong> These are the “thinking” layers. Each neuron in a hidden layer takes inputs from the previous layer, multiplies them by specific values (called <strong>weights</strong>, $w$), adds a <strong>bias</strong> ($b$), and then passes the result through an <strong>activation function</strong> ($\sigma$). This activation function introduces non-linearity, allowing the network to learn complex patterns. <ul> <li>The “net input” to a neuron $j$ in layer $l$ from layer $l-1$ can be written as: $ z_j^l = \sum_k w_{jk}^l a_k^{l-1} + b_j^l $</li> <li>The “activation” (output) of that neuron is then: $ a_j^l = \sigma(z_j^l) $</li> </ul> </li> <li> <strong>Output Layer:</strong> This layer gives us the network’s final prediction ($\hat{y}$).</li> </ol> <p>So, during the forward pass, information flows from left to right, input to output, layer by layer. At the very beginning, all the weights and biases are usually randomized. So, the first prediction ($\hat{y}$) will likely be way off!</p> <h3 id="quantifying-the-wrongness-the-loss-function">Quantifying the “Wrongness”: The Loss Function</h3> <p>Once our network makes a prediction $\hat{y}$, we need to compare it to the <em>actual</em> correct answer, $y$. This comparison is done using a <strong>loss function</strong> (or cost function). The loss function tells us <em>how wrong</em> our prediction was. A common loss function for regression tasks is the <strong>Mean Squared Error (MSE)</strong>:</p> <p>$ L = \frac{1}{2} \sum_{i=1}^N (y_i - \hat{y}_i)^2 $</p> <p>Here, $y_i$ is the true value, $\hat{y}_i$ is the network’s prediction, and $N$ is the number of samples. The $1/2$ is just for mathematical convenience (it simplifies the derivative later). Our ultimate goal is to <strong>minimize this loss</strong>. We want $L$ to be as close to zero as possible.</p> <h3 id="finding-the-path-downhill-gradient-descent">Finding the Path Downhill: Gradient Descent</h3> <p>Imagine you’re blindfolded on a mountain, and your goal is to find the lowest point. You can’t see the whole mountain, but you can feel the slope right where you’re standing. What do you do? You take a small step in the direction that feels steepest <em>downhill</em>. You repeat this process, slowly but surely making your way down the mountain.</p> <p>This, my friends, is the intuition behind <strong>Gradient Descent</strong>.</p> <p>In our neural network, the “mountain” is the loss function, and its “terrain” is shaped by all the weights ($W$) and biases ($b$) in the network. Our goal is to find the combination of $W$ and $b$ that puts us at the bottom of the loss mountain.</p> <p>The “slope” at any point on this mountain is given by the <strong>gradient</strong>. Mathematically, the gradient is a vector of partial derivatives, indicating the direction of the <em>steepest ascent</em>. If we want to go downhill, we move in the <em>opposite</em> direction of the gradient.</p> <p>So, for each weight $W$ and bias $b$ in our network, we want to update them using this rule:</p> <p>$ W_{new} = W_{old} - \alpha \frac{\partial L}{\partial W_{old}} $</p> <p>$ b_{new} = b_{old} - \alpha \frac{\partial L}{\partial b_{old}} $</p> <p>Here:</p> <ul> <li>$W_{new}$ and $b_{new}$ are the updated values.</li> <li>$W_{old}$ and $b_{old}$ are the current values.</li> <li>$\alpha$ (alpha) is the <strong>learning rate</strong>, a small positive number that determines the size of our step. Too large, and we might jump over the minimum; too small, and learning will be very slow.</li> <li>$ \frac{\partial L}{\partial W_{old}} $ and $ \frac{\partial L}{\partial b_{old}} $ are the <strong>partial derivatives</strong> of the loss function with respect to $W_{old}$ and $b_{old}$, respectively. These tell us how much a tiny change in $W$ or $b$ would affect the loss.</li> </ul> <p>This is where the real challenge begins: <strong>How do we calculate these partial derivatives for <em>every single weight and bias</em> in our network?</strong> Especially when a typical deep neural network can have millions of them! This is precisely the problem Backpropagation elegantly solves.</p> <h3 id="the-chain-rules-grand-tour-backpropagation-explained">The Chain Rule’s Grand Tour: Backpropagation Explained</h3> <p>Backpropagation is essentially an efficient way to calculate the gradients of the loss function with respect to every weight and bias in the network, using the <strong>chain rule</strong> from calculus.</p> <p>Let’s break down the chain rule first. If you have a function $f(g(x))$, and you want to find its derivative with respect to $x$, the chain rule states:</p> <p>$ \frac{df}{dx} = \frac{df}{dg} \cdot \frac{dg}{dx} $</p> <p>Think of it like this: if A depends on B, and B depends on C, then how much does A depend on C? You first figure out how much A changes with B, then how much B changes with C, and multiply those rates of change.</p> <p>In our neural network, the loss $L$ depends on the output $\hat{y}$, which depends on the activations of the previous layer, which depend on <em>its</em> weights and biases, and so on, all the way back to the input. It’s a long chain of dependencies!</p> <p>Backpropagation works by starting at the output layer and propagating the “error signal” backwards through the network, layer by layer, calculating the gradient for each weight and bias along the way.</p> <h4 id="step-1-calculate-the-error-at-the-output-layer">Step 1: Calculate the Error at the Output Layer</h4> <p>This is our starting point. We need to know how sensitive the total loss is to the output of our final layer.</p> <p>For a neuron $k$ in the output layer, its activation is $a_k^L = \sigma(z_k^L)$. The loss $L$ directly depends on these output activations. So, we calculate $ \frac{\partial L}{\partial a_k^L} $.</p> <p>Then, we need to know how the loss changes with the <em>net input</em> to that output neuron, $z_k^L$. This is a crucial “error term” we’ll propagate backward. Using the chain rule:</p> <p>$ \delta_k^L = \frac{\partial L}{\partial z_k^L} = \frac{\partial L}{\partial a_k^L} \cdot \frac{\partial a_k^L}{\partial z_k^L} $</p> <p>Where $\frac{\partial a_k^L}{\partial z_k^L}$ is simply the derivative of the activation function at $z_k^L$, i.e., $\sigma’(z_k^L)$.</p> <p>So, for an MSE loss and a sigmoid activation function $\sigma(x) = \frac{1}{1 + e^{-x}}$ (whose derivative is $\sigma’(x) = \sigma(x)(1 - \sigma(x))$), this becomes:</p> <p>$ \delta_k^L = (a_k^L - y_k) \cdot \sigma’(z_k^L) $</p> <p>This $\delta_k^L$ represents the “error” or “responsibility for the error” for the $k$-th neuron in the output layer.</p> <h4 id="step-2-calculate-gradients-for-output-layer-weights-and-biases">Step 2: Calculate Gradients for Output Layer Weights and Biases</h4> <p>Now that we have the error for the output layer’s net inputs ($ \delta_k^L $), we can calculate the gradients for its weights and biases.</p> <ul> <li> <p><strong>For biases ($b_k^L$):</strong> $ \frac{\partial L}{\partial b_k^L} = \frac{\partial L}{\partial z_k^L} \cdot \frac{\partial z_k^L}{\partial b_k^L} $ Since $z_k^L = \sum_j w_{kj}^L a_j^{L-1} + b_k^L$, we have $ \frac{\partial z_k^L}{\partial b_k^L} = 1 $. So, $ \frac{\partial L}{\partial b_k^L} = \delta_k^L $. (The error for a neuron is its bias gradient!)</p> </li> <li> <p><strong>For weights ($w_{kj}^L$):</strong> These weights connect the previous (hidden) layer’s activations ($a_j^{L-1}$) to the current (output) layer’s neuron $k$. $ \frac{\partial L}{\partial w_{kj}^L} = \frac{\partial L}{\partial z_k^L} \cdot \frac{\partial z_k^L}{\partial w_{kj}^L} $ Since $z_k^L = \sum_j w_{kj}^L a_j^{L-1} + b_k^L$, we have $ \frac{\partial z_k^L}{\partial w_{kj}^L} = a_j^{L-1} $. So, $ \frac{\partial L}{\partial w_{kj}^L} = \delta_k^L \cdot a_j^{L-1} $. (The error for a weight is the error of the target neuron multiplied by the activation from the source neuron.)</p> </li> </ul> <h4 id="step-3-propagate-the-error-backwards-to-the-previous-hidden-layer">Step 3: Propagate the Error Backwards to the Previous Hidden Layer</h4> <p>This is the “back” in Backpropagation! We need to calculate the error for the hidden layer $l$ (which is $L-1$ in our two-layer example). This means we need $ \delta_j^l = \frac{\partial L}{\partial z_j^l} $.</p> <p>How does the error from the output layer ($ \delta_k^L $) affect the net input of a neuron $j$ in the previous layer ($z_j^l$)? Well, neuron $j$’s activation $a_j^l$ contributed to the net inputs $z_k^L$ of <em>all</em> the neurons $k$ in the next layer, each weighted by $w_{kj}^L$.</p> <p>So, using the chain rule again:</p> <p>$ \delta_j^l = \frac{\partial L}{\partial z_j^l} = \left( \sum_k w_{kj}^{l+1} \delta_k^{l+1} \right) \cdot \sigma’(z_j^l) $</p> <p>Let’s break that down:</p> <ul> <li>$ \sum_k w_{kj}^{l+1} \delta_k^{l+1} $: This part sums up the “responsibility for error” that neuron $j$ passes on to <em>all</em> the neurons in the next layer ($l+1$), weighted by the connection strengths ($w_{kj}^{l+1}$). It’s like asking: “How much did my output affect the errors of the neurons I fed into?”</li> <li>$ \sigma’(z_j^l) $: We multiply this sum by the derivative of the activation function of neuron $j$. This accounts for how sensitive neuron $j$’s <em>own</em> output is to its net input.</li> </ul> <p>This is the truly ingenious part: We don’t need to re-calculate $ \frac{\partial L}{\partial z_k^L} $ from scratch for each layer. We simply <em>reuse</em> the error terms from the layer ahead and propagate them backward, multiplying by the weights.</p> <h4 id="step-4-calculate-gradients-for-hidden-layer-weights-and-biases">Step 4: Calculate Gradients for Hidden Layer Weights and Biases</h4> <p>Once we have $ \delta_j^l $ for the hidden layer, the process for calculating its weights and biases is identical to Step 2:</p> <ul> <li> <strong>For biases ($b_j^l$):</strong> $ \frac{\partial L}{\partial b_j^l} = \delta_j^l $</li> <li> <strong>For weights ($w_{ji}^l$):</strong> $ \frac{\partial L}{\partial w_{ji}^l} = \delta_j^l \cdot a_i^{l-1} $ (where $a_i^{l-1}$ is the activation from the layer <em>before</em> the current hidden layer, or the input layer if $l=1$).</li> </ul> <p>We repeat this process, moving backward through the network, layer by layer, until we’ve calculated all the gradients for all weights and biases.</p> <h3 id="the-full-cycle-training-a-network">The Full Cycle: Training a Network</h3> <p>To summarize the training cycle for a neural network:</p> <ol> <li> <strong>Initialize:</strong> Randomly set all weights and biases.</li> <li> <strong>Forward Pass:</strong> Feed input data through the network to get a prediction ($\hat{y}$).</li> <li> <strong>Calculate Loss:</strong> Compare $\hat{y}$ with the true label $y$ using a loss function ($L$).</li> <li> <strong>Backward Pass (Backpropagation):</strong> <ul> <li>Start at the output layer and calculate the error terms ($\delta$) and gradients for weights and biases in that layer.</li> <li>Propagate the error terms backward, layer by layer, calculating $\delta$ and gradients for each preceding layer’s weights and biases.</li> </ul> </li> <li> <strong>Update Parameters:</strong> Use Gradient Descent (or its variants) to adjust all weights and biases using the calculated gradients and the learning rate ($\alpha$).</li> <li> <strong>Repeat:</strong> Go back to step 2 with the updated weights and biases, repeating the process for many iterations (epochs) and many batches of data until the loss is minimized, and the network makes accurate predictions.</li> </ol> <h3 id="why-is-backpropagation-so-revolutionary">Why is Backpropagation So Revolutionary?</h3> <p>Before Backpropagation, training multi-layer neural networks was incredibly difficult and computationally expensive. You could try to calculate all derivatives manually, but that quickly becomes a combinatorial nightmare for anything beyond a trivial network.</p> <p>Backpropagation provided an elegant, efficient, and computationally feasible way to train deep networks. It leveraged the chain rule to <strong>reuse intermediate calculations</strong>, avoiding redundant computations. This efficiency is what allowed neural networks to scale from theoretical curiosities to the powerful AI systems we use today, from image recognition to natural language processing.</p> <p>Without Backpropagation, the AI revolution as we know it simply wouldn’t have happened. It’s the silent hero, the unsung architect behind the learning capabilities of most modern AI.</p> <h3 id="a-glimpse-beyond-challenges-and-evolution">A Glimpse Beyond: Challenges and Evolution</h3> <p>While revolutionary, Backpropagation isn’t without its challenges. Issues like <strong>vanishing gradients</strong> (where gradients become extremely small in early layers, slowing down learning) and <strong>exploding gradients</strong> (where they become too large, making training unstable) arose, particularly in very deep networks.</p> <p>These challenges led to further innovations:</p> <ul> <li>New <strong>activation functions</strong> like ReLU (Rectified Linear Unit) which help mitigate vanishing gradients.</li> <li>More sophisticated <strong>optimizers</strong> like Adam, RMSprop, and Adagrad, which are smarter versions of Gradient Descent, dynamically adjusting the learning rate.</li> <li> <strong>Batch Normalization</strong> and <strong>Residual Connections</strong> to stabilize and deepen networks even further.</li> </ul> <p>Backpropagation laid the foundation, and these subsequent innovations built a magnificent skyscraper of AI on top of it.</p> <h3 id="wrapping-up">Wrapping Up</h3> <p>So, the next time you see a machine learning model performing an amazing feat, take a moment to appreciate the unsung hero working tirelessly beneath the surface: Backpropagation. It’s not just a mathematical trick; it’s the fundamental mechanism that breathes life into neural networks, allowing them to learn, adapt, and eventually, impress us with their intelligence.</p> <p>Understanding Backpropagation isn’t just about passing a course; it’s about gaining a deeper appreciation for the ingenuity that underpins so much of the technology shaping our world. Keep exploring, keep questioning, and keep learning! There’s a whole universe of AI waiting to be understood.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>