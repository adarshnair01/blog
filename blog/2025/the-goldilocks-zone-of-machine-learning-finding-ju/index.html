<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Goldilocks Zone of Machine Learning: Finding 'Just Right' with Overfitting and Underfitting | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/the-goldilocks-zone-of-machine-learning-finding-ju/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Goldilocks Zone of Machine Learning: Finding 'Just Right' with Overfitting and Underfitting</h1> <p class="post-meta"> Created on December 03, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/overfitting"> <i class="fa-solid fa-hashtag fa-sm"></i> Overfitting</a>   <a href="/blog/blog/tag/underfitting"> <i class="fa-solid fa-hashtag fa-sm"></i> Underfitting</a>   <a href="/blog/blog/tag/model-evaluation"> <i class="fa-solid fa-hashtag fa-sm"></i> Model Evaluation</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>Welcome back to my journal – a place where I jot down my thoughts and learnings from the exciting, sometimes bewildering, world of data science and machine learning. Today, I want to talk about a concept that’s absolutely fundamental, something that tripped me up quite a bit in my early days, and frankly, still keeps me on my toes: <strong>Overfitting and Underfitting</strong>.</p> <p>Think of it like this: building a machine learning model is a bit like cooking. You want to prepare a meal that’s not just delicious for the tasters in your kitchen, but also for any new guests who might drop by. You want a recipe that’s “just right.” If your dish is too bland (underfitting) or too eccentric (overfitting), your guests won’t be happy. This “just right” spot? That’s our <strong>Goldilocks Zone</strong> in machine learning.</p> <p>The core goal of any machine learning model is <strong>generalization</strong>. We don’t just want our model to memorize the data it was trained on; we want it to learn the <em>underlying patterns</em> so it can make accurate predictions on <em>new, unseen data</em>. This is where the delicate balance between overfitting and underfitting comes into play.</p> <p>Let’s dive in!</p> <h3 id="underfitting-the-too-simple-problem">Underfitting: The “Too Simple” Problem</h3> <p>Imagine you’re trying to explain the complex orbital mechanics of planets to someone using only basic arithmetic. You might try to fit a simple linear equation to a highly non-linear, elliptical path. The result? Your predictions would be wildly off. This, my friends, is <strong>underfitting</strong>.</p> <p>In machine learning terms, an underfit model is too simple to capture the underlying structure of the data. It hasn’t learned enough from the training data, often because it lacks the complexity to do so.</p> <p><strong>What it looks like:</strong></p> <ul> <li> <strong>Poor performance on both training and test data.</strong> The model can’t even get things right on the data it’s seen, let alone new data.</li> <li> <strong>High Bias:</strong> The model makes strong, often incorrect, assumptions about the data. It’s ‘biased’ towards a simpler interpretation, ignoring complexity.</li> <li>Visually, if you plot your data points, an underfit model might draw a straight line through a dataset that clearly shows a curve. It misses all the nuanced relationships.</li> </ul> <p><strong>Why does it happen?</strong></p> <ol> <li> <strong>Model is too simple:</strong> Using a linear model for data that has complex, non-linear relationships.</li> <li> <strong>Insufficient Features:</strong> Not giving the model enough relevant information to learn from. Imagine trying to predict house prices without knowing the number of bedrooms or location!</li> <li> <strong>Too much Regularization:</strong> Regularization techniques (which we’ll discuss later) prevent overfitting, but too much can push the model into underfitting.</li> <li> <strong>Insufficient Training Time/Epochs:</strong> For iterative models like neural networks, not training long enough can mean the model hasn’t had a chance to learn the patterns.</li> </ol> <p><strong>How to fix underfitting (make your dish more flavorful!):</strong></p> <ul> <li> <strong>Increase Model Complexity:</strong> Try a more sophisticated model (e.g., polynomial regression instead of linear, or a neural network with more layers/neurons).</li> <li> <strong>Add More Features:</strong> Introduce new, relevant features that can help the model understand the data better. This might involve feature engineering.</li> <li> <strong>Decrease Regularization:</strong> If you’re using regularization, try reducing its strength.</li> </ul> <h3 id="overfitting-the-too-complex-problem">Overfitting: The “Too Complex” Problem</h3> <p>Now, let’s swing to the other extreme. Remember that example of preparing for an exam? Imagine you memorize every single practice question, every minute detail, every possible trick question, but you don’t actually understand the core concepts. When the exam comes, and the questions are slightly different, you’re lost. You’ve <strong>overfit</strong> to the practice material.</p> <p>An overfit model is excessively complex. It doesn’t just learn the underlying patterns; it learns the <em>noise</em> and <em>random fluctuations</em> in the training data too. It tries to explain every single data point perfectly, including the outliers, at the expense of generalization.</p> <p><strong>What it looks like:</strong></p> <ul> <li> <strong>Excellent performance on training data, but poor performance on test (unseen) data.</strong> This is the classic hallmark. Your model boasts near-perfect accuracy on the data it’s seen, only to fall flat when given new inputs.</li> <li> <strong>High Variance:</strong> The model is too sensitive to the training data. Small changes in the training data would lead to vastly different models. It lacks consistency.</li> <li>Visually, an overfit model might draw a wiggly line that passes through every single training data point, even those that are clearly outliers. It’s like drawing a map that accounts for every single pebble on a road, making it useless for navigating the actual road.</li> </ul> <p><strong>Why does it happen?</strong></p> <ol> <li> <strong>Model is too complex:</strong> Using a very high-degree polynomial for data that has a simpler relationship, or a deep neural network on a small dataset.</li> <li> <strong>Too many features:</strong> Including irrelevant or noisy features can cause the model to latch onto spurious correlations.</li> <li> <strong>Not enough data:</strong> If your dataset is small, the model has fewer examples to learn from, making it easier to memorize the noise instead of the general pattern.</li> <li> <strong>Too much training time/epochs:</strong> For iterative models, training for too long can lead to overfitting, as the model starts to “memorize” the training data.</li> </ol> <p><strong>How to fix overfitting (make your dish ‘just right’!):</strong></p> <ul> <li> <strong>More Data:</strong> The single best cure for overfitting. The more diverse data your model sees, the harder it is for it to memorize noise.</li> <li> <strong>Feature Selection/Engineering:</strong> Choose only the most relevant features and discard noisy or redundant ones. You might even create new, more informative features.</li> <li> <strong>Regularization:</strong> This is a crucial technique. Regularization methods (like L1 - Lasso, and L2 - Ridge) add a penalty term to the model’s loss function based on the magnitude of its coefficients. This encourages the model to use smaller weights, effectively simplifying it and preventing it from becoming too sensitive to individual data points. <ul> <li>For example, in L2 regularization (Ridge), the objective function becomes: $ J(\theta) = \text{Loss}(\theta) + \lambda \sum_{i=1}^n \theta_i^2 $ Here, $\text{Loss}(\theta)$ is your original cost function (e.g., Mean Squared Error), and $\lambda$ (lambda) is the regularization parameter, controlling the strength of the penalty. A larger $\lambda$ means more regularization, shrinking coefficients closer to zero.</li> </ul> </li> <li> <strong>Cross-Validation:</strong> Instead of just one train-test split, K-Fold Cross-Validation splits your data into K subsets. The model is trained K times, each time using a different subset as the validation set. This provides a more robust estimate of your model’s performance on unseen data.</li> <li> <strong>Early Stopping:</strong> For iterative training algorithms, monitor the model’s performance on a separate validation set. Stop training when the validation error starts to increase, even if the training error is still decreasing. This catches the model just before it starts to overfit.</li> <li> <strong>Ensemble Methods:</strong> Techniques like Bagging (e.g., Random Forests) and Boosting (e.g., Gradient Boosting Machines) combine multiple models to reduce variance and improve generalization.</li> </ul> <h3 id="the-goldilocks-zone-finding-just-right">The Goldilocks Zone: Finding “Just Right”</h3> <p>So, how do we find that perfect balance, that sweet spot where our model is complex enough to capture the underlying patterns but simple enough not to memorize the noise? This is known as the <strong>Bias-Variance Trade-off</strong>.</p> <ul> <li> <strong>Bias</strong> is the error introduced by approximating a real-world problem (which may be complicated) by a simplified model. Underfit models have high bias.</li> <li> <strong>Variance</strong> is the amount that the estimate of the target function will change if different training data was used. Overfit models have high variance.</li> </ul> <p>As you increase the complexity of your model, its bias generally decreases (it can fit the training data better), but its variance tends to increase (it becomes more sensitive to the specific training data). Our goal is to find the model complexity where both bias and variance are minimized, leading to the lowest total error on unseen data.</p> <p><strong>My Approach to Finding “Just Right”:</strong></p> <ol> <li> <strong>Start Simple:</strong> I usually begin with a relatively simple model and establish a baseline performance.</li> <li> <strong>Iterate and Evaluate:</strong> I then gradually increase model complexity or add features, constantly monitoring performance on a <strong>separate validation set</strong>. This is critical! If your model performs well on training data but poorly on validation data, you’re likely overfitting. If it performs poorly on both, you’re underfitting.</li> <li> <strong>Learning Curves:</strong> These are invaluable diagnostic tools. A learning curve plots the model’s performance (e.g., error) on both the training and validation sets as a function of the training set size or model complexity (e.g., number of iterations/epochs). <ul> <li> <strong>Underfitting learning curve:</strong> Both training and validation error are high and converge to a similar high value. Adding more data won’t help much here.</li> <li> <strong>Overfitting learning curve:</strong> Training error is low, but validation error is significantly higher, and there’s a large gap between them. As you add more data, this gap usually shrinks.</li> <li> <strong>Just Right learning curve:</strong> Both training and validation errors are relatively low and close to each other.</li> </ul> </li> <li> <strong>Hyperparameter Tuning:</strong> Many models have hyperparameters (e.g., the $\lambda$ in regularization, the depth of a decision tree, the number of layers in a neural network) that control their complexity. Tuning these carefully using techniques like Grid Search or Random Search with cross-validation helps zero in on the optimal configuration.</li> </ol> <h3 id="a-personal-anecdote-the-case-of-the-overzealous-weather-predictor">A Personal Anecdote: The Case of the Overzealous Weather Predictor</h3> <p>I remember working on a project to predict local weather patterns using historical data. Initially, I threw almost every available feature into a complex model – temperature, humidity, wind speed, pressure, dew point, cloud cover, and many other variables. The model was phenomenal on the historical data, boasting an accuracy of 99.8%. I was ecstatic!</p> <p>Then came the real test. I fed it tomorrow’s weather data. The predictions were terrible, worse than simply guessing. The model had overfit spectacularly. It had learned to associate specific combinations of historical values with specific outcomes, rather than understanding the underlying meteorological principles. It was like memorizing every cloud formation from last year, instead of grasping how air pressure and fronts actually work.</p> <p>My solution involved a lot of feature engineering (discarding irrelevant features, creating new ones like ‘temperature change in last 24 hours’), rigorous cross-validation, and the application of L2 regularization. Slowly, painstakingly, the validation accuracy started to climb, and the gap between training and validation performance narrowed. It was a powerful lesson in humility and the importance of the Goldilocks Zone!</p> <h3 id="conclusion-the-journey-to-generalization">Conclusion: The Journey to Generalization</h3> <p>Understanding and addressing overfitting and underfitting is not just a theoretical exercise; it’s a fundamental skill for any aspiring data scientist or MLE. It’s an iterative process, a continuous balancing act, and often involves a fair bit of experimentation. There’s no single magic bullet, but rather a toolkit of strategies to help you navigate this terrain.</p> <p>The next time you build a model, don’t just celebrate its performance on your training data. Ask yourself: “Is my model truly learning, or is it just memorizing? Is it too simple, or too complex? Am I in the Goldilocks Zone?”</p> <p>Keep learning, keep experimenting, and happy modeling!</p> <hr> <p><em>Stay curious,</em> <em>[Your Name/Portfolio Name]</em></p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>