<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Learning Like We Do: My Deep Dive into Reinforcement Learning's Magic | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/learning-like-we-do-my-deep-dive-into-reinforcemen/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Learning Like We Do: My Deep Dive into Reinforcement Learning's Magic</h1> <p class="post-meta"> Created on July 05, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/reinforcement-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Reinforcement Learning</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>As a budding data scientist and machine learning enthusiast, I’m constantly drawn to areas of AI that feel genuinely “intelligent.” While supervised learning has given us incredible image classifiers and language models, and unsupervised learning helps us find hidden patterns, there’s a certain magic to systems that learn <em>how to act</em> in complex environments. This fascination led me down a rabbit hole into the world of <strong>Reinforcement Learning (RL)</strong>, and frankly, it’s one of the most exciting paradigms I’ve encountered.</p> <p>Join me on a journey to explore what Reinforcement Learning is, how it works, and why it’s at the forefront of creating truly adaptive and intelligent agents. We’ll strip away some of the intimidating jargon and look at the core ideas that power everything from self-driving cars to AI that can beat grandmasters at Go.</p> <h3 id="the-core-idea-learning-by-doing">The Core Idea: Learning by Doing</h3> <p>Think about how a baby learns to walk. They don’t have a dataset of perfectly labeled “walking” and “falling” examples. Instead, they try, they stumble, they fall (negative feedback!), they push themselves up, and they take another step (positive feedback!). Over time, through countless interactions with their environment, they figure out the optimal sequence of muscle movements to achieve their goal: walking.</p> <p>This “learning by doing,” “trial and error” approach is the heart of Reinforcement Learning. Instead of being explicitly programmed or shown examples, an RL agent learns optimal behavior by interacting with its environment, receiving feedback in the form of rewards or penalties. Its ultimate goal is to maximize the total cumulative reward it receives over time.</p> <p>This contrasts sharply with:</p> <ul> <li> <strong>Supervised Learning:</strong> Where we provide an agent with input data <em>and</em> the correct output labels.</li> <li> <strong>Unsupervised Learning:</strong> Where we provide input data and let the agent find hidden structures or patterns <em>without</em> any labels.</li> </ul> <p>RL is unique because the agent has to <em>discover</em> the optimal actions on its own, through active experimentation.</p> <h3 id="the-agent-environment-interaction-our-rl-sandbox">The Agent-Environment Interaction: Our RL Sandbox</h3> <p>To understand RL, we need to meet its main characters and the fundamental loop that drives learning.</p> <h4 id="the-cast">The Cast:</h4> <ol> <li> <strong>The Agent:</strong> This is our learner, the decision-maker. It’s the entity that performs actions in the environment.</li> <li> <strong>The Environment:</strong> This is everything outside the agent. It’s the world the agent interacts with, reacting to the agent’s actions and presenting new situations.</li> </ol> <h4 id="the-interaction-loop">The Interaction Loop:</h4> <p>At each discrete time step $t$:</p> <ul> <li>The agent observes the current <strong>state</strong> ($S_t$) of the environment.</li> <li>Based on $S_t$, the agent selects and performs an <strong>action</strong> ($A_t$).</li> <li>The environment transitions to a new state ($S_{t+1}$) and provides a <strong>reward</strong> ($R_{t+1}$) to the agent.</li> <li>This loop continues until a terminal state is reached (e.g., game over), or indefinitely for continuous tasks.</li> </ul> <p>Let’s break down these elements with an analogy: imagine training a robot to navigate a maze.</p> <ul> <li> <strong>State ($S_t$):</strong> The robot’s current position in the maze (e.g., “row 3, column 5”).</li> <li> <strong>Action ($A_t$):</strong> The robot’s movement choice (e.g., “move North,” “move East”).</li> <li> <strong>Reward ($R_{t+1}$):</strong> <ul> <li>Positive: +10 for reaching the exit.</li> <li>Negative: -1 for hitting a wall, -10 for falling into a pit.</li> <li>Small negative: -0.1 for each step taken (encourages finding the shortest path).</li> </ul> </li> <li> <strong>Policy ($\pi$):</strong> This is the agent’s strategy, a mapping from states to actions. Essentially, it tells the agent <em>what to do in any given situation</em>. Our robot’s policy might be: “If I’m at (3,5), go North. If I’m at (3,6), go East.” The goal of RL is to find an <em>optimal policy</em> ($\pi^*$) that maximizes the total expected reward.</li> </ul> <h3 id="the-goal-maximizing-cumulative-reward">The Goal: Maximizing Cumulative Reward</h3> <p>The agent isn’t just interested in immediate rewards. It wants to maximize the <em>total</em> reward it receives over the long run. However, future rewards are often less certain or less impactful than immediate ones. To account for this, we introduce a <strong>discount factor</strong> ($\gamma$), where $0 \le \gamma \le 1$.</p> <p>The <strong>return</strong> ($G_t$) from time step $t$ is the total discounted future reward: $G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$</p> <p>A $\gamma$ closer to 0 makes the agent “myopic,” focusing on immediate rewards. A $\gamma$ closer to 1 makes the agent “farsighted,” valuing long-term rewards almost as much as immediate ones.</p> <h3 id="the-markov-decision-process-mdp-formalizing-the-environment">The Markov Decision Process (MDP): Formalizing the Environment</h3> <p>Most RL problems are formalized as <strong>Markov Decision Processes (MDPs)</strong>. An MDP provides a mathematical framework for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision-maker.</p> <p>An MDP is defined by:</p> <ul> <li>A set of <strong>states</strong> ($\mathcal{S}$).</li> <li>A set of <strong>actions</strong> ($\mathcal{A}$).</li> <li> <table> <tbody> <tr> <td> <strong>Transition Probabilities</strong> ($P(s’</td> <td>s,a)$): The probability of moving to state $s’$ from state $s$ after taking action $a$.</td> </tr> </tbody> </table> </li> <li> <strong>Reward Function</strong> ($R(s,a,s’)$): The expected reward received when transitioning from state $s$ to state $s’$ after taking action $a$.</li> <li> <strong>Discount Factor</strong> ($\gamma$).</li> </ul> <p>The crucial concept here is the <strong>Markov Property</strong>: “The future is independent of the past given the present.” This means that the current state $S_t$ contains all the information needed to determine the probabilities of future states and rewards, regardless of how the agent arrived at $S_t$. This simplifies things immensely, allowing us to focus only on the current state.</p> <h3 id="value-functions-estimating-goodness">Value Functions: Estimating “Goodness”</h3> <p>How does an agent know which state is “good” or which action is “best”? It uses <strong>value functions</strong> to estimate the expected future reward.</p> <ol> <li> <p><strong>State-Value Function ($V^\pi(s)$):</strong> This tells us how good it is for the agent to be in a particular state $s$ <em>if it follows policy $\pi$ from then on</em>. $V^\pi(s) = E_\pi [G_t | S_t = s]$ This is the expected return starting from state $s$ and following policy $\pi$.</p> </li> <li> <p><strong>Action-Value Function ($Q^\pi(s,a)$):</strong> This tells us how good it is for the agent to take a particular action $a$ in a particular state $s$ <em>and then follow policy $\pi$ from then on</em>. This is often more useful because it directly helps the agent choose actions. $Q^\pi(s,a) = E_\pi [G_t | S_t = s, A_t = a]$ This is the expected return starting from state $s$, taking action $a$, and then following policy $\pi$.</p> </li> </ol> <p>The goal of RL is to find the <em>optimal policy</em> ($\pi^<em>$), which means finding the optimal value functions: $V^</em>(s)$ and $Q^*(s,a)$. These optimal functions satisfy the <strong>Bellman Optimality Equations</strong>:</p> <table> <tbody> <tr> <td>$V^*(s) = \max_a \sum_{s’, r} P(s’, r</td> <td>s,a) [r + \gamma V^*(s’)]$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$Q^*(s,a) = \sum_{s’, r} P(s’, r</td> <td>s,a) [r + \gamma \max_{a’} Q^*(s’,a’)]$</td> </tr> </tbody> </table> <p>In simple terms, these equations state that the optimal value of a state (or state-action pair) is the expected immediate reward plus the discounted optimal value of the <em>next</em> state, assuming the agent always chooses the best possible action. This recursive relationship is fundamental to solving RL problems.</p> <h3 id="the-exploration-exploitation-dilemma">The Exploration-Exploitation Dilemma</h3> <p>One of the central challenges in RL is balancing <strong>exploration</strong> and <strong>exploitation</strong>.</p> <ul> <li> <strong>Exploration:</strong> Trying out new actions to discover potentially better rewards. (Trying a new restaurant you’ve never been to).</li> <li> <strong>Exploitation:</strong> Taking the action that is currently known to yield the highest reward. (Going to your favorite restaurant because you know it’s good).</li> </ul> <p>If our robot only exploits, it might get stuck in a suboptimal path in the maze because it never tried the path that leads to the quickest exit. If it only explores, it might wander aimlessly and never finish the maze efficiently. A good RL agent needs a strategy to do both effectively. Common strategies include $\epsilon$-greedy (explore randomly with probability $\epsilon$, exploit otherwise) or using more sophisticated methods like Upper Confidence Bound (UCB).</p> <h3 id="how-agents-learn-a-glimpse-into-algorithms">How Agents Learn: A Glimpse into Algorithms</h3> <p>Now that we understand the core components, how do we actually <em>train</em> an agent? There are several classes of algorithms.</p> <h4 id="1-value-based-methods-learning-how-good-statesactions-are">1. Value-Based Methods: Learning “How Good” States/Actions Are</h4> <p>These algorithms focus on estimating the optimal value functions ($Q^<em>(s,a)$ or $V^</em>(s)$). Once we have $Q^*(s,a)$, finding the optimal policy is trivial: just pick the action with the highest $Q$-value in any given state.</p> <ul> <li> <p><strong>Q-Learning:</strong> This is one of the most popular and foundational model-free (meaning it doesn’t need to know the transition probabilities or reward function of the environment beforehand) RL algorithms. It’s an <strong>off-policy</strong> algorithm, meaning it can learn the optimal policy while following a different (e.g., exploratory) policy.</p> <p>The Q-Learning update rule for $Q(s,a)$ (our estimate of $Q^*(s,a)$) after taking action $A_t$ in state $S_t$, observing reward $R_{t+1}$ and new state $S_{t+1}$ is:</p> <p>$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma \max_{a’} Q(S_{t+1}, a’) - Q(S_t, A_t)]$</p> <p>Let’s break this down:</p> <ul> <li>$Q(S_t, A_t)$: Our current estimate of the value of taking action $A_t$ in state $S_t$.</li> <li>$\alpha$: The <strong>learning rate</strong> ($0 &lt; \alpha \le 1$). How much we update our estimate based on the new experience.</li> <li>$R_{t+1}$: The actual reward received.</li> <li>$\gamma \max_{a’} Q(S_{t+1}, a’)$: The estimated optimal future reward from the next state $S_{t+1}$. We assume the agent will take the best possible action $a’$ in $S_{t+1}$ according to its <em>current</em> best $Q$-values.</li> <li>$[R_{t+1} + \gamma \max_{a’} Q(S_{t+1}, a’) - Q(S_t, A_t)]$: This is the <strong>temporal difference (TD) error</strong>. It represents the difference between our new, more informed estimate of the Q-value (the “target”) and our old estimate. We use this error to nudge our $Q(S_t, A_t)$ value in the right direction.</li> </ul> </li> <li> <p><strong>SARSA (State-Action-Reward-State-Action):</strong> Similar to Q-Learning but is an <strong>on-policy</strong> algorithm. This means it learns the Q-value of the policy <em>it is currently following</em>, not necessarily the optimal policy independent of its exploration strategy. Its update rule uses the Q-value of the <em>next chosen action</em> $A_{t+1}$, not the maximum possible Q-value:</p> <p>$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]$</p> </li> </ul> <h4 id="2-policy-based-methods-directly-learning-the-strategy">2. Policy-Based Methods: Directly Learning the Strategy</h4> <table> <tbody> <tr> <td>Instead of learning value functions and deriving a policy from them, these methods directly learn a parameterized policy $\pi(a</td> <td>s; \theta)$, where $\theta$ are the parameters of the policy. This is particularly useful for environments with continuous action spaces (e.g., steering angle of a car), where enumerating Q-values for all possible actions is impractical. Algorithms like REINFORCE and Actor-Critic fall into this category.</td> </tr> </tbody> </table> <h4 id="3-deep-reinforcement-learning-drl-the-game-changer">3. Deep Reinforcement Learning (DRL): The Game Changer</h4> <p>The problem with traditional Q-Learning or SARSA is that they rely on tables to store $Q(s,a)$ values. This works for small, discrete state spaces (like our simple maze). But what about complex environments like a game of Go (where the number of states is astronomical) or a robot controlling its joints (continuous state and action spaces)? The lookup table becomes impossibly large.</p> <p>This is where <strong>Deep Reinforcement Learning (DRL)</strong> comes in. DRL combines the power of deep neural networks (DNNs) with RL algorithms. Instead of a table, a neural network is used to approximate the Q-value function (or the policy directly).</p> <ul> <li> <strong>Deep Q-Network (DQN):</strong> A landmark DRL algorithm introduced by DeepMind in 2013 that learned to play Atari games better than humans using only raw pixel data. DQN uses a convolutional neural network to take raw pixel data as input and output Q-values for each possible action. To stabilize training (which can be notoriously difficult with DNNs and RL), DQN uses two key innovations: <ol> <li> <strong>Experience Replay:</strong> The agent stores its experiences $(S_t, A_t, R_{t+1}, S_{t+1})$ in a replay buffer. During training, it samples random batches from this buffer, breaking the correlation between consecutive experiences.</li> <li> <strong>Target Network:</strong> It uses a separate “target” Q-network, which is a delayed copy of the main Q-network, to compute the target values ($R_{t+1} + \gamma \max_{a’} Q_{target}(S_{t+1}, a’)$). This helps stabilize the training targets.</li> </ol> </li> </ul> <p>DQN proved that neural networks could learn complex control policies directly from high-dimensional sensor data, paving the way for many subsequent DRL breakthroughs.</p> <h3 id="applications-and-the-future-of-rl">Applications and the Future of RL</h3> <p>The impact of Reinforcement Learning is already profound and rapidly expanding:</p> <ul> <li> <strong>Gaming:</strong> AlphaGo’s victory over the world Go champion, OpenAI Five beating top Dota 2 players, and DRL agents mastering classic Atari games.</li> <li> <strong>Robotics:</strong> Learning complex manipulation tasks, locomotion, and grasping objects in unstructured environments.</li> <li> <strong>Autonomous Driving:</strong> Training self-driving cars to navigate traffic, make safe decisions, and adapt to unforeseen circumstances.</li> <li> <strong>Resource Management:</strong> Optimizing energy consumption in data centers (Google uses RL to cool their data centers, saving millions).</li> <li> <strong>Finance:</strong> Algorithmic trading, portfolio optimization.</li> <li> <strong>Healthcare:</strong> Drug discovery, personalized treatment recommendations.</li> </ul> <p>Despite these successes, RL still faces challenges:</p> <ul> <li> <strong>Sample Efficiency:</strong> DRL often requires an enormous amount of data (experiences) to learn, which can be expensive or impractical in the real world.</li> <li> <strong>Safety and Robustness:</strong> Ensuring RL agents behave predictably and safely, especially in critical applications.</li> <li> <strong>Transfer Learning:</strong> How can an agent learn a skill in one environment and transfer it to a similar but different one without starting from scratch?</li> </ul> <p>The field is constantly evolving, with new algorithms and techniques emerging to address these issues. I believe the future holds even more sophisticated agents capable of learning from minimal interaction, collaborating with humans, and solving problems that are currently beyond our reach.</p> <h3 id="my-continuing-journey">My Continuing Journey</h3> <p>Diving into Reinforcement Learning has been an exhilarating experience. It bridges the gap between theoretical AI concepts and practical, problem-solving applications in a way few other fields do. From the elegant simplicity of the agent-environment loop to the complexity of deep Q-networks, every step feels like uncovering a new layer of intelligence.</p> <p>If you’re fascinated by machines that learn to act intelligently in dynamic environments, I highly encourage you to explore Reinforcement Learning further. Pick up a good textbook (Sutton &amp; Barto’s “Reinforcement Learning: An Introduction” is the Bible), try implementing a simple Q-Learning agent for a classic problem like the Frozen Lake environment, and watch as your agent slowly but surely figures out how to navigate its world. It’s a truly rewarding experience, both literally and figuratively!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>