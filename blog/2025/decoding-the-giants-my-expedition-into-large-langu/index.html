<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Decoding the Giants: My Expedition into Large Language Models | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/decoding-the-giants-my-expedition-into-large-langu/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Decoding the Giants: My Expedition into Large Language Models</h1> <p class="post-meta"> Created on July 30, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> NLP</a>   <a href="/blog/blog/tag/large-language-models"> <i class="fa-solid fa-hashtag fa-sm"></i> Large Language Models</a>   <a href="/blog/blog/tag/transformers"> <i class="fa-solid fa-hashtag fa-sm"></i> Transformers</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>The first time I really <em>felt</em> the power of a Large Language Model (LLM) wasn’t just when it summarized a dense research paper for me, or whipped up a Python script in seconds. It was when I asked it to write a short story in the style of a specific author, and it delivered something eerily close to the real thing, complete with nuanced tone and characteristic phrasing. That moment transformed my understanding from “cool tech” to “mind-bending frontier.”</p> <p>This isn’t just about chatbots anymore. We’re talking about a class of artificial intelligence that can process, understand, and generate human language with astonishing fluency. But what exactly <em>are</em> these digital titans, and how do they work their magic? Let’s embark on an expedition to decode the giants.</p> <h3 id="from-rules-to-deep-learning-a-brief-history-of-language-understanding">From Rules to Deep Learning: A Brief History of Language Understanding</h3> <p>Before we dive into the “how” of LLMs, it’s worth a quick look back at how we <em>used</em> to teach computers language. For decades, Natural Language Processing (NLP) relied on a mix of handcrafted rules, dictionaries, and statistical models. Think of it like a meticulous librarian trying to categorize every book by hand – effective for simple tasks, but limited, brittle, and unable to grasp context or nuance.</p> <p>Then came the age of neural networks. We started feeding computers vast amounts of text, and they learned patterns. Word embeddings, like Word2Vec, allowed words to be represented as numerical vectors, where words with similar meanings were closer in vector space. This was a game-changer! Suddenly, “king” - “man” + “woman” could get you “queen.” RNNs (Recurrent Neural Networks) and LSTMs (Long Short-Term Memory networks) followed, capable of processing sequences of words, remembering information over short spans. However, they struggled with very long sentences or documents because information would “fade” over time, and they processed words sequentially, which was slow.</p> <p>The problem? Language isn’t always linear. To truly understand a sentence like “The cat, which had chased the mouse all morning, finally caught it,” you need to connect “cat” to “it” and “chased.” Traditional models found this challenging. Enter the <strong>Transformer</strong>.</p> <h3 id="the-transformer-the-engine-of-modern-llms">The Transformer: The Engine of Modern LLMs</h3> <p>In 2017, a groundbreaking paper titled “Attention Is All You Need” introduced the Transformer architecture. This was the paradigm shift. The Transformer completely ditched the sequential processing of RNNs and LSTMs in favor of a mechanism called <strong>self-attention</strong>.</p> <p>Imagine you’re reading a sentence. Instead of reading word by word and trying to remember everything from the beginning, what if you could instantaneously glance at <em>all</em> the words and decide which ones are most relevant to the word you’re currently focusing on? That’s the core idea behind self-attention.</p> <h4 id="the-magic-of-self-attention-q-k-v">The Magic of Self-Attention (Q, K, V)</h4> <p>At its heart, self-attention allows each word in a sequence to weigh the importance of every other word. To achieve this, the Transformer assigns three vectors to each word:</p> <ol> <li> <strong>Query (Q)</strong>: Think of this as “What am I looking for?” or “What information do I need from other words?”</li> <li> <strong>Key (K)</strong>: This is “Do I have what you’re looking for?” or “What information do I offer?”</li> <li> <strong>Value (V)</strong>: If a Query matches a Key, this is “Here’s the information I’m offering.”</li> </ol> <p>The process works like this: For each word, we calculate how “relevant” it is to every other word by taking the dot product of its Query vector with the Key vector of every other word (including itself). This gives us attention scores. These scores are then scaled and passed through a softmax function to turn them into probabilities, ensuring they sum to 1. Finally, these probabilities are multiplied by the Value vectors and summed up, creating a new representation for the word that has “paid attention” to the most relevant parts of the entire input.</p> <p>Mathematically, the scaled dot-product attention can be expressed as:</p> <p>$ \text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V $</p> <p>Where $Q$ is the matrix of queries, $K$ is the matrix of keys, $V$ is the matrix of values, and $d_k$ is the dimension of the key vectors (used for scaling to prevent tiny gradients when $d_k$ is large). This simple yet powerful formula allows the model to capture long-range dependencies efficiently.</p> <h4 id="multi-head-attention-multiple-perspectives">Multi-Head Attention: Multiple Perspectives</h4> <p>Why just one “attention”? The Transformer uses <strong>Multi-Head Attention</strong>, which is like having several parallel self-attention mechanisms, each learning to focus on different aspects of the input. One “head” might focus on grammatical relationships, another on semantic connections. The outputs from these multiple “heads” are then concatenated and linearly transformed, giving a richer, more comprehensive understanding of the input.</p> <h4 id="positional-encoding-preserving-order">Positional Encoding: Preserving Order</h4> <p>Since Transformers process all words simultaneously, they lose the inherent order of words. “Dog bites man” and “Man bites dog” would look the same without something to denote position. This is solved by <strong>Positional Encoding</strong>. We inject information about the absolute or relative position of words into their input embeddings using specific mathematical functions (often sine and cosine waves). This allows the model to know <em>where</em> a word is in the sequence, even while processing it in parallel.</p> <h3 id="the-large-in-llms-scale-beyond-imagination">The “Large” in LLMs: Scale Beyond Imagination</h3> <p>The “Large” in Large Language Models refers to three critical factors:</p> <ol> <li> <strong>Vast Datasets</strong>: LLMs are trained on truly colossal datasets – often trillions of words scraped from the internet, including books, articles, websites, and more. This sheer volume of text allows them to learn an incredibly wide range of language patterns, facts, and reasoning abilities.</li> <li> <strong>Billions of Parameters</strong>: A parameter is essentially a value that the model learns during training. While early neural networks had thousands or millions of parameters, LLMs boast <em>billions</em>, even <em>trillions</em>. For example, GPT-3 has 175 billion parameters. This immense complexity allows them to capture incredibly subtle nuances in language.</li> <li> <strong>Immense Compute Power</strong>: Training these models requires mind-boggling computational resources, often involving thousands of GPUs running for weeks or months.</li> </ol> <p>This scale isn’t just about making things “bigger”; it leads to <strong>emergent abilities</strong>. These are capabilities that weren’t explicitly programmed or obvious in smaller models but appear spontaneously once a certain scale is reached. Things like few-shot learning (performing a new task with just a few examples), complex reasoning, or even creative writing.</p> <h3 id="the-training-journey-pre-training-to-alignment">The Training Journey: Pre-training to Alignment</h3> <p>LLMs undergo a multi-stage training process:</p> <ol> <li> <p><strong>Pre-training</strong>: This is the heavy lifting. The model is fed vast amounts of text and trained on a simple, self-supervised task: <strong>predicting the next word</strong>. If the model sees “The cat sat on the…”, it learns to predict “mat” (or “rug,” “couch,” etc.). By repeatedly predicting the next word across trillions of examples, the model develops a deep statistical understanding of language, grammar, facts, and even some forms of common-sense reasoning.</p> <p>This is often done with a “decoder-only” Transformer, which means it only focuses on generating text auto-regressively, one token at a time, based on the previous tokens.</p> </li> <li> <p><strong>Fine-tuning &amp; Alignment</strong>: A raw pre-trained LLM might be good at predicting the next word, but it might not be helpful, truthful, or harmless. This is where fine-tuning comes in.</p> <ul> <li> <strong>Instruction Tuning</strong>: Models are fine-tuned on datasets of instructions and desired responses (e.g., “Summarize this article,” “Write a poem about X”). This teaches the model to follow instructions.</li> <li> <strong>Reinforcement Learning from Human Feedback (RLHF)</strong>: This crucial step is what makes models like ChatGPT so good. Human evaluators rank model responses for helpfulness, accuracy, and safety. This human feedback is then used to train a reward model, which in turn guides the LLM to generate better, more aligned responses. It’s like having a helpful, ethical guide teaching the LLM good manners and responsible behavior.</li> </ul> </li> </ol> <h3 id="what-can-llms-do-and-what-cant-they">What Can LLMs Do (and What Can’t They)?</h3> <p>The capabilities of LLMs are truly astounding:</p> <ul> <li> <strong>Text Generation</strong>: Writing articles, stories, poetry, emails, marketing copy.</li> <li> <strong>Summarization</strong>: Condensing long documents into key points.</li> <li> <strong>Translation</strong>: Bridging language barriers.</li> <li> <strong>Question Answering</strong>: Providing information based on vast knowledge.</li> <li> <strong>Code Generation &amp; Debugging</strong>: Writing code snippets, finding errors, explaining complex code.</li> <li> <strong>Creative Tasks</strong>: Brainstorming ideas, role-playing, generating dialogues.</li> </ul> <p>However, it’s crucial to understand their limitations:</p> <ul> <li> <strong>Hallucinations</strong>: They can confidently generate factually incorrect information. They don’t “know” facts; they predict statistically plausible sequences of words.</li> <li> <strong>Bias</strong>: As they learn from human-generated data, they can inherit and perpetuate biases present in that data.</li> <li> <strong>Lack of True Understanding</strong>: LLMs are incredibly sophisticated pattern matchers. They don’t possess consciousness, common sense, or real-world understanding in the way humans do.</li> <li> <strong>Context Window Limits</strong>: While better than RNNs, there’s still a limit to how much context (how many tokens) an LLM can effectively consider at once.</li> <li> <strong>Up-to-Date Information</strong>: Their knowledge is typically capped at their last training cutoff.</li> </ul> <h3 id="my-personal-take-a-world-of-discovery">My Personal Take: A World of Discovery</h3> <p>The speed at which LLMs are evolving is breathtaking. Every few months, new models emerge that push the boundaries of what’s possible. As a data science and MLE enthusiast, it feels like we’re living through a technological revolution. Experimenting with different prompts, fine-tuning smaller models, or just exploring the latent capabilities of these giants is an endlessly fascinating endeavor.</p> <p>There are also immense ethical considerations: bias, misinformation, job displacement, and the potential for misuse. As we build and deploy these powerful tools, understanding their inner workings and inherent limitations becomes not just an academic exercise, but a societal responsibility.</p> <h3 id="the-road-ahead-whats-next">The Road Ahead: What’s Next?</h3> <p>The future of LLMs is bursting with possibilities:</p> <ul> <li> <strong>Multimodality</strong>: Integrating text with images, audio, and video to understand and generate across different data types.</li> <li> <strong>Smarter Reasoning</strong>: Developing models with enhanced logical inference and problem-solving capabilities.</li> <li> <strong>Efficiency</strong>: Creating smaller, more efficient models that can run on less powerful hardware, making AI more accessible.</li> <li> <strong>Personalization &amp; Agents</strong>: Developing LLMs that act as intelligent agents, understanding individual users and performing complex tasks autonomously.</li> </ul> <p>We are still in the early chapters of the LLM story. The journey from rules-based systems to the self-attending giants of today is a testament to human ingenuity. Understanding the core mechanisms, especially the Transformer’s attention, demystifies much of the “magic” and empowers us to not just use these tools, but to contribute to their development and guide their responsible application.</p> <p>So, the next time you interact with an LLM, take a moment to appreciate the billions of parameters, trillions of words, and the ingenious Transformer architecture humming beneath the surface. It’s not just talking to a machine; it’s peeking into a new frontier of intelligence. And trust me, it’s an adventure worth taking.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>