<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Unraveling Tomorrow's Secrets: A Journey into Markov Chains | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/unraveling-tomorrows-secrets-a-journey-into-markov/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Unraveling Tomorrow's Secrets: A Journey into Markov Chains</h1> <p class="post-meta"> Created on February 14, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/markov-chains"> <i class="fa-solid fa-hashtag fa-sm"></i> Markov Chains</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/probability"> <i class="fa-solid fa-hashtag fa-sm"></i> Probability</a>   <a href="/blog/blog/tag/stochastic-processes"> <i class="fa-solid fa-hashtag fa-sm"></i> Stochastic Processes</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Have you ever tried to guess what the weather will be like tomorrow? Or which song will play next on your shuffled playlist? Perhaps you’ve noticed patterns in a game you’re playing, trying to anticipate your opponent’s next move. We often intuitively look for patterns and connections between past events and future outcomes. But what if the past, beyond the immediate present, didn’t matter at all?</p> <p>Welcome to the captivating realm of <strong>Markov Chains</strong>, a concept so elegant in its simplicity yet so profound in its applications that it underpins everything from Google’s search algorithm to predicting stock market trends (with caveats, of course!). As a data enthusiast, when I first encountered Markov Chains, I was struck by their intuitive logic and their sheer power to model seemingly random phenomena. Let’s embark on a journey to demystify them.</p> <h3 id="the-heart-of-the-matter-memorylessness-the-markov-property">The Heart of the Matter: Memorylessness (The Markov Property)</h3> <p>At the core of every Markov Chain lies a brilliant, almost philosophical, idea known as the <strong>Markov Property</strong>. It states that <em>the future state of a system depends only on its current state, not on the sequence of events that preceded it.</em></p> <p>Think of it like this: If I’m trying to decide what to wear tomorrow, a Markov Chain would suggest that my choice only depends on what I’m wearing <em>today</em>. It doesn’t care what I wore last Tuesday, or even last year. My current outfit is the only piece of information relevant to my next outfit decision.</p> <p>Mathematically, if $X_n$ represents the state of our system at time $n$, the Markov Property can be expressed as:</p> <p>$P(X_{n+1} = j \mid X_n = i, X_{n-1} = k, …, X_0 = l) = P(X_{n+1} = j \mid X_n = i)$</p> <p>This means the probability of transitioning to state $j$ at the next step ($n+1$), given all past states, is the same as the probability of transitioning to state $j$ given <em>only</em> the current state $i$. This “memoryless” property simplifies things immensely, turning complex sequences into manageable chunks.</p> <h3 id="building-blocks-states-transitions-and-probabilities">Building Blocks: States, Transitions, and Probabilities</h3> <p>To truly grasp Markov Chains, let’s break down their essential components:</p> <ol> <li> <p><strong>States:</strong> These are the possible situations or conditions our system can be in. In our weather example, the states might be “Sunny,” “Cloudy,” or “Rainy.” If we’re modeling a simple board game, states could be “Start,” “Square 1,” “Square 2,” …, “End.” These states must be exhaustive (cover all possibilities) and mutually exclusive (the system can only be in one state at a time).</p> </li> <li> <p><strong>Transitions:</strong> These are the movements or changes from one state to another. If it’s sunny today, tomorrow it might be rainy – that’s a transition.</p> </li> <li> <p><strong>Transition Probabilities:</strong> This is where the magic happens. For every possible transition from state $i$ to state $j$, there’s a probability $P_{ij}$ that this transition will occur. For a valid Markov Chain, the sum of probabilities for all possible transitions <em>from</em> a given state must equal 1. (You have to go <em>somewhere</em>!).</p> <p>Let’s stick with our simplified weather example. Suppose we observe the weather for many years and find these probabilities:</p> <ul> <li>If it’s <strong>Sunny</strong> today: <ul> <li>There’s a 90% chance it will be <strong>Sunny</strong> tomorrow ($P_{SS} = 0.9$).</li> <li>There’s a 10% chance it will be <strong>Rainy</strong> tomorrow ($P_{SR} = 0.1$).</li> </ul> </li> <li>If it’s <strong>Rainy</strong> today: <ul> <li>There’s a 30% chance it will be <strong>Sunny</strong> tomorrow ($P_{RS} = 0.3$).</li> <li>There’s a 70% chance it will be <strong>Rainy</strong> tomorrow ($P_{RR} = 0.7$).</li> </ul> </li> </ul> <p>We can represent these probabilities elegantly in a <strong>Transition Matrix (P)</strong>:</p> <p>$P = \begin{pmatrix} P_{SS} &amp; P_{SR} \ P_{RS} &amp; P_{RR} \end{pmatrix} = \begin{pmatrix} 0.9 &amp; 0.1 \ 0.3 &amp; 0.7 \end{pmatrix}$</p> <p>Here, the rows represent the “current state” and the columns represent the “next state.” Each row must sum to 1. This matrix is the blueprint for our Markov Chain; it completely describes its behavior.</p> </li> </ol> <h3 id="visualizing-the-journey-state-diagrams">Visualizing the Journey: State Diagrams</h3> <p>For many, visual aids make complex concepts click. Markov Chains can be beautifully represented using <strong>state diagrams</strong>:</p> <ul> <li>Each state is a node (a circle).</li> <li>Transitions are directed arrows (edges) between nodes.</li> <li>Each arrow is labeled with its corresponding transition probability.</li> </ul> <p>Let’s draw our weather example:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      (0.9)
   ┌───────────┐
   │           ▼
(Sunny)───────(Rainy)
   ▲           │
   └───────────┘
      (0.3)
(0.1)      (0.7)
</code></pre></div></div> <p>(Imagine an arrow from Sunny to Sunny with 0.9, Sunny to Rainy with 0.1, Rainy to Sunny with 0.3, and Rainy to Rainy with 0.7). This diagram visually encapsulates all the information in our transition matrix, making it easy to see the possible paths and their likelihoods.</p> <h3 id="predicting-the-future-a-step-at-a-time">Predicting the Future: A Step at a Time</h3> <p>Now that we have our components, how do we use this to predict what happens in, say, two days? Or a week?</p> <p>Let’s say we start today, and it’s Sunny. Our initial state distribution (a row vector $\pi_0$) would be $\begin{pmatrix} 1 &amp; 0 \end{pmatrix}$ (100% chance of Sunny, 0% chance of Rainy).</p> <p>To find the probability distribution for tomorrow ($\pi_1$), we multiply our initial distribution by the transition matrix:</p> <p>$\pi_1 = \pi_0 P = \begin{pmatrix} 1 &amp; 0 \end{pmatrix} \begin{pmatrix} 0.9 &amp; 0.1 \ 0.3 &amp; 0.7 \end{pmatrix} = \begin{pmatrix} (1 \cdot 0.9 + 0 \cdot 0.3) &amp; (1 \cdot 0.1 + 0 \cdot 0.7) \end{pmatrix} = \begin{pmatrix} 0.9 &amp; 0.1 \end{pmatrix}$</p> <p>This makes sense: if it’s sunny today, there’s a 90% chance it’s sunny tomorrow and a 10% chance it’s rainy.</p> <p>What about the day after tomorrow ($\pi_2$)? We just apply the matrix again:</p> <p>$\pi_2 = \pi_1 P = \begin{pmatrix} 0.9 &amp; 0.1 \end{pmatrix} \begin{pmatrix} 0.9 &amp; 0.1 \ 0.3 &amp; 0.7 \end{pmatrix} = \begin{pmatrix} (0.9 \cdot 0.9 + 0.1 \cdot 0.3) &amp; (0.9 \cdot 0.1 + 0.1 \cdot 0.7) \end{pmatrix}$ $\pi_2 = \begin{pmatrix} (0.81 + 0.03) &amp; (0.09 + 0.07) \end{pmatrix} = \begin{pmatrix} 0.84 &amp; 0.16 \end{pmatrix}$</p> <p>So, if it’s sunny today, there’s an 84% chance it will be sunny two days from now, and a 16% chance it will be rainy.</p> <p>Notice a pattern? To find the state distribution after $n$ steps, you just multiply the initial distribution by the transition matrix raised to the power of $n$:</p> <p>$\pi_n = \pi_0 P^n$</p> <p>This is incredibly powerful! With simple matrix multiplication, we can peer into the probabilistic future of our system.</p> <h3 id="the-long-run-stationary-distribution">The Long Run: Stationary Distribution</h3> <p>Now, what if we fast-forward far into the future – say, 100 days? Or 1000 days? Will the weather probabilities still depend on whether it was sunny or rainy <em>today</em>?</p> <p>For many Markov Chains (specifically, those that are <strong>irreducible</strong>, meaning you can eventually get from any state to any other state, and <strong>aperiodic</strong>, meaning they don’t get stuck in predictable cycles), something amazing happens: the system eventually reaches a <strong>stationary distribution</strong>, also known as a steady-state distribution.</p> <p>This stationary distribution, often denoted as $\pi_s$, represents the long-term probabilities of being in each state, regardless of the initial starting state. It’s like the system “forgets” where it started and settles into a stable rhythm.</p> <p>Mathematically, a stationary distribution $\pi_s$ satisfies the equation:</p> <p>$\pi_s P = \pi_s$</p> <p>This means that if the system is already in the stationary distribution, applying the transition matrix one more time doesn’t change the distribution. It’s stable.</p> <p>To find $\pi_s$ for our weather example, we’d solve a system of linear equations. Let $\pi_s = \begin{pmatrix} s_S &amp; s_R \end{pmatrix}$, where $s_S$ is the long-term probability of being Sunny and $s_R$ is the long-term probability of being Rainy.</p> <p>$\begin{pmatrix} s_S &amp; s_R \end{pmatrix} \begin{pmatrix} 0.9 &amp; 0.1 \ 0.3 &amp; 0.7 \end{pmatrix} = \begin{pmatrix} s_S &amp; s_R \end{pmatrix}$</p> <p>This gives us two equations:</p> <ol> <li>$0.9 s_S + 0.3 s_R = s_S$</li> <li>$0.1 s_S + 0.7 s_R = s_R$</li> </ol> <p>And we also know that $s_S + s_R = 1$ (because it must be either sunny or rainy).</p> <p>From equation 1: $0.3 s_R = 0.1 s_S \Rightarrow s_S = 3 s_R$. Substitute into the sum equation: $3 s_R + s_R = 1 \Rightarrow 4 s_R = 1 \Rightarrow s_R = 0.25$. Then $s_S = 3 \cdot 0.25 = 0.75$.</p> <p>So, our stationary distribution is $\pi_s = \begin{pmatrix} 0.75 &amp; 0.25 \end{pmatrix}$. This means that in the long run, our fictional town will be sunny 75% of the time and rainy 25% of the time, irrespective of whether it was sunny or rainy on the very first day we started observing!</p> <h3 id="where-do-we-use-markov-chains-mind-blowing-applications">Where Do We Use Markov Chains? Mind-Blowing Applications!</h3> <p>The simplicity and mathematical elegance of Markov Chains belie their incredible utility across diverse fields:</p> <ol> <li> <p><strong>Google PageRank:</strong> This is perhaps the most famous application. Imagine every webpage as a state, and every hyperlink as a transition. A user randomly clicking links creates a massive Markov Chain. The stationary distribution of this chain tells us the probability of a “random surfer” being on any given page. Pages with higher probabilities are considered more important and thus rank higher in search results. Pure genius!</p> </li> <li> <p><strong>Natural Language Processing (NLP):</strong> Markov Chains are fundamental for language modeling. Given a sequence of words, they can predict the next most likely word. For example, after “The quick brown,” what’s the most probable next word? “Fox”! This is used in predictive text, spell checkers, and even machine translation.</p> </li> <li> <p><strong>Finance:</strong> While financial markets are notoriously complex and don’t strictly adhere to the memoryless property, simplified Markov models can be used to model stock price movements (e.g., up, down, stable) or to price options.</p> </li> <li> <p><strong>Biology &amp; Bioinformatics:</strong> Modeling DNA sequences, protein folding, and even the spread of diseases can leverage Markov Chains. Each nucleotide (A, C, G, T) or amino acid could be a state.</p> </li> <li> <p><strong>Reinforcement Learning:</strong> Markov Decision Processes (MDPs), an extension of Markov Chains, are the bedrock of reinforcement learning. An agent (like an AI playing a game) moves between states, performs actions, and receives rewards, aiming to learn the optimal policy to maximize its long-term reward.</p> </li> </ol> <h3 id="a-data-scientists-toolkit">A Data Scientist’s Toolkit</h3> <p>From a data science perspective, Markov Chains are invaluable. We often estimate the transition probabilities ($P_{ij}$) directly from observed data. If we track many instances of a system moving from state to state, we can simply count the transitions and divide by the total number of departures from a given state to get our probabilities.</p> <p>For example, if we observed 100 days where it was Sunny, and 90 times it stayed Sunny while 10 times it became Rainy, then $P_{SS} = 90/100 = 0.9$ and $P_{SR} = 10/100 = 0.1$.</p> <p>While implementing this in Python might involve libraries like <code class="language-plaintext highlighter-rouge">numpy</code> for matrix operations and <code class="language-plaintext highlighter-rouge">random</code> for simulating sequences, the core logic remains the same: define states, estimate transition probabilities, and then use matrix multiplication to forecast or calculate long-term behavior.</p> <h3 id="beyond-the-horizon">Beyond the Horizon</h3> <p>Of course, no model is perfect. The assumption of memorylessness is a strong one and might not hold true for all real-world phenomena (e.g., human behavior or complex financial systems often have longer “memories”). However, even in these cases, Markov Chains often serve as excellent approximations or foundational elements for more sophisticated models.</p> <h3 id="concluding-thoughts">Concluding Thoughts</h3> <p>Markov Chains are a testament to the power of mathematics to model and understand complexity. With just a few simple rules – states and memoryless transitions – we can build models that predict long-term behavior, uncover hidden structures, and drive some of the most impactful technologies of our time.</p> <p>So, the next time you use Google, or your phone suggests the next word, or you simply ponder tomorrow’s weather, remember the elegant simplicity of the Markov Chain, quietly working behind the scenes, unraveling the secrets of tomorrow. What seemingly random process will <em>you</em> model next? The journey into stochastic processes is just beginning!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>