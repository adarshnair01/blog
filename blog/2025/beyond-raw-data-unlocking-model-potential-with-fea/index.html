<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Beyond Raw Data: Unlocking Model Potential with Feature Engineering | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/beyond-raw-data-unlocking-model-potential-with-fea/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Beyond Raw Data: Unlocking Model Potential with Feature Engineering</h1> <p class="post-meta"> Created on December 24, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/feature-engineering"> <i class="fa-solid fa-hashtag fa-sm"></i> Feature Engineering</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/data-transformation"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Transformation</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>As a budding data scientist, I often find myself reflecting on the moments that truly shifted my perspective. One of the most profound was when I fully grasped the power — and necessity — of <strong>Feature Engineering</strong>. Before that, I assumed that once I had my data, the model would simply “figure it out.” Oh, how naive I was!</p> <p>It’s like cooking. You can have the finest raw ingredients – fresh vegetables, premium cuts of meat. But if you just throw them into a pot without chopping, seasoning, or understanding how different flavors combine, you’re unlikely to create a gourmet meal. Feature Engineering is precisely that: the art and science of transforming raw data into the delectable, model-ready features that your machine learning algorithm can truly learn from.</p> <h3 id="what-even-is-a-feature-and-why-do-we-need-to-engineer-them">What Even <em>Is</em> a Feature? (And Why Do We Need to Engineer Them?)</h3> <p>At its core, a <strong>feature</strong> is an individual measurable property or characteristic of a phenomenon being observed. In the context of machine learning, features are the independent variables (columns in your dataset) that you feed into your model to predict an outcome (the dependent variable).</p> <p>Think about predicting house prices:</p> <ul> <li> <strong>Raw data:</strong> <code class="language-plaintext highlighter-rouge">['Address: 123 Main St', 'Built: 2005-03-15', 'Area: 1500 sqft', 'Bedrooms: 3', 'Baths: 2']</code> </li> <li> <strong>Features:</strong> <code class="language-plaintext highlighter-rouge">year_built=2005</code>, <code class="language-plaintext highlighter-rouge">age_of_house=19</code> (if current year is 2024), <code class="language-plaintext highlighter-rouge">living_area_sqft=1500</code>, <code class="language-plaintext highlighter-rouge">num_bedrooms=3</code>, <code class="language-plaintext highlighter-rouge">num_bathrooms=2</code>.</li> </ul> <p>Why don’t we just use the raw data directly?</p> <ol> <li> <strong>Models Speak Math:</strong> Most machine learning algorithms are mathematical beasts. They need numerical input. Text like “123 Main St” or dates like “2005-03-15” are meaningless to them unless converted.</li> <li> <strong>Hidden Information:</strong> Often, the most predictive information isn’t explicitly in the raw data, but <em>implied</em> by it. The difference between ‘2005-03-15’ and ‘2024-03-15’ isn’t just two dates; it’s the <em>age</em> of the house, which is likely a crucial predictor.</li> <li> <strong>Better Representation:</strong> Sometimes, raw data is noisy, sparse, or poorly distributed. Engineering new features can create more robust and meaningful representations that help your model converge faster and perform better.</li> </ol> <p>This transformation process, from raw data to a set of features that best represents the underlying problem for a given model, is what we call <strong>Feature Engineering</strong>.</p> <h3 id="the-heartbeat-domain-knowledge">The Heartbeat: Domain Knowledge</h3> <p>Before diving into techniques, I need to emphasize one crucial aspect: <strong>Domain Knowledge</strong>. This isn’t just a buzzword; it’s your superpower. Understanding the subject matter (e.g., finance, healthcare, e-commerce) allows you to make informed hypotheses about what features might be important.</p> <p>For example, if you’re predicting customer churn for a telecom company, domain knowledge tells you that <code class="language-plaintext highlighter-rouge">average_call_duration_last_month</code> or <code class="language-plaintext highlighter-rouge">number_of_customer_service_calls</code> might be more indicative than just the raw <code class="language-plaintext highlighter-rouge">call_log_data</code>. You can’t just code these out of thin air; you need to know what to look for!</p> <h3 id="my-toolkit-of-feature-engineering-techniques">My Toolkit of Feature Engineering Techniques</h3> <p>Let’s explore some common and powerful techniques I keep in my feature engineering arsenal.</p> <h4 id="1-numerical-feature-engineering">1. Numerical Feature Engineering</h4> <p>Numerical data often needs a little tweaking to perform optimally.</p> <ul> <li> <strong>Transformations (Scaling, Log, Square Root, Polynomial):</strong> <ul> <li> <strong>Logarithmic/Square Root Transformations:</strong> Used for features with a skewed distribution (e.g., income, house prices). Many models (especially linear ones) assume normally distributed features. Applying $log(x)$ or $\sqrt{x}$ can pull in outliers and make the distribution more symmetrical, helping the model learn more effectively. <ul> <li> <em>Example:</em> If <code class="language-plaintext highlighter-rouge">price</code> is highly skewed, <code class="language-plaintext highlighter-rouge">log_price</code> ($log(\text{price})$) might be a better feature.</li> </ul> </li> <li> <strong>Polynomial Features:</strong> For capturing non-linear relationships. If your target changes with $x^2$ rather than just $x$, you can create a feature like $x^2$. <ul> <li> <em>Example:</em> <code class="language-plaintext highlighter-rouge">area_squared</code> ($area^2$) could be a feature for house price prediction if the value increases non-linearly with size.</li> </ul> </li> <li> <strong>Scaling:</strong> While often considered a pre-processing step, it’s fundamental for many algorithms (e.g., K-Nearest Neighbors, SVMs, neural networks) that are sensitive to the magnitude of features. <ul> <li> <strong>Standardization:</strong> Transforms data to have a mean of 0 and standard deviation of 1 ($x’ = (x - \mu) / \sigma$).</li> <li> <strong>Normalization:</strong> Scales data to a fixed range, usually 0 to 1 ($x’ = (x - min(x)) / (max(x) - min(x))$).</li> </ul> </li> </ul> </li> <li> <strong>Binning/Discretization:</strong> <ul> <li>Converting continuous numerical data into discrete categories (bins). This can make a model more robust to outliers and potentially capture non-linear relationships if the model struggles with raw continuous values.</li> <li> <em>Example:</em> Instead of <code class="language-plaintext highlighter-rouge">age</code> (0-100), you could have <code class="language-plaintext highlighter-rouge">age_group</code> (0-18, 19-35, 36-60, 61+).</li> </ul> </li> <li> <strong>Interaction Features:</strong> <ul> <li>Creating new features by combining existing ones, often through multiplication or division, to capture how features influence each other.</li> <li> <em>Example:</em> If predicting crop yield, <code class="language-plaintext highlighter-rouge">rainfall_per_fertilizer_unit</code> ($rainfall / fertilizer_amount$) might be more predictive than <code class="language-plaintext highlighter-rouge">rainfall</code> and <code class="language-plaintext highlighter-rouge">fertilizer_amount</code> separately. Or <code class="language-plaintext highlighter-rouge">price_per_square_foot</code> ($price / square_footage$).</li> </ul> </li> </ul> <h4 id="2-categorical-feature-engineering">2. Categorical Feature Engineering</h4> <p>Categorical data (like <code class="language-plaintext highlighter-rouge">color</code>, <code class="language-plaintext highlighter-rouge">city</code>, <code class="language-plaintext highlighter-rouge">gender</code>) cannot be fed directly into most models.</p> <ul> <li> <strong>One-Hot Encoding:</strong> <ul> <li>This is the most common technique. It converts each category into a new binary (0 or 1) feature. If a feature <code class="language-plaintext highlighter-rouge">color</code> has values <code class="language-plaintext highlighter-rouge">red</code>, <code class="language-plaintext highlighter-rouge">blue</code>, <code class="language-plaintext highlighter-rouge">green</code>, it becomes three new features: <code class="language-plaintext highlighter-rouge">color_red</code>, <code class="language-plaintext highlighter-rouge">color_blue</code>, <code class="language-plaintext highlighter-rouge">color_green</code>.</li> <li> <em>Why:</em> It prevents the model from assuming an arbitrary ordinal relationship between categories (e.g., <code class="language-plaintext highlighter-rouge">red</code> is not “greater” than <code class="language-plaintext highlighter-rouge">blue</code>).</li> <li> <em>Caution:</em> Can lead to a high-dimensional dataset if a categorical feature has many unique values.</li> </ul> </li> <li> <strong>Label Encoding:</strong> <ul> <li>Assigns a unique integer to each category (e.g., <code class="language-plaintext highlighter-rouge">red=0</code>, <code class="language-plaintext highlighter-rouge">blue=1</code>, <code class="language-plaintext highlighter-rouge">green=2</code>).</li> <li> <em>Why:</em> Useful when there <em>is</em> a natural order (ordinality) to the categories (e.g., <code class="language-plaintext highlighter-rouge">small=0</code>, <code class="language-plaintext highlighter-rouge">medium=1</code>, <code class="language-plaintext highlighter-rouge">large=2</code>). Also can be used for tree-based models (like Decision Trees, Random Forests, XGBoost) as they are less sensitive to implied ordinality.</li> </ul> </li> <li> <strong>Target Encoding (Mean Encoding):</strong> <ul> <li>Replaces a categorical value with the mean of the target variable for that category.</li> <li> <em>Example:</em> For <code class="language-plaintext highlighter-rouge">city</code>, replace <code class="language-plaintext highlighter-rouge">New York</code> with the average house price in New York.</li> <li> <em>Why:</em> Can capture predictive power efficiently and reduce dimensionality.</li> <li> <em>Caution:</em> Prone to <strong>data leakage</strong> and overfitting if not implemented carefully (e.g., using only training data statistics or K-fold cross-validation).</li> </ul> </li> </ul> <h4 id="3-datetime-feature-engineering">3. Date/Time Feature Engineering</h4> <p>Date and time stamps are treasure troves of information. Don’t just leave them as raw <code class="language-plaintext highlighter-rouge">datetime</code> objects!</p> <ul> <li> <strong>Extracting Components:</strong> <ul> <li>Break down dates into <code class="language-plaintext highlighter-rouge">year</code>, <code class="language-plaintext highlighter-rouge">month</code>, <code class="language-plaintext highlighter-rouge">day</code>, <code class="language-plaintext highlighter-rouge">day_of_week</code>, <code class="language-plaintext highlighter-rouge">day_of_year</code>, <code class="language-plaintext highlighter-rouge">week_of_year</code>, <code class="language-plaintext highlighter-rouge">hour</code>, <code class="language-plaintext highlighter-rouge">minute</code>, <code class="language-plaintext highlighter-rouge">second</code>.</li> <li> <em>Example:</em> For predicting ride-share demand, <code class="language-plaintext highlighter-rouge">hour_of_day</code> and <code class="language-plaintext highlighter-rouge">day_of_week</code> are far more useful than the full timestamp.</li> </ul> </li> <li> <strong>Time Differences:</strong> <ul> <li>Calculate the duration between two events (e.g., <code class="language-plaintext highlighter-rouge">time_since_last_purchase</code>, <code class="language-plaintext highlighter-rouge">days_since_signup</code>).</li> </ul> </li> <li> <strong>Cyclical Features:</strong> <ul> <li>For cyclical data like <code class="language-plaintext highlighter-rouge">hour_of_day</code> (0-23) or <code class="language-plaintext highlighter-rouge">month_of_year</code> (1-12), converting them into sine and cosine components can preserve their cyclical nature without implying a linear relationship where none exists.</li> <li> <em>Example:</em> For <code class="language-plaintext highlighter-rouge">hour</code> (0-23), you could create two features: $hour_sin = sin(2\pi \times hour / 24)$ and $hour_cos = cos(2\pi \times hour / 24)$.</li> </ul> </li> </ul> <h4 id="4-text-feature-engineering-briefly">4. Text Feature Engineering (Briefly)</h4> <p>Text data has its own universe of feature engineering.</p> <ul> <li> <strong>Basic Statistics:</strong> <code class="language-plaintext highlighter-rouge">word_count</code>, <code class="language-plaintext highlighter-rouge">char_count</code>, <code class="language-plaintext highlighter-rouge">average_word_length</code>, <code class="language-plaintext highlighter-rouge">sentiment_score</code>.</li> <li> <strong>TF-IDF (Term Frequency-Inverse Document Frequency):</strong> A statistical measure that reflects how important a word is to a document in a collection or corpus.</li> <li> <strong>Word Embeddings (Word2Vec, GloVe, BERT):</strong> Represent words (or phrases) as dense vectors in a continuous vector space, capturing semantic relationships. This is a vast field in itself!</li> </ul> <h3 id="the-feature-engineering-workflow-my-own-playbook">The Feature Engineering Workflow: My Own Playbook</h3> <p>My journey through feature engineering usually follows a pattern:</p> <ol> <li> <strong>Understand the Data &amp; Problem:</strong> <ul> <li>Extensive Exploratory Data Analysis (EDA).</li> <li>Deep dive into the business context. What are we trying to predict? Why? What factors <em>should</em> influence it? This is where domain knowledge shines.</li> </ul> </li> <li> <strong>Brainstorm &amp; Hypothesize:</strong> <ul> <li>Based on EDA and domain knowledge, I brainstorm potential new features. “What if I combine X and Y? What if I extract this part of the date? What if this text length matters?” I write these down, even if they seem wild.</li> </ul> </li> <li> <strong>Create the Features (Code!):</strong> <ul> <li>Using libraries like Pandas and NumPy, I start coding up my new features. This is often the most iterative and code-intensive part. I ensure proper handling of missing values and edge cases.</li> </ul> </li> <li> <strong>Evaluate &amp; Validate:</strong> <ul> <li>Train a baseline model <em>with and without</em> the new features. Does performance improve?</li> <li>Look at feature importance (if available from the model, e.g., tree-based models).</li> <li>Visualize relationships between new features and the target variable.</li> <li>Check for correlation between new features and existing ones to avoid multicollinearity and redundancy.</li> </ul> </li> <li> <strong>Refine &amp; Iterate:</strong> <ul> <li>Based on evaluation, I might go back to step 2. Maybe a feature didn’t work as expected, or maybe it sparked an idea for an even better one. This loop is crucial for success.</li> </ul> </li> </ol> <h3 id="common-pitfalls-and-best-practices-ive-learned">Common Pitfalls and Best Practices I’ve Learned</h3> <ul> <li> <strong>Data Leakage is Your Arch-Nemesis:</strong> This is when your training data contains information that would not be available at prediction time, causing your model to report overly optimistic performance. <ul> <li> <em>Classic Example:</em> Using statistics derived from the <em>entire</em> dataset (including the test set) to engineer features, or using future information. If you’re building a feature like <code class="language-plaintext highlighter-rouge">average_price_per_category</code>, ensure these averages are calculated <em>only</em> from the training set, and then applied to both training and test sets.</li> <li> <em>Rule of Thumb:</em> Split your data into training and test sets <em>before</em> performing any feature engineering steps that use target information or involve aggregation over the dataset.</li> </ul> </li> <li> <p><strong>Don’t Over-Engineer:</strong> Sometimes, a simpler set of features performs better and is easier to maintain. Start simple, then add complexity incrementally. Too many features can lead to overfitting and make your model harder to interpret.</p> </li> <li> <p><strong>Feature Scaling Matters:</strong> Remember to scale your numerical features, especially for algorithms sensitive to magnitudes (SVMs, K-Means, Neural Networks, Gradient Descent based models).</p> </li> <li> <p><strong>Consistency is Key:</strong> Ensure that the same feature engineering steps applied to your training data are <em>exactly</em> applied to your validation, test, and future production data.</p> </li> <li> <strong>Version Control Your Features:</strong> Just like code, keep track of your feature engineering scripts. It’s easy to get lost in a sea of <code class="language-plaintext highlighter-rouge">df['new_feature_v2']</code>.</li> </ul> <h3 id="wrapping-up-your-ml-superpower">Wrapping Up: Your ML Superpower</h3> <p>Feature Engineering truly is the secret sauce of machine learning. It’s where creativity meets technical skill, and where deep understanding of your data can elevate a mediocre model to an exceptional one. It’s often more impactful than trying countless different algorithms or hyperparameter tuning.</p> <p>I encourage you to embrace it! Don’t just blindly feed raw data into your models. Take the time to understand your features, brainstorm new ones, and iteratively improve them. It’s a journey of discovery, problem-solving, and continuous learning, and mastering it will undoubtedly be one of your most valuable skills in the world of data science and machine learning. Go forth and engineer!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>