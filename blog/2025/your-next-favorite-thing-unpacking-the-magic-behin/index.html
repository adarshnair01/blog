<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Your Next Favorite Thing: Unpacking the Magic Behind Recommender Systems | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/your-next-favorite-thing-unpacking-the-magic-behin/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Your Next Favorite Thing: Unpacking the Magic Behind Recommender Systems</h1> <p class="post-meta"> Created on May 12, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/recommender-systems"> <i class="fa-solid fa-hashtag fa-sm"></i> Recommender Systems</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/collaborative-filtering"> <i class="fa-solid fa-hashtag fa-sm"></i> Collaborative Filtering</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h3 id="hey-there-fellow-curious-mind">Hey there, fellow curious mind!</h3> <p>Have you ever found yourself scrolling through endless options on a streaming service, only to land on something truly amazing, seemingly plucked from your deepest desires? Or maybe you’ve been casually browsing an online store, and an item pops up that you <em>didn’t even know you needed</em> but immediately added to your cart? If so, you’ve experienced the silent, subtle, yet incredibly powerful influence of a <strong>Recommender System</strong>.</p> <p>For me, it started with a simple question: “How do they <em>know</em>?” This curiosity led me down a rabbit hole into the exciting field of Data Science and Machine Learning, and it’s a journey I’m thrilled to share with you. Today, we’re going to pull back the curtain on these digital matchmakers, exploring the algorithms and techniques that power our personalized online experiences.</p> <h3 id="the-problem-too-much-choice">The Problem: Too Much Choice!</h3> <p>In our hyper-connected world, we’re drowning in information and options. Think about it:</p> <ul> <li>Millions of songs on Spotify</li> <li>Hundreds of thousands of movies/shows on Netflix</li> <li>Billions of products on Amazon</li> </ul> <p>Without help, finding what you genuinely like or need would be like finding a needle in a haystack – an incredibly vast, ever-growing haystack. This is where Recommender Systems step in. Their primary goal is to <strong>cut through the noise</strong> and present you with items you’re most likely to engage with, enjoy, or purchase. Essentially, they transform information overload into personalized discovery.</p> <h3 id="how-do-they-work-the-core-approaches">How Do They Work? The Core Approaches</h3> <p>At their heart, recommender systems use data about users and items to predict what a user might like. There are several principal ways to do this, each with its own strengths and weaknesses.</p> <h4 id="1-content-based-filtering-if-you-liked-that-youll-like-this">1. Content-Based Filtering: “If you liked <em>that</em>, you’ll like <em>this</em>.”</h4> <p>Imagine you’re a big fan of science fiction movies starring specific actors and directed by certain visionary filmmakers. A content-based system works much like a personal assistant who knows your tastes. It recommends items that are <em>similar</em> to the ones you’ve enjoyed in the past.</p> <p><strong>How it operates:</strong></p> <ul> <li> <strong>Item Features:</strong> Each item (movie, book, product) is described by its features (genre, director, actors, keywords, price, color, etc.). We can represent these features as a vector.</li> <li> <strong>User Profile:</strong> Your preferences are built by analyzing the features of items you’ve interacted with positively. For instance, if you’ve liked 5 sci-fi movies, your profile will have a strong “sci-fi” component.</li> <li> <strong>Matching:</strong> The system then looks for items whose features match your user profile.</li> </ul> <p>Let’s say we have item features like <code class="language-plaintext highlighter-rouge">[is_sci_fi, is_drama, star_wars_related, budget_high]</code>. If you loved “Dune” (features: <code class="language-plaintext highlighter-rouge">[1, 0, 0, 1]</code>), the system might look for other movies with high <code class="language-plaintext highlighter-rouge">is_sci_fi</code> and <code class="language-plaintext highlighter-rouge">budget_high</code> scores.</p> <p>A common way to measure similarity between your profile vector ($\mathbf{P}$) and an item’s feature vector ($\mathbf{I}$) is using <strong>Cosine Similarity</strong>:</p> <table> <tbody> <tr> <td>$ \text{cosine_similarity}(\mathbf{P}, \mathbf{I}) = \frac{\mathbf{P} \cdot \mathbf{I}}{</td> <td> </td> <td>\mathbf{P}</td> <td> </td> <td>\cdot</td> <td> </td> <td>\mathbf{I}</td> <td> </td> <td>} $</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>Here, $\mathbf{P} \cdot \mathbf{I}$ is the dot product of the two vectors, and $</td> <td> </td> <td>\mathbf{P}</td> <td> </td> <td>$ and $</td> <td> </td> <td>\mathbf{I}</td> <td> </td> <td>$ are their magnitudes. This value ranges from -1 to 1, with 1 meaning identical, and 0 meaning no similarity.</td> </tr> </tbody> </table> <p><strong>Pros:</strong></p> <ul> <li>Can recommend niche items specific to your tastes.</li> <li>No “cold start” for new items (as long as we have their features).</li> <li>Recommendations are easy to explain (“because it’s a sci-fi movie, and you like sci-fi!”).</li> </ul> <p><strong>Cons:</strong></p> <ul> <li> <strong>Overspecialization:</strong> You might get stuck in a “filter bubble,” only seeing very similar things.</li> <li> <strong>Cold Start for Users:</strong> If you’re a new user, the system doesn’t know your tastes yet.</li> <li>Requires detailed feature engineering for items.</li> </ul> <h4 id="2-collaborative-filtering-people-like-you-also-liked-this">2. Collaborative Filtering: “People like <em>you</em> also liked <em>this</em>.”</h4> <p>This is often considered the “magic” of recommender systems because it doesn’t need to understand the <em>content</em> of items. Instead, it leverages the collective wisdom of the crowd. The core idea is: if two users have similar tastes in the past, they’re likely to have similar tastes in the future.</p> <p>There are two main types of collaborative filtering:</p> <p><strong>a) Neighborhood-Based Collaborative Filtering:</strong></p> <ul> <li> <strong>User-User Collaborative Filtering:</strong> <ul> <li>Find users who are similar to you (e.g., you both rated the same movies similarly).</li> <li>Recommend items that these “similar users” liked but you haven’t seen yet.</li> <li> <em>Analogy:</em> Your friend Alex loves all the same bands as you. If Alex discovers a new band and loves it, chances are you will too!</li> <li> <em>Challenge:</em> Scalability becomes an issue with millions of users. Finding the ‘most similar’ users among millions for every recommendation can be computationally expensive.</li> </ul> </li> <li> <strong>Item-Item Collaborative Filtering:</strong> <ul> <li>Find items that are similar to items you’ve liked (e.g., people who liked Movie A also liked Movie B).</li> <li>Recommend these “similar items.”</li> <li> <em>Analogy:</em> If you liked “Inception,” the system finds other movies that people who liked “Inception” also rated highly.</li> <li> <em>Advantage:</em> Item-item similarities tend to be more stable than user-user similarities (user tastes change, but item-item relationships are more fixed). This can be precomputed, making it highly scalable for many users. Most major platforms use this approach heavily.</li> </ul> </li> </ul> <p><strong>b) Model-Based Collaborative Filtering (Matrix Factorization):</strong></p> <p>This is where things get a bit more abstract and incredibly powerful. Instead of directly comparing users or items, model-based methods try to understand the <em>underlying reasons</em> for preferences. They do this by “decomposing” the user-item interaction data into a set of lower-dimensional “latent factors.”</p> <ul> <li> <p><strong>The Idea:</strong> Imagine a hidden set of characteristics (like a movie being “action-packed,” “dialogue-heavy,” or “visually stunning”) that influence a user’s rating. We don’t explicitly know what these factors are, but the model tries to discover them. Each user and each item can be represented as a combination of these latent factors.</p> </li> <li> <p><strong>Matrix Factorization:</strong> The user-item interaction data can be seen as a large, sparse matrix $R$, where $r_{ui}$ is the rating user $u$ gave to item $i$. Matrix factorization aims to approximate this matrix by multiplying two smaller matrices: a user-factor matrix $P$ and an item-factor matrix $Q$.</p> <p>$ R \approx P \cdot Q^T $</p> <p>Where:</p> <ul> <li>$P$ is an $M \times K$ matrix, with $M$ users and $K$ latent factors. Each row $p_u$ represents user $u$’s preference for each of the $K$ factors.</li> <li>$Q$ is an $N \times K$ matrix, with $N$ items and $K$ latent factors. Each row $q_i$ represents item $i$’s strength in each of the $K$ factors.</li> <li>$Q^T$ is the transpose of $Q$.</li> </ul> <p>The predicted rating for user $u$ on item $i$ is simply the dot product of their respective latent factor vectors: $\hat{r}_{ui} = p_u q_i^T$.</p> <p>The goal is to find $P$ and $Q$ that minimize the difference between predicted and actual ratings for all known ratings, often with regularization to prevent overfitting:</p> <table> <tbody> <tr> <td>$ \min<em>{P,Q} \sum</em>{(u,i) \in R<em>{known}} (r</em>{ui} - p_u q_i^T)^2 + \lambda (</td> <td> </td> <td>P</td> <td> </td> <td>^2 +</td> <td> </td> <td>Q</td> <td> </td> <td>^2) $</td> </tr> </tbody> </table> <p>Common techniques for this include Singular Value Decomposition (SVD) or Alternating Least Squares (ALS).</p> </li> </ul> <p><strong>Pros:</strong></p> <ul> <li>Handles sparse data very well (can make good predictions even with few ratings).</li> <li>Discover hidden, complex patterns in data.</li> <li>Highly scalable once the model is trained.</li> </ul> <p><strong>Cons:</strong></p> <ul> <li>Latent factors are often hard to interpret (“What does latent factor 3 represent?”).</li> <li>Can suffer from the “cold start” problem for new users or items.</li> </ul> <h4 id="3-hybrid-recommender-systems-the-best-of-both-worlds">3. Hybrid Recommender Systems: The Best of Both Worlds</h4> <p>Most real-world recommender systems, like those used by Netflix or Amazon, don’t rely on just one technique. They combine multiple approaches to overcome individual limitations. A hybrid system might:</p> <ul> <li>Use content-based filtering for new users/items (cold start).</li> <li>Switch to collaborative filtering once enough interaction data is gathered.</li> <li>Combine predictions from different models.</li> </ul> <p>This “ensemble” approach often leads to much more robust and accurate recommendations.</p> <h3 id="the-elephant-in-the-room-challenges-for-recommender-systems">The Elephant in the Room: Challenges for Recommender Systems</h3> <p>Building a perfect recommender system is incredibly challenging. Here are some hurdles data scientists constantly face:</p> <ol> <li> <strong>The Cold Start Problem:</strong> <ul> <li> <strong>New Users:</strong> With no past interactions, how do you recommend anything? (Solution: Content-based (ask preferences), popularity-based, or random recommendations).</li> <li> <strong>New Items:</strong> With no ratings, how do you know if an item is good? (Solution: Content-based, editorial curation, or recommend to specific early adopters).</li> </ul> </li> <li> <p><strong>Sparsity:</strong> The vast majority of user-item interactions are unknown. Most people only interact with a tiny fraction of available items. This makes the user-item matrix incredibly sparse, making patterns hard to find.</p> </li> <li> <p><strong>Scalability:</strong> Imagine making real-time recommendations for millions of users and billions of items. Algorithms need to be incredibly efficient.</p> </li> <li> <p><strong>Serendipity and Diversity:</strong> A good recommender shouldn’t just show you things that are extremely similar to what you already like (filter bubble!). It should also introduce you to new, surprising, yet relevant items – true serendipity. It also needs to offer diversity, not just variations of the same thing.</p> </li> <li> <p><strong>Bias and Fairness:</strong> Recommender systems learn from historical data, which can contain biases. If certain groups of users or items are underrepresented, the system might perpetuate or even amplify these biases, leading to unfair or unhelpful recommendations.</p> </li> <li> <strong>Explainability:</strong> Sometimes users want to know <em>why</em> an item was recommended. “Because similar users liked it” is often less satisfying than “because you liked this genre and this actor.” Providing clear, transparent explanations is a growing field (Explainable AI - XAI).</li> </ol> <h3 id="how-do-we-know-if-its-good-evaluation-metrics">How Do We Know if It’s Good? Evaluation Metrics</h3> <p>To improve recommender systems, we need to measure their performance. Common metrics include:</p> <ul> <li> <strong>RMSE (Root Mean Squared Error):</strong> Used when predicting numerical ratings. Lower is better. $ \text{RMSE} = \sqrt{\frac{1}{N} \sum_{i=1}^N (y_i - \hat{y}_i)^2} $</li> <li> <strong>Precision and Recall:</strong> Used for “top-N” recommendations. Precision tells us what proportion of recommended items were relevant, and Recall tells us what proportion of all relevant items were recommended.</li> <li> <strong>Coverage, Diversity, Novelty:</strong> More advanced metrics to ensure the system isn’t just recommending popular items or falling into a filter bubble.</li> </ul> <h3 id="the-future-is-bright-and-smart">The Future is Bright (and Smart!)</h3> <p>Recommender systems are continually evolving. Here are some exciting trends:</p> <ul> <li> <strong>Deep Learning:</strong> Neural networks are revolutionizing recommenders, especially through learning rich, dense “embeddings” for users and items. These embeddings capture complex relationships in a powerful low-dimensional space.</li> <li> <strong>Reinforcement Learning (RL):</strong> Instead of just predicting what a user might like <em>next</em>, RL aims to optimize for long-term user engagement and satisfaction by learning from sequences of interactions.</li> <li> <strong>Context-Aware Recommendations:</strong> Taking into account factors like time of day, location, or user’s mood.</li> <li> <strong>Session-Based Recommendations:</strong> Recommending based on the current browsing session rather than long-term user history, crucial for e-commerce.</li> </ul> <h3 id="your-next-discovery-awaits">Your Next Discovery Awaits…</h3> <p>From content creators to e-commerce giants, recommenders are at the heart of our digital economy, shaping our experiences and helping us navigate an ocean of choices. They represent a beautiful blend of statistics, machine learning, and a deep understanding of human psychology.</p> <p>I hope this journey into Recommender Systems has sparked your own curiosity! The field is dynamic, challenging, and incredibly rewarding. So, the next time your streaming service nails a recommendation, take a moment to appreciate the complex symphony of algorithms working tirelessly behind the scenes – and perhaps, consider diving deeper yourself. Your next favorite thing might just be learning how to build these systems!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>