<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Q-Learning: The Secret Sauce for Smart Decisions (and How I Learned to Love Reinforcement Learning) | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/q-learning-the-secret-sauce-for-smart-decisions-an/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Q-Learning: The Secret Sauce for Smart Decisions (and How I Learned to Love Reinforcement Learning)</h1> <p class="post-meta"> Created on August 06, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/reinforcement-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Reinforcement Learning</a>   <a href="/blog/blog/tag/q-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Q-Learning</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   <a href="/blog/blog/tag/algorithms"> <i class="fa-solid fa-hashtag fa-sm"></i> Algorithms</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h3 id="hey-there-future-ai-architects-and-data-explorers">Hey there, future AI architects and data explorers!</h3> <p>Have you ever found yourself in a new city, trying to figure out the best way to get to your destination? You might try one route, realize it’s full of traffic, and then on your next trip, you try a different path. Slowly, through trial and error, you learn the <em>optimal</em> routes – the ones that get you where you need to go fastest or with the least hassle.</p> <p>This very human process of learning through experience, of making decisions to maximize a desired outcome, is exactly what <strong>Reinforcement Learning (RL)</strong> is all about. And at its heart, for many of us just starting out, sits a wonderfully elegant algorithm called <strong>Q-Learning</strong>.</p> <p>When I first encountered RL, it felt a bit like magic. How could a machine, without being explicitly programmed for every single scenario, learn to perform complex tasks? Q-Learning was my gateway, the first concept that truly clicked and revealed the underlying brilliance. It’s a fundamental building block that explains how an agent can learn to make optimal choices in an uncertain environment. And today, I want to share that journey with you.</p> <hr> <h3 id="diving-into-the-world-of-reinforcement-learning-the-fundamentals">Diving into the World of Reinforcement Learning: The Fundamentals</h3> <p>Before we get to Q-Learning specifically, let’s set the stage with the core players in any Reinforcement Learning scenario:</p> <ul> <li> <strong>Agent</strong>: This is our decision-maker. Think of it as the learner – a robot, a game character, a recommendation engine.</li> <li> <strong>Environment</strong>: This is the world the agent interacts with. It could be a maze, a video game, a stock market, or even the user interface of an app.</li> <li> <strong>State ($s$)</strong>: A specific situation or configuration of the environment at a given time. If our agent is a robot in a maze, its current position <code class="language-plaintext highlighter-rouge">(x, y)</code> is a state.</li> <li> <strong>Action ($a$)</strong>: A move or choice the agent can make from a given state. Our robot might choose to move <code class="language-plaintext highlighter-rouge">UP</code>, <code class="language-plaintext highlighter-rouge">DOWN</code>, <code class="language-plaintext highlighter-rouge">LEFT</code>, or <code class="language-plaintext highlighter-rouge">RIGHT</code>.</li> <li> <strong>Reward ($R$)</strong>: A numerical feedback signal the environment gives the agent after it performs an action in a state. This is how the agent knows if it did “good” or “bad.” A positive reward encourages the action, a negative one discourages it. For our robot, reaching the exit might give a large positive reward, hitting a wall a small negative reward.</li> <li> <strong>Policy ($\pi$)</strong>: The agent’s strategy or “brain.” It’s a mapping from states to actions, telling the agent what action to take in each state. The ultimate goal of RL is to find an <em>optimal policy</em> that maximizes the total accumulated reward over time.</li> </ul> <p>The cycle is simple: The <strong>Agent</strong> observes the <strong>State</strong>, chooses an <strong>Action</strong>, the <strong>Environment</strong> reacts, gives a <strong>Reward</strong>, and transitions to a new <strong>State</strong>. This loop repeats, and through this interaction, the agent learns.</p> <hr> <h3 id="enter-q-learning-whats-in-a-q">Enter Q-Learning: What’s in a “Q”?</h3> <p>Q-Learning is a <strong>model-free</strong> reinforcement learning algorithm. “Model-free” means the agent doesn’t need to know how the environment works (its rules, transition probabilities, rewards) in advance. It learns purely from experience. This is crucial because, in many real-world scenarios, we don’t have a perfect model of the environment.</p> <p>So, what does the “Q” stand for? It stands for “Quality” or “Q-value.” In Q-Learning, our agent learns a function, often represented as a <strong>Q-table</strong>, which maps every possible <em>state-action pair</em> to a numerical value. This value, $Q(s, a)$, represents the <strong>expected future reward</strong> an agent can get by taking action $a$ in state $s$, and then following an optimal policy thereafter.</p> <p>Think of the Q-table as a sophisticated “cheat sheet” or a strategy guide. If our robot is in state <code class="language-plaintext highlighter-rouge">(x, y)</code>, it looks at its Q-table, finds the Q-values for moving <code class="language-plaintext highlighter-rouge">UP</code>, <code class="language-plaintext highlighter-rouge">DOWN</code>, <code class="language-plaintext highlighter-rouge">LEFT</code>, and <code class="language-plaintext highlighter-rouge">RIGHT</code>, and then chooses the action that has the highest Q-value. That’s the action it <em>believes</em> will lead to the best long-term outcome.</p> <p>Initially, this Q-table is empty or filled with zeros. The agent starts knowing nothing. But through repeated interactions with the environment, it gradually updates and refines these Q-values.</p> <hr> <h3 id="the-q-learning-algorithm-how-the-magic-happens">The Q-Learning Algorithm: How the Magic Happens</h3> <p>Let’s break down the core steps and the famous update rule that makes Q-Learning tick.</p> <p><strong>1. Initialization:</strong> We start by creating our Q-table. It will have rows representing states and columns representing actions. All entries are typically initialized to zero.</p> <p><strong>2. Exploration vs. Exploitation ($\epsilon$-greedy policy):</strong> This is a critical dilemma in RL.</p> <ul> <li> <strong>Exploration:</strong> The agent tries new, potentially suboptimal actions to discover more about the environment. If our robot always takes the known “best” path, it might never discover a hidden shortcut!</li> <li> <strong>Exploitation:</strong> The agent uses its current knowledge (the Q-table) to choose the action it believes will yield the highest reward. This is about leveraging what it already knows.</li> </ul> <p>To balance these, Q-Learning often uses an <strong>$\epsilon$-greedy policy</strong>.</p> <ul> <li>With a small probability $\epsilon$ (epsilon), the agent chooses a random action (exploration).</li> <li>With probability $(1 - \epsilon)$, the agent chooses the action with the highest Q-value for its current state (exploitation).</li> </ul> <p>Initially, $\epsilon$ is usually set high (e.g., 0.9 or 1.0) to encourage exploration. As the agent gains more experience and its Q-table becomes more accurate, $\epsilon$ is gradually <em>decayed</em> (reduced) over time, shifting the balance towards exploitation. This ensures the agent eventually settles on the optimal path rather than endlessly trying new things.</p> <p><strong>3. The Q-Value Update Rule (The Heart of Q-Learning):</strong> This is where the learning happens. After the agent takes an action $a$ in state $s$, receives a reward $R$, and transitions to a new state $s’$, it updates the Q-value for the state-action pair $(s, a)$ using the following formula:</p> <p>$Q(s, a) \leftarrow Q(s, a) + \alpha [R + \gamma \max_{a’} Q(s’, a’) - Q(s, a)]$</p> <p>Let’s break down each component of this powerful equation:</p> <ul> <li>$Q(s, a)$: This is the <em>current estimated Q-value</em> for taking action $a$ in state $s$. It’s what we want to update.</li> <li>$\alpha$ (alpha) - <strong>Learning Rate</strong>: This value (between 0 and 1) determines how much we update the Q-value based on new information. <ul> <li>If $\alpha = 0$, the agent learns nothing.</li> <li>If $\alpha = 1$, the agent completely overwrites its old knowledge with the new information.</li> <li>A common value is around 0.1, meaning it takes a small step towards the new estimate.</li> </ul> </li> <li>$R$: This is the <strong>immediate reward</strong> the agent receives for taking action $a$ in state $s$ and landing in $s’$.</li> <li>$\gamma$ (gamma) - <strong>Discount Factor</strong>: This value (between 0 and 1) determines the importance of <em>future rewards</em> versus <em>immediate rewards</em>. <ul> <li>If $\gamma = 0$, the agent is “myopic” and only considers immediate rewards.</li> <li>If $\gamma = 1$, the agent considers future rewards equally important as immediate ones.</li> <li>A common value like 0.9 encourages the agent to seek long-term rewards but with a slight preference for sooner rewards, preventing infinite loops or very distant, uncertain gains.</li> </ul> </li> <li>$\max_{a’} Q(s’, a’)$: This is the <strong>maximum expected future reward</strong> for the <em>next state</em> ($s’$). The agent looks at all possible actions $a’$ it could take from the new state $s’$ and picks the highest Q-value among them. This term represents the “optimal future” value.</li> <li>$[R + \gamma \max_{a’} Q(s’, a’) - Q(s, a)]$: This entire bracketed term is known as the <strong>Temporal Difference (TD) error</strong>. <ul> <li> <code class="language-plaintext highlighter-rouge">R + \gamma \max_{a'} Q(s', a')</code>: This is our <em>new estimate</em> of the “true” value of $Q(s, a)$, based on what just happened (immediate reward) and the <em>best possible future</em> from the next state.</li> <li> <code class="language-plaintext highlighter-rouge">- Q(s, a)</code>: We subtract our <em>old estimate</em> from the new one. The difference tells us how “wrong” our old estimate was.</li> </ul> </li> </ul> <p>So, in plain English, the update rule says: “Take your current estimate for $Q(s, a)$, and adjust it a little bit (controlled by $\alpha$) in the direction of a better estimate, which is based on the immediate reward you just got, plus the best possible discounted future reward you can expect from your new situation.”</p> <p>This process is repeated over many, many episodes (complete runs from start to finish). Over time, if the environment is static and the agent explores sufficiently, the Q-values will converge to the optimal values, giving us the best possible policy.</p> <hr> <h3 id="a-simple-example-the-grid-world">A Simple Example: The Grid World</h3> <p>Imagine a 3x3 grid. Our agent starts at (0,0) and wants to reach a goal at (2,2).</p> <ul> <li> <strong>States</strong>: (0,0), (0,1), …, (2,2) - 9 states.</li> <li> <strong>Actions</strong>: Up, Down, Left, Right - 4 actions.</li> <li> <strong>Rewards</strong>: +10 for reaching (2,2). -1 for hitting a wall. 0 for any other move.</li> </ul> <p>Initially, our Q-table is all zeros. Let’s say the agent is at (0,0).</p> <ol> <li> <strong>Exploration</strong>: With $\epsilon$-greedy, it might randomly choose to move <code class="language-plaintext highlighter-rouge">RIGHT</code>.</li> <li> <strong>Environment reaction</strong>: It moves to (0,1). Reward $R=0$.</li> <li> <strong>Update</strong>: Now we update $Q((0,0), \text{RIGHT})$. <ul> <li>The agent is now in state (0,1). It looks at what it <em>could</em> do from (0,1) and what the max $Q$ value is for those actions (which are still 0 initially). So, $\max_{a’} Q((0,1), a’)$ is 0.</li> <li>$Q((0,0), \text{RIGHT}) \leftarrow Q((0,0), \text{RIGHT}) + \alpha [0 + \gamma * 0 - Q((0,0), \text{RIGHT})]$</li> <li>If $Q((0,0), \text{RIGHT})$ was initially 0, and $\alpha=0.1, \gamma=0.9$, the update looks like: $0 \leftarrow 0 + 0.1 [0 + 0.9 * 0 - 0] \Rightarrow 0$. Not much change yet! This is normal for early steps.</li> </ul> </li> </ol> <p>Now, let’s fast forward many episodes. Imagine the agent finally stumbles into the goal state (2,2) from (1,2) by moving <code class="language-plaintext highlighter-rouge">RIGHT</code>. It gets a reward of +10. Now, it updates $Q((1,2), \text{RIGHT})$:</p> <ul> <li>$R = +10$.</li> <li>The next state $s’$ is (2,2) (goal state). From the goal state, there are no more actions, so $\max_{a’} Q((2,2), a’)$ is usually considered 0 (or some terminal value).</li> <li>$Q((1,2), \text{RIGHT}) \leftarrow Q((1,2), \text{RIGHT}) + \alpha [10 + \gamma * 0 - Q((1,2), \text{RIGHT})]$</li> <li>Let’s assume $\alpha=0.1, \gamma=0.9$ and $Q((1,2), \text{RIGHT})$ was 0: $Q((1,2), \text{RIGHT}) \leftarrow 0 + 0.1 [10 + 0 - 0] = 1$. So, $Q((1,2), \text{RIGHT})$ is now 1.</li> </ul> <p>On subsequent episodes, if the agent moves <code class="language-plaintext highlighter-rouge">RIGHT</code> from (1,2) again, the $Q$-value will keep getting updated, moving closer to the true expected reward (which would eventually be 10 if $\alpha$ were 1 and $\gamma$ were 1, or $10\alpha$ if $Q$ was $0$). More importantly, Q-values of states <em>leading to</em> (1,2) will also start getting updated, propagating the positive reward backward through the states. This is how the “path” to the goal is learned.</p> <hr> <h3 id="why-q-learning-is-so-powerful-and-its-limits">Why Q-Learning is So Powerful (and Its Limits)</h3> <p><strong>Advantages:</strong></p> <ul> <li> <strong>Model-Free</strong>: It doesn’t require knowing the environment’s dynamics, making it suitable for complex, unknown systems.</li> <li> <strong>Learns Optimal Policy</strong>: Given enough time and exploration, Q-Learning is guaranteed to find the optimal policy (the best sequence of actions) that maximizes cumulative reward for <strong>finite Markov Decision Processes (MDPs)</strong>.</li> <li> <strong>Simplicity</strong>: Conceptually, it’s quite intuitive and relatively straightforward to implement for smaller problems.</li> </ul> <p><strong>Limitations:</strong></p> <ul> <li> <strong>State-Space Explosion</strong>: The biggest challenge! If your environment has a huge number of states (e.g., a complex video game with millions of pixel combinations, or a robot navigating a continuous 3D space), storing a Q-table for every state-action pair becomes computationally impossible and requires immense memory. Imagine trying to store Q-values for every possible board configuration in chess or Go!</li> <li> <strong>Discrete States and Actions</strong>: Q-Learning, in its basic form, is designed for discrete states and actions. Dealing with continuous environments (like controlling a robot arm where angles and speeds can be any real number) is tricky.</li> <li> <strong>Slow Convergence</strong>: For very large Q-tables, even if memory isn’t an issue, it can take an incredibly long time for the agent to explore all states sufficiently and for the Q-values to converge.</li> </ul> <hr> <h3 id="beyond-the-q-table-a-glimpse-into-deep-q-learning-dqn">Beyond the Q-Table: A Glimpse into Deep Q-Learning (DQN)</h3> <p>The limitations of the Q-table, particularly the state-space explosion, paved the way for a revolutionary advancement: <strong>Deep Q-Networks (DQN)</strong>. Instead of storing Q-values in a table, DQN uses a <strong>neural network</strong> to <em>approximate</em> the Q-function.</p> <p>This means the neural network takes the state as input (e.g., raw pixels from a game screen) and outputs the Q-values for all possible actions. This allows the agent to generalize from seen states to unseen states, making it capable of handling incredibly complex, high-dimensional environments that were previously impossible for tabular Q-Learning. This bridge between Q-Learning and neural networks is what truly ignited the field of <strong>Deep Reinforcement Learning</strong> and led to AI breakthroughs like AlphaGo and self-driving cars.</p> <hr> <h3 id="conclusion-my-q-learning-takeaway">Conclusion: My Q-Learning Takeaway</h3> <p>Q-Learning, for me, was more than just an algorithm; it was a revelation. It showed me that complex intelligence and seemingly “smart” behavior can emerge from surprisingly simple rules and persistent trial-and-error. It’s a foundational concept that underpins much of what we see in modern AI agents learning to master games, control robots, or make strategic decisions.</p> <p>If you’re looking to dive deeper into the fascinating world of AI and machine learning, Q-Learning is an excellent starting point. It teaches you the core principles of an agent interacting with an environment, learning from rewards, and balancing exploration with exploitation. Try implementing a simple Q-Learning agent in a grid world or a simple game – the feeling of watching your agent learn and improve is incredibly rewarding.</p> <p>So, go forth, explore, and let your curiosity be your $\epsilon$-greedy policy! The world of Reinforcement Learning is vast and exciting, and Q-Learning is your first trusty map.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>