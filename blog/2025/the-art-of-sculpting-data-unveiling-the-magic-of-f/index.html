<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Art of Sculpting Data: Unveiling the Magic of Feature Engineering | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/the-art-of-sculpting-data-unveiling-the-magic-of-f/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Art of Sculpting Data: Unveiling the Magic of Feature Engineering</h1> <p class="post-meta"> Created on June 14, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/feature-engineering"> <i class="fa-solid fa-hashtag fa-sm"></i> Feature Engineering</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/data-preprocessing"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Preprocessing</a>   <a href="/blog/blog/tag/predictive-modeling"> <i class="fa-solid fa-hashtag fa-sm"></i> Predictive Modeling</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey there, fellow data adventurer!</p> <p>Have you ever looked at a breathtaking sculpture and wondered how the artist saw that intricate form within a block of raw stone? Or perhaps you’ve marvelled at a master chef turning humble ingredients into a gourmet feast? In the world of Data Science and Machine Learning, we have our own version of this artistry: it’s called <strong>Feature Engineering</strong>.</p> <p>It’s the unsung hero, the quiet powerhouse, the secret ingredient that often separates a mediocre machine learning model from an award-winning one. And today, we’re going to pull back the curtain and explore why it’s so incredibly vital, how we do it, and why every aspiring data scientist needs to master this craft.</p> <h3 id="the-raw-material-vs-the-masterpiece-what-is-feature-engineering">The Raw Material vs. The Masterpiece: What is Feature Engineering?</h3> <p>Imagine you’re trying to predict if a student will pass an exam based on their study habits. Your raw data might include things like:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">hours_slept</code> (e.g., 7)</li> <li> <code class="language-plaintext highlighter-rouge">hours_studied_per_day</code> (e.g., [1, 2, 0, 3, 1, 0, 2])</li> <li> <code class="language-plaintext highlighter-rouge">exam_date</code> (e.g., ‘2024-05-15’)</li> <li> <code class="language-plaintext highlighter-rouge">textbook_read_status</code> (e.g., ‘partially read’)</li> </ul> <p>Now, give this raw data directly to a machine learning model. How much “sense” will it make?</p> <ul> <li> <code class="language-plaintext highlighter-rouge">hours_slept</code>: The model can probably use this directly.</li> <li> <code class="language-plaintext highlighter-rouge">hours_studied_per_day</code>: This is a list for a <em>single</em> student. A model can’t easily ingest a list of varying length. It needs a single number.</li> <li> <code class="language-plaintext highlighter-rouge">exam_date</code>: What does ‘2024-05-15’ mean to a model? Is it the day of the week, the month, or something else?</li> <li> <code class="language-plaintext highlighter-rouge">textbook_read_status</code>: ‘partially read’ is text. Models prefer numbers.</li> </ul> <p>This is where Feature Engineering swoops in! It’s the process of <strong>transforming raw data into features that better represent the underlying problem to predictive models, thereby improving model accuracy.</strong></p> <p>Think of it as preparing your ingredients before you cook. You don’t just throw a whole potato into a stew; you peel it, chop it, maybe even par-boil it. Similarly, we don’t just dump raw data into our models. We clean it, shape it, and combine it in ways that highlight the important patterns.</p> <h3 id="why-is-it-so-important-the-garbage-in-garbage-out-principle">Why Is It So Important? The “Garbage In, Garbage Out” Principle</h3> <p>You’ve probably heard the phrase “Garbage In, Garbage Out.” It’s particularly true in machine learning. Even the most sophisticated algorithms, like deep neural networks, will struggle if the features you feed them don’t adequately capture the information needed to make a prediction.</p> <p>Here’s why good feature engineering is a game-changer:</p> <ol> <li> <strong>Improved Model Performance:</strong> This is the big one. Well-engineered features can drastically increase your model’s accuracy, precision, recall, or F1-score. Sometimes, a simpler model with great features outperforms a complex model with raw, unoptimized data.</li> <li> <strong>Better Model Interpretability:</strong> When you create features based on domain knowledge, the model’s decisions often become easier to understand. If you create a <code class="language-plaintext highlighter-rouge">study_consistency_score</code> feature, and the model highly values it, you immediately know why.</li> <li> <strong>Reduced Overfitting:</strong> Thoughtful feature engineering can sometimes lead to more robust models that generalize better to unseen data by simplifying complex relationships.</li> <li> <strong>Faster Training:</strong> Fewer, more informative features mean your model has less “noise” to sift through, often leading to quicker training times.</li> </ol> <h3 id="the-toolbox-of-a-feature-engineer-common-techniques">The Toolbox of a Feature Engineer: Common Techniques</h3> <p>So, how do we transform those raw ingredients? Let’s look at some common techniques:</p> <h4 id="1-numerical-features-the-art-of-nuance">1. Numerical Features: The Art of Nuance</h4> <p>Numerical data often needs shaping to reveal its true potential.</p> <ul> <li> <strong>Binning/Discretization:</strong> Converting a continuous numerical feature into categorical bins. <ul> <li> <em>Example:</em> Instead of <code class="language-plaintext highlighter-rouge">age</code> (e.g., 23, 45, 67), create <code class="language-plaintext highlighter-rouge">age_group</code> (e.g., ‘young’, ‘middle-aged’, ‘senior’). This can help capture non-linear relationships.</li> </ul> </li> <li> <strong>Transformations:</strong> Applying mathematical functions to change the distribution of a feature. <ul> <li> <em>Example:</em> <code class="language-plaintext highlighter-rouge">salary</code> might be heavily skewed. Applying a <code class="language-plaintext highlighter-rouge">log</code> transformation ($log(x)$ or $log(x+1)$ if $x$ can be zero) can make it more Gaussian-like, which helps some models perform better. Other common transforms include square root or reciprocal.</li> <li> <em>Mathematical Example:</em> If our data points for <code class="language-plaintext highlighter-rouge">income</code> are [1000, 2000, 5000, 100000], applying $log_{10}(x)$ yields [3, 3.3, 3.7, 5]. This compresses the larger values, making the distribution more uniform.</li> </ul> </li> <li> <strong>Scaling:</strong> Adjusting the range of numerical features. This is critical for algorithms sensitive to feature magnitudes (e.g., K-Nearest Neighbors, Support Vector Machines, Neural Networks). <ul> <li> <strong>Standardization (Z-score normalization):</strong> Transforms data to have a mean of 0 and standard deviation of 1. $x’ = (x - \mu) / \sigma$ where $\mu$ is the mean and $\sigma$ is the standard deviation.</li> <li> <strong>Normalization (Min-Max scaling):</strong> Scales data to a fixed range, usually 0 to 1. $x’ = (x - min) / (max - min)$</li> </ul> </li> <li> <strong>Interaction Features:</strong> Combining two or more features to create a new one that captures their interaction. <ul> <li> <em>Example:</em> If you have <code class="language-plaintext highlighter-rouge">length</code> and <code class="language-plaintext highlighter-rouge">width</code>, creating <code class="language-plaintext highlighter-rouge">area = length * width</code> might be highly predictive. For our student example, <code class="language-plaintext highlighter-rouge">total_study_hours = sum(hours_studied_per_day)</code> or <code class="language-plaintext highlighter-rouge">study_efficiency = total_study_hours / exam_difficulty</code>.</li> </ul> </li> </ul> <h4 id="2-categorical-features-giving-labels-a-voice">2. Categorical Features: Giving Labels a Voice</h4> <p>Categorical data, like ‘red’, ‘green’, ‘blue’, needs to be converted into a numerical format for most models.</p> <ul> <li> <strong>One-Hot Encoding:</strong> Creates a new binary (0 or 1) column for each unique category. <ul> <li> <em>Example:</em> <code class="language-plaintext highlighter-rouge">color</code> = ‘red’, ‘blue’, ‘green’ becomes: <code class="language-plaintext highlighter-rouge">color_red</code> (1 if red, 0 otherwise) <code class="language-plaintext highlighter-rouge">color_blue</code> (1 if blue, 0 otherwise) <code class="language-plaintext highlighter-rouge">color_green</code> (1 if green, 0 otherwise)</li> <li>This is ideal for nominal categories (no inherent order).</li> </ul> </li> <li> <strong>Label Encoding/Ordinal Encoding:</strong> Assigns a unique integer to each category. <ul> <li> <em>Example:</em> <code class="language-plaintext highlighter-rouge">size</code> = ‘small’, ‘medium’, ‘large’ could become 0, 1, 2.</li> <li>This is suitable for ordinal categories (where order matters). Be careful not to use it for nominal categories, as it might imply an artificial order to the model.</li> </ul> </li> <li> <strong>Target Encoding (Mean Encoding):</strong> Replaces a category with the mean of the target variable for that category. <ul> <li> <em>Example:</em> For a <code class="language-plaintext highlighter-rouge">city</code> feature, replace ‘New York’ with the average house price in New York. This can be powerful but requires careful handling to avoid data leakage (using target information from the validation/test set).</li> </ul> </li> </ul> <h4 id="3-date-and-time-features-unlocking-temporal-patterns">3. Date and Time Features: Unlocking Temporal Patterns</h4> <p>Dates and times are a goldmine for features, but models can’t understand ‘2024-05-15’ directly.</p> <ul> <li> <strong>Extracting Components:</strong> Break down dates into granular features. <ul> <li> <em>Example:</em> From <code class="language-plaintext highlighter-rouge">exam_date</code>, extract <code class="language-plaintext highlighter-rouge">year</code>, <code class="language-plaintext highlighter-rouge">month</code>, <code class="language-plaintext highlighter-rouge">day_of_week</code>, <code class="language-plaintext highlighter-rouge">day_of_year</code>, <code class="language-plaintext highlighter-rouge">quarter</code>, <code class="language-plaintext highlighter-rouge">is_weekend</code>, <code class="language-plaintext highlighter-rouge">hour_of_day</code>.</li> </ul> </li> <li> <strong>Cyclical Features:</strong> For features like <code class="language-plaintext highlighter-rouge">hour_of_day</code> or <code class="language-plaintext highlighter-rouge">month_of_year</code>, there’s a cyclical nature (23:00 is close to 00:00). We can represent this using sine and cosine transformations. <ul> <li> <em>Example:</em> For <code class="language-plaintext highlighter-rouge">day_of_year</code> (1-365): <code class="language-plaintext highlighter-rouge">day_of_year_sin = sin(2 * pi * day_of_year / 365)</code> <code class="language-plaintext highlighter-rouge">day_of_year_cos = cos(2 * pi * day_of_year / 365)</code> </li> <li>This helps models understand that January 1st and December 31st are closer than January 1st and July 1st.</li> </ul> </li> <li> <strong>Time Differences:</strong> Calculate duration or elapsed time between events. <ul> <li> <em>Example:</em> <code class="language-plaintext highlighter-rouge">days_since_last_purchase</code>, <code class="language-plaintext highlighter-rouge">time_until_exam</code>.</li> </ul> </li> </ul> <h4 id="4-text-features-decoding-language">4. Text Features: Decoding Language</h4> <p>Text is arguably the most complex data type, but feature engineering helps us make sense of it.</p> <ul> <li> <strong>Bag-of-Words (BoW):</strong> Counts the occurrences of each word in a document.</li> <li> <strong>TF-IDF (Term Frequency-Inverse Document Frequency):</strong> Weighs words based on how often they appear in a document relative to how often they appear in the entire corpus. This helps identify important words unique to a document.</li> <li> <strong>Word Embeddings:</strong> (More advanced, but worth knowing!) Transforms words into dense numerical vectors that capture semantic meaning. Words with similar meanings have similar vector representations.</li> </ul> <h3 id="the-feature-engineering-journey-its-iterative">The Feature Engineering Journey: It’s Iterative!</h3> <p>Feature engineering isn’t a one-and-done step; it’s an ongoing, iterative process that’s deeply intertwined with Exploratory Data Analysis (EDA) and model building:</p> <ol> <li> <strong>Understand the Problem &amp; Data:</strong> What are you trying to predict? What does your data represent? This is where your domain knowledge shines.</li> <li> <strong>Exploratory Data Analysis (EDA):</strong> Visualize your data! Look for patterns, distributions, outliers, and relationships between features and your target variable. This will spark ideas for new features.</li> <li> <strong>Brainstorm &amp; Create Features:</strong> Based on your understanding and EDA, hypothesize potential features and implement them.</li> <li> <strong>Model Training &amp; Evaluation:</strong> Train your model with the new features and evaluate its performance. Did it improve?</li> <li> <strong>Feature Selection/Importance:</strong> Which features are most impactful? Are some redundant? Can we remove less important ones to simplify the model?</li> <li> <strong>Refine &amp; Iterate:</strong> Go back to step 1 or 2. Can you combine features differently? Are there other aspects of the raw data you haven’t explored?</li> </ol> <p>This cycle of exploration, creation, evaluation, and refinement is the heart of effective feature engineering.</p> <h3 id="your-superpower-domain-knowledge">Your Superpower: Domain Knowledge</h3> <p>While there are many generic techniques, your ultimate superpower in feature engineering is <strong>domain knowledge</strong>. Understanding the context of your data – whether it’s student performance, housing prices, or customer behavior – allows you to invent truly insightful features that generic approaches might miss.</p> <ul> <li> <em>Example (student data):</em> Knowing that <code class="language-plaintext highlighter-rouge">sleep_deprivation_index = (hours_slept &lt; 6) * 1</code> or <code class="language-plaintext highlighter-rouge">study_to_rest_ratio = total_study_hours / total_rest_hours</code> might be a powerful predictor. A computer wouldn’t come up with this on its own.</li> </ul> <h3 id="tools-of-the-trade">Tools of the Trade</h3> <p>You don’t need fancy software to do feature engineering. Your primary tools will be:</p> <ul> <li> <strong>Pandas:</strong> The go-to library for data manipulation in Python. Perfect for creating new columns, applying functions, and transforming dataframes.</li> <li> <strong>NumPy:</strong> For powerful numerical operations, especially when dealing with arrays.</li> <li> <strong>Scikit-learn (sklearn.preprocessing):</strong> Offers a fantastic suite of pre-built transformers for scaling, encoding, and other common tasks (e.g., <code class="language-plaintext highlighter-rouge">StandardScaler</code>, <code class="language-plaintext highlighter-rouge">MinMaxScaler</code>, <code class="language-plaintext highlighter-rouge">OneHotEncoder</code>, <code class="language-plaintext highlighter-rouge">LabelEncoder</code>).</li> </ul> <h3 id="a-few-best-practices">A Few Best Practices</h3> <ul> <li> <strong>Start Simple:</strong> Don’t over-engineer from the start. Build a baseline model with basic features, then add complexity incrementally.</li> <li> <strong>Beware of Data Leakage:</strong> Ensure your feature engineering process doesn’t inadvertently use information from your test set during training. For instance, when using target encoding, calculate the mean only on the training data.</li> <li> <strong>Document Everything:</strong> Keep track of the features you create and why you created them. Your future self (and your team) will thank you!</li> <li> <strong>Experiment Fearlessly:</strong> Feature engineering is an art. There’s no single “right” way. Try different transformations, combinations, and encodings.</li> </ul> <h3 id="conclusion-the-unsung-hero-of-data-science">Conclusion: The Unsung Hero of Data Science</h3> <p>Feature Engineering is more than just data preprocessing; it’s where the art and science of data collide. It’s about looking at raw numbers and text, and seeing the hidden stories, the crucial relationships, and the predictive power waiting to be unleashed.</p> <p>It requires creativity, domain expertise, and a willingness to iterate and experiment. It’s often the most time-consuming part of a machine learning project, but it’s also where you can make the biggest impact.</p> <p>So, as you embark on your data science journey, remember the sculptor and the chef. Don’t just hand your model raw ingredients; carefully prepare, shape, and transform them. Master the art of Feature Engineering, and you’ll unlock the true potential of your data and your models. Happy sculpting!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>