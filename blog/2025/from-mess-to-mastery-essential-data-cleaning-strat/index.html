<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> From Mess to Mastery: Essential Data Cleaning Strategies for Aspiring Data Scientists | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/2025/from-mess-to-mastery-essential-data-cleaning-strat/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">From Mess to Mastery: Essential Data Cleaning Strategies for Aspiring Data Scientists</h1> <p class="post-meta"> Created on October 16, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/data-cleaning"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Cleaning</a>   <a href="/blog/blog/tag/data-preprocessing"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Preprocessing</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/python"> <i class="fa-solid fa-hashtag fa-sm"></i> Python</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Ever heard the phrase, “Garbage In, Garbage Out”? It’s the unspoken mantra of every data scientist, and it’s particularly true when it comes to the raw, untamed data we encounter in the real world. Imagine trying to bake a cake with spoiled ingredients or build a sturdy house with crooked bricks. The result? A disaster. In data science, messy data is those spoiled ingredients and crooked bricks.</p> <p>This isn’t a secret held by seasoned professionals; it’s a fundamental truth I learned early in my journey. No matter how sophisticated your machine learning algorithm, its performance is fundamentally limited by the quality of the data it’s fed. That’s why data cleaning—the process of detecting and correcting (or removing) corrupt or inaccurate records from a dataset—is arguably the most critical, yet often least glamorous, step in the entire data science pipeline. It’s often cited as taking up 60-80% of a data scientist’s time, and for good reason!</p> <p>In this post, I want to share my go-to strategies for tackling the common data cleaning challenges. Think of this as your practical guide to transforming chaotic datasets into sparkling, model-ready gold.</p> <h3 id="the-data-detectives-mindset-before-you-clean-investigate">The Data Detective’s Mindset: Before You Clean, Investigate!</h3> <p>Before you even <em>think</em> about cleaning, you need to understand your data. This is where you become a data detective. What does each column represent? What’s the expected range of values? Are there relationships between features?</p> <p>Tools like <code class="language-plaintext highlighter-rouge">df.info()</code>, <code class="language-plaintext highlighter-rouge">df.describe()</code>, and <code class="language-plaintext highlighter-rouge">df.value_counts()</code> are your magnifying glass and fingerprint kit. They reveal data types, statistical summaries, and the distribution of values, which are crucial for spotting anomalies. Visualizations like histograms, box plots, and scatter plots are also incredibly powerful for surfacing issues that numbers alone might hide. Embrace this exploratory phase; it will save you hours of pain later.</p> <p>Let’s dive into some common cleaning scenarios and the strategies to conquer them.</p> <h3 id="strategy-1-taming-the-missing-values-beast">Strategy 1: Taming the Missing Values Beast</h3> <p>Missing data is perhaps the most common adversary. It’s like having blank spaces in a puzzle. If you just leave them, your picture will be incomplete.</p> <p><strong>How to Spot It:</strong> Using libraries like Pandas, it’s straightforward:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="c1"># Assuming 'df' is your DataFrame
</span><span class="nf">print</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="nf">isnull</span><span class="p">().</span><span class="nf">sum</span><span class="p">())</span> <span class="c1"># Counts missing values per column
</span></code></pre></div></div> <p>This will give you a quick overview of how many <code class="language-plaintext highlighter-rouge">NaN</code> (Not a Number) or <code class="language-plaintext highlighter-rouge">None</code> values are lurking in each column.</p> <p><strong>Understanding Why (And Why It Matters):</strong> Missing data isn’t always random. Sometimes a value is missing because it truly doesn’t apply (e.g., “number of children” for an unmarried person), or because of a data entry error, or even a system failure. The <em>reason</em> for missingness often guides your cleaning strategy:</p> <ul> <li> <strong>Missing Completely At Random (MCAR):</strong> The missingness isn’t related to any other variable or the variable itself. If a sensor randomly fails, that’s MCAR.</li> <li> <strong>Missing At Random (MAR):</strong> Missingness is related to <em>other observed variables</em> but not the variable itself. For example, men might be less likely to fill out a certain survey question than women.</li> <li> <strong>Missing Not At Random (MNAR):</strong> Missingness is related to the value of the variable itself, even if that value is unobserved. For instance, people with very high incomes might be less likely to report their income.</li> </ul> <p><strong>Tactics for Handling Missing Values:</strong></p> <ol> <li> <strong>Deletion:</strong> <ul> <li> <strong>Row-wise deletion (<code class="language-plaintext highlighter-rouge">df.dropna()</code>):</strong> If a row has <em>any</em> missing values, remove the entire row. This is simple but can lead to significant data loss if many rows have even one missing value. Use this if the number of missing rows is small (e.g., &lt;5% of your data) and the rows aren’t critical.</li> <li> <strong>Column-wise deletion (<code class="language-plaintext highlighter-rouge">df.drop()</code>):</strong> If a column has too many missing values (e.g., &gt;70-80%), it might be better to drop the entire column. It likely won’t provide useful information anyway.</li> </ul> </li> <li> <p><strong>Imputation (Filling in the Blanks):</strong> This is often preferred as it preserves more data.</p> <ul> <li> <strong>Mean/Median/Mode Imputation:</strong> <ul> <li> <strong>Mean:</strong> For numerical features, replace <code class="language-plaintext highlighter-rouge">NaN</code> with the average value. <code class="language-plaintext highlighter-rouge">$ \bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i $</code>. Good for normally distributed data.</li> <li> <strong>Median:</strong> For numerical features, replace <code class="language-plaintext highlighter-rouge">NaN</code> with the middle value. More robust to outliers than the mean.</li> <li> <strong>Mode:</strong> For categorical features, replace <code class="language-plaintext highlighter-rouge">NaN</code> with the most frequent value. <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">numerical_column</span><span class="sh">'</span><span class="p">].</span><span class="nf">fillna</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">numerical_column</span><span class="sh">'</span><span class="p">].</span><span class="nf">mean</span><span class="p">(),</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">categorical_column</span><span class="sh">'</span><span class="p">].</span><span class="nf">fillna</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">categorical_column</span><span class="sh">'</span><span class="p">].</span><span class="nf">mode</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div> </div> </li> </ul> </li> <li> <strong>Forward-Fill/Backward-Fill (<code class="language-plaintext highlighter-rouge">ffill</code>/<code class="language-plaintext highlighter-rouge">bfill</code>):</strong> Especially useful for time-series data. <code class="language-plaintext highlighter-rouge">ffill</code> propagates the last valid observation forward, while <code class="language-plaintext highlighter-rouge">bfill</code> propagates the next valid observation backward.</li> <li> <strong>Interpolation:</strong> More sophisticated. It estimates missing values based on surrounding valid points. Linear interpolation is common. <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">numerical_column</span><span class="sh">'</span><span class="p">].</span><span class="nf">interpolate</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="sh">'</span><span class="s">linear</span><span class="sh">'</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div> </div> </li> <li> <strong>Model-based Imputation (Advanced):</strong> Using a machine learning model (e.g., K-Nearest Neighbors, MICE) to predict missing values based on other features in the dataset. This is powerful but more complex.</li> </ul> </li> </ol> <p>My advice: Start simple. Visualize your data. If mean/median seems reasonable, go for it. If not, explore more advanced methods.</p> <h3 id="strategy-2-decluttering-with-duplicate-records">Strategy 2: Decluttering with Duplicate Records</h3> <p>Duplicate rows are redundant and can skew your analysis or lead to overfitting in models, making your data appear more robust than it truly is.</p> <p><strong>How to Spot It:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="nf">duplicated</span><span class="p">().</span><span class="nf">sum</span><span class="p">())</span> <span class="c1"># Counts exact duplicate rows
</span></code></pre></div></div> <p><strong>Understanding Why:</strong> Duplicates often arise from data entry errors, combining datasets from different sources, or issues during data extraction.</p> <p><strong>Tactics for Handling Duplicates:</strong></p> <ol> <li> <strong>Removing Exact Duplicates:</strong> This is the simplest form. Pandas can remove entire rows that are identical across all columns. <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">.</span><span class="nf">drop_duplicates</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div> </div> <p>You can also specify a subset of columns to consider for uniqueness. For example, if you know <code class="language-plaintext highlighter-rouge">customer_id</code> and <code class="language-plaintext highlighter-rouge">order_id</code> together should be unique:</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">.</span><span class="nf">drop_duplicates</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">customer_id</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">order_id</span><span class="sh">'</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div> </div> </li> <li> <strong>Handling “Fuzzy” Duplicates:</strong> Sometimes records aren’t <em>exactly</em> the same but refer to the same entity (e.g., “New York” vs. “NY”). This is where string matching algorithms (like Levenshtein distance) come in handy to identify similar-looking strings. This is more of an advanced technique often requiring custom code.</li> </ol> <p>Always remove duplicates <em>after</em> handling missing values and inconsistencies, as these can make identical records appear different.</p> <h3 id="strategy-3-standardizing-inconsistent-data-and-correcting-typos">Strategy 3: Standardizing Inconsistent Data and Correcting Typos</h3> <p>Inconsistent data, especially in categorical features, can create many unique categories where there should be only a few. This leads to poor analysis and inefficient model training.</p> <p><strong>How to Spot It:</strong> The <code class="language-plaintext highlighter-rouge">value_counts()</code> method is your best friend here.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">city</span><span class="sh">'</span><span class="p">].</span><span class="nf">value_counts</span><span class="p">())</span>
</code></pre></div></div> <p>You might see entries like “New York”, “new york”, “NY”, “NewYork”, which all refer to the same city.</p> <p><strong>Understanding Why:</strong> Human error during data entry, lack of validation, or merging datasets with different naming conventions are common culprits.</p> <p><strong>Tactics for Standardization:</strong></p> <ol> <li> <strong>Case Normalization:</strong> Convert all text to a consistent case (e.g., lowercase or uppercase). <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">city</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">city</span><span class="sh">'</span><span class="p">].</span><span class="nb">str</span><span class="p">.</span><span class="nf">lower</span><span class="p">()</span>
</code></pre></div> </div> </li> <li> <strong>Spelling Correction and Aliases:</strong> Use <code class="language-plaintext highlighter-rouge">replace()</code> or <code class="language-plaintext highlighter-rouge">map()</code> to standardize variations. <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">city</span><span class="sh">'</span><span class="p">].</span><span class="nf">replace</span><span class="p">({</span><span class="sh">'</span><span class="s">ny</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">new york</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">newyork</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">new york</span><span class="sh">'</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="c1"># Or, for more entries:
</span><span class="n">mapping</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">ny</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">new york</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">newyork</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">new york</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">la</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">los angeles</span><span class="sh">'</span><span class="p">}</span>
<span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">city</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">city</span><span class="sh">'</span><span class="p">].</span><span class="nf">replace</span><span class="p">(</span><span class="n">mapping</span><span class="p">)</span>
</code></pre></div> </div> </li> <li> <strong>Removing Leading/Trailing Spaces:</strong> Whitespace can make identical strings appear different. <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">city</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">city</span><span class="sh">'</span><span class="p">].</span><span class="nb">str</span><span class="p">.</span><span class="nf">strip</span><span class="p">()</span>
</code></pre></div> </div> </li> <li> <strong>Regular Expressions:</strong> For more complex pattern matching and extraction.</li> <li> <strong>Data Type Conversion:</strong> Ensure columns have the correct data types. Numbers stored as strings, or dates as general objects, can lead to incorrect calculations or errors. <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">numeric_col</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">to_numeric</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">numeric_col</span><span class="sh">'</span><span class="p">],</span> <span class="n">errors</span><span class="o">=</span><span class="sh">'</span><span class="s">coerce</span><span class="sh">'</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">date_col</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">date_col</span><span class="sh">'</span><span class="p">],</span> <span class="n">errors</span><span class="o">=</span><span class="sh">'</span><span class="s">coerce</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div> </div> <p>The <code class="language-plaintext highlighter-rouge">errors='coerce'</code> argument is a lifesaver, as it will turn any values it can’t convert into <code class="language-plaintext highlighter-rouge">NaN</code>, which you can then handle with your missing value strategies.</p> </li> </ol> <h3 id="strategy-4-conquering-outliers">Strategy 4: Conquering Outliers</h3> <p>Outliers are data points that significantly deviate from other observations. They can drastically skew statistical analyses and impact model performance, especially for algorithms sensitive to distances (like K-Means, Linear Regression).</p> <p><strong>How to Spot It:</strong></p> <ol> <li> <strong>Visualizations:</strong> <ul> <li> <strong>Box Plots:</strong> Clearly show the median, quartiles, and points outside the “whiskers” as potential outliers.</li> <li> <strong>Histograms/Distribution Plots:</strong> Can reveal extreme values far from the main bulk of data.</li> <li> <strong>Scatter Plots:</strong> For multivariate analysis, outliers can appear as points far from the general cluster of other points.</li> </ul> </li> <li> <strong>Statistical Methods:</strong> <ul> <li> <strong>IQR (Interquartile Range) Method:</strong> A robust way to define a range for “normal” data. <ul> <li>Calculate the first quartile ($Q_1$) and third quartile ($Q_3$).</li> <li>Compute the IQR: $IQR = Q_3 - Q_1$</li> <li>Define bounds: <ul> <li>Lower Bound = $Q_1 - 1.5 \times IQR$</li> <li>Upper Bound = $Q_3 + 1.5 \times IQR$</li> </ul> </li> <li>Any data point outside these bounds is considered an outlier.</li> </ul> </li> <li> <strong>Z-score:</strong> Measures how many standard deviations a data point is from the mean. <ul> <li>$Z = \frac{x - \mu}{\sigma}$</li> <li>Where $x$ is the data point, $\mu$ is the mean, and $\sigma$ is the standard deviation.</li> <li> <table> <tbody> <tr> <td>A common threshold for outliers is $</td> <td>Z</td> <td>&gt; 2$ or $</td> <td>Z</td> <td>&gt; 3$. This method assumes your data is normally distributed.</td> </tr> </tbody> </table> </li> </ul> </li> </ul> </li> </ol> <p><strong>Understanding Why:</strong> Outliers can be:</p> <ul> <li> <strong>Errors:</strong> Data entry mistakes, measurement errors (e.g., typing “1000” instead of “100”).</li> <li> <strong>Natural Variation:</strong> A truly rare but legitimate observation (e.g., a billionaire in a salary dataset).</li> <li> <strong>Novelty/Anomaly:</strong> An unusual event or behavior you might be interested in detecting.</li> </ul> <p>It’s crucial to investigate outliers with domain knowledge. Is it an error, or is it a valid extreme value? This decision impacts your next step.</p> <p><strong>Tactics for Handling Outliers:</strong></p> <ol> <li> <p><strong>Removal:</strong> If an outlier is clearly an error and doesn’t represent true data, and there are very few of them, you can safely remove the corresponding rows. Be cautious not to remove too much data.</p> </li> <li> <strong>Transformation:</strong> <ul> <li> <strong>Log Transformation:</strong> For right-skewed data, taking the natural logarithm ($ \ln(x) $) or base-10 logarithm ($ \log_{10}(x) $) can compress the range of values, bringing outliers closer to the distribution.</li> <li> <strong>Square Root Transformation:</strong> Similar to log transformation but less aggressive.</li> <li>These are especially useful if your model assumes normally distributed errors.</li> </ul> </li> <li> <strong>Capping/Winsorization:</strong> Instead of removing outliers, you can “cap” them. This involves replacing values beyond a certain percentile (e.g., 95th percentile) with the value at that percentile. Similarly, values below the 5th percentile might be replaced with the 5th percentile value. <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="n">Q1</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">column</span><span class="sh">'</span><span class="p">].</span><span class="nf">quantile</span><span class="p">(</span><span class="mf">0.25</span><span class="p">)</span>
<span class="n">Q3</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">column</span><span class="sh">'</span><span class="p">].</span><span class="nf">quantile</span><span class="p">(</span><span class="mf">0.75</span><span class="p">)</span>
<span class="n">IQR</span> <span class="o">=</span> <span class="n">Q3</span> <span class="o">-</span> <span class="n">Q1</span>
<span class="n">lower_bound</span> <span class="o">=</span> <span class="n">Q1</span> <span class="o">-</span> <span class="mf">1.5</span> <span class="o">*</span> <span class="n">IQR</span>
<span class="n">upper_bound</span> <span class="o">=</span> <span class="n">Q3</span> <span class="o">+</span> <span class="mf">1.5</span> <span class="o">*</span> <span class="n">IQR</span>

<span class="c1"># Cap outliers
</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">column</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">column</span><span class="sh">'</span><span class="p">].</span><span class="nf">clip</span><span class="p">(</span><span class="n">lower</span><span class="o">=</span><span class="n">lower_bound</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="n">upper_bound</span><span class="p">)</span>
</code></pre></div> </div> </li> <li> <p><strong>Treating as Missing Values:</strong> If you’re unsure if an outlier is an error or a valid extreme, you can convert it to <code class="language-plaintext highlighter-rouge">NaN</code> and then use your chosen imputation strategy.</p> </li> <li> <strong>Using Robust Models:</strong> Some machine learning models (like tree-based models such as Decision Trees, Random Forests, Gradient Boosting Machines) are naturally less sensitive to outliers because they partition data based on thresholds rather than continuous values or distances.</li> </ol> <h3 id="strategy-5-feature-scaling-a-quick-mention">Strategy 5: Feature Scaling (A Quick Mention)</h3> <p>While strictly a preprocessing step often done <em>after</em> basic cleaning, feature scaling is vital for many ML algorithms. It standardizes the range of independent variables or features.</p> <ul> <li> <strong>Min-Max Scaling (Normalization):</strong> Scales values to a fixed range, usually 0 to 1. $ X_{scaled} = \frac{X - X_{min}}{X_{max} - X_{min}} $</li> <li> <strong>Standardization (Z-score Scaling):</strong> Transforms data to have a mean of 0 and a standard deviation of 1. $ X_{scaled} = \frac{X - \mu}{\sigma} $ This is critical for algorithms that use distance metrics (like K-NN, SVM) or gradient descent (like Linear Regression, Neural Networks) to prevent features with larger scales from dominating.</li> </ul> <h3 id="best-practices-and-the-iterative-nature-of-cleaning">Best Practices and The Iterative Nature of Cleaning</h3> <p>Data cleaning is rarely a one-shot process. It’s iterative, requiring you to go back and forth between exploration, cleaning, and re-evaluation.</p> <ol> <li> <strong>Document Everything:</strong> Keep a clear record of the cleaning steps you’ve taken. Use comments in your code. A clean script is a reproducible script.</li> <li> <strong>Keep Original Data:</strong> Always work on a copy of your dataset (<code class="language-plaintext highlighter-rouge">df_cleaned = df.copy()</code>). Never overwrite your original raw data.</li> <li> <strong>Visualize, Visualize, Visualize:</strong> After each major cleaning step, visualize your data again. Did the changes have the intended effect? Did you inadvertently introduce new problems?</li> <li> <strong>Leverage Domain Knowledge:</strong> Talk to experts who understand the data. Their insights can be invaluable in deciding whether a value is an outlier or a legitimate data point, or what an appropriate imputation strategy might be.</li> <li> <strong>Automation vs. Manual:</strong> While small datasets might allow for some manual fixes, strive to automate your cleaning process as much as possible, especially for recurring tasks.</li> </ol> <h3 id="conclusion-embrace-the-mess-build-a-better-model">Conclusion: Embrace the Mess, Build a Better Model</h3> <p>Data cleaning might not be the most glamorous part of data science, but it’s where the rubber meets the road. It’s the gritty, essential work that transforms raw, unreliable information into a solid foundation for robust analysis and powerful machine learning models.</p> <p>By mastering these strategies—handling missing values, de-duplicating, standardizing inconsistencies, and intelligently managing outliers—you’re not just tidying up; you’re developing a critical skill that will empower you to tackle almost any real-world dataset. So, go forth, embrace the mess, and build better, more reliable models! Your algorithms (and your future insights) will thank you.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>