<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> My Journey into the Data Vortex: Taming High Dimensions with Dimensionality Reduction | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/my-journey-into-the-data-vortex-taming-high-dimens/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">My Journey into the Data Vortex: Taming High Dimensions with Dimensionality Reduction</h1> <p class="post-meta"> Created on March 17, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/dimensionality-reduction"> <i class="fa-solid fa-hashtag fa-sm"></i> Dimensionality Reduction</a>   <a href="/blog/blog/tag/pca"> <i class="fa-solid fa-hashtag fa-sm"></i> PCA</a>   <a href="/blog/blog/tag/t-sne"> <i class="fa-solid fa-hashtag fa-sm"></i> t-SNE</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>If you’re anything like me, when you first started diving deep into the world of data science, you probably felt that exhilarating rush of possibility. So much data, so many insights waiting to be discovered! But then, you hit a wall. A very, very tall wall made of… <em>too many features</em>.</p> <p>I remember one of my first real-world datasets for a classification task. It had over 500 features – everything from user demographics to intricate behavioral patterns. My laptop groaned, my models took ages to train, and trying to visualize anything beyond a simple scatter plot felt like trying to draw a portrait blindfolded. That’s when I realized I was staring down the barrel of what data scientists lovingly (or perhaps fearfully) call the “<strong>Curse of Dimensionality</strong>.”</p> <p>This isn’t some ancient data science folklore; it’s a very real problem. When your dataset has a high number of features (or “dimensions”), things get complicated, fast. Data points become incredibly sparse, meaning they’re all super far apart, making it tough for algorithms to find meaningful patterns. Imagine trying to find a specific grain of sand in a vast desert versus a small sandbox – the desert is your high-dimensional space. High dimensionality leads to:</p> <ul> <li> <strong>Computational Overload</strong>: More features mean more calculations, longer training times, and more memory consumption.</li> <li> <strong>Overfitting</strong>: Models can get confused by noise and irrelevant features, learning the quirks of your training data rather than true underlying patterns. They become specialists, not generalists.</li> <li> <strong>Poor Visualization</strong>: Our brains (and our screens) are limited to 2D or 3D. How do you visualize a 100-dimensional dataset? You can’t, directly.</li> <li> <strong>Increased Storage Needs</strong>: More features mean larger files, which might not be a huge issue for smaller datasets, but scales up quickly.</li> </ul> <p>So, what’s a budding data scientist to do? Enter my hero: <strong>Dimensionality Reduction</strong>.</p> <h2 id="what-is-dimensionality-reduction-really">What is Dimensionality Reduction, Really?</h2> <p>At its core, dimensionality reduction is about transforming data from a high-dimensional space into a low-dimensional space while trying to retain as much meaningful information as possible. It’s not just about throwing away data; it’s about finding the <em>essence</em> of your data. Think of it like summarizing a very long book into a few key chapters, or even a single compelling sentence, without losing the main plot.</p> <p>There are broadly two categories of dimensionality reduction:</p> <ol> <li> <strong>Feature Selection</strong>: This is like carefully picking the most important original features from your dataset and discarding the rest. “Which 5 questions are most important to describe this person?”</li> <li> <strong>Feature Extraction</strong>: This is where the real magic often happens. Instead of just picking original features, we <em>create new, synthetic features</em> that are combinations or transformations of the original ones. “Can we invent 5 new concepts that capture everything important about this person, even if those concepts weren’t directly measured?” This is what we’ll focus on today!</li> </ol> <h2 id="principal-component-analysis-pca-the-grandmaster-of-linear-transformation">Principal Component Analysis (PCA): The Grandmaster of Linear Transformation</h2> <p>When I first wrapped my head around PCA, it felt like unlocking a secret level in data analysis. <strong>Principal Component Analysis (PCA)</strong> is arguably the most famous and widely used dimensionality reduction technique. It’s a linear method, meaning it looks for straight-line relationships in your data.</p> <h3 id="the-intuition-behind-pca-casting-the-best-shadow">The Intuition Behind PCA: Casting the Best Shadow</h3> <p>Imagine you have a 3D object, like a complex sculpture, and you want to understand its shape by looking at its shadow. If you shine a light from directly above, you might get a shadow that looks like a flat circle or square – not very informative. But if you carefully choose the angle of your light, you can cast a shadow that captures the <em>most variation</em> or “spread” of the object, revealing its contours and details.</p> <p>PCA does something similar. It finds new, orthogonal (at 90 degrees to each other) directions in your data, called <strong>Principal Components (PCs)</strong>. These PCs are chosen sequentially:</p> <ol> <li>The <strong>First Principal Component</strong> captures the largest possible variance in the data. It’s the direction where your data is most spread out.</li> <li>The <strong>Second Principal Component</strong> is orthogonal to the first and captures the next largest amount of remaining variance.</li> <li>And so on, until you have as many principal components as your original dimensions.</li> </ol> <p>The beauty is, you typically only need the first few principal components to capture a significant portion of the total variance, effectively reducing your dimensions.</p> <h3 id="how-pca-works-a-peek-under-the-hood">How PCA Works (A Peek Under the Hood):</h3> <p>Let’s get a <em>little</em> mathematical, but don’t worry, we’ll keep it high-level!</p> <ol> <li> <strong>Standardize the Data</strong>: First, we scale our data so each feature has a mean of 0 and a standard deviation of 1. This prevents features with larger ranges from dominating the analysis.</li> <li> <strong>Compute the Covariance Matrix ($\Sigma$)</strong>: This matrix tells us how much each pair of features varies together. A positive covariance means they tend to increase/decrease together, while a negative covariance means one increases as the other decreases. \(\Sigma = \frac{1}{n-1}(X - \bar{X})^T(X - \bar{X})\) Where $X$ is your data matrix, $\bar{X}$ is the mean vector for each feature, and $n$ is the number of data points.</li> <li> <strong>Calculate Eigenvalues and Eigenvectors</strong>: This is the heart of PCA. We find the eigenvectors and corresponding eigenvalues of the covariance matrix. \(\Sigma v = \lambda v\) <ul> <li> <strong>Eigenvectors ($v$)</strong>: These are our principal components. They are the new directions (axes) in our feature space. They tell us <em>where</em> the data varies most.</li> <li> <strong>Eigenvalues ($\lambda$)</strong>: Each eigenvalue corresponds to an eigenvector and represents the magnitude of variance captured along that eigenvector. A larger eigenvalue means that its corresponding principal component captures more “information” or variance.</li> </ul> </li> <li> <strong>Sort and Select</strong>: We sort the eigenvectors by their eigenvalues in descending order. The eigenvector with the largest eigenvalue is PC1, the next largest is PC2, and so on. We then choose the top $k$ eigenvectors (the ones with the largest eigenvalues) to form a “projection matrix.”</li> <li> <strong>Project Data</strong>: Finally, we project our original standardized data onto these $k$ principal components. The result is a new dataset with $k$ dimensions, where each new dimension is a principal component.</li> </ol> <h3 id="benefits-of-pca">Benefits of PCA:</h3> <ul> <li> <strong>Computational Efficiency</strong>: Faster model training.</li> <li> <strong>Noise Reduction</strong>: By focusing on directions of high variance, PCA can sometimes filter out noisy features that contribute less to the overall data structure.</li> <li> <strong>Reduced Multicollinearity</strong>: Principal components are orthogonal, meaning they are uncorrelated, which can be beneficial for certain models.</li> <li> <strong>Visualization</strong>: Crucial for visualizing high-dimensional data in 2D or 3D.</li> </ul> <h3 id="limitations-of-pca">Limitations of PCA:</h3> <ul> <li> <strong>Linearity</strong>: PCA assumes linear relationships in your data. If your data has complex, non-linear patterns, PCA might miss them.</li> <li> <strong>Interpretability</strong>: The new principal components are linear combinations of original features, making them harder to interpret directly compared to original features. “What does PC1 <em>mean</em>?” can be a tough question!</li> </ul> <h2 id="beyond-linearity-t-sne-and-umap-for-the-visually-inclined">Beyond Linearity: t-SNE and UMAP for the Visually Inclined</h2> <p>While PCA is a powerful workhorse for general dimensionality reduction and preprocessing, sometimes our data’s true story isn’t linear. For exploring complex, non-linear relationships, especially when our goal is visualization, we turn to other techniques.</p> <h3 id="t-sne-t-distributed-stochastic-neighbor-embedding-the-cluster-whisperer">t-SNE (t-Distributed Stochastic Neighbor Embedding): The Cluster Whisperer</h3> <p><strong>t-SNE</strong> is a non-linear dimensionality reduction technique primarily used for <strong>visualization</strong>. When I first saw t-SNE plots, it felt like magic – suddenly, distinct clusters appeared from what was once an undifferentiated blob of points!</p> <p>The core idea of t-SNE is to preserve <em>local</em> structure. It aims to map high-dimensional points into a low-dimensional space (typically 2D or 3D) such that points that were close together in the high-dimensional space remain close together, and points that were far apart remain far apart.</p> <p>Think of it this way: imagine crumpling a piece of paper (your high-dimensional data) and then trying to flatten it out (your low-dimensional representation). t-SNE tries to do this in a way that preserves the neighborhoods – if two points were neighbors on the crumpled paper, they should ideally still be neighbors after flattening. It’s like finding a map of a city where neighborhoods are preserved, even if the overall shape of the city changes drastically.</p> <p><strong>Pros of t-SNE</strong>:</p> <ul> <li>Excellent for revealing clusters and separating complex, non-linear data structures for visualization.</li> <li>Produces visually appealing and interpretable plots.</li> </ul> <p><strong>Cons of t-SNE</strong>:</p> <ul> <li>Computationally expensive, especially for very large datasets (can take a long time).</li> <li>Stochastic (randomized), meaning different runs might produce slightly different arrangements of clusters (though the clusters themselves should remain).</li> <li>Doesn’t preserve <em>global</em> structure as well as UMAP. The absolute distances between clusters in a t-SNE plot don’t necessarily reflect actual high-dimensional distances.</li> </ul> <h3 id="umap-uniform-manifold-approximation-and-projection-the-new-kid-on-the-block">UMAP (Uniform Manifold Approximation and Projection): The New Kid on the Block</h3> <p><strong>UMAP</strong> is a relatively newer technique that often feels like t-SNE’s faster, more robust cousin. It shares a similar goal: non-linear dimensionality reduction for visualization, aiming to preserve both local and global data structure.</p> <p>UMAP builds on manifold learning, essentially assuming that high-dimensional data lies on a lower-dimensional “manifold” (a fancy word for a curved surface) within that high-dimensional space. It constructs a graph representation of the high-dimensional data and then optimizes a low-dimensional graph to be as structurally similar as possible.</p> <p>If t-SNE is a beautifully detailed map of a few specific neighborhoods, UMAP is often capable of producing a beautifully detailed map of the entire city, and it does so much, much quicker.</p> <p><strong>Pros of UMAP</strong>:</p> <ul> <li>Significantly faster than t-SNE, making it suitable for larger datasets.</li> <li>Often better at preserving <em>global</em> structure while still maintaining excellent local structure preservation.</li> <li>More consistent results across different runs than t-SNE.</li> </ul> <p><strong>Cons of UMAP</strong>:</p> <ul> <li>Still not as straightforward to interpret distances as PCA’s linear transformations.</li> </ul> <h2 id="why-bother-with-dimensionality-reduction-the-real-world-impact">Why Bother with Dimensionality Reduction? The Real-World Impact</h2> <p>After exploring these techniques, you might wonder, “Is it always necessary?” My experience tells me it’s almost always worth considering.</p> <ul> <li> <strong>Clarity in Visualization</strong>: Suddenly, those intimidating 500 features can become a beautiful 2D plot revealing distinct customer segments or disease subtypes.</li> <li> <strong>Speeding Up Training</strong>: Imagine cutting model training time from hours to minutes, or even seconds. That’s a game-changer for iterative development.</li> <li> <strong>Fighting Overfitting</strong>: By reducing noise and focusing on the most informative aspects of your data, your models become more robust and generalize better to unseen data.</li> <li> <strong>Simplifying Data Pipelines</strong>: Less complex data can make your entire machine learning pipeline more efficient and easier to manage.</li> </ul> <h2 id="my-personal-takeaways-and-when-to-choose-what">My Personal Takeaways and When to Choose What</h2> <p>Through countless datasets and experiments, I’ve developed a few heuristics:</p> <ul> <li> <strong>For General Purpose Reduction &amp; Preprocessing</strong>: <strong>PCA</strong> is often my first stop. It’s fast, well-understood, and excellent for linearly structured data or when you need fewer features for a subsequent model. It’s also great if you need to remove multicollinearity.</li> <li> <strong>For Visualizing Clusters &amp; Exploring Non-Linear Structures</strong>: <strong>UMAP</strong> is usually my go-to. Its speed and ability to preserve both local and global structure make it incredibly powerful for gaining insights into complex datasets. If UMAP doesn’t quite give me what I need, I’ll sometimes try <strong>t-SNE</strong> as a secondary option, though less frequently now.</li> <li> <strong>Remember the Goal</strong>: Always consider <em>why</em> you’re reducing dimensionality. Is it for faster models? Better visualization? Less memory? Your goal will guide your choice.</li> </ul> <h2 id="conquering-the-data-vortex">Conquering the Data Vortex!</h2> <p>Dimensionality Reduction isn’t just a technical trick; it’s a superpower for data scientists. It transforms overwhelming, high-dimensional chaos into manageable, insightful clarity. It allows us to understand our data better, build more efficient models, and ultimately, extract more value from the vast seas of information around us.</p> <p>So, the next time you face a dataset with more features than you can shake a stick at, don’t despair! Embrace the tools of dimensionality reduction. Experiment, visualize, and watch as your data starts to reveal its hidden stories.</p> <p>Happy coding, and may your dimensions be ever reduced!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>