<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Unmasking the Magic: A Deep Dive into the GPT Architecture | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/unmasking-the-magic-a-deep-dive-into-the-gpt-archi/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Unmasking the Magic: A Deep Dive into the GPT Architecture</h1> <p class="post-meta"> Created on August 05, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> NLP</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/transformers"> <i class="fa-solid fa-hashtag fa-sm"></i> Transformers</a>   <a href="/blog/blog/tag/gpt"> <i class="fa-solid fa-hashtag fa-sm"></i> GPT</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>As a budding data scientist, I often find myself marveling at the sheer “magic” of modern AI. And few things feel more magical than watching a Generative Pre-trained Transformer (GPT) model effortlessly craft coherent, contextually relevant text on virtually any topic. It’s like having a hyper-intelligent scribe at your fingertips, capable of writing poetry, summarizing articles, or even debugging code.</p> <p>But beneath this seemingly magical facade lies a beautifully engineered architecture that, once understood, reveals its elegant logic. Today, I want to take you on a journey, from the foundational concepts to the specific innovations that make GPT models so incredibly powerful. My goal is to demystify GPT, breaking down its core components in a way that’s both accessible and deeply insightful.</p> <h3 id="the-ancestor-why-we-needed-a-transformer">The Ancestor: Why We Needed a Transformer</h3> <p>Before GPT, the world of Natural Language Processing (NLP) was largely dominated by Recurrent Neural Networks (RNNs) and their more sophisticated cousins, Long Short-Term Memory networks (LSTMs). These models processed text word by word, maintaining a “memory” of previous words to understand context.</p> <p>Think of it like reading a book one word at a time, trying to remember everything that came before. While effective for short sentences, they had two major Achilles’ heels:</p> <ol> <li> <strong>Sequential Bottleneck:</strong> They <em>had</em> to process words one after another. This made them slow and couldn’t fully leverage the parallel processing power of modern GPUs.</li> <li> <strong>Long-Range Dependencies:</strong> Remembering information from paragraphs ago was incredibly difficult. By the time an RNN reached the end of a long sentence, the initial context might have faded, leading to issues with understanding complex relationships.</li> </ol> <p>In 2017, a groundbreaking paper titled “Attention Is All You Need” introduced the <strong>Transformer architecture</strong>, which completely revolutionized how we handle sequences. The core idea? Ditch sequential processing for a mechanism called <strong>self-attention</strong>.</p> <h3 id="deconstructing-the-transformer-block-gpts-blueprint">Deconstructing the Transformer Block: GPT’s Blueprint</h3> <p>GPT models are built upon a specific, decoder-only variant of the Transformer architecture. Let’s break down the key components you’d find in each of its many stacked layers.</p> <h4 id="1-input-embedding-giving-words-a-voice">1. Input Embedding: Giving Words a Voice</h4> <p>Our computers don’t understand words like “cat” or “algorithm.” They understand numbers. So, the first step is to convert each word (or sub-word token) into a numerical vector – a list of numbers that represents its meaning. This is called <strong>embedding</strong>. Words with similar meanings will have similar embedding vectors.</p> <h4 id="2-positional-encoding-because-order-still-matters">2. Positional Encoding: Because Order Still Matters</h4> <p>The brilliance of the Transformer is its ability to process all words in a sentence <em>simultaneously</em>. But herein lies a problem: if words are processed in parallel, how do we know their order? “Dog bites man” means something very different from “Man bites dog.”</p> <p>This is where <strong>Positional Encoding</strong> comes in. Instead of relying on sequential processing, we inject information about the position of each word directly into its embedding vector. Imagine adding a small, unique “tag” to each word that tells the model where it sits in the sequence.</p> <p>The original Transformer uses sine and cosine functions of different frequencies to generate these position vectors:</p> <p>$PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{model}})$ $PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d_{model}})$</p> <p>Where:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">pos</code> is the position of the token in the sequence.</li> <li> <code class="language-plaintext highlighter-rouge">i</code> is the dimension within the embedding vector.</li> <li> <code class="language-plaintext highlighter-rouge">d_model</code> is the dimension of the embedding vector.</li> </ul> <p>By adding this positional encoding to the word embedding, each token carries information about both its meaning and its location in the sequence.</p> <h4 id="3-the-heart-of-the-transformer-multi-head-self-attention">3. The Heart of the Transformer: Multi-Head Self-Attention</h4> <p>This is arguably the most crucial component. Self-attention allows each word in a sequence to “look at” and “weigh” the importance of every other word in that same sequence to better understand its own context.</p> <p>Think of it like a committee meeting: when someone speaks, everyone else listens and gauges how relevant that statement is to their own understanding.</p> <p>For each word, three main vectors are derived from its embedded representation:</p> <ul> <li> <strong>Query (Q):</strong> What am I looking for? (Like a search query)</li> <li> <strong>Key (K):</strong> What do I have to offer? (Like a tag on information)</li> <li> <strong>Value (V):</strong> What information do I actually carry? (The actual information)</li> </ul> <p>The self-attention mechanism works by calculating a score for each word’s Query against all other words’ Keys. This score tells us how much “attention” a word should pay to other words. These scores are then scaled and passed through a softmax function to get attention weights, which are then multiplied by the Value vectors to get the final output.</p> <p>The formula for Scaled Dot-Product Attention looks like this:</p> <p>$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$</p> <p>Where:</p> <ul> <li>$Q$, $K$, $V$ are matrices stacked with query, key, and value vectors for all words.</li> <li>$d_k$ is the dimension of the key vectors (used to scale the dot product, preventing large values from pushing the softmax into regions with tiny gradients).</li> </ul> <p><strong>Multi-Head Attention:</strong> Instead of just one committee meeting, imagine having several specialized sub-committees, each focusing on a different aspect of the problem. This is Multi-Head Attention. It allows the model to simultaneously pay attention to different parts of the input sequence (e.g., one “head” might focus on grammatical dependencies, another on semantic relationships). The outputs from these different “heads” are then concatenated and linearly transformed.</p> <p><strong>Crucially for GPT: Masked Self-Attention:</strong> GPT models are <em>generative</em> and <em>autoregressive</em>, meaning they predict the <em>next</em> word based on <em>previous</em> words. To ensure this behavior during training, we employ <strong>masked self-attention</strong>. When the model is processing a word, it is prevented from “peeking” at future words in the sequence. This is done by setting the attention scores for future words to negative infinity before the softmax, effectively zeroing them out. This forces the model to learn to predict words one by one, just like a human speaking or writing.</p> <h4 id="4-feed-forward-network-processing-the-attended-information">4. Feed-Forward Network: Processing the “Attended” Information</h4> <p>After the attention mechanism has allowed words to gather context from each other, a simple, position-wise <strong>Feed-Forward Network (FFN)</strong> is applied independently to each word’s representation. This is typically a two-layer neural network with a ReLU activation in between. It helps the model process the information it just “attended” to, adding non-linearity and increasing the model’s capacity to learn complex patterns.</p> <h4 id="5-residual-connections-and-layer-normalization-stability-and-speed">5. Residual Connections and Layer Normalization: Stability and Speed</h4> <p>Throughout the Transformer block, you’ll find <strong>Residual Connections</strong> (or skip connections) and <strong>Layer Normalization</strong>.</p> <ul> <li> <strong>Residual Connections:</strong> These add the input of a sub-layer to its output. This helps combat the vanishing gradient problem in deep networks, allowing information to flow more easily through many layers and speeding up training.</li> <li> <strong>Layer Normalization:</strong> This technique normalizes the activations of each layer, making training more stable and robust to different input scales.</li> </ul> <h3 id="from-transformer-to-gpt-the-generative-leap">From Transformer to GPT: The Generative Leap</h3> <p>Now that we understand the core components, let’s see how they come together in GPT.</p> <p>The original Transformer architecture has two main parts: an <strong>Encoder</strong> (which processes an input sequence to understand it) and a <strong>Decoder</strong> (which generates an output sequence).</p> <p>GPT models are <strong>Decoder-only</strong> Transformers. Why? Because their primary job is text generation, which is an autoregressive task. We give it a prompt, and it generates the <em>next</em> most probable word, then the <em>next</em>, and so on, until it completes the thought or reaches a specified length. The masked self-attention we discussed earlier is critical for this.</p> <h4 id="1-pre-training-learning-the-language-of-the-internet">1. Pre-training: Learning the Language of the Internet</h4> <p>The “P” in GPT stands for “Pre-trained,” and this is where a massive amount of its power comes from. Instead of training from scratch for every specific task, GPT models undergo an extensive <strong>pre-training phase</strong>.</p> <p>Imagine giving a student access to almost the entire internet – billions of words from books, articles, websites, and conversations – and asking them to simply predict the <em>next word</em> in every possible sentence. This is essentially what pre-training does.</p> <p>During pre-training, the model learns:</p> <ul> <li> <strong>Grammar and Syntax:</strong> How words fit together correctly.</li> <li> <strong>Facts and World Knowledge:</strong> Relationships between concepts and real-world information.</li> <li> <strong>Reasoning Abilities:</strong> Patterns that allow it to infer logical connections.</li> <li> <strong>Different Writing Styles:</strong> From formal essays to casual chat.</li> </ul> <table> <tbody> <tr> <td>This objective is called <strong>Language Modeling</strong>, specifically next-token prediction: $P(w_i</td> <td>w_1, …, w_{i-1})$. The model is fed a sequence of words $w_1, …, w_{i-1}$ and tries to predict $w_i$. Because of the masked attention, it can only see words that have <em>already occurred</em> in the sequence.</td> </tr> </tbody> </table> <p>This unsupervised learning approach, on such a vast scale, enables the model to develop an incredibly rich and general understanding of language.</p> <h4 id="2-fine-tuning-historical-context-adapting-the-giant">2. Fine-tuning (Historical Context): Adapting the Giant</h4> <p>While modern GPT models (like GPT-3 and beyond) often leverage few-shot or zero-shot prompting rather than explicit fine-tuning, earlier GPT versions (and many other Transformer models) utilized a <strong>fine-tuning</strong> step.</p> <p>After pre-training, the model has a general understanding of language. Fine-tuning involves taking this pre-trained model and training it for a specific downstream task (e.g., sentiment analysis, question answering, summarization) using a smaller, task-specific labeled dataset. This allows the model to adapt its generalized knowledge to perform exceptionally well on specialized tasks with relatively little data compared to training from scratch.</p> <h3 id="why-is-gpt-so-good-the-recipe-for-success">Why is GPT So Good? The Recipe for Success</h3> <ol> <li> <strong>The Transformer Architecture:</strong> Its parallel processing capability and sophisticated self-attention mechanism effectively solve the long-standing problems of long-range dependencies and sequential bottlenecks that plagued RNNs.</li> <li> <strong>Scale:</strong> GPT models are massive. They have billions (or even trillions) of parameters, allowing them to capture an enormous amount of linguistic nuance and world knowledge. This scale, combined with vast training data, leads to emergent abilities that simply aren’t present in smaller models.</li> <li> <strong>Pre-training on Massive Data:</strong> By learning to predict the next word on an unprecedented amount of text, GPT develops a profound understanding of language, which it can then apply to a myriad of tasks.</li> <li> <strong>Decoder-only Design for Generation:</strong> The autoregressive nature of the decoder, coupled with masked attention, makes it inherently suited for generating coherent, flowing text, one token at a time.</li> </ol> <h3 id="the-road-ahead-a-glimpse-into-the-future">The Road Ahead: A Glimpse into the Future</h3> <p>While GPT models are astonishing, they’re not perfect. They can sometimes “hallucinate” facts, propagate biases present in their training data, and are incredibly computationally expensive to train and run. The field is constantly evolving, with researchers exploring more efficient architectures, better training methods, and robust ways to ensure safety and ethical use.</p> <h3 id="conclusion-unmasking-the-magic">Conclusion: Unmasking the Magic</h3> <p>So, the “magic” of GPT isn’t really magic at all – it’s brilliant engineering and an immense amount of data and computational power. It’s a testament to the power of a well-designed architecture (the Transformer) combined with a simple yet powerful unsupervised objective (next-token prediction) scaled to unprecedented levels.</p> <p>As I reflect on this journey, I’m constantly reminded that behind every impressive AI feat lies a foundation of elegant mathematical principles and clever algorithmic design. Understanding these foundations isn’t just academic; it’s empowering. It allows us to not only appreciate the current state of AI but also to envision and contribute to its future. The world of AI is moving at an incredible pace, and understanding its core components, like the GPT architecture, is your key to unlocking its potential. Keep exploring, keep questioning, and keep building!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>