<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Unraveling the Neural Network: A Personal Voyage into AI's Brain | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/unraveling-the-neural-network-a-personal-voyage-in/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Unraveling the Neural Network: A Personal Voyage into AI's Brain</h1> <p class="post-meta"> Created on September 30, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/neural-networks"> <i class="fa-solid fa-hashtag fa-sm"></i> Neural Networks</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="my-first-brush-with-ai-a-curious-minds-dive">My First Brush with AI: A Curious Mind’s Dive</h2> <p>It feels like just yesterday I was staring at a blank screen, a thousand questions swirling in my head about Artificial Intelligence. How do these algorithms <em>learn</em>? Can a machine truly <em>think</em>? Like many of you, I’d heard the buzzwords: AI, Machine Learning, Deep Learning. But it wasn’t until I truly started digging that I encountered the magnificent architecture known as the <strong>Neural Network</strong>. It wasn’t just a fancy algorithm; it was an attempt to mimic the very structure of our own brains, albeit in a highly simplified form.</p> <p>My journey into neural networks wasn’t just about understanding code; it was about understanding a paradigm shift in how we approach problem-solving with computers. From recognizing cats in photos to predicting stock prices, neural networks are everywhere. So, let’s peel back the layers and discover what makes these digital brains tick.</p> <h2 id="the-humble-neuron-ais-smallest-thinker">The Humble Neuron: AI’s Smallest Thinker</h2> <p>Imagine trying to teach a baby to recognize a cat. You show them pictures, point to real cats, and say “cat.” Their brain processes these inputs and forms connections. A neural network operates on a similar, albeit much simpler, principle. Its fundamental building block is the <strong>neuron</strong> (or node).</p> <p>Think of a neuron as a tiny decision-maker. It receives several inputs, processes them, and then spits out an output. Let’s break down this process:</p> <ol> <li> <p><strong>Inputs ($x_1, x_2, …, x_n$):</strong> These are the pieces of information the neuron receives. If we’re trying to predict house prices, inputs might be the number of bedrooms, square footage, and zip code.</p> </li> <li> <p><strong>Weights ($w_1, w_2, …, w_n$):</strong> Each input comes with a “weight.” These weights determine the importance of each input. A higher weight means that input has a stronger influence on the neuron’s decision. Initially, these weights are random, but as the network learns, they adjust.</p> </li> <li> <p><strong>Summation:</strong> The neuron calculates a weighted sum of its inputs. This is like adding up all the clues, but giving more importance to some clues than others. We also add a <strong>bias ($b$)</strong> term, which allows the neuron to activate even if all inputs are zero, or to shift the activation function.</p> <p>Mathematically, this looks like:</p> <p>$z = (\sum_{i=1}^{n} x_i w_i) + b$</p> <p>Or, written out:</p> <p>$z = (x_1 w_1 + x_2 w_2 + … + x_n w_n) + b$</p> </li> <li> <p><strong>Activation Function ($\sigma$):</strong> This is where things get interesting! After the weighted sum ($z$) is calculated, it passes through an <strong>activation function</strong>. Why? Without it, a neural network would just be a fancy linear regression model, incapable of learning complex, non-linear patterns. Activation functions introduce non-linearity, allowing the network to model intricate relationships in data.</p> <p>Common activation functions include:</p> <ul> <li> <strong>Sigmoid:</strong> Squashes values between 0 and 1, useful for probabilities. ($\sigma(z) = \frac{1}{1 + e^{-z}}$)</li> <li> <strong>ReLU (Rectified Linear Unit):</strong> Simple yet powerful, outputs the input directly if positive, otherwise 0. ($\sigma(z) = \max(0, z)$) This is a popular choice due to its computational efficiency and ability to mitigate vanishing gradient problems (which we won’t dive deep into here, but it’s a big deal!).</li> <li> <strong>Tanh (Hyperbolic Tangent):</strong> Similar to Sigmoid but squashes values between -1 and 1.</li> </ul> <p>So, the final output of a single neuron is:</p> <p>$a = \sigma(z)$</p> <p>Where $a$ is the activated output. This output then becomes an input for other neurons, or it’s the final answer.</p> </li> </ol> <h2 id="from-neuron-to-network-building-the-brain">From Neuron to Network: Building the Brain</h2> <p>A single neuron, while interesting, isn’t very powerful. The real magic happens when you connect many neurons together to form a <strong>network</strong>. This is where the term “neural network” truly comes alive!</p> <p>A typical neural network is organized into layers:</p> <ol> <li> <strong>Input Layer:</strong> This is where our raw data (the $x_i$ values) enters the network. Each node in this layer corresponds to an input feature.</li> <li> <strong>Hidden Layers:</strong> These are the “thinking” layers. Neurons in hidden layers take inputs from the previous layer, perform their weighted sum and activation, and pass their outputs to the next layer. A network can have one, two, or even hundreds of hidden layers. The more hidden layers, the “deeper” the network, leading to the term “Deep Learning.” These layers are where the network learns to extract increasingly complex features from the data.</li> <li> <strong>Output Layer:</strong> This layer produces the network’s final prediction. The number of neurons here depends on the problem. For binary classification (e.g., “cat” or “not cat”), you might have one neuron with a Sigmoid activation. For multi-class classification (e.g., identifying different types of animals), you might have one neuron per class with a Softmax activation (which gives probabilities for each class). For regression (e.g., predicting house prices), you might have one neuron with no activation (or a linear one).</li> </ol> <p>Information flows through the network in one direction, from the input layer, through the hidden layers, and finally to the output layer. This process is called <strong>feedforward propagation</strong>.</p> <h2 id="the-learning-process-how-networks-get-smart-backpropagation-explained">The Learning Process: How Networks Get Smart (Backpropagation Explained!)</h2> <p>Now, this is the million-dollar question: How does a network <em>learn</em>? Initially, those weights and biases ($w_i$ and $b$) are just random guesses. The network will make terrible predictions. The learning process is all about intelligently adjusting these weights and biases so that the network makes better and better predictions.</p> <p>Here’s the general idea:</p> <ol> <li> <strong>Make a Prediction (Feedforward):</strong> We feed an input (e.g., an image of a cat) through the network, and it produces an output (e.g., “dog”).</li> <li> <strong>Measure the Error (Loss Function):</strong> We compare the network’s prediction with the actual correct answer (the “label”). This difference is called the <strong>error</strong> or <strong>loss</strong>. A <strong>loss function</strong> quantifies how “wrong” the network’s prediction was. <ul> <li>For regression tasks, a common loss function is <strong>Mean Squared Error (MSE)</strong>: $L = \frac{1}{m} \sum_{j=1}^{m} (y_j - \hat{y}_j)^2$ where $y_j$ is the true value, $\hat{y}_j$ is the predicted value, and $m$ is the number of samples.</li> <li>For classification tasks, <strong>Cross-Entropy Loss</strong> is frequently used.</li> </ul> </li> <li> <strong>Adjust Weights (Gradient Descent &amp; Backpropagation):</strong> This is the core of learning. We want to minimize the loss. Imagine the loss as a landscape, and we’re trying to find the lowest point (the minimum loss). We take small steps downhill. The direction of the steepest descent is given by the <strong>gradient</strong> of the loss function with respect to each weight and bias. <ul> <li> <p><strong>Gradient Descent:</strong> This optimization algorithm iteratively adjusts weights and biases in the direction that decreases the loss function. It’s like feeling your way down a dark hill – you take a small step in the direction that feels like it’s going down. $w_{new} = w_{old} - \alpha \frac{\partial L}{\partial w}$ $b_{new} = b_{old} - \alpha \frac{\partial L}{\partial b}$ Here, $\alpha$ is the <strong>learning rate</strong>, a small positive number that controls how big each step is. A larger learning rate can make learning faster but might overshoot the minimum; a smaller one is slower but more precise.</p> </li> <li> <p><strong>Backpropagation:</strong> This is the ingenious algorithm that efficiently calculates these gradients ($\frac{\partial L}{\partial w}$ and $\frac{\partial L}{\partial b}$) for <em>all</em> the weights and biases in the network. It does this by propagating the error backwards from the output layer, through the hidden layers, to the input layer. It essentially figures out how much each weight and bias contributed to the final error, assigning “credit” or “blame” accordingly. This is a chain rule application from calculus, but its practical implementation is what made deep learning truly feasible. Without backpropagation, training deep networks would be computationally impossible.</p> </li> </ul> </li> </ol> <p>This entire cycle of feedforward, calculate loss, and backpropagate to adjust weights and biases is repeated thousands or millions of times using large datasets. Each pass through the training data is called an <strong>epoch</strong>. With each epoch, the network refines its internal representations, making its predictions more accurate.</p> <h2 id="why-are-neural-networks-so-powerful">Why Are Neural Networks So Powerful?</h2> <ul> <li> <strong>Universal Approximation Theorem:</strong> This fascinating theorem states that a neural network with just one hidden layer (and enough neurons) can approximate any continuous function to an arbitrary degree of accuracy. In simpler terms, given enough data and complexity, a neural network can learn virtually any pattern!</li> <li> <strong>Feature Learning:</strong> Unlike traditional machine learning algorithms where you often have to manually design “features” (e.g., edges, textures for image recognition), deep neural networks can learn these features directly from the raw data. This is incredibly powerful for complex data like images, audio, and text.</li> <li> <strong>Scalability:</strong> With vast amounts of data and computational power (GPUs), neural networks can scale to solve incredibly complex problems that were once thought intractable.</li> </ul> <h2 id="a-glimpse-beyond-diverse-architectures">A Glimpse Beyond: Diverse Architectures</h2> <p>What we’ve discussed is the foundational <strong>Feedforward Neural Network</strong> (sometimes called a Multi-Layer Perceptron or MLP). But the world of neural networks is vast and diverse:</p> <ul> <li> <strong>Convolutional Neural Networks (CNNs):</strong> Master of image and video processing, they excel at spatial patterns. Think object detection and facial recognition.</li> <li> <strong>Recurrent Neural Networks (RNNs):</strong> Designed for sequential data, like text or time series, allowing them to remember past information. Language translation and speech recognition are their strong suit.</li> <li> <strong>Transformers:</strong> The new kid on the block, revolutionized Natural Language Processing (NLP) and powers models like GPT-3.</li> </ul> <p>Each architecture is a specialized tool, designed to tackle particular types of data and problems, pushing the boundaries of what AI can achieve.</p> <h2 id="the-journey-continues">The Journey Continues</h2> <p>My journey with neural networks taught me that AI isn’t some mystical black box; it’s a sophisticated interplay of simple mathematical operations, scaled to an incredible degree. It’s about designing systems that can learn from data, identify patterns, and make intelligent decisions.</p> <p>For those of you just starting out, don’t be intimidated by the math or the complexity. Begin with the core concepts: the neuron, the layers, the forward pass, and the beautiful dance of backpropagation adjusting weights. Build a simple network, play with the learning rate, and see the magic unfold. The field is constantly evolving, and there’s always something new to learn, build, and innovate.</p> <p>So, go forth and explore! The digital brains are waiting for you to teach them. What will you build next?</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>