<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Cracking the Code: A Deep Dive into How Neural Networks Learn (and Think!) | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/cracking-the-code-a-deep-dive-into-how-neural-netw/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Cracking the Code: A Deep Dive into How Neural Networks Learn (and Think!)</h1> <p class="post-meta"> Created on January 16, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/neural-networks"> <i class="fa-solid fa-hashtag fa-sm"></i> Neural Networks</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Welcome, curious minds! Today, I want to take you on a journey, a personal exploration into one of the most exciting and transformative technologies of our time: Neural Networks. As someone deeply passionate about data science and machine learning, I often find myself marveling at their capabilities, and I want to demystify them for you.</p> <p>You’ve probably heard the buzzwords: AI, Deep Learning, Machine Learning. But what actually makes an AI “smart”? How does it learn? For a long time, this felt like magic to me. But once you peel back the layers, you discover a beautiful blend of biology, mathematics, and computation working in harmony. Think of this as our journal entry into understanding these digital brains.</p> <h3 id="the-spark-of-inspiration-our-own-brains">The Spark of Inspiration: Our Own Brains</h3> <p>Let’s start with a radical idea: what if we could build machines that learn like us? The human brain, with its billions of interconnected neurons, is the ultimate learning machine. Each neuron is a tiny processor, taking inputs, deciding if they’re important enough, and then passing signals on.</p> <p>This biological marvel inspired the very first conceptual “artificial neuron” in 1943 by Warren McCulloch and Walter Pitts, and later, the Perceptron by Frank Rosenblatt in 1958. The idea was simple yet profound: simulate this input-process-output mechanism digitally.</p> <h3 id="the-artificial-neuron-our-digital-building-block">The Artificial Neuron: Our Digital Building Block</h3> <p>Imagine a single neuron. What does it do? It receives signals from other neurons. If the combined strength of these signals crosses a certain threshold, it “fires” and sends its own signal onward.</p> <p>Our artificial neuron, often called a <strong>node</strong> or <strong>perceptron</strong>, works similarly.</p> <ol> <li> <strong>Inputs ($x_i$):</strong> These are like the signals from other neurons. In a neural network, these could be pixels from an image, words from a sentence, or features from a dataset.</li> <li> <strong>Weights ($w_i$):</strong> Each input signal is multiplied by a “weight.” Think of a weight as the importance or strength assigned to that particular input. A higher weight means that input has a stronger influence on the neuron’s decision.</li> <li> <p><strong>Summation:</strong> All the weighted inputs are summed up. We also add a <strong>bias ($b$)</strong>, which is like a neuron’s inherent readiness to fire, regardless of its inputs. Mathematically, this looks like: \(z = \sum_{i=1}^{n} w_i x_i + b\) Here, $n$ is the number of inputs, $x_i$ are the inputs, $w_i$ are their corresponding weights, and $b$ is the bias.</p> </li> <li> <strong>Activation Function ($f$):</strong> This is the “decision-maker.” The sum $z$ is passed through an activation function, which determines the neuron’s output. Why do we need this? It introduces non-linearity. Without it, stacking multiple layers of neurons would just be like having one big linear neuron, limiting the network’s ability to learn complex patterns. Some common activation functions you might encounter are: <ul> <li> <strong>Sigmoid:</strong> Squashes values between 0 and 1, useful for probabilities.</li> <li> <strong>ReLU (Rectified Linear Unit):</strong> Outputs the input if it’s positive, otherwise zero. $f(z) = \max(0, z)$. It’s simple and very popular!</li> <li> <strong>Tanh (Hyperbolic Tangent):</strong> Squashes values between -1 and 1.</li> </ul> <p>So, the final output of our neuron is: \(a = f(z)\)</p> </li> </ol> <p>This tiny little unit, taking inputs, weighing them, summing them up, and making a “decision,” is the fundamental building block of all neural networks. Pretty neat, right?</p> <h3 id="from-single-neuron-to-a-network-building-layers">From Single Neuron to a Network: Building Layers</h3> <p>A single neuron can make simple decisions. But to tackle complex problems like recognizing cats in photos or understanding human speech, we need more. We connect these neurons together in layers, forming a <strong>Neural Network</strong>.</p> <ul> <li> <strong>Input Layer:</strong> This is where our raw data enters the network. Each node here represents an input feature (e.g., a pixel value).</li> <li> <strong>Hidden Layers:</strong> These are the “thinking” layers. The output of one layer becomes the input for the next. As information passes through these layers, the network learns to extract increasingly complex features from the data. The “deep” in <strong>Deep Learning</strong> simply refers to networks with many hidden layers.</li> <li> <strong>Output Layer:</strong> This layer produces the final result. For classifying images into “cat” or “dog,” it might have two output nodes. For predicting a house price, it might have one output node.</li> </ul> <p>Information flows from the input layer, through the hidden layers, to the output layer. This process is called <strong>forward propagation</strong>. Each connection between neurons has its own weight, and each neuron has its own bias. The sheer number of these weights and biases in a large network can be astounding – millions, even billions!</p> <h3 id="the-magic-of-learning-how-weights-get-adjusted-backpropagation">The Magic of Learning: How Weights Get Adjusted (Backpropagation)</h3> <p>Here’s where the real “learning” happens. Initially, all those weights and biases are random. If we show the network an image of a cat, it will likely output a random guess like “it’s a car” or “it’s a table.” Clearly, that’s not very useful.</p> <p>The goal is to adjust these weights and biases so that the network makes accurate predictions. How do we do that? Through a process that feels like magic but is pure calculus and optimization:</p> <ol> <li> <strong>Making a Guess:</strong> The network performs forward propagation, takes an input, and makes a prediction ($\hat{y}$).</li> <li> <strong>Measuring the Error (Loss Function):</strong> We compare the network’s prediction ($\hat{y}$) with the actual correct answer ($y$). The difference between these two is our “error” or <strong>loss</strong>. For example, for a simple regression problem (predicting a number), we might use the Mean Squared Error: \(L = \frac{1}{2m} \sum_{j=1}^{m} (y^{(j)} - \hat{y}^{(j)})^2\) Where $m$ is the number of examples, $y^{(j)}$ is the true value, and $\hat{y}^{(j)}$ is the predicted value for the $j$-th example. Our goal is to minimize this loss.</li> <li> <strong>Adjusting the Weights (Gradient Descent &amp; Backpropagation):</strong> This is the core of learning. We want to know how much each weight and bias contributed to the error and in what direction we should change it to reduce the error. <ul> <li> <strong>Gradient Descent:</strong> Imagine you’re blindfolded on a hilly terrain, trying to find the lowest point (the minimum loss). You’d feel the slope around you and take a small step downhill. This is what gradient descent does. It calculates the “slope” (gradient) of the loss function with respect to each weight and bias.</li> <li> <strong>Updating Rule:</strong> We update each weight $w$ using the following rule: \(w_{new} = w_{old} - \alpha \frac{\partial L}{\partial w}\) Here, $\frac{\partial L}{\partial w}$ is the gradient (how much the loss changes with a tiny change in $w$), and $\alpha$ is the <strong>learning rate</strong>. The learning rate determines how big of a “step” we take downhill. Too big, and we might overshoot the minimum; too small, and learning will be very slow.</li> <li> <strong>Backpropagation:</strong> Calculating these gradients for every single weight in a large network efficiently is computationally intensive. Backpropagation is a clever algorithm that does exactly this, working backward from the output layer to the input layer, distributing the blame for the error across all the weights. It’s an application of the chain rule from calculus, but its intuition is simpler: if the output was wrong, which connections were most responsible, and how should they be tweaked?</li> </ul> </li> </ol> <p>This entire process – forward propagation, calculating loss, and then backpropagation to update weights – is repeated thousands, millions, even billions of times over many iterations (called <strong>epochs</strong>) using vast amounts of data. Slowly but surely, the network’s weights and biases converge to values that allow it to make incredibly accurate predictions.</p> <h3 id="why-are-neural-networks-so-powerful">Why are Neural Networks so Powerful?</h3> <p>What makes these seemingly simple interconnected nodes capable of such complex feats?</p> <ol> <li> <strong>Universal Approximation Theorem:</strong> Theoretically, a neural network with just one hidden layer can approximate any continuous function to any desired accuracy, given enough neurons. This means they can learn incredibly complex, non-linear relationships in data that traditional linear models simply cannot.</li> <li> <strong>Hierarchical Feature Learning:</strong> As data passes through multiple hidden layers, the network learns to extract features at different levels of abstraction. For example, in an image, the first layer might detect edges and corners, the next might combine these to detect shapes (eyes, ears), and subsequent layers might combine shapes to recognize entire objects (a cat’s face). This automatic feature extraction is a massive advantage over older machine learning techniques where features had to be hand-engineered by humans.</li> <li> <strong>Scalability:</strong> With the advent of powerful GPUs (Graphics Processing Units) and massive datasets, neural networks can scale to incredible sizes, allowing them to learn from truly enormous amounts of information.</li> </ol> <h3 id="real-world-impact-neural-networks-all-around-us">Real-World Impact: Neural Networks All Around Us</h3> <p>It’s not an exaggeration to say that neural networks are powering much of the AI we interact with daily:</p> <ul> <li> <strong>Image Recognition:</strong> Your phone’s face unlock, tagging friends in photos, self-driving cars recognizing pedestrians and traffic signs.</li> <li> <strong>Natural Language Processing (NLP):</strong> Google Translate, autocorrect, chatbots like ChatGPT, spam detection.</li> <li> <strong>Speech Recognition:</strong> Siri, Alexa, Google Assistant.</li> <li> <strong>Recommendation Systems:</strong> What movies Netflix suggests, what products Amazon shows you.</li> <li> <strong>Medical Diagnosis:</strong> Analyzing medical images (X-rays, MRIs) for early detection of diseases.</li> </ul> <p>They are silently, yet profoundly, changing our world.</p> <h3 id="the-road-ahead-challenges-and-the-future">The Road Ahead: Challenges and the Future</h3> <p>While powerful, neural networks aren’t without their complexities:</p> <ul> <li> <strong>Data Hunger:</strong> They often require vast amounts of labeled data to train effectively.</li> <li> <strong>Computational Cost:</strong> Training large models can require significant computing power and time.</li> <li> <strong>Interpretability (The Black Box Problem):</strong> Sometimes, it’s hard to understand <em>why</em> a neural network made a particular decision, especially in deep networks. This “black box” nature can be a concern in critical applications like healthcare or autonomous driving.</li> <li> <strong>Adversarial Attacks:</strong> Small, imperceptible changes to input data can sometimes trick a network into making drastically wrong predictions.</li> </ul> <p>Researchers are actively working on these challenges, exploring areas like explainable AI (XAI), more efficient architectures, and methods to train models with less data. The field is constantly evolving, and that’s what makes it so exciting!</p> <h3 id="conclusion-our-journey-continues">Conclusion: Our Journey Continues</h3> <p>So, there you have it – a peek behind the curtain of neural networks. We’ve explored their biological inspiration, dissected the artificial neuron, understood how layers combine to form networks, and grasped the iterative dance of forward propagation and backpropagation that enables them to learn.</p> <p>From simple mathematical operations to world-changing applications, neural networks represent a pinnacle of human ingenuity. They’re not magic; they’re elegant mathematical systems that learn patterns from data. As you delve deeper into data science and machine learning, you’ll find countless opportunities to build, train, and deploy these incredible models.</p> <p>I hope this journal entry has sparked your curiosity and given you a solid foundation to explore further. The world of AI is vast and ever-expanding, and understanding its core building blocks like neural networks is your first step to becoming a builder in this new era. Keep learning, keep experimenting, and who knows what amazing things you’ll create!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>