<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Algorithmic Compass: Navigating Data with Decision Trees | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/2025/the-algorithmic-compass-navigating-data-with-decis/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Algorithmic Compass: Navigating Data with Decision Trees</h1> <p class="post-meta"> Created on May 01, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/decision-trees"> <i class="fa-solid fa-hashtag fa-sm"></i> Decision Trees</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/supervised-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Supervised Learning</a>   <a href="/blog/blog/tag/algorithms"> <i class="fa-solid fa-hashtag fa-sm"></i> Algorithms</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hello fellow data adventurers!</p> <p>Today, I want to take you on a journey through one of the most fundamental and surprisingly intuitive algorithms in machine learning: the Decision Tree. When I first started diving into the world of AI, I was often intimidated by complex mathematical equations and abstract concepts. But then I met Decision Trees, and it was like a breath of fresh air. They just <em>make sense</em>.</p> <h3 id="the-everyday-art-of-decision-making">The Everyday Art of Decision Making</h3> <p>Think about your daily life. Every morning, you make a series of decisions:</p> <ul> <li>Is it sunny? If yes, wear sunglasses. If no, check for rain.</li> <li>Is it raining? If yes, take an umbrella. If no, just enjoy the walk.</li> </ul> <p>Or perhaps you’re trying to decide what movie to watch:</p> <ul> <li>Do I want action? <ul> <li>Yes: Is it a superhero movie? <ul> <li>Yes: Watch Marvel.</li> <li>No: Watch a spy thriller.</li> </ul> </li> <li>No: Do I want comedy? <ul> <li>Yes: Watch a rom-com.</li> <li>No: Watch a drama.</li> </ul> </li> </ul> </li> </ul> <p>What you’ve just done, instinctively, is create a decision tree! It’s a flowchart that guides you to a final decision by asking a series of questions. In the realm of machine learning, Decision Trees do precisely this, but with data.</p> <h3 id="what-exactly-is-a-decision-tree">What Exactly Is a Decision Tree?</h3> <p>At its core, a Decision Tree is a non-parametric supervised learning algorithm used for both classification and regression tasks. It builds a model in the form of a tree structure, where:</p> <ul> <li> <strong>Root Node:</strong> This is where the tree starts, representing the entire dataset.</li> <li> <strong>Internal Nodes (Decision Nodes):</strong> These represent a “test” on a particular attribute or feature (e.g., “Is it raining?”). Each branch from an internal node represents the outcome of that test.</li> <li> <strong>Branches:</strong> These are the paths leading from a decision node to the next node, based on the answer to the question.</li> <li> <strong>Leaf Nodes (Terminal Nodes):</strong> These are the end points of the tree, representing the final decision or prediction (e.g., “Wear a jacket,” “Watch Marvel”).</li> </ul> <p>Imagine you’re trying to predict if a customer will buy a product. A Decision Tree might ask: “Is the customer’s age &gt; 30?” If yes, it might then ask: “Does the customer live in an urban area?” This continues until it reaches a leaf node that says “Likely to buy” or “Unlikely to buy.”</p> <h3 id="how-do-decision-trees-learn-the-splitting-magic">How Do Decision Trees Learn? The “Splitting” Magic</h3> <p>This is where the real “learning” happens. The fundamental challenge in building a Decision Tree is deciding <em>which</em> question to ask at each step and <em>what order</em> to ask them in. The goal is to create “pure” leaf nodes – meaning, a leaf node ideally contains samples that belong to only one class.</p> <p>The algorithm achieves this by recursively partitioning the data into subsets based on the features that provide the “best split.” But how do we define “best split”? This is where we bring in some awesome mathematical concepts: <strong>Entropy</strong>, <strong>Information Gain</strong>, and <strong>Gini Impurity</strong>.</p> <h4 id="1-entropy-measuring-disorder">1. Entropy: Measuring Disorder</h4> <p>Think of entropy as a measure of impurity or disorder within a set of data. If a set of data is perfectly mixed (e.g., an equal number of “Yes” and “No” outcomes), its entropy is high. If a set is perfectly pure (e.g., all “Yes” outcomes), its entropy is zero.</p> <p>I like to imagine a basket of fruits. If your basket contains only apples, it’s very “pure” – low entropy. If it contains a mix of apples, bananas, and oranges, it’s “disordered” – high entropy.</p> <p>Mathematically, for a given set $S$ (our data at a node) with $c$ classes, the entropy is calculated as:</p> \[H(S) = - \sum_{i=1}^{c} p_i \log_2(p_i)\] <p>Where:</p> <ul> <li>$p_i$ is the proportion (probability) of samples belonging to class $i$ in the set $S$.</li> <li>The sum goes over all possible classes.</li> <li>The $\log_2$ is used because we’re often thinking in terms of bits of information (binary choices).</li> </ul> <p>Let’s quickly demystify this:</p> <ul> <li>If $p_i = 0$ for a class, $p_i \log_2(p_i)$ is taken as 0 (as $x \log x \to 0$ as $x \to 0$).</li> <li>If a set is perfectly pure (e.g., all samples are class 1, so $p_1 = 1$, and $p_i = 0$ for all other $i$), then $H(S) = - (1 \log_2(1)) = - (1 \cdot 0) = 0$. Perfect purity, zero entropy!</li> <li>If a set is perfectly mixed (e.g., two classes with $p_1 = 0.5, p_2 = 0.5$), then $H(S) = - (0.5 \log_2(0.5) + 0.5 \log_2(0.5)) = - (0.5 \cdot -1 + 0.5 \cdot -1) = - (-0.5 - 0.5) = 1$. Maximum disorder for two classes, maximum entropy!</li> </ul> <h4 id="2-information-gain-finding-the-best-question">2. Information Gain: Finding the Best Question</h4> <p>Now that we know how to measure impurity, how do we find the “best split”? We use <strong>Information Gain (IG)</strong>. Information Gain quantifies how much the entropy of the data decreases <em>after</em> splitting it on a particular attribute. We want to choose the attribute that gives us the <em>highest</em> information gain, as this means it’s the most effective at making our subsets purer.</p> <p>The formula for Information Gain when splitting a set $S$ on an attribute $A$ is:</p> \[IG(S, A) = H(S) - \sum_{v \in Values(A)} \frac{|S_v|}{|S|} H(S_v)\] <p>Where:</p> <ul> <li>$H(S)$ is the entropy of the parent set $S$ before the split.</li> <li>$Values(A)$ are all the possible values (outcomes) of attribute $A$.</li> <li>$S_v$ is the subset of $S$ for which attribute $A$ has value $v$.</li> <li> <table> <tbody> <tr> <td>$</td> <td>S_v</td> <td>$ is the number of elements in subset $S_v$.</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>$</td> <td>S</td> <td>$ is the total number of elements in the parent set $S$.</td> </tr> </tbody> </table> </li> <li>$H(S_v)$ is the entropy of the subset $S_v$.</li> </ul> <p>In simple terms, Information Gain is the entropy of the parent node minus the weighted average of the entropy of the child nodes. A larger Information Gain means the split is more useful for classification.</p> <h4 id="3-gini-impurity-a-simpler-alternative">3. Gini Impurity: A Simpler Alternative</h4> <p>While Information Gain (based on Entropy) is widely used, another popular metric for splitting is <strong>Gini Impurity</strong>. It’s often computationally faster because it doesn’t involve logarithmic calculations.</p> <p>Gini Impurity measures the probability of incorrectly classifying a randomly chosen element in the dataset if it were randomly labeled according to the class distribution in the dataset. A Gini Impurity of 0 means the set is perfectly pure.</p> <p>The formula for Gini Impurity for a set $S$ with $c$ classes is:</p> \[G(S) = 1 - \sum_{i=1}^{c} (p_i)^2\] <p>Where:</p> <ul> <li>$p_i$ is the proportion of samples belonging to class $i$ in the set $S$.</li> </ul> <p>Comparing Gini and Entropy:</p> <ul> <li>Both Gini Impurity and Entropy aim to find the most “pure” splits.</li> <li>Gini tends to isolate the most frequent class in its own branch, while Entropy tends to produce more balanced trees.</li> <li>In practice, they often lead to very similar trees, and the choice between them can depend on specific dataset characteristics or computational preference.</li> </ul> <h3 id="building-the-tree-recursive-partitioning-in-action">Building the Tree: Recursive Partitioning in Action</h3> <p>The process of building a Decision Tree is recursive:</p> <ol> <li> <strong>Start at the root node:</strong> Consider all features in your dataset.</li> <li> <strong>Find the best split:</strong> For each feature, calculate the Information Gain (or Gini Impurity reduction) from splitting the data based on that feature. Choose the feature and split point that yields the highest gain.</li> <li> <strong>Create child nodes:</strong> Divide the dataset into subsets based on the chosen split.</li> <li> <strong>Recurse:</strong> Apply steps 1-3 to each child node. This continues until: <ul> <li>All samples in a node belong to the same class (perfect purity).</li> <li>No more features are left to split on.</li> <li>A predefined stopping criterion (like maximum depth) is met.</li> <li>The number of samples in a node falls below a minimum threshold.</li> </ul> </li> </ol> <h3 id="the-peril-of-overfitting-and-how-to-prune-it">The Peril of Overfitting and How to Prune It</h3> <p>Decision Trees are incredibly powerful, but they have a notorious weakness: <strong>overfitting</strong>. A tree can become too complex, learning every tiny detail and noise in the training data, essentially “memorizing” it rather than “learning” generalized rules. When this over-complex tree encounters new, unseen data, its performance drops dramatically.</p> <p>Imagine trying to predict if you’ll enjoy a new movie based on your friend’s extremely detailed review of every single frame. You might overfit to <em>that specific movie</em> and fail to predict if you’ll enjoy other movies with similar genres or actors.</p> <p>To combat overfitting, we use <strong>pruning</strong> techniques:</p> <ol> <li> <strong>Pre-pruning (Stopping Early):</strong> This involves setting limits <em>before</em> the tree is fully grown. Common hyperparameters include: <ul> <li> <code class="language-plaintext highlighter-rouge">max_depth</code>: The maximum depth of the tree. A smaller depth prevents the tree from becoming too deep and complex.</li> <li> <code class="language-plaintext highlighter-rouge">min_samples_leaf</code>: The minimum number of samples required to be at a leaf node.</li> <li> <code class="language-plaintext highlighter-rouge">min_samples_split</code>: The minimum number of samples required to split an internal node.</li> <li> <code class="language-plaintext highlighter-rouge">max_features</code>: The number of features to consider when looking for the best split.</li> </ul> </li> <li> <strong>Post-pruning (Pruning After Growth):</strong> In this approach, a full tree is grown, and then branches are removed or simplified from the bottom-up. This is often done by evaluating the performance of subtrees on a validation set or using cost-complexity pruning (e.g., <code class="language-plaintext highlighter-rouge">ccp_alpha</code> in scikit-learn).</li> </ol> <h3 id="advantages-of-decision-trees">Advantages of Decision Trees</h3> <ul> <li> <strong>Interpretability:</strong> They are “white-box” models. You can easily visualize and understand the decision path, making them excellent for explaining predictions to non-technical stakeholders.</li> <li> <strong>Handles Various Data Types:</strong> They can work with both numerical and categorical data without extensive preprocessing (like one-hot encoding for categories).</li> <li> <strong>Minimal Data Preparation:</strong> Unlike many other algorithms, Decision Trees don’t require feature scaling (e.g., normalization or standardization).</li> <li> <strong>Feature Importance:</strong> They naturally highlight which features are most important for making decisions.</li> </ul> <h3 id="disadvantages-of-decision-trees">Disadvantages of Decision Trees</h3> <ul> <li> <strong>Overfitting:</strong> As discussed, they are prone to overfitting, which can lead to poor generalization performance.</li> <li> <strong>Instability:</strong> Small changes in the training data can sometimes lead to a completely different tree structure, making them somewhat unstable.</li> <li> <strong>Bias:</strong> They can be biased towards features with more levels or dominant classes if not handled carefully.</li> </ul> <h3 id="beyond-a-single-tree-ensemble-methods">Beyond a Single Tree: Ensemble Methods</h3> <p>While individual Decision Trees are powerful, their limitations (especially instability and overfitting) led to the development of <strong>ensemble methods</strong>. These methods combine predictions from multiple Decision Trees to create a more robust and accurate model.</p> <p>Two of the most popular ensemble techniques are:</p> <ol> <li> <strong>Random Forests:</strong> Builds multiple Decision Trees independently and averages their predictions. This reduces variance and improves generalization.</li> <li> <strong>Gradient Boosting (e.g., XGBoost, LightGBM):</strong> Builds trees sequentially, where each new tree tries to correct the errors made by the previous ones. This focuses on reducing bias.</li> </ol> <p>These powerful ensembles are built directly upon the foundational understanding of single Decision Trees, underscoring just how crucial this algorithm is!</p> <h3 id="conclusion-your-algorithmic-compass">Conclusion: Your Algorithmic Compass</h3> <p>Decision Trees are more than just an algorithm; they’re an intuitive framework for thinking about data and making predictions. They mirror our human decision-making process, making them incredibly accessible yet surprisingly powerful. From determining loan eligibility to diagnosing medical conditions or predicting customer churn, Decision Trees serve as a reliable algorithmic compass, guiding us through complex data landscapes.</p> <p>I hope this journey into Decision Trees has been insightful for you. They are truly an essential tool in any data scientist’s toolkit and a fantastic starting point for understanding more advanced machine learning concepts. So go forth, explore your data, and happy tree building!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>