<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Invisible Tug-of-War: Why Accuracy Isn't Enough (A Deep Dive into Precision vs Recall) | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/the-invisible-tug-of-war-why-accuracy-isnt-enough/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Invisible Tug-of-War: Why Accuracy Isn't Enough (A Deep Dive into Precision vs Recall)</h1> <p class="post-meta"> Created on August 07, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/model-evaluation"> <i class="fa-solid fa-hashtag fa-sm"></i> Model Evaluation</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/classification"> <i class="fa-solid fa-hashtag fa-sm"></i> Classification</a>   <a href="/blog/blog/tag/metrics"> <i class="fa-solid fa-hashtag fa-sm"></i> Metrics</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>It feels like just yesterday I was getting started with machine learning, and honestly, the sheer volume of metrics, algorithms, and concepts felt a bit like trying to drink from a firehose. One of the first “aha!” moments I had, something that fundamentally shifted my understanding of what makes a “good” model, revolved around two seemingly simple terms: <em>Precision</em> and <em>Recall</em>.</p> <p>You see, when you’re first learning, “accuracy” is king. It’s intuitive: how many predictions did your model get right? But as I built more complex systems and started thinking about real-world consequences, I quickly learned that accuracy can be a deceptive friend. Sometimes, getting 95% of predictions correct isn’t good enough, or even worse, it might be hiding a critical flaw.</p> <p>Today, I want to take you on a journey through the often-misunderstood relationship between Precision and Recall. We’ll explore why they matter, when to prioritize one over the other, and how understanding this invisible tug-of-war is absolutely essential for anyone building intelligent systems. Whether you’re a high school student just dipping your toes into AI or a seasoned data scientist, I promise this will give you a deeper appreciation for the nuances of model evaluation.</p> <h3 id="the-problem-with-simple-accuracy-a-medical-mystery">The Problem with Simple Accuracy: A Medical Mystery</h3> <p>Let’s imagine a scenario close to home, one where the stakes are incredibly high. Our task is to build a machine learning model that can detect a rare, aggressive form of cancer from patient data. This cancer, if left undiagnosed, is deadly, but if caught early, it’s highly treatable.</p> <p>Our initial thought might be to just aim for high accuracy. Say, 98% accuracy! Sounds fantastic, right? But let’s pause and think about what that 2% error might mean.</p> <p>Imagine out of 1000 patients:</p> <ul> <li>990 patients are healthy.</li> <li>10 patients actually have the aggressive cancer.</li> </ul> <p>If our model achieves 98% accuracy, it means it got 980 predictions right. But <em>which</em> 980?</p> <p>What if our model simply predicted “healthy” for <em>every single patient</em>?</p> <ul> <li>It would correctly identify all 990 healthy patients (True Negatives).</li> <li>It would incorrectly identify all 10 sick patients as healthy (False Negatives).</li> </ul> <p>In this case, the accuracy would be $\frac{990}{1000} = 0.99$, or 99%! Even better than 98%! Yet, it missed every single person with cancer. This model, despite its high accuracy, is completely useless – it’s a death sentence for 10 people.</p> <p>This extreme example highlights why accuracy alone isn’t enough. We need to understand <em>what kind</em> of errors our model is making.</p> <h3 id="the-confusion-matrix-unpacking-the-errors">The Confusion Matrix: Unpacking the Errors</h3> <p>To truly understand our model’s performance, we need to break down its predictions into four categories. This is where the <strong>Confusion Matrix</strong> comes in, and it’s a cornerstone of classification evaluation.</p> <p>Let’s stick with our cancer detection example.</p> <ul> <li> <strong>Positive (P):</strong> The patient <em>actually has</em> cancer.</li> <li> <strong>Negative (N):</strong> The patient <em>does not have</em> cancer.</li> </ul> <p>And our model makes predictions:</p> <ul> <li> <strong>Predicted Positive (P’):</strong> Model says the patient has cancer.</li> <li> <strong>Predicted Negative (N’):</strong> Model says the patient does not have cancer.</li> </ul> <p>Now, let’s combine these:</p> <ol> <li> <strong>True Positives (TP):</strong> Our model predicted ‘cancer’ (P’), and the patient <em>actually had cancer</em> (P). <ul> <li> <em>Outcome:</em> Excellent! Correct diagnosis, lives potentially saved.</li> </ul> </li> <li> <strong>True Negatives (TN):</strong> Our model predicted ‘no cancer’ (N’), and the patient <em>actually didn’t have cancer</em> (N). <ul> <li> <em>Outcome:</em> Excellent! Correctly identified healthy patient, avoided unnecessary stress and tests.</li> </ul> </li> <li> <strong>False Positives (FP):</strong> Our model predicted ‘cancer’ (P’), but the patient <em>actually didn’t have cancer</em> (N). <ul> <li> <em>Outcome:</em> Bad. A “false alarm.” The patient is told they have cancer when they don’t, leading to immense stress, anxiety, and potentially expensive, invasive follow-up tests. This is also called a <strong>Type I Error</strong>.</li> </ul> </li> <li> <strong>False Negatives (FN):</strong> Our model predicted ‘no cancer’ (N’), but the patient <em>actually had cancer</em> (P). <ul> <li> <em>Outcome:</em> Catastrophic. The model missed an actual cancer case. The patient is told they are healthy when they are not, delaying critical treatment and potentially proving fatal. This is also called a <strong>Type II Error</strong>.</li> </ul> </li> </ol> <table> <thead> <tr> <th style="text-align: left"> </th> <th style="text-align: left"><strong>Actual Positive (P)</strong></th> <th style="text-align: left"><strong>Actual Negative (N)</strong></th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Predicted Positive (P’)</strong></td> <td style="text-align: left">True Positive (TP)</td> <td style="text-align: left">False Positive (FP)</td> </tr> <tr> <td style="text-align: left"><strong>Predicted Negative (N’)</strong></td> <td style="text-align: left">False Negative (FN)</td> <td style="text-align: left">True Negative (TN)</td> </tr> </tbody> </table> <p>Understanding these four quadrants is the foundation for everything that follows.</p> <h3 id="precision-when-your-yes-really-means-yes">Precision: When Your “Yes” Really Means “Yes”</h3> <p>Now that we have our confusion matrix, we can define Precision. Think of Precision as asking: <strong>“When my model says ‘yes’ (it predicts positive), how often is it actually correct?”</strong></p> <p>In our cancer example, if the model tells a patient, “You have cancer,” how confident can we be that the patient <em>actually</em> has cancer?</p> <p>The formula for Precision is:</p> <p>$Precision = \frac{TP}{TP + FP}$</p> <p>Let’s break that down:</p> <ul> <li> <strong>TP</strong> is the number of times our model correctly predicted cancer.</li> <li> <strong>TP + FP</strong> is the total number of times our model <em>predicted cancer</em> (both correctly and incorrectly).</li> </ul> <p>So, Precision tells us the proportion of our <em>positive predictions</em> that were truly positive.</p> <p><strong>Why is high Precision important?</strong> In our cancer diagnosis scenario, high precision means that when our model flags someone for cancer, there’s a very low chance it’s a false alarm. This is crucial because a false positive (telling a healthy person they have cancer) can lead to enormous psychological distress, unnecessary biopsies, invasive procedures, and financial burden. Imagine the emotional rollercoaster! If our precision is low, doctors might lose trust in the system due to too many false alarms.</p> <ul> <li> <strong>Example where Precision is paramount:</strong> <ul> <li> <strong>Spam Detection (False Positives are terrible):</strong> If a legitimate email (a True Negative in this case) is wrongly classified as spam (a False Positive), you might miss an important work email or a flight confirmation. You’d rather have some spam in your inbox (False Negatives) than miss a crucial email.</li> <li> <strong>Recommending a very expensive, irreversible treatment:</strong> You want to be highly precise that the person truly needs it before proceeding.</li> </ul> </li> </ul> <h3 id="recall-catching-all-the-fish">Recall: Catching All the Fish</h3> <p>On the other side of the coin, we have Recall. Recall asks: <strong>“Out of all the actual positive cases, how many did my model successfully identify?”</strong></p> <p>In our cancer example, out of all the patients who <em>actually have cancer</em>, how many did our model manage to find?</p> <p>The formula for Recall is:</p> <p>$Recall = \frac{TP}{TP + FN}$</p> <p>Let’s break that down:</p> <ul> <li> <strong>TP</strong> is the number of times our model correctly identified cancer.</li> <li> <strong>TP + FN</strong> is the total number of patients who <em>actually had cancer</em> (those we caught, plus those we missed).</li> </ul> <p>So, Recall tells us the proportion of all <em>actual positive cases</em> that our model correctly identified.</p> <p><strong>Why is high Recall important?</strong> In our cancer diagnosis scenario, high recall means our model is very good at catching <em>all</em> the patients who truly have cancer. This is paramount because a false negative (missing an actual cancer case) means a patient goes home thinking they’re healthy, while a deadly disease progresses untreated. The cost here is literally a human life. We want to avoid false negatives at all costs.</p> <ul> <li> <strong>Example where Recall is paramount:</strong> <ul> <li> <strong>Disease Outbreak Detection:</strong> If you’re tracking a highly contagious disease, you want to identify as many infected individuals as possible (high recall) to prevent further spread, even if it means some false alarms.</li> <li> <strong>Fraud Detection:</strong> Missing actual fraudulent transactions (False Negatives) can cost a bank millions. It’s often better to flag a few legitimate transactions for review (False Positives) than to let fraud slip through.</li> <li> <strong>Security Screening (e.g., airport scanners):</strong> Missing a dangerous item (False Negative) is catastrophic. Having to rescan a few harmless items (False Positive) is an inconvenience, but acceptable.</li> </ul> </li> </ul> <h3 id="the-invisible-tug-of-war-precision-vs-recall-trade-off">The Invisible Tug-of-War: Precision vs Recall Trade-off</h3> <p>This is the core concept: <strong>you usually can’t maximize both Precision and Recall simultaneously.</strong> They often have an inverse relationship. Improving one tends to decrease the other. This is the “tug-of-war.”</p> <p>Why? Because models often make decisions based on a “threshold.” Imagine your model outputs a probability score (e.g., 0 to 1) that a patient has cancer. You then set a threshold, say 0.5:</p> <ul> <li>If probability &gt; 0.5, predict ‘cancer’.</li> <li>If probability &lt;= 0.5, predict ‘no cancer’.</li> </ul> <p>Let’s see how adjusting this threshold impacts Precision and Recall:</p> <ul> <li> <strong>To increase Recall (catch more sick people):</strong> You might lower your threshold (e.g., to 0.3). <ul> <li>Now, more patients will be predicted ‘cancer’ (P’).</li> <li>This will likely increase your TP (you’re catching more actual sick people).</li> <li>But it will also likely increase your FP (you’re also getting more false alarms from healthy people).</li> <li> <em>Result:</em> Higher Recall (good for avoiding FN), but lower Precision (more FP).</li> </ul> </li> <li> <strong>To increase Precision (reduce false alarms):</strong> You might raise your threshold (e.g., to 0.7). <ul> <li>Now, fewer patients will be predicted ‘cancer’ (P’).</li> <li>This means your model is being very cautious. The predictions it <em>does</em> make will be very reliable (high TP, low FP).</li> <li>But it will also likely miss some actual sick people who had scores between 0.3 and 0.7 (increasing FN).</li> <li> <em>Result:</em> Higher Precision (good for avoiding FP), but lower Recall (more FN).</li> </ul> </li> </ul> <p>This trade-off is fundamental. Your choice of where to set this threshold – or more generally, how you design your model and its objective function – will directly determine the balance between Precision and Recall.</p> <h3 id="beyond-the-binary-f1-score-and-the-real-world">Beyond the Binary: F1-Score and The Real World</h3> <p>So, how do you decide which one to prioritize? It boils down to understanding the <strong>cost of different errors</strong> in your specific problem.</p> <ul> <li> <strong>When the cost of a False Positive (FP) is high</strong>, you’ll likely prioritize <strong>Precision</strong>. (e.g., telling someone they have a deadly disease when they don’t, flagging a legitimate user’s account for fraud, wasting resources on false alarms).</li> <li> <strong>When the cost of a False Negative (FN) is high</strong>, you’ll likely prioritize <strong>Recall</strong>. (e.g., missing an actual deadly disease, failing to detect a critical security threat, missing a truly fraudulent transaction).</li> </ul> <p>Sometimes, you need a balance between the two. That’s where the <strong>F1-Score</strong> comes in handy. It’s the harmonic mean of Precision and Recall:</p> <p>$F1-Score = 2 \times \frac{Precision \times Recall}{Precision + Recall}$</p> <p>The F1-Score gives equal weight to Precision and Recall. If either Precision or Recall is very low, the F1-Score will also be low, penalizing models that perform well on one but poorly on the other. It’s a good single metric when you need a reasonable balance.</p> <p>There are also generalized F-beta scores ($F_\beta$) where you can weigh Recall ($\beta &gt; 1$) or Precision ($\beta &lt; 1$) more heavily. For example, an $F_2$-score weights Recall twice as much as Precision, useful in scenarios like our cancer detection where False Negatives are especially detrimental.</p> <h3 id="my-personal-takeaway">My Personal Takeaway</h3> <p>My journey into data science has taught me that the technical details are only half the story. The other, equally crucial half, is understanding the <em>context</em> and the <em>impact</em> of your models on the real world. A perfect algorithm might yield terrible results if its evaluation metric doesn’t align with the problem’s actual goals and risks.</p> <p>Precision and Recall aren’t just formulas; they’re reflections of the consequences of your model’s decisions. They force you to ask tough questions: What are the real costs of a false alarm? What are the real costs of a missed opportunity or a missed threat?</p> <p>So, as you build your next model, resist the urge to just glance at accuracy. Dig deeper. Construct that confusion matrix. Calculate Precision and Recall. And most importantly, have a thoughtful conversation with stakeholders about which type of error is more tolerable. That’s where true machine learning mastery begins, transforming you from a model builder into a problem solver.</p> <p>Keep exploring, keep questioning, and happy modeling!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>