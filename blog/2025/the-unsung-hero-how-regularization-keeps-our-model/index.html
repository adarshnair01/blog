<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Unsung Hero: How Regularization Keeps Our Models Honest (and Smart!) | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/2025/the-unsung-hero-how-regularization-keeps-our-model/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Unsung Hero: How Regularization Keeps Our Models Honest (and Smart!)</h1> <p class="post-meta"> Created on October 04, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/regularization"> <i class="fa-solid fa-hashtag fa-sm"></i> Regularization</a>   <a href="/blog/blog/tag/overfitting"> <i class="fa-solid fa-hashtag fa-sm"></i> Overfitting</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/model-optimization"> <i class="fa-solid fa-hashtag fa-sm"></i> Model Optimization</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>Welcome back to my little corner of the internet where we demystify the magic behind data science. Today, I want to talk about something incredibly fundamental, yet often overlooked until you’ve bumped your head against the wall a few times: <strong>Regularization</strong>.</p> <p>I remember when I first started building machine learning models. I was so proud of my model’s ability to achieve 99% accuracy on my training data. “Look at it go!” I thought. Then came the moment of truth: testing it on new, unseen data. And… <em>thud</em>. A spectacular flop. My model, which seemed like a genius just moments ago, was now acting like it had never seen a data point in its life.</p> <p>Sound familiar? This, my friends, is the classic tale of <strong>overfitting</strong>.</p> <h3 id="the-overzealous-learner-understanding-overfitting">The Overzealous Learner: Understanding Overfitting</h3> <p>Imagine you’re studying for a history exam. You could try to memorize every single date, name, and specific detail from your textbook, word-for-word. You’d probably ace any question that’s an exact replica of what’s in the book. But what if the teacher asks a question that requires you to <em>understand</em> the broader historical context, or to analyze an event not explicitly detailed in the text? Your pure memorization strategy would likely fail. You’ve “overfit” to the training data (your textbook).</p> <p>In machine learning, overfitting happens when our model learns the training data <em>too</em> well. It doesn’t just learn the underlying patterns; it also memorizes the noise, the random fluctuations, and even the outliers present in that specific dataset. When presented with new data, which inevitably has different noise and specific quirks, the overfit model performs poorly because it hasn’t learned the general rules – it’s just memorized exceptions.</p> <p>Think of it like a tailor making a suit for you. An overfit model is like a suit that fits <em>perfectly</em> when you stand still in one specific pose, but restricts movement and looks terrible as soon as you try to walk or sit down. It’s too specialized.</p> <p>On the other end of the spectrum, we have <strong>underfitting</strong>, where the model is too simple to capture the underlying patterns at all – like a child’s drawing trying to represent a complex landscape. But today, our focus is on saving the overzealous learner.</p> <h3 id="enter-regularization-the-guiding-hand">Enter Regularization: The Guiding Hand</h3> <p>So, how do we tell our model, “Hey, calm down! Don’t memorize everything; try to understand the bigger picture”? This is where <strong>regularization</strong> comes in.</p> <p>Regularization is a technique designed to <em>discourage overly complex models</em>. It does this by adding a “penalty” term to the model’s loss function.</p> <p>Let’s quickly recall what a loss function is. It’s the mathematical expression that measures how “wrong” our model’s predictions are compared to the actual values. Our model’s goal during training is to minimize this loss function.</p> <p>For a linear regression model, for instance, a common loss function is the Mean Squared Error (MSE):</p> <p>$ J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})^2 $</p> <p>Here, $h_\theta(x^{(i)})$ is our model’s prediction for the $i$-th data point, $y^{(i)}$ is the actual value, $m$ is the number of training examples, and $\theta$ represents the model’s parameters (the coefficients or weights).</p> <p>Regularization simply takes this existing loss function and adds something extra to it:</p> <p>$ J_{regularized}(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})^2 + \text{Regularization Term} $</p> <p>This “Regularization Term” is the magic ingredient. What it essentially does is penalize large values for the model’s coefficients ($\theta_j$). Why? Because larger coefficients often indicate a more complex model that’s trying too hard to fit every single data point, including the noise. By penalizing these large coefficients, we encourage the model to find a simpler, more generalized solution.</p> <p>The strength of this penalty is controlled by a hyperparameter, usually denoted by <strong>$\lambda$ (lambda)</strong>.</p> <ul> <li>If $\lambda$ is 0, there’s no penalty, and we’re back to our original, potentially overfit, model.</li> <li>If $\lambda$ is very large, the penalty for large coefficients becomes so significant that the model might shrink them all close to zero, leading to underfitting.</li> <li>The sweet spot for $\lambda$ is usually found through techniques like cross-validation. It’s a balancing act: you want enough penalty to prevent overfitting, but not so much that you induce underfitting.</li> </ul> <h3 id="the-two-main-flavors-l1-vs-l2-regularization">The Two Main Flavors: L1 vs. L2 Regularization</h3> <p>There are two primary types of regularization you’ll encounter most often: L1 and L2. They both add a penalty based on the magnitude of the coefficients, but they do it in slightly different ways, leading to distinct effects.</p> <h4 id="1-l2-regularization-ridge-regression">1. L2 Regularization (Ridge Regression)</h4> <p>Also known as Ridge Regression when applied to linear regression, L2 regularization adds the sum of the <em>squares</em> of the coefficients to the loss function.</p> <p>The regularization term looks like this:</p> <p>$ \text{Regularization Term (L2)} = \lambda \sum_{j=1}^{n} \theta_j^2 $</p> <p>So, our full L2-regularized loss function becomes:</p> <p>$ J_{L2}(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda \sum_{j=1}^{n} \theta_j^2 $</p> <p><strong>Intuition:</strong></p> <ul> <li> <strong>Shrinks Coefficients:</strong> L2 regularization tends to shrink all coefficients towards zero, but it rarely makes them exactly zero.</li> <li> <strong>Why squared?</strong> Squaring the coefficients means that larger coefficients are penalized much more heavily than smaller ones. This “gentle push” helps distribute the weight across all features.</li> <li> <strong>Analogy:</strong> Imagine a group of students (features) contributing to a project (prediction). L2 regularization tells them, “Everyone, dial back your individual contributions a bit so the overall project is more balanced.” No one gets completely cut out, but everyone’s impact is reduced.</li> <li> <strong>When to use:</strong> It’s particularly useful when you have many features that are all somewhat relevant, and you want to reduce the impact of each without completely eliminating any. It’s also less sensitive to outliers than L1.</li> </ul> <h4 id="2-l1-regularization-lasso-regression">2. L1 Regularization (Lasso Regression)</h4> <p>Known as Lasso Regression (Least Absolute Shrinkage and Selection Operator) for linear models, L1 regularization adds the sum of the <em>absolute values</em> of the coefficients to the loss function.</p> <p>The regularization term looks like this:</p> <table> <tbody> <tr> <td>$ \text{Regularization Term (L1)} = \lambda \sum_{j=1}^{n}</td> <td>\theta_j</td> <td>$</td> </tr> </tbody> </table> <p>And the full L1-regularized loss function:</p> <table> <tbody> <tr> <td>$ J_{L1}(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda \sum_{j=1}^{n}</td> <td>\theta_j</td> <td>$</td> </tr> </tbody> </table> <p><strong>Intuition:</strong></p> <ul> <li> <strong>Feature Selection (Sparsity):</strong> This is L1’s superstar feature! Because it uses the absolute value, L1 regularization has a tendency to shrink some coefficients <em>exactly</em> to zero. This effectively “selects” features, discarding the less important ones from the model.</li> <li> <strong>Why absolute value?</strong> The geometry of the absolute value penalty encourages solutions where coefficients lie on the axes, leading to zeros. (This is often visualized with contour plots of the loss function and the regularization penalty, where the L1 penalty forms a diamond shape with “corners” at the axes).</li> <li> <strong>Analogy:</strong> Going back to our project analogy, L1 regularization is like a project manager (the model) saying, “Okay, we have too many people doing similar things. Let’s identify the most crucial contributors and let others go, so we can focus our resources.”</li> <li> <strong>When to use:</strong> L1 is ideal when you suspect that many features in your dataset are irrelevant or redundant. It provides automatic feature selection, leading to simpler, more interpretable models.</li> </ul> <h4 id="elastic-net-regularization">Elastic Net Regularization</h4> <p>What if you want the best of both worlds? That’s where <strong>Elastic Net</strong> comes in. It combines both L1 and L2 penalties:</p> <table> <tbody> <tr> <td>$ J_{ElasticNet}(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda_1 \sum_{j=1}^{n}</td> <td>\theta_j</td> <td>+ \lambda_2 \sum_{j=1}^{n} \theta_j^2 $</td> </tr> </tbody> </table> <p>Here, you have two $\lambda$ parameters ($\lambda_1$ for L1 and $\lambda_2$ for L2), giving you even more fine-grained control. It’s particularly useful when you have many highly correlated features, as L1 tends to pick just one of them while L2 keeps them all. Elastic Net can group correlated variables together.</p> <h3 id="beyond-l1l2-other-regularization-techniques">Beyond L1/L2: Other Regularization Techniques</h3> <p>While L1 and L2 are workhorses, especially in traditional statistical modeling and linear models, regularization isn’t limited to just these. For instance, in the world of deep learning (neural networks):</p> <ul> <li> <strong>Dropout:</strong> Randomly “drops out” (sets to zero) a percentage of neurons during training. This forces the network to learn more robust features and prevents over-reliance on any single neuron, much like forcing a team to work effectively even if some members are absent.</li> <li> <strong>Early Stopping:</strong> Simply stopping the training process once the model’s performance on a validation set starts to degrade, even if it’s still improving on the training set. It’s like telling your model, “You’ve learned enough; pushing further will only lead to memorization.”</li> </ul> <h3 id="the-power-of-balance">The Power of Balance</h3> <p>Regularization, at its core, is about achieving a balance. It’s about finding that sweet spot between a model that’s too simple (underfit) and one that’s too complex (overfit). It’s a crucial tool in any data scientist’s toolkit because, in the real world, our goal isn’t just to make accurate predictions on data we’ve already seen, but to make reliable predictions on data we <em>haven’t</em> seen yet.</p> <p>By understanding L1 and L2 regularization, you gain a powerful way to guide your models towards generalization, making them not just smart, but truly wise. So, the next time your model is getting a bit too enthusiastic about your training data, remember the quiet hero: regularization, the unsung champion fighting against overfitting!</p> <p>Keep learning, keep building, and keep your models honest!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>