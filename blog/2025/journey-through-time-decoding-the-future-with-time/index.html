<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Journey Through Time: Decoding the Future with Time Series Analysis | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/journey-through-time-decoding-the-future-with-time/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Journey Through Time: Decoding the Future with Time Series Analysis</h1> <p class="post-meta"> Created on July 29, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/time-series-analysis"> <i class="fa-solid fa-hashtag fa-sm"></i> Time Series Analysis</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/forecasting"> <i class="fa-solid fa-hashtag fa-sm"></i> Forecasting</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/statistics"> <i class="fa-solid fa-hashtag fa-sm"></i> Statistics</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>Have you ever gazed at a graph that shows how something changes over days, months, or years? Maybe it was stock prices fluctuating, the temperature rising and falling with the seasons, or even the number of YouTube views on your favorite video. What we’re looking at in these scenarios isn’t just a collection of numbers; it’s a story unfolding through time. And that, my friends, is the heart of <strong>Time Series Analysis</strong>.</p> <p>Welcome to my personal journal entry on a topic that truly fascinates me: making sense of data that has a memory. It’s not just about what happened, but <em>when</em> it happened, and how that sequence gives us clues about what <em>might</em> happen next.</p> <h3 id="whats-so-special-about-time">What’s So Special About Time?</h3> <p>In most data science problems, the order of your data points doesn’t inherently matter. If you’re predicting whether an email is spam, the order you received emails won’t typically change your prediction for a <em>single</em> email. But with time series data, the chronological order is paramount. It’s the very backbone of our analysis.</p> <p>Imagine you have a series of daily temperatures. Yesterday’s temperature ($T_{yesterday}$) is usually a pretty good predictor of today’s temperature ($T_{today}$). But a temperature from three months ago ($T_{3_months_ago}$) might not be. This dependency on past observations is what makes time series data unique and exciting.</p> <h3 id="deconstructing-the-dance-components-of-a-time-series">Deconstructing the Dance: Components of a Time Series</h3> <p>When we look at a time series, it often looks like a messy, wiggly line. But beneath that surface, there are usually several underlying patterns playing a rhythmic dance. Understanding these components is the first step to truly “seeing” our data. We often model a time series $Y_t$ as a combination of these elements:</p> <p>$Y_t = T_t + S_t + C_t + R_t$ (Additive Model)</p> <p>Or sometimes:</p> <p>$Y_t = T_t \times S_t \times C_t \times R_t$ (Multiplicative Model)</p> <p>Where:</p> <ol> <li> <p><strong>Trend ($T_t$)</strong>: This is the long-term, underlying movement in the series. Is the data generally increasing, decreasing, or staying flat over time? Think of the global average temperature slowly rising over decades, or a company’s sales steadily growing year after year. It’s the big-picture direction.</p> </li> <li> <p><strong>Seasonality ($S_t$)</strong>: These are predictable, repeating patterns that occur over a fixed period, like a day, week, month, or year. Air conditioner sales spike in summer and dip in winter. Electricity consumption peaks during the day and falls at night. These patterns are consistent and easy to spot once you know the period.</p> </li> <li> <p><strong>Cyclical ($C_t$)</strong>: These are fluctuations that are not of fixed period, usually longer than seasonal patterns, and often associated with economic or business cycles (e.g., recession, expansion). They are less regular and harder to predict than seasonal patterns. Think of the overall ebb and flow of a country’s economy over several years. Sometimes, ‘Cyclical’ and ‘Trend’ are grouped together, especially if the cycles are very long.</p> </li> <li> <p><strong>Irregular / Residual ($R_t$)</strong>: This is the random, unpredictable component of the series. It’s what’s left over after we’ve accounted for the trend, seasonality, and cyclical patterns. Think of a sudden, unexpected news event that causes a stock price to briefly drop, or a random error in a sensor reading. This is the noise, the unexplained variability that we hope to minimize but can never fully eliminate.</p> </li> </ol> <p>Visualizing these components separately is often the first thing a data scientist does. Tools like <code class="language-plaintext highlighter-rouge">statsmodels</code> in Python can <em>decompose</em> a time series into these very parts, revealing the hidden layers of its story.</p> <h3 id="the-holy-grail-stationarity">The Holy Grail: Stationarity</h3> <p>This is a big word, and a <em>huge</em> concept in time series analysis. Many traditional time series models, like ARIMA (which we’ll touch upon), assume that the series we are working with is <strong>stationary</strong>.</p> <p>But what does stationary mean? Imagine a calm, still lake. Its average water level doesn’t change much, and the ripples on its surface behave similarly no matter where you look. Now imagine a turbulent river. Its water level might change drastically, and the currents vary wildly from one bend to the next.</p> <p>A time series is considered stationary if its statistical properties – specifically its <strong>mean</strong>, <strong>variance</strong>, and <strong>autocorrelation</strong> – remain constant over time.</p> <ul> <li> <strong>Constant Mean</strong>: The average value of the series doesn’t systematically increase or decrease. No trend.</li> <li> <strong>Constant Variance</strong>: The variability or spread of the data around its mean remains consistent. The fluctuations don’t get dramatically larger or smaller over time.</li> <li> <strong>Constant Autocorrelation</strong>: The relationship between a value and its past values stays the same over time. The “memory” of the series doesn’t change.</li> </ul> <p>Why is stationarity so important? Because if these properties are stable, we can make reliable inferences about future values based on past observations. If the mean is constantly shifting, how can we predict what the <em>next</em> mean will be?</p> <p>Most real-world time series are <em>non-stationary</em>. They have trends, seasonality, or changing variance. So, a crucial step in time series analysis is to <strong>transform</strong> a non-stationary series into a stationary one. The most common technique for this is <strong>differencing</strong>.</p> <p>Differencing involves calculating the difference between consecutive observations. For example, a first-order difference is $Y_t’ = Y_t - Y_{t-1}$. This often removes trends. Seasonal differencing ($Y_t’ = Y_t - Y_{t-season_period}$) can remove seasonality. It’s like taking the river’s water level and instead looking at how much the level <em>changed</em> from one moment to the next.</p> <h3 id="peeking-into-the-past-autocorrelation-and-partial-autocorrelation">Peeking into the Past: Autocorrelation and Partial Autocorrelation</h3> <p>How do we quantify the “memory” of a time series? We use <strong>autocorrelation</strong>.</p> <ul> <li> <p><strong>Autocorrelation Function (ACF)</strong>: This measures the correlation between a time series and a lagged version of itself. In simpler terms, it tells us how much the value at time $t$ is related to the value at time $t-1$, $t-2$, $t-3$, and so on. If the ACF plot shows significant spikes at certain lags, it suggests that values at those lags have a strong relationship. For example, a strong correlation at lag 7 in daily data indicates a weekly seasonal pattern.</p> </li> <li> <p><strong>Partial Autocorrelation Function (PACF)</strong>: This is a bit more nuanced. While ACF measures the <em>total</em> correlation between $Y_t$ and $Y_{t-k}$ (including the indirect correlation transmitted through intermediate lags $Y_{t-1}, …, Y_{t-k+1}$), PACF measures the <em>direct</em> correlation between $Y_t$ and $Y_{t-k}$, after removing the influence of the values in between. Think of it this way: how much information does $Y_{t-k}$ provide about $Y_t$ that $Y_{t-1}, …, Y_{t-k+1}$ couldn’t already explain?</p> </li> </ul> <p>ACF and PACF plots are indispensable tools. They help us identify the appropriate order for ARIMA models by showing us at which lags the correlations are statistically significant. For example, a sharp drop in PACF after lag $p$ often suggests an AutoRegressive (AR) component of order $p$. Similarly, a sharp drop in ACF after lag $q$ suggests a Moving Average (MA) component of order $q$.</p> <h3 id="building-predictors-the-arima-family">Building Predictors: The ARIMA Family</h3> <p>Now that we understand the ingredients, let’s talk about some of the workhorse models in time series analysis: the <strong>ARIMA</strong> family. ARIMA stands for <strong>A</strong>uto<strong>R</strong>egressive <strong>I</strong>ntegrated <strong>M</strong>oving <strong>A</strong>verage. It’s typically denoted as ARIMA(p, d, q).</p> <ul> <li> <p><strong>AR (Autoregressive) - ‘p’</strong>: This component models the relationship between an observation and a number of lagged observations. Essentially, it’s a linear regression of the current value of the series against its own past values. If we say AR(1), it means the current value depends on the immediately preceding value: $Y_t = c + \phi_1 Y_{t-1} + \epsilon_t$ where $\phi_1$ is the coefficient for the lag-1 term, and $\epsilon_t$ is white noise. An AR(p) model includes $p$ such lagged terms.</p> </li> <li> <p><strong>I (Integrated) - ‘d’</strong>: This part handles the non-stationarity we discussed earlier. The ‘d’ stands for the number of times the raw observations are differenced to make the time series stationary. If $d=1$, we apply first-order differencing. If $d=2$, we difference the differenced series.</p> </li> <li> <p><strong>MA (Moving Average) - ‘q’</strong>: This component models the relationship between an observation and a lagged forecast error. Instead of using past observations directly, it uses the past <em>errors</em> (the difference between what we predicted and what actually happened). An MA(1) model looks like: $Y_t = c + \theta_1 \epsilon_{t-1} + \epsilon_t$ where $\theta_1$ is the coefficient for the lag-1 error term. An MA(q) model includes $q$ such lagged error terms.</p> </li> </ul> <p>Combining these gives us the ARIMA(p,d,q) model, a powerful framework for forecasting.</p> <p>But what about seasonality? That’s where <strong>SARIMA</strong> (Seasonal ARIMA) comes in, denoted as SARIMA(p, d, q)(P, D, Q, S). The second set of (P, D, Q, S) parameters handles the seasonal components, where ‘S’ is the length of the seasonal period (e.g., 12 for monthly data, 7 for daily data).</p> <p>Beyond ARIMA, the world of time series models is vast. We have:</p> <ul> <li> <strong>Exponential Smoothing Models (ETS)</strong>: Like Holt-Winters, which are great for data with trends and seasonality.</li> <li> <strong>Prophet</strong>: Developed by Facebook, known for handling missing data, outliers, and having an intuitive approach for trends and multiple seasonalities.</li> <li> <strong>Machine Learning Models</strong>: Random Forests, Gradient Boosting, or even Deep Learning models like <strong>Recurrent Neural Networks (RNNs)</strong> and <strong>LSTMs (Long Short-Term Memory networks)</strong>, which are particularly adept at capturing complex, long-term dependencies in sequential data. These models often treat the time series forecasting problem as a supervised learning problem by creating lagged features.</li> </ul> <h3 id="a-conceptual-walkthrough-forecasting-ice-cream-sales">A Conceptual Walkthrough: Forecasting Ice Cream Sales</h3> <p>Let’s imagine we’re data scientists for a company selling ice cream, and we want to predict next month’s sales.</p> <ol> <li> <strong>Data Collection &amp; Loading</strong>: We gather historical monthly sales data for the past few years.</li> <li> <strong>Visualization</strong>: We plot the sales over time. Immediately, we notice an upward <strong>trend</strong> (business is growing!) and clear <strong>seasonality</strong> (sales spike in summer, dip in winter).</li> <li> <strong>Decomposition</strong>: We use a statistical method to separate the sales into its trend, seasonal, and residual components. This confirms our visual observations and helps us quantify them.</li> <li> <strong>Stationarity Check</strong>: We perform a statistical test (like the Augmented Dickey-Fuller test, ADF) to check if the series is stationary. It’s probably not, due to the trend and seasonality.</li> <li> <strong>Differencing</strong>: To achieve stationarity, we apply differencing. We might apply a first-order difference to remove the trend ($d=1$). We’d also apply a seasonal difference (e.g., lag 12 for monthly data, so $D=1$) to remove the yearly seasonality.</li> <li> <strong>ACF/PACF Analysis</strong>: We examine the ACF and PACF plots of our <em>differenced</em> series. This is where the magic happens for ARIMA. We look for significant spikes or cut-offs to determine our ‘p’ and ‘q’ values (and ‘P’ and ‘Q’ for seasonality).</li> <li> <strong>Model Selection &amp; Fitting</strong>: Based on the ACF/PACF plots and possibly some trial and error, we choose our SARIMA(p,d,q)(P,D,Q,S) model. We then ‘fit’ this model to our historical data.</li> <li> <strong>Forecasting</strong>: Once the model is trained, we use it to predict future sales, e.g., for the next 6-12 months.</li> <li> <strong>Evaluation</strong>: We compare our model’s predictions with actual sales from a ‘hold-out’ period (data the model hasn’t seen). Metrics like Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), or Mean Absolute Percentage Error (MAPE) help us understand how accurate our forecasts are.</li> </ol> <h3 id="the-road-ahead-challenges-and-considerations">The Road Ahead: Challenges and Considerations</h3> <p>Time series analysis isn’t without its quirks:</p> <ul> <li> <strong>Missing Data and Outliers</strong>: Gaps in data or unusually high/low values can significantly throw off models. Imputation and outlier detection become critical.</li> <li> <strong>Non-linear Relationships</strong>: Not all patterns are linear. Sometimes, complex machine learning models are needed to capture these nuances.</li> <li> <strong>Changing Dynamics</strong>: What if the trend suddenly changes? Or a new seasonal pattern emerges? Models need to be regularly updated and retrained.</li> <li> <strong>Exogenous Variables</strong>: Sometimes, external factors (like marketing spend, holidays, competitor actions) also influence the time series. Incorporating these “exogenous variables” into models (e.g., SARIMAX) can significantly improve accuracy.</li> <li> <strong>Forecast Horizon</strong>: Predicting one step ahead is often easier than predicting far into the future. Uncertainty generally increases with the forecast horizon.</li> </ul> <h3 id="concluding-thoughts">Concluding Thoughts</h3> <p>Time series analysis is a powerful blend of statistics, mathematics, and intuition. It’s about recognizing patterns, understanding dependencies, and using that knowledge to peer into the future. Whether you’re forecasting stock prices, predicting energy demand, or understanding the spread of a disease, the ability to analyze and model time-dependent data is an indispensable skill in the data science toolkit.</p> <p>It’s a field that’s constantly evolving, with new techniques emerging from machine learning and deep learning, pushing the boundaries of what’s possible. So, dive in, explore some datasets, build your first forecasting model, and start decoding the rhythmic stories hidden within time!</p> <p>Happy forecasting!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>