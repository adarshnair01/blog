<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Whisperers of Tomorrow: My Deep Dive into Large Language Models | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/the-whisperers-of-tomorrow-my-deep-dive-into-large/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Whisperers of Tomorrow: My Deep Dive into Large Language Models</h1> <p class="post-meta"> Created on August 30, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/natural-language-processing"> <i class="fa-solid fa-hashtag fa-sm"></i> Natural Language Processing</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/transformers"> <i class="fa-solid fa-hashtag fa-sm"></i> Transformers</a>   <a href="/blog/blog/tag/artificial-intelligence"> <i class="fa-solid fa-hashtag fa-sm"></i> Artificial Intelligence</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>As a budding Data Scientist and MLE, few technologies have captured my imagination quite like Large Language Models (LLMs). There’s something almost magical about typing a simple prompt and watching an AI generate a coherent essay, solve a complex coding problem, or even craft a sonnet. It feels like we’re peeking into the future, a future where machines don’t just process information but <em>understand</em> and <em>create</em>.</p> <p>But how does this magic actually work? What are these “brains” behind the chatbot? I remember the first time I started digging into the technical papers, feeling a mix of awe and bewilderment. Billions of parameters? Attention mechanisms? Reinforcement Learning from Human Feedback? It felt like climbing Mount Everest. But piece by piece, the puzzle started to come together, and what I discovered was a blend of elegant mathematics, immense computational power, and truly clever engineering.</p> <p>So, let’s embark on this journey together. Whether you’re a high school student curious about AI or a fellow data enthusiast, I hope to demystify LLMs and share the wonder I found in their architecture and capabilities.</p> <h2 id="what-is-a-language-model-really">What <em>is</em> a Language Model, Really?</h2> <p>Before we get to the “Large” part, let’s understand the core concept: a <strong>Language Model</strong>. At its heart, a language model is a system designed to predict the next word in a sequence, given the words that came before it. Think of it like the autocomplete feature on your phone, but on steroids.</p> <p>For example, if I start a sentence: “The cat sat on the…”, what’s the most likely next word? “Mat,” “roof,” “fence”? A language model learns these probabilities from vast amounts of text data.</p> <p>Early language models were quite simple, like <em>n</em>-gram models, which would look at the previous <code class="language-plaintext highlighter-rouge">n</code> words to predict the next. Then came recurrent neural networks (RNNs) and LSTMs, which could process sequences more effectively by maintaining a “memory” of previous words. These were cool, but they struggled with very long sentences and couldn’t process information in parallel, making them slow to train on massive datasets.</p> <h2 id="the-large-leap-scale-and-emergence">The “Large” Leap: Scale and Emergence</h2> <p>The revolution began when models started getting <em>big</em>. Really, really big. The “Large” in LLM refers to two main things:</p> <ol> <li> <strong>The Number of Parameters</strong>: These are the tunable weights and biases within the neural network that the model learns during training. While early neural networks might have millions of parameters, LLMs boast billions, even trillions. GPT-3, for instance, has 175 billion parameters. Imagine the complexity of a machine with 175 billion knobs to tune!</li> <li> <strong>The Scale of Training Data</strong>: LLMs are trained on truly colossal datasets, often comprising petabytes of text scraped from the internet (like Common Crawl), digitized books, articles, and more. This gargantuan exposure to human language allows them to absorb an incredible amount of information about grammar, facts, common sense, and even different writing styles.</li> </ol> <p>This combination of massive parameters and data leads to what we call <strong>emergent abilities</strong>. It’s like a phase transition in physics: a small amount of water behaves predictably, but when you add enough molecules and lower the temperature, it suddenly <em>freezes</em> into ice, exhibiting entirely new properties. Similarly, when language models cross a certain threshold in size and data, they suddenly start showing abilities they weren’t explicitly programmed for: complex reasoning, common-sense understanding, multi-step problem solving, and even a rudimentary form of creativity.</p> <h2 id="the-transformer-the-secret-sauce">The Transformer: The Secret Sauce</h2> <p>So, how do these huge models process information so efficiently and effectively? The answer lies primarily in a groundbreaking architecture introduced in 2017 by Google researchers: the <strong>Transformer</strong>. This architecture ditched the sequential processing of RNNs and LSTMs, allowing for unparalleled parallelization and the ability to “see” the entire input sequence at once.</p> <p>The star of the Transformer show is the <strong>Attention Mechanism</strong>.</p> <h3 id="1-attention-noticing-whats-important">1. Attention: Noticing What’s Important</h3> <p>Imagine you’re reading a complex paragraph. Your brain doesn’t just process word by word sequentially; it highlights important words, connects ideas across sentences, and pays more attention to certain parts of the text to understand the context. The Attention Mechanism does something similar for the LLM.</p> <p>When the model processes a word, it doesn’t just look at the immediately preceding words. It looks at <em>all</em> other words in the input sequence and calculates how relevant each of them is to understanding the current word.</p> <p>Mathematically, this “attention score” is often calculated using three concepts:</p> <ul> <li> <strong>Query (Q)</strong>: What I’m currently looking for (e.g., the current word).</li> <li> <strong>Key (K)</strong>: What each other word can offer (e.g., the context of other words).</li> <li> <strong>Value (V)</strong>: The actual information from other words that I want to aggregate.</li> </ul> <p>The core idea of scaled dot-product attention can be expressed as:</p> <p>$Attention(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$</p> <p>Don’t let the math scare you! In simple terms:</p> <ol> <li>We multiply Query (current word’s representation) by the Transpose of Keys (all other words’ representations) to get similarity scores.</li> <li>We divide by $\sqrt{d_k}$ (the square root of the dimension of the keys) to prevent the dot products from getting too large, stabilizing training.</li> <li>We apply <code class="language-plaintext highlighter-rouge">softmax</code> to turn these scores into probabilities, telling us <em>how much</em> attention to give each word (summing to 1).</li> <li>Finally, we multiply these probabilities by the Values (the actual content of other words) and sum them up. This gives us a new, context-rich representation for our current word.</li> </ol> <h3 id="2-multi-head-attention-multiple-perspectives">2. Multi-Head Attention: Multiple Perspectives</h3> <p>Just like a detective might look at a crime scene from multiple angles, the Transformer uses <strong>Multi-Head Attention</strong>. Instead of just one set of Q, K, V, it uses several sets (e.g., 8 or 12 “heads”). Each head learns to focus on different aspects of the relationships between words. One head might focus on grammatical dependencies, another on semantic relationships, and yet another on coreferencing (e.g., “he” referring to “John”). The outputs from all these heads are then concatenated and linearly transformed to form the final attention output.</p> <h3 id="3-positional-encoding-understanding-order">3. Positional Encoding: Understanding Order</h3> <p>Since the Transformer processes words in parallel, it loses the inherent order information that RNNs provided. To solve this, <strong>Positional Encodings</strong> are added to the input word embeddings. These are special vectors that tell the model the absolute or relative position of each word in the sequence. It’s like giving each word a little tag indicating its place in line, ensuring the model knows that “dog bites man” is different from “man bites dog.”</p> <p>Most modern LLMs, like the GPT (Generative Pre-trained Transformer) series, primarily use the <strong>decoder-only</strong> architecture of the Transformer. This means they are excellent at generating sequences of text, predicting the next token based on all preceding tokens.</p> <h2 id="training-llms-a-glimpse-behind-the-curtain">Training LLMs: A Glimpse Behind the Curtain</h2> <p>Training an LLM is a two-stage process, often compared to a marathon followed by a sprint.</p> <h3 id="1-pre-training-the-marathon-of-unsupervised-learning">1. Pre-training: The Marathon of Unsupervised Learning</h3> <p>This is where the “Large” data and “Large” parameters truly come into play. The model is fed vast amounts of raw, unlabeled text data and tasked with predicting missing words or the next word in a sequence. It’s a form of <strong>unsupervised learning</strong>. For instance, in a sentence like “The quick brown fox jumps over the lazy dog,” the model might be asked to predict “jumps” given the rest of the sentence, or to predict “dog” given “The quick brown fox jumps over the lazy”.</p> <p>This phase requires immense computational resources – thousands of powerful GPUs running for weeks or months. During pre-training, the model learns the statistical properties of language, grammar, factual knowledge, and even some reasoning patterns implicitly embedded in the text. This is where it builds its foundational “world model.”</p> <h3 id="2-fine-tuning--instruction-tuning-the-sprint-for-alignment">2. Fine-tuning &amp; Instruction Tuning: The Sprint for Alignment</h3> <p>After pre-training, you have a powerful but somewhat raw language model. It knows a lot, but it might not be good at following specific instructions or generating responses that are helpful, harmless, and honest. This is where <strong>fine-tuning</strong> comes in.</p> <ul> <li> <strong>Instruction Tuning</strong>: The model is further trained on smaller, high-quality datasets consisting of input-output pairs (e.g., a question and a desired answer). This teaches the model to follow instructions, understand different prompt formats, and generate appropriate responses.</li> <li> <strong>Reinforcement Learning from Human Feedback (RLHF)</strong>: This is a crucial step that aligns the model’s behavior with human preferences. Humans rate various responses generated by the model, indicating which ones are better, safer, or more helpful. This feedback is then used to train a “reward model,” which in turn guides the LLM to generate higher-quality and more desirable outputs. This is largely responsible for making models like ChatGPT feel so conversational and helpful.</li> </ul> <p>The distinction between a “base model” (after pre-training) and a “chat model” (after fine-tuning/RLHF) is important. A base model might simply complete a sentence, while a chat model will try to answer a question or follow a command.</p> <h2 id="why-do-they-work-so-well-emergent-abilities">Why Do They Work So Well? Emergent Abilities</h2> <p>The most mind-boggling aspect of LLMs is their <strong>emergent abilities</strong>. These are capabilities that weren’t explicitly programmed or obvious in smaller models, but appear almost magically when the models scale up:</p> <ul> <li> <strong>In-context Learning</strong>: The ability to learn from a few examples given in the prompt, without explicit fine-tuning.</li> <li> <strong>Reasoning</strong>: Performing logical deductions, solving math problems, or generating code.</li> <li> <strong>Common Sense</strong>: Understanding typical real-world scenarios.</li> <li> <strong>Multilingualism</strong>: Often able to translate and understand multiple languages even if not explicitly trained for it.</li> </ul> <p>It’s as if, by learning to predict the next word over vast amounts of text, the model builds an internal representation of the world and the relationships within it, allowing it to “reason” and “understand” in ways we are still trying to fully comprehend.</p> <h2 id="challenges-and-limitations">Challenges and Limitations</h2> <p>Despite their incredible power, LLMs are not without their flaws:</p> <ul> <li> <strong>Hallucinations</strong>: They sometimes confidently generate factually incorrect information, or “make things up.” This is because they are optimized for generating <em>plausible</em> text, not necessarily <em>truthful</em> text.</li> <li> <strong>Bias</strong>: Since they learn from human-generated text, LLMs can inherit and even amplify biases present in the training data (e.g., gender stereotypes, racial prejudices).</li> <li> <strong>Computational Cost</strong>: Training and running these models requires immense computing power, energy, and financial resources.</li> <li> <strong>Lack of True Understanding</strong>: While they can simulate understanding, LLMs don’t possess genuine consciousness, common sense, or a true grasp of reality in the way humans do. They are sophisticated pattern-matching machines.</li> <li> <strong>Safety and Ethics</strong>: Concerns around misuse, generating harmful content, and job displacement are significant considerations for their development and deployment.</li> </ul> <h2 id="the-future-is-now-and-beyond">The Future is Now (and Beyond)</h2> <p>LLMs have already transformed many fields. They power sophisticated chatbots, help developers write and debug code, assist content creators with drafting and ideation, and can translate languages in real-time.</p> <p>Looking ahead, the research community is exploring even more exciting avenues:</p> <ul> <li> <strong>Multimodal LLMs</strong>: Combining text with images, audio, and video to enable even richer interactions and understanding.</li> <li> <strong>Agentic AI</strong>: Models that can break down complex goals into smaller steps, interact with tools, and learn from feedback loops to achieve long-term objectives.</li> <li> <strong>Personalized AI</strong>: Tailoring LLMs to individual users’ needs and preferences while maintaining privacy.</li> <li> <strong>Ethical AI</strong>: Developing robust methods to ensure fairness, transparency, and safety as these models become more integrated into our lives.</li> </ul> <p>My journey into LLMs has been nothing short of exhilarating. It’s a field moving at lightning speed, constantly pushing the boundaries of what’s possible. As a data scientist and MLE, understanding these models isn’t just a technical skill; it’s a doorway to shaping the future of human-computer interaction. The magic, I’ve learned, isn’t really magic at all – it’s brilliant engineering and mathematics, constantly evolving. And that, to me, is even more fascinating.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>