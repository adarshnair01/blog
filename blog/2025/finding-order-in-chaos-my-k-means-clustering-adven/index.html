<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Finding Order in Chaos: My K-Means Clustering Adventure | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/finding-order-in-chaos-my-k-means-clustering-adven/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Finding Order in Chaos: My K-Means Clustering Adventure</h1> <p class="post-meta"> Created on August 29, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/unsupervised-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Unsupervised Learning</a>   <a href="/blog/blog/tag/k-means"> <i class="fa-solid fa-hashtag fa-sm"></i> K-Means</a>   <a href="/blog/blog/tag/clustering"> <i class="fa-solid fa-hashtag fa-sm"></i> Clustering</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hello fellow data explorers!</p> <p>Have you ever looked at a messy room, a sprawling collection of books, or even just a mixed bag of candies and thought, “There has to be a better way to organize this?” As humans, our brains are wired to find patterns, to group similar things together. We instinctively sort clothes by type, books by genre, or candies by flavor.</p> <p>What if we could teach a computer to do the same? Not by explicitly telling it “these are shirts, these are pants,” but by letting it figure out the inherent groupings on its own? This, my friends, is the magic of <strong>clustering</strong>, and today, I want to take you on a personal journey into one of the most fundamental and widely used clustering algorithms: <strong>K-Means Clustering</strong>.</p> <p>It’s a journey from scattered data points to clear, insightful groups, and it’s surprisingly simple yet incredibly powerful. Ready to dive in?</p> <h3 id="what-exactly-is-k-means-clustering">What Exactly <em>Is</em> K-Means Clustering?</h3> <p>Imagine you have a giant pile of data points – maybe customer purchasing habits, different species of flowers, or even just points on a 2D graph. You don’t have any labels telling you which customer belongs to which “segment” or which flower is which species. This is where K-Means shines: it’s an <strong>unsupervised learning</strong> algorithm. It learns <em>without</em> predefined labels.</p> <p>The core idea behind K-Means is to partition <code class="language-plaintext highlighter-rouge">n</code> data points into <code class="language-plaintext highlighter-rouge">k</code> distinct, non-overlapping subgroups or “clusters”. The goal is to make sure that data points within the same cluster are as similar as possible to each other, while data points in different clusters are as dissimilar as possible.</p> <p>Think of it like this: you want to sort your messy room into <code class="language-plaintext highlighter-rouge">k</code> distinct piles. You don’t know beforehand what these piles will contain (clothes, books, gadgets), but you want everything in one pile to be “similar” and different from items in other piles. K-Means does this by finding the “center” of each pile (we call these <strong>centroids</strong>) and making sure every item is assigned to the pile whose center it’s closest to.</p> <h3 id="the-k-means-algorithm-a-step-by-step-dance">The K-Means Algorithm: A Step-by-Step Dance</h3> <p>The K-Means algorithm is iterative, meaning it repeats a set of steps until it reaches a stable state. Let’s break it down:</p> <p><strong>Step 1: Initialization - Choose Your ‘K’ and Plant Your Seeds</strong></p> <p>The first crucial decision you have to make is to pick the number of clusters, <code class="language-plaintext highlighter-rouge">k</code>. This <code class="language-plaintext highlighter-rouge">k</code> is the “K” in K-Means. How many piles do you want to sort your data into? Sometimes domain knowledge helps, other times we’ll need a trick (which we’ll discuss later!).</p> <p>Once <code class="language-plaintext highlighter-rouge">k</code> is chosen, the algorithm randomly places <code class="language-plaintext highlighter-rouge">k</code> <strong>centroids</strong> (our initial “pile centers”) somewhere in the data space. These centroids are just imaginary points at first, representing the initial guess for the center of each cluster.</p> <ul> <li> <em>My personal thought:</em> This random placement always feels a bit like throwing darts at a map and hoping for the best. It’s surprisingly effective, but sometimes leads to suboptimal results, which we’ll also touch upon!</li> </ul> <p><strong>Step 2: Assignment Step (The ‘E’ in EM - Expectation)</strong></p> <p>Now that we have our <code class="language-plaintext highlighter-rouge">k</code> centroids, it’s time to assign each and every data point to its nearest centroid. For each data point <code class="language-plaintext highlighter-rouge">x</code>, we calculate its distance to every single centroid <code class="language-plaintext highlighter-rouge">c_j</code> and assign <code class="language-plaintext highlighter-rouge">x</code> to the cluster <code class="language-plaintext highlighter-rouge">C_j</code> whose centroid <code class="language-plaintext highlighter-rouge">c_j</code> is closest.</p> <p>How do we measure “closest”? Most commonly, we use <strong>Euclidean distance</strong>. If you have a data point $\mathbf{x} = (x_1, x_2, \ldots, x_n)$ and a centroid $\mathbf{c} = (c_1, c_2, \ldots, c_n)$, the Euclidean distance is:</p> <p>$d(\mathbf{x}, \mathbf{c}) = \sqrt{\sum_{i=1}^n (x_i - c_i)^2}$</p> <p>This is just the straight-line distance you’d measure with a ruler in 2D or 3D space, extended to multiple dimensions.</p> <ul> <li> <em>My personal thought:</em> This step is like drawing imaginary lines on your floor, assigning each book to the closest pile center.</li> </ul> <p><strong>Step 3: Update Step (The ‘M’ in EM - Maximization)</strong></p> <p>After all data points have been assigned to a cluster, the centroids are no longer just random points; they’re the <em>centers</em> of their respective assigned groups. So, it’s time to move them! We recalculate the position of each centroid <code class="language-plaintext highlighter-rouge">c_j</code> by taking the <strong>mean</strong> (average) of all the data points currently assigned to its cluster <code class="language-plaintext highlighter-rouge">C_j</code>.</p> <p>For each cluster <code class="language-plaintext highlighter-rouge">j</code>, the new centroid $\mathbf{c}_j$ is calculated as:</p> <table> <tbody> <tr> <td>$\mathbf{c}_j = \frac{1}{</td> <td>C_j</td> <td>} \sum_{\mathbf{x} \in C_j} \mathbf{x}$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>Here, $</td> <td>C_j</td> <td>$ is the number of data points in cluster <code class="language-plaintext highlighter-rouge">j</code>. This literally means “sum up all the coordinates of the points in cluster <code class="language-plaintext highlighter-rouge">j</code> and divide by the number of points.”</td> </tr> </tbody> </table> <ul> <li> <em>My personal thought:</em> Now that your books are in piles, you physically move the “pile center” to the actual middle of where all the books landed. Makes sense, right?</li> </ul> <p><strong>Step 4: Convergence - Repeat Until Stable</strong></p> <p>Steps 2 and 3 are repeated. We re-assign data points to the <em>new</em> centroids, and then recalculate the centroids based on these <em>new</em> assignments. This iterative process continues until one of two conditions is met:</p> <ol> <li> <strong>Convergence:</strong> The centroids no longer move significantly between iterations (they’ve found their “happy place”).</li> <li> <strong>Maximum Iterations:</strong> A predefined maximum number of iterations is reached (to prevent infinite loops in tricky cases).</li> </ol> <p>The algorithm guarantees that with each iteration, the sum of squared distances between data points and their assigned centroids (also known as the <strong>Within-Cluster Sum of Squares, WCSS</strong>) will decrease or stay the same, eventually leading to a local optimum.</p> <h3 id="a-simple-example-visualizing-the-dance">A Simple Example: Visualizing the Dance</h3> <p>Imagine you have a scatter plot of points.</p> <ol> <li> <strong>Start:</strong> You pick <code class="language-plaintext highlighter-rouge">k=3</code> and place three random centroids.</li> <li> <strong>Iteration 1:</strong> <ul> <li>Each point gets assigned to the closest of the three initial centroids. This creates three initial, messy clusters.</li> <li>The centroids then move to the center of their newly assigned points.</li> </ul> </li> <li> <strong>Iteration 2:</strong> <ul> <li>With the new centroid positions, some points might now be closer to a <em>different</em> centroid. They switch clusters.</li> <li>Centroids move again to the new average of their assigned points.</li> </ul> </li> <li> <strong>Repeat:</strong> This continues. You’d see the centroids “dancing” around the data space, pulling points towards them, until they settle down, each having claimed a distinct group of points.</li> </ol> <p>It’s truly fascinating to watch this process unfold visually!</p> <h3 id="the-k-conundrum-how-many-clusters-do-i-need">The “K” Conundrum: How Many Clusters Do I Need?</h3> <p>This is often the trickiest part of K-Means. How do you choose the “right” <code class="language-plaintext highlighter-rouge">k</code>? If you pick too few, you’ll lump distinct groups together. Too many, and you might split meaningful groups or create tiny, insignificant clusters.</p> <p>One of the most popular methods for selecting <code class="language-plaintext highlighter-rouge">k</code> is the <strong>Elbow Method</strong>.</p> <p><strong>The Elbow Method</strong></p> <p>The idea is to run K-Means for a range of <code class="language-plaintext highlighter-rouge">k</code> values (e.g., from 1 to 10). For each <code class="language-plaintext highlighter-rouge">k</code>, we calculate the <strong>Within-Cluster Sum of Squares (WCSS)</strong>. This is the sum of the squared distances between each point and its assigned centroid. As we increase <code class="language-plaintext highlighter-rouge">k</code>, the WCSS will naturally decrease because points will be closer to their centroids if there are more centroids to choose from.</p> <table> <tbody> <tr> <td>$WCSS = \sum_{j=1}^k \sum_{\mathbf{x} \in C_j}</td> <td> </td> <td>\mathbf{x} - \mathbf{c}_j</td> <td> </td> <td>^2$</td> </tr> </tbody> </table> <p>We then plot WCSS against <code class="language-plaintext highlighter-rouge">k</code>. What we look for is an “elbow” in the graph. This is the point where the rate of decrease in WCSS slows down significantly. Beyond this “elbow,” adding more clusters doesn’t explain much more of the variance in the data, indicating that the additional clusters might just be splitting existing, meaningful groups.</p> <ul> <li> <em>My personal thought:</em> It’s like bending your arm – the elbow joint is a distinct point where the angle changes dramatically. Before the elbow, adding more clusters reduces error significantly. After it, the gains diminish.</li> </ul> <p>Other methods like the Silhouette Score can also help, but the Elbow Method offers a good intuitive starting point. Often, domain knowledge is also paramount – if you know you’re looking for, say, “3 types of customers,” <code class="language-plaintext highlighter-rouge">k=3</code> might be your initial best bet.</p> <h3 id="strengths-and-weaknesses-no-algorithm-is-perfect">Strengths and Weaknesses: No Algorithm is Perfect</h3> <p>Like any tool, K-Means has its perks and pitfalls:</p> <p><strong>Strengths:</strong></p> <ol> <li> <strong>Simplicity:</strong> It’s incredibly easy to understand and implement.</li> <li> <strong>Efficiency:</strong> It’s computationally very fast, especially for large datasets, making it suitable for practical applications.</li> <li> <strong>Scalability:</strong> It scales well to a large number of data points.</li> <li> <strong>Interpretability:</strong> The clusters are often easy to interpret (e.g., “these are my high-value customers”).</li> </ol> <p><strong>Weaknesses:</strong></p> <ol> <li> <strong>Requires ‘k’:</strong> You have to specify the number of clusters (<code class="language-plaintext highlighter-rouge">k</code>) upfront, which can be challenging.</li> <li> <strong>Sensitive to Initial Centroids:</strong> Random initialization can lead to different results each time, potentially converging to a local optimum rather than the global optimum. (A common improvement: <strong>K-Means++</strong>, which initializes centroids more smartly by spreading them out, helps mitigate this).</li> <li> <strong>Sensitive to Outliers:</strong> Outliers can drastically pull centroids towards them, distorting the clusters.</li> <li> <strong>Assumes Spherical Clusters:</strong> K-Means works best when clusters are roughly spherical and of similar size and density. It struggles with irregularly shaped clusters (like crescent moons) or clusters with varying densities.</li> <li> <strong>Sensitive to Feature Scaling:</strong> Since it uses distance calculations, features with larger ranges can dominate the distance calculation. It’s often necessary to scale your features (e.g., standardization or normalization) before applying K-Means.</li> </ol> <h3 id="beyond-the-basics-quick-glimpses">Beyond the Basics: Quick Glimpses</h3> <p>While K-Means is a fantastic starting point, there are variations and related algorithms that address some of its limitations:</p> <ul> <li> <strong>K-Means++:</strong> A smarter initialization strategy that selects initial centroids that are far apart from each other, improving the chances of finding a better solution.</li> <li> <strong>Mini-Batch K-Means:</strong> For extremely large datasets, this uses subsets of the data (mini-batches) to update centroids, significantly speeding up computation.</li> <li> <strong>K-Medoids (PAM - Partitioning Around Medoids):</strong> Instead of using the mean, it uses an <em>actual data point</em> (the medoid) as the cluster center, making it more robust to outliers.</li> </ul> <h3 id="real-world-applications-where-k-means-shines">Real-World Applications: Where K-Means Shines</h3> <p>K-Means is a workhorse in many industries:</p> <ul> <li> <strong>Customer Segmentation:</strong> Grouping customers based on purchasing behavior or demographics for targeted marketing campaigns.</li> <li> <strong>Image Compression:</strong> Quantizing colors in an image (e.g., reducing a million colors to just 256 for a GIF image) by clustering similar colors.</li> <li> <strong>Document Clustering:</strong> Grouping news articles, research papers, or emails by topic.</li> <li> <strong>Anomaly Detection:</strong> Identifying unusual data points that don’t fit into any cluster (e.g., fraudulent transactions).</li> <li> <strong>Geospatial Analysis:</strong> Identifying areas with similar characteristics based on geographical data.</li> </ul> <h3 id="my-concluding-thoughts-an-elegant-simplicity">My Concluding Thoughts: An Elegant Simplicity</h3> <p>My journey with K-Means has truly highlighted how a seemingly simple algorithm, built on basic concepts like distance and averages, can unlock profound insights from complex, unlabelled data. It’s a testament to the elegance of mathematics and computation.</p> <p>While it has its limitations, K-Means remains a fundamental tool in any data scientist’s toolkit. It’s often the first algorithm I reach for when starting to explore unstructured data, providing a quick and intuitive way to understand inherent groupings.</p> <p>So, the next time you encounter a pile of messy, unorganized data, remember K-Means. It might just be the quiet, diligent organizer you need to find order in the chaos.</p> <p>Happy clustering!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>