<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Goldilocks Problem of Machine Learning: Finding the "Just Right" Model | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/the-goldilocks-problem-of-machine-learning-finding/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Goldilocks Problem of Machine Learning: Finding the "Just Right" Model</h1> <p class="post-meta"> Created on November 21, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/model-evaluation"> <i class="fa-solid fa-hashtag fa-sm"></i> Model Evaluation</a>   <a href="/blog/blog/tag/overfitting"> <i class="fa-solid fa-hashtag fa-sm"></i> Overfitting</a>   <a href="/blog/blog/tag/underfitting"> <i class="fa-solid fa-hashtag fa-sm"></i> Underfitting</a>   <a href="/blog/blog/tag/bias-variance-tradeoff"> <i class="fa-solid fa-hashtag fa-sm"></i> Bias-Variance Tradeoff</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey there, fellow data explorers and aspiring AI builders!</p> <p>If you’re anything like me, you’ve probably spent countless hours wrestling with datasets, tweaking models, and staring at metrics, all in pursuit of that elusive goal: building a machine learning model that just <em>works</em>. Not just on the data it’s seen before, but on <strong>new, unseen data</strong> in the wild. This, my friends, is the holy grail of machine learning: generalization.</p> <p>But here’s the kicker: achieving generalization is often a delicate dance. Sometimes our models try <em>too hard</em> to learn, memorizing every tiny detail and quirk of the training data. Other times, they don’t try <em>hard enough</em>, barely scratching the surface of the underlying patterns. These two extremes are what we call <strong>overfitting</strong> and <strong>underfitting</strong>, and understanding them is fundamental to becoming a successful data scientist or MLE.</p> <p>Think of it like the classic story of Goldilocks and the Three Bears. Goldilocks wasn’t looking for the coldest porridge or the hottest, the hardest bed or the softest. She wanted something “just right.” In machine learning, our “just right” model is one that learns enough to be useful without getting bogged down by the noise.</p> <p>Let’s dive in!</p> <h3 id="the-student-analogy-preparing-for-the-big-exam">The Student Analogy: Preparing for the Big Exam</h3> <p>Imagine you’re a student preparing for a big, important exam. You have your textbook, your notes, and a set of practice problems.</p> <ul> <li> <p><strong>The Underfitting Student:</strong> This student skims the textbook, doesn’t really engage with the material, and barely attempts the practice problems. They don’t understand the core concepts. When the exam comes, they struggle with both the questions that are similar to the practice problems <em>and</em> the completely new ones. Their performance is poor across the board.</p> </li> <li> <p><strong>The Overfitting Student:</strong> This student meticulously memorizes <em>every single practice problem</em>. They know the exact numbers, the phrasing, even the minor typos. They’ve essentially created a perfect mental map of the practice set. When the exam arrives, if a question is <em>identical</em> to a practice problem, they ace it! But if a question is phrased slightly differently, or requires applying a concept to a new scenario, they’re completely lost. Their performance on the <em>practice</em> is perfect, but on the <em>actual, slightly different exam</em>, it’s a disaster. They’ve confused memorization with understanding.</p> </li> <li> <p><strong>The “Just Right” Student:</strong> This student engages with the material, understands the underlying concepts and principles, and uses the practice problems to test their comprehension and application skills. They can solve the practice problems, but more importantly, they can apply their knowledge to novel situations and slightly varied questions on the real exam. They do well on both practice and the actual exam.</p> </li> </ul> <p>This analogy perfectly encapsulates the core challenge of model generalization. We want our machine learning models to be the “just right” student.</p> <h3 id="underfitting-the-case-of-the-overly-simplistic-model-high-bias">Underfitting: The Case of the Overly Simplistic Model (High Bias)</h3> <p>An underfit model is like our first student: it’s simply <strong>too simple</strong> to capture the underlying patterns in the training data. It hasn’t learned enough.</p> <h4 id="what-does-it-look-like">What does it look like?</h4> <ul> <li> <strong>Poor performance on both training data AND test data.</strong> The model can’t even perform well on the data it has seen, which is a major red flag.</li> <li> <strong>High training error and high validation/test error.</strong> Both are high, indicating that the model is fundamentally unable to grasp the complexities of the problem.</li> </ul> <p>Imagine trying to fit a straight line ($y = mx + c$) to data that clearly follows a complex curve, like a parabola ($y = ax^2 + bx + c$). No matter how you adjust $m$ or $c$, that straight line will never fully capture the bend of the parabola. It’s too restrictive.</p> <h4 id="the-mathy-bit-bias">The Mathy Bit: Bias</h4> <p>In machine learning, we often talk about the <strong>Bias-Variance Trade-off</strong>. Underfitting is characterized by <strong>high bias</strong>.</p> <p><strong>Bias</strong> refers to the error introduced by approximating a real-world problem (which might be complicated) with a simpler model. A high-bias model makes strong assumptions about the data, often leading it to miss relevant relations between features and target outputs. It consistently misses the mark, even on training data.</p> <h4 id="causes-of-underfitting">Causes of Underfitting:</h4> <ol> <li> <strong>Model is too simple:</strong> Using a linear model for inherently non-linear data.</li> <li> <strong>Insufficient features:</strong> Not providing the model with enough relevant information. If you’re trying to predict house prices but only give the model the number of bedrooms, it’s missing out on crucial factors like square footage, location, and age.</li> <li> <strong>Too much regularization:</strong> Regularization techniques (which we’ll discuss later) prevent overfitting, but too much can constrain the model excessively, leading to underfitting.</li> <li> <strong>Insufficient training time/epochs:</strong> For iterative models like neural networks, not training long enough can prevent the model from learning sufficient patterns.</li> </ol> <h4 id="how-to-combat-underfitting">How to Combat Underfitting:</h4> <ol> <li> <strong>Increase Model Complexity:</strong> <ul> <li>Use a more sophisticated algorithm (e.g., switch from linear regression to polynomial regression, or a decision tree, or a neural network with more layers/neurons).</li> <li>For neural networks, add more hidden layers or neurons.</li> </ul> </li> <li> <strong>Add More Features:</strong> Incorporate more relevant independent variables that could help the model understand the target better.</li> <li> <strong>Reduce Regularization:</strong> If you’re using regularization, try decreasing its strength ($\lambda$).</li> <li> <strong>Feature Engineering:</strong> Create new features from existing ones that might better represent the underlying relationships (e.g., combining height and weight to create BMI).</li> </ol> <h3 id="overfitting-the-case-of-the-overly-complex-model-high-variance">Overfitting: The Case of the Overly Complex Model (High Variance)</h3> <p>An overfit model is like our second student: it has learned the training data <em>too well</em>, essentially memorizing it, including its noise and random fluctuations. It struggles to generalize to any data it hasn’t explicitly seen.</p> <h4 id="what-does-it-look-like-1">What does it look like?</h4> <ul> <li> <strong>Excellent performance on training data.</strong> The model appears to be a genius!</li> <li> <strong>Poor performance on unseen test data.</strong> When faced with new examples, its performance drops significantly.</li> <li> <strong>Low training error and high validation/test error.</strong> This is the classic signature of overfitting: a large gap between how well it performs on seen vs. unseen data.</li> </ul> <p>Imagine trying to fit a complex, wiggly line that perfectly passes through every single data point in your training set, even the noisy outliers. While it might hit every training point, this line will likely make wild, incorrect predictions when it encounters a new point that’s slightly different.</p> <h4 id="the-mathy-bit-variance">The Mathy Bit: Variance</h4> <p>Overfitting is characterized by <strong>high variance</strong>.</p> <p><strong>Variance</strong> refers to the amount that the estimate of the target function will change if different training data was used. A high-variance model is overly sensitive to the specific dataset it was trained on. Small changes in the training data can lead to drastically different models. It’s too flexible and picks up too much of the noise.</p> <h4 id="causes-of-overfitting">Causes of Overfitting:</h4> <ol> <li> <strong>Model is too complex:</strong> Using a very flexible model (e.g., a deep neural network with many layers and parameters, or a decision tree grown to full depth) with insufficient data.</li> <li> <strong>Too little training data:</strong> With a small dataset, a complex model can easily “memorize” the limited examples instead of learning general patterns.</li> <li> <strong>Too many features:</strong> When you have many features, especially relative to the number of data points, the model might start to find spurious correlations that don’t generalize (the “curse of dimensionality”).</li> <li> <strong>Lack of regularization:</strong> No constraints on the model’s complexity, allowing it to fit the noise.</li> </ol> <h4 id="how-to-combat-overfitting">How to Combat Overfitting:</h4> <ol> <li> <strong>Simplify the Model:</strong> <ul> <li>Use a simpler algorithm.</li> <li>For neural networks, reduce the number of layers or neurons, or use simpler activation functions.</li> <li>Prune decision trees.</li> </ul> </li> <li> <strong>Get More Training Data:</strong> This is often the most effective solution. More data helps the model learn the true underlying patterns rather than memorizing specific examples.</li> <li> <strong>Feature Selection/Engineering:</strong> Reduce the number of features by selecting only the most relevant ones or creating more meaningful composite features.</li> <li> <strong>Regularization:</strong> This is a crucial set of techniques! Regularization adds a penalty to the loss function for having large coefficients, encouraging simpler models. <ul> <li> <table> <tbody> <tr> <td> <strong>L1 Regularization (Lasso):</strong> Adds $\lambda \sum_{i=1}^{n}</td> <td>\theta_i</td> <td>$ to the cost function. It tends to drive some coefficients exactly to zero, effectively performing feature selection.</td> </tr> </tbody> </table> </li> <li> <strong>L2 Regularization (Ridge/Weight Decay):</strong> Adds $\lambda \sum_{i=1}^{n} \theta_i^2$ to the cost function. It shrinks coefficients towards zero without necessarily making them exactly zero.</li> <li> <strong>Dropout (for Neural Networks):</strong> Randomly drops out (sets to zero) a percentage of neurons during training. This prevents neurons from co-adapting too much and forces the network to learn more robust features.</li> </ul> </li> <li> <strong>Early Stopping:</strong> For iterative models, stop training when the performance on a separate validation set starts to degrade, even if the training error is still decreasing. This finds the sweet spot before the model starts overfitting.</li> <li> <strong>Cross-Validation:</strong> While not directly preventing overfitting, cross-validation provides a more robust estimate of how your model will perform on unseen data, helping you diagnose overfitting more reliably.</li> </ol> <h3 id="the-bias-variance-trade-off-the-sweet-spot">The Bias-Variance Trade-off: The Sweet Spot</h3> <p>The terms “bias” and “variance” are two sides of the same coin when it comes to model error. As we increase model complexity, we typically reduce bias (the model can better fit the underlying patterns) but increase variance (it becomes more sensitive to training data fluctuations). Conversely, simplifying a model increases bias but reduces variance.</p> <p>The total error of a model can be conceptually broken down as:</p> <p>$Total Error = Bias^2 + Variance + Irreducible Error$</p> <p>The <strong>Irreducible Error</strong> is the noise inherent in the data itself that no model, no matter how perfect, can eliminate. Our goal is to minimize the sum of bias squared and variance.</p> <p>This relationship means we’re always trying to find a balance. We can’t eliminate both bias and variance simultaneously. The art of machine learning lies in finding that “just right” sweet spot where the combined error from bias and variance is minimized. This is where our model generalizes best to new data.</p> <h3 id="practical-strategies-for-finding-just-right">Practical Strategies for Finding “Just Right”</h3> <p>So, how do we actually find this sweet spot in practice?</p> <ol> <li> <strong>Data Splitting:</strong> Always split your data into <strong>training, validation, and test sets</strong>. <ul> <li> <strong>Training Set:</strong> Used to train the model.</li> <li> <strong>Validation Set:</strong> Used to tune hyperparameters and make model selection decisions (e.g., “should I use more layers or less?”). This helps prevent overfitting to the <em>test set</em>.</li> <li> <strong>Test Set:</strong> Used <em>only once</em> at the very end to evaluate the final model’s performance on truly unseen data.</li> </ul> </li> <li> <strong>Learning Curves:</strong> These plots show the training error and validation error as a function of the training set size or model complexity. They are incredibly useful for diagnosing both overfitting and underfitting. <ul> <li>If both errors are high and close together: <strong>Underfitting</strong>.</li> <li>If training error is low and validation error is high, with a large gap: <strong>Overfitting</strong>.</li> </ul> </li> <li> <strong>Cross-Validation:</strong> For smaller datasets, or to get a more robust estimate of performance, k-fold cross-validation is invaluable. It trains and validates the model multiple times on different subsets of the data.</li> <li> <strong>Hyperparameter Tuning:</strong> Many of the “knobs” we turn on our models (e.g., learning rate, number of layers, regularization strength) are called hyperparameters. Systematically searching for the best combination of these using techniques like Grid Search, Random Search, or Bayesian Optimization helps us optimize the bias-variance trade-off.</li> </ol> <h3 id="conclusion-the-journey-continues">Conclusion: The Journey Continues</h3> <p>Understanding overfitting and underfitting isn’t just about memorizing definitions; it’s about developing an intuition for how models learn and generalize. It’s about being the “just right” student in our own data science journey.</p> <p>The path to building robust, generalized machine learning models is an iterative one. You’ll constantly be building, evaluating, diagnosing, and refining. You’ll start with a hypothesis, train a model, see if it’s underfitting or overfitting (or hopefully, just right!), and then apply the appropriate strategies to improve it.</p> <p>This fundamental concept will be a cornerstone of almost every machine learning project you undertake. So, embrace the challenge, keep experimenting, and remember to always strive for that “just right” balance! Happy modeling!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>