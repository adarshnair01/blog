<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Deep Learning Duel: My Journey Through PyTorch vs. TensorFlow | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/the-deep-learning-duel-my-journey-through-pytorch/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Deep Learning Duel: My Journey Through PyTorch vs. TensorFlow</h1> <p class="post-meta"> Created on August 31, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/pytorch"> <i class="fa-solid fa-hashtag fa-sm"></i> PyTorch</a>   <a href="/blog/blog/tag/tensorflow"> <i class="fa-solid fa-hashtag fa-sm"></i> TensorFlow</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>As a budding data scientist and machine learning engineer, few decisions feel as weighty as choosing the right tools. When I first dove headfirst into the exhilarating, sometimes bewildering, world of deep learning, two names echoed through every tutorial, every research paper, every online forum: PyTorch and TensorFlow. It felt like standing at a crossroads, two mighty paths diverging, each promising to lead to the promised land of neural networks and intelligent AI.</p> <p>But here’s the secret I wish someone had told me sooner: it’s not about picking a “winner” in a gladiatorial battle. It’s about understanding their philosophies, their strengths, and ultimately, which one resonates best with your problem, your team, and your personal coding style. Join me as I recount my own journey navigating this deep learning duel, exploring what makes these frameworks tick, and how they’ve evolved to become the incredible powerhouses they are today.</p> <h3 id="the-foundation-tensors-and-the-need-for-speed">The Foundation: Tensors and the Need for Speed</h3> <p>Before we dissect the frameworks themselves, let’s talk about their fundamental building block: <strong>tensors</strong>. If you’ve ever worked with NumPy, you’re already halfway there. A tensor is essentially a multi-dimensional array, a generalization of vectors and matrices, designed to hold numerical data. In deep learning, everything — from your input images and text to your model’s weights and biases — is represented as a tensor.</p> <p>Both PyTorch and TensorFlow provide highly optimized tensor libraries, often leveraging your GPU (Graphics Processing Unit) for lightning-fast computations. This is crucial because deep learning involves an astronomical number of mathematical operations. Without GPU acceleration, training even a moderately complex model would take an eternity.</p> <p>Let’s look at a simple tensor creation:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># PyTorch tensor
</span><span class="n">pt_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">]])</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">PyTorch Tensor:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">pt_tensor</span><span class="p">)</span>

<span class="c1"># TensorFlow tensor
</span><span class="n">tf_tensor</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">constant</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">]])</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">TensorFlow Tensor:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">tf_tensor</span><span class="p">)</span>

<span class="c1"># Example: Adding two tensors
</span><span class="n">tensor_a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]])</span>
<span class="n">tensor_b</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([[</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">]])</span>
<span class="n">tensor_sum</span> <span class="o">=</span> <span class="n">tensor_a</span> <span class="o">+</span> <span class="n">tensor_b</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">PyTorch Tensor Sum:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">tensor_sum</span><span class="p">)</span>
</code></pre></div></div> <p>Notice the similarity? At this basic level, both frameworks feel very intuitive, largely due to their Pythonic interfaces.</p> <h3 id="the-engine-room-automatic-differentiation-autograd">The Engine Room: Automatic Differentiation (Autograd)</h3> <p>Here’s where the real magic happens, and where PyTorch and TensorFlow traditionally had their most significant philosophical difference. Training neural networks involves a process called <strong>backpropagation</strong>, which requires calculating gradients (derivatives) of the loss function with respect to every single weight in the network. Manually computing these derivatives for millions of parameters would be impossible. This is where <strong>automatic differentiation (Autograd)</strong> comes in.</p> <p>Autograd systems build a <strong>computation graph</strong> that tracks all operations performed on tensors. When it’s time to update weights, this graph is traversed backward to compute all necessary gradients efficiently.</p> <p>Imagine a simple function: $f(x) = x^2$. The derivative, $df/dx = 2x$. If we had a chain of operations, say $y = x^2$ and $z = y + 3$, then $dz/dx$ would be computed using the chain rule. Autograd handles this complexity for us.</p> <h4 id="pytorch-the-dynamic-graph-evangelist">PyTorch: The Dynamic Graph Evangelist</h4> <p>PyTorch’s approach to computation graphs is famously <strong>dynamic</strong>. What does this mean? It means the computation graph is built <em>on the fly</em> as your code executes. Each forward pass of data through your network constructs a new graph.</p> <p>Think of it like cooking a new dish every time. You read the recipe (your model definition), and you perform the steps, creating the “graph” of ingredients and actions as you go. If you decide to add a pinch of this or that based on taste (an <code class="language-plaintext highlighter-rouge">if</code> statement or loop), you can easily change the recipe in real-time.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># PyTorch example: Dynamic graph
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">y</span> <span class="o">+</span> <span class="mi">3</span>
<span class="n">z</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span> <span class="c1"># Computes gradients
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">PyTorch: dz/dx at x=2 is</span><span class="sh">"</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span> <span class="c1"># Expected: 2*x = 4
</span></code></pre></div></div> <p>This dynamic nature offers several advantages:</p> <ol> <li> <strong>Debugging:</strong> It feels just like debugging regular Python code. You can use standard debuggers, print statements, and step through your model line by line, inspecting tensors at any point.</li> <li> <strong>Flexibility:</strong> Easily handle variable-length inputs, recurrent neural networks (RNNs), and complex control flow (like <code class="language-plaintext highlighter-rouge">if</code> statements and loops) within your model architecture, as the graph adapts with each pass.</li> <li> <strong>Intuitiveness:</strong> For many, this “imperative” style feels more natural and Pythonic.</li> </ol> <h4 id="tensorflow-from-static-powerhouse-to-eager-explorer-tf2x">TensorFlow: From Static Powerhouse to Eager Explorer (TF2.x)</h4> <p>Historically, TensorFlow (especially TF1.x) used a <strong>static computation graph</strong>. This meant you first defined the <em>entire</em> graph (all operations, placeholders for data) before you ran any computations within a session.</p> <p>Imagine designing an entire factory blueprint before a single bolt is turned. Once the blueprint is complete, it’s highly optimized and efficient for production, but making changes mid-operation is difficult.</p> <p>The advantages of static graphs in TF1.x included:</p> <ol> <li> <strong>Optimization:</strong> The framework could perform global optimizations on the entire graph before execution, potentially leading to faster training and inference.</li> <li> <strong>Deployment:</strong> The graph could be saved and deployed independently of the Python code, making it easier to run models on different platforms (mobile, C++, etc.).</li> </ol> <p>However, static graphs came with significant drawbacks:</p> <ol> <li> <strong>Debugging:</strong> Infamously difficult. Debugging involved inspecting the graph definition, not the actual values flowing through it until runtime.</li> <li> <strong>Complexity:</strong> Handling dynamic control flow was cumbersome, often requiring special TensorFlow operators.</li> <li> <strong>Steep Learning Curve:</strong> The session-based execution model felt less intuitive than standard Python.</li> </ol> <p>This led to a major shift with <strong>TensorFlow 2.x</strong>. Recognizing PyTorch’s success and the developer preference for dynamic execution, TensorFlow embraced <strong>Eager Execution</strong> as its default. In Eager Execution, operations are executed immediately, just like in PyTorch. This brought TF2.x much closer to PyTorch’s development experience.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># TensorFlow 2.x example with Eager Execution
</span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nc">Variable</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span>
<span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nc">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">y</span> <span class="o">+</span> <span class="mi">3</span>
<span class="n">gradients</span> <span class="o">=</span> <span class="n">tape</span><span class="p">.</span><span class="nf">gradient</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">TensorFlow 2.x: dz/dx at x=2 is</span><span class="sh">"</span><span class="p">,</span> <span class="n">gradients</span><span class="p">.</span><span class="nf">numpy</span><span class="p">())</span> <span class="c1"># Expected: 2*x = 4
</span></code></pre></div></div> <p>While Eager Execution is the default, TF2.x still allows you to “compile” your Eager code into a static graph using <code class="language-plaintext highlighter-rouge">tf.function</code> for production deployment, getting the best of both worlds. This convergence significantly narrowed the philosophical gap between the two frameworks.</p> <h3 id="the-ecosystem-and-production-readiness">The Ecosystem and Production Readiness</h3> <p>While core functionalities have converged, the broader ecosystems still offer distinct strengths.</p> <h4 id="tensorflows-production-prowess">TensorFlow’s Production Prowess</h4> <p>Historically, TensorFlow has been the king of production deployment. Google’s vast resources and experience in deploying AI at scale have manifested in a comprehensive ecosystem:</p> <ul> <li> <strong>TensorFlow Serving:</strong> A flexible, high-performance serving system for machine learning models in production.</li> <li> <strong>TensorFlow Lite:</strong> For deploying models on mobile and embedded devices.</li> <li> <strong>TensorFlow.js:</strong> For running ML models directly in the browser or Node.js.</li> <li> <strong>TPU Support:</strong> Seamless integration with Google’s Tensor Processing Units for ultra-fast training.</li> <li> <strong>Keras:</strong> Integrated as TensorFlow’s high-level API, Keras makes building and experimenting with neural networks incredibly fast and easy. It abstracts away much of the low-level TensorFlow details, allowing you to focus on the model architecture.</li> </ul> <p>This maturity made TensorFlow a natural choice for large enterprises aiming for robust, scalable AI solutions.</p> <h4 id="pytorchs-research-agility-and-growing-maturity">PyTorch’s Research Agility and Growing Maturity</h4> <p>PyTorch, emerging from Facebook AI Research (FAIR), quickly became the darling of the research community due to its flexibility, Pythonic nature, and ease of debugging. This led to a massive surge in research papers and open-source projects being implemented in PyTorch.</p> <p>Over time, PyTorch has rapidly built out its own production capabilities:</p> <ul> <li> <strong>TorchScript:</strong> A way to serialize and optimize PyTorch models, allowing them to be run in C++ environments, often without the need for Python.</li> <li> <strong>PyTorch Mobile:</strong> For deploying models on iOS and Android.</li> <li> <strong>ONNX (Open Neural Network Exchange):</strong> A standard for representing ML models, allowing interoperability between frameworks (e.g., train in PyTorch, deploy with ONNX Runtime).</li> <li> <strong>PyTorch Lightning / Fastai:</strong> These high-level libraries built on PyTorch aim to simplify common deep learning tasks, reduce boilerplate code, and promote best practices, similar to how Keras simplifies TensorFlow. They are increasingly popular for making PyTorch more production-friendly.</li> </ul> <p>PyTorch is no longer just for researchers; its growing ecosystem makes it a strong contender for production-grade applications.</p> <h3 id="when-to-choose-which-my-personal-take">When to Choose Which (My Personal Take)</h3> <p>Given their convergence, the choice often comes down to personal preference, team expertise, and specific project requirements.</p> <p><strong>Choose PyTorch if:</strong></p> <ul> <li> <strong>You’re doing cutting-edge research or experimentation.</strong> Its flexibility and dynamic graph make it easier to implement novel architectures, debug complex models, and iterate quickly. Many new research papers release their code in PyTorch.</li> <li> <strong>You prefer a more “Pythonic” and imperative coding style.</strong> If you enjoy the feeling of writing standard Python and want to leverage its debugging tools directly, PyTorch might feel more natural.</li> <li><strong>Your primary goal is rapid prototyping and development cycle.</strong></li> <li> <strong>You’re starting out and want a potentially gentler entry point.</strong> (Though with TF2.x, this gap has narrowed).</li> </ul> <p><strong>Choose TensorFlow if:</strong></p> <ul> <li> <strong>Your project demands robust, large-scale production deployment.</strong> Especially if you need to deploy across a wide range of devices (mobile, web, embedded) and use tools like TF Serving.</li> <li> <strong>You are working within a Google-centric ecosystem</strong> (e.g., using Google Cloud AI Platform, TPUs).</li> <li> <strong>You value a highly mature, broad, and well-documented ecosystem</strong> that covers everything from data preprocessing to deployment and monitoring.</li> <li><strong>You are working on an existing project that already uses TensorFlow.</strong></li> <li> <strong>You prefer the high-level abstraction of Keras</strong> for quickly building and deploying models.</li> </ul> <h3 id="the-grand-convergence-a-win-for-everyone">The Grand Convergence: A Win for Everyone</h3> <p>My journey through PyTorch and TensorFlow has shown me one undeniable truth: the “duel” has largely turned into a <strong>convergence</strong>. TensorFlow 2.x, with its embrace of Eager Execution, has adopted many of the beloved features of PyTorch. Conversely, PyTorch has significantly matured its production deployment story.</p> <p>This convergence is a massive win for the deep learning community. It means that developers now have more freedom to choose based on preference rather than being locked into a framework purely for its unique features. Both frameworks are incredibly powerful, actively developed, and backed by huge communities.</p> <h3 id="conclusion-your-best-tool-is-your-well-understood-tool">Conclusion: Your Best Tool is Your Well-Understood Tool</h3> <p>In the end, the “best” framework isn’t an objective truth; it’s the one you understand best, the one that fits your problem, and the one that allows you to be most productive. I’ve personally found myself gravitating towards PyTorch for rapid prototyping and research-heavy tasks due to its immediate feedback and Pythonic feel, but I deeply appreciate TensorFlow’s comprehensive ecosystem when thinking about scaling and deployment.</p> <p>My advice? Don’t get bogged down in endless comparisons. Pick one, get your hands dirty, build something, and understand its core principles. Once you grasp one, picking up the other becomes significantly easier because the underlying deep learning concepts remain the same. The titans have evolved, and their combined strengths offer an unparalleled toolkit for building the future of AI. Go forth, experiment, and happy deep learning!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>