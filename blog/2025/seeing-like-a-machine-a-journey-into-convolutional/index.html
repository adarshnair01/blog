<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Seeing Like a Machine: A Journey into Convolutional Neural Networks | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/seeing-like-a-machine-a-journey-into-convolutional/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Seeing Like a Machine: A Journey into Convolutional Neural Networks</h1> <p class="post-meta"> Created on February 07, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/computer-vision"> <i class="fa-solid fa-hashtag fa-sm"></i> Computer Vision</a>   <a href="/blog/blog/tag/neural-networks"> <i class="fa-solid fa-hashtag fa-sm"></i> Neural Networks</a>   <a href="/blog/blog/tag/cnns"> <i class="fa-solid fa-hashtag fa-sm"></i> CNNs</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>As someone deeply fascinated by how machines learn, one of the first truly mind-blowing concepts I encountered was the Convolutional Neural Network (CNN). It felt like unlocking a secret chamber in the world of Artificial Intelligence. How can a computer, which only “sees” numbers (pixel values), understand the intricate patterns that make up a cat, a car, or even a human emotion? This question led me down a rabbit hole, and today, I want to share that journey with you, demystifying the incredible power of CNNs.</p> <h3 id="the-challenge-why-seeing-is-hard-for-computers">The Challenge: Why “Seeing” is Hard for Computers</h3> <p>Imagine you’re looking at a photo of a cat. You instantly recognize it. Easy, right? But for a computer, that image is just a massive grid of numbers, where each number represents the intensity of a pixel. A standard image might be 256x256 pixels with three color channels (Red, Green, Blue). That’s $256 \times 256 \times 3 = 196,608$ numbers!</p> <p>If we tried to feed these numbers directly into a traditional fully connected neural network (where every input is connected to every neuron in the next layer), we’d run into a few huge problems:</p> <ol> <li> <strong>Too Many Parameters:</strong> A single layer connecting 196,608 inputs to, say, 1,000 neurons would require nearly 200 million weights! Training such a network is computationally expensive and prone to overfitting.</li> <li> <strong>Loss of Spatial Information:</strong> A fully connected network treats each pixel as an independent feature. It loses the crucial information that pixels are arranged in a 2D grid, and their neighbors are highly relevant. The relative position of pixels is key to recognizing patterns like edges, corners, or entire objects.</li> <li> <strong>Lack of Invariance:</strong> If a cat is in the top-left corner versus the bottom-right, a traditional network might treat them as completely different patterns because the exact pixel values have shifted. We need our model to understand that a cat is a cat, regardless of where it appears in the image.</li> </ol> <p>This is where Convolutional Neural Networks step in, offering an elegant solution to these challenges. They are specifically designed to process data that has a known grid-like topology, like image pixels.</p> <h3 id="the-core-idea-feature-detectors-at-work">The Core Idea: Feature Detectors at Work</h3> <p>At its heart, a CNN learns to identify features within an image, starting with simple ones like edges and corners, and progressively building up to more complex features like eyes, ears, or wheels. It does this through a series of specialized layers. Let’s break them down.</p> <h4 id="1-the-convolutional-layer-the-eye-of-the-network">1. The Convolutional Layer: The Eye of the Network</h4> <p>This is the namesake layer and arguably the most important. Instead of looking at every pixel individually, the convolutional layer uses a small “filter” (also called a kernel) that slides over the input image, performing a specific operation.</p> <p>Imagine you have a small magnifying glass, and you’re moving it across a large painting. Every time you stop, you observe a small section, process what you see, and then move to the next section. That’s essentially what a filter does!</p> <p><strong>What is a Filter?</strong> A filter is a small matrix of numbers (e.g., $3 \times 3$ or $5 \times 5$). These numbers are the <em>weights</em> that the network learns during training. Each filter is designed to detect a specific type of feature. For instance, one filter might become excellent at detecting vertical edges, another for horizontal edges, and yet another for certain textures or colors.</p> <p>Let’s visualize the <strong>Convolution Operation</strong>:</p> <ol> <li>The filter is placed over a section of the input image (a patch of pixels the same size as the filter).</li> <li>Element-wise multiplication occurs between the filter’s values and the corresponding pixel values in that image section.</li> <li>All these products are summed up to produce a single number.</li> <li>This single number becomes one pixel in a new output image, called a <strong>feature map</strong> or <strong>activation map</strong>.</li> <li>The filter then slides (or “convolves”) to the next section of the input image, repeating the process.</li> </ol> <p>The mathematical representation of this operation is: $ (I * K)(i, j) = \sum_m \sum_n I(i-m, j-n)K(m, n) $ Where $I$ is the input image, $K$ is the kernel (filter), and $(i, j)$ are the coordinates in the output feature map.</p> <p><strong>Example Intuition:</strong></p> <ul> <li>If a filter for “vertical edges” slides over a region with a strong vertical line, the multiplication and summation will yield a high value in the feature map, indicating the presence of that vertical edge.</li> <li>If it slides over a uniform, featureless region, the output will be low.</li> </ul> <p><strong>Parameters that control the convolution:</strong></p> <ul> <li> <strong>Stride:</strong> This determines how many pixels the filter shifts at a time. A stride of 1 means it moves one pixel at a time, capturing every possible overlap. A stride of 2 means it jumps two pixels, reducing the size of the output feature map.</li> <li> <strong>Padding:</strong> When a filter slides over an image, pixels at the edges are only involved in a few calculations. This can lead to the output feature map being smaller than the input and losing information from the edges. Padding involves adding extra rows and columns of zeros around the input image’s borders, allowing the filter to cover the edges adequately and often resulting in an output feature map of the same size.</li> </ul> <p>A convolutional layer typically uses <em>multiple</em> filters (hundreds, sometimes thousands!). Each filter generates its own feature map, and these feature maps are then stacked together, forming a multi-dimensional output that captures various features from the input. This output then becomes the input for the next layer.</p> <h4 id="2-the-activation-layer-introducing-non-linearity">2. The Activation Layer: Introducing Non-Linearity</h4> <p>After the convolution operation, the data still consists of linear transformations. To enable the network to learn more complex patterns and non-linear relationships, we introduce an activation function. The most common choice in CNNs is the <strong>Rectified Linear Unit (ReLU)</strong>.</p> <p>The ReLU function is incredibly simple: $ f(x) = \max(0, x) $</p> <p>It simply converts all negative values to zero and keeps positive values as they are. Why is this important? Without non-linearity, stacking multiple layers would be equivalent to a single linear transformation, limiting the network’s learning capacity. ReLU introduces the necessary non-linearity, allowing the network to model more intricate functions. Plus, it’s computationally efficient!</p> <h4 id="3-the-pooling-layer-downsampling-and-robustness">3. The Pooling Layer: Downsampling and Robustness</h4> <p>After convolution and activation, our feature maps can still be quite large. The pooling layer comes in to reduce the spatial dimensions (width and height) of the feature maps, which helps in two ways:</p> <ol> <li> <strong>Reduces Computation:</strong> Fewer parameters mean faster training and less memory usage.</li> <li> <strong>Achieves Translation Invariance:</strong> By summarizing features in a region, pooling makes the network less sensitive to the exact location of a feature within that region. If a vertical edge shifts slightly to the left or right, a pooling layer can still detect its presence.</li> </ol> <p>The most popular pooling technique is <strong>Max Pooling</strong>. Here’s how it works:</p> <ol> <li>A small window (e.g., $2 \times 2$) slides over the feature map.</li> <li>Within each window, the maximum value is selected.</li> <li>This maximum value becomes a single pixel in the new, downsampled feature map.</li> </ol> <p>Intuition: “Is this feature <em>strongly present</em> anywhere in this region?” If a strong feature (high value) exists, it’s preserved; otherwise, it’s likely discarded. This effectively summarizes the information. Other pooling methods like Average Pooling also exist, but Max Pooling is generally preferred for its robustness.</p> <h3 id="building-a-deep-network-stacking-the-blocks">Building a Deep Network: Stacking the Blocks</h3> <p>A typical CNN architecture will stack these layers: <code class="language-plaintext highlighter-rouge">Input Image -&gt; Conv -&gt; ReLU -&gt; Pool -&gt; Conv -&gt; ReLU -&gt; Pool ...</code></p> <p>Each successive convolutional layer learns to detect more complex, abstract features based on the features detected by the previous layers. For example:</p> <ul> <li>The first convolutional layer might detect basic edges and corners.</li> <li>The second layer might combine these to detect shapes like circles or rectangles.</li> <li>Later layers might combine shapes to detect parts of objects (e.g., an eye, a wheel, a nose).</li> <li>The final convolutional layers then piece these parts together to recognize entire objects (a face, a car, an animal).</li> </ul> <p>This hierarchical learning is what gives CNNs their incredible power and ability to understand complex visual information.</p> <h4 id="4-the-fully-connected-layer-making-the-final-decision">4. The Fully Connected Layer: Making the Final Decision</h4> <p>After several rounds of convolution, activation, and pooling, we have a set of highly abstract and condensed feature maps. At this point, the spatial information has been compressed into meaningful representations.</p> <p>To perform classification (e.g., “Is this a cat or a dog?”), we need to convert these 2D/3D feature maps into a 1D vector. This process is called <strong>“flattening.”</strong></p> <p>Once flattened, this vector is fed into one or more traditional fully connected neural network layers. These layers act as a classifier, taking the high-level features extracted by the convolutional layers and using them to make a final prediction. The last fully connected layer typically uses a <strong>softmax</strong> activation function, which outputs a probability distribution over the possible classes (e.g., 90% cat, 8% dog, 2% bird).</p> <h3 id="the-learning-process-teaching-a-cnn-to-see">The Learning Process: Teaching a CNN to See</h3> <p>So, how do these filters “know” what features to detect? This is where the magic of training comes in. Just like other neural networks, CNNs learn through a process called <strong>backpropagation</strong> and <strong>gradient descent</strong>.</p> <ol> <li> <strong>Forward Pass:</strong> An image is fed through the network, and a prediction is made.</li> <li> <strong>Loss Calculation:</strong> The predicted output is compared to the actual correct label (e.g., “cat”). A “loss function” calculates how wrong the prediction was.</li> <li> <strong>Backward Pass (Backpropagation):</strong> The error is propagated backward through the network.</li> <li> <strong>Weight Adjustment (Gradient Descent):</strong> An optimizer (like Adam or SGD) uses the calculated gradients to slightly adjust the weights in <em>all</em> the filters and fully connected layers. These adjustments are made in a direction that would reduce the loss for that specific image.</li> </ol> <p>This process is repeated over millions of images and many “epochs” (full passes through the training data). Gradually, the filters in the convolutional layers learn to recognize increasingly relevant and complex features that lead to accurate classifications. It’s truly a testament to the power of iterative optimization!</p> <h3 id="beyond-pictures-the-real-world-impact">Beyond Pictures: The Real-World Impact</h3> <p>CNNs have revolutionized computer vision and beyond:</p> <ul> <li> <strong>Image Classification &amp; Object Detection:</strong> Identifying objects in photos (e.g., Google Photos), powering self-driving cars to detect pedestrians and other vehicles.</li> <li> <strong>Medical Imaging:</strong> Assisting doctors in diagnosing diseases by detecting anomalies in X-rays, MRIs, and CT scans.</li> <li> <strong>Facial Recognition:</strong> Unlocking your phone, security systems.</li> <li> <strong>Satellite Imagery Analysis:</strong> Monitoring deforestation, urban development.</li> <li> <strong>Generative AI:</strong> The fundamental principles of CNNs underpin advanced generative models like Generative Adversarial Networks (GANs) and Diffusion Models, which create realistic images, art, and even videos.</li> </ul> <h3 id="wrapping-up">Wrapping Up</h3> <p>My journey into CNNs was an eye-opener. It showed me how intricate problems can be solved with elegant, modular designs. They bridge the gap between pixels and perception, giving machines a powerful sense of “sight” that was once thought to be exclusively human.</p> <p>While this dive was just scratching the surface, I hope you now have a clearer understanding of the building blocks and the intuitive power behind Convolutional Neural Networks. They are a testament to human ingenuity in mimicking natural intelligence, and their potential continues to unfold in exciting ways every single day.</p> <p>Keep learning, keep building, and maybe you’ll be the one to unlock the next breakthrough in how machines see the world!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>