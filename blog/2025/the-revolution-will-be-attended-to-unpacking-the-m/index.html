<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Revolution Will Be Attended To: Unpacking the Magic of Transformers | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/the-revolution-will-be-attended-to-unpacking-the-m/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Revolution Will Be Attended To: Unpacking the Magic of Transformers</h1> <p class="post-meta"> Created on February 09, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> NLP</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/transformers"> <i class="fa-solid fa-hashtag fa-sm"></i> Transformers</a>   <a href="/blog/blog/tag/attention"> <i class="fa-solid fa-hashtag fa-sm"></i> Attention</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>As someone deeply fascinated by the rapid evolution of Artificial Intelligence, I often find myself reflecting on the “aha!” moments that have truly reshaped the field. For me, one of the biggest and most impactful revelations came with the Transformer architecture. When I first dove into the paper “Attention Is All You Need,” I remember a feeling of both bewilderment and profound excitement. It felt like unlocking a secret cheat code for AI, especially in natural language processing (NLP).</p> <p>Today, I want to demystify Transformers for you. Whether you’re a high school student curious about AI or a budding data scientist, my goal is to break down this incredible innovation into digestible, engaging pieces. Forget the intimidating jargon for a moment; let’s embark on a journey to understand how a model, seemingly built on a simple idea – attention – could lead to powerhouses like ChatGPT, BERT, and DALL-E.</p> <h3 id="the-world-before-transformers-when-ai-had-short-term-memory">The World Before Transformers: When AI Had Short-Term Memory</h3> <p>Before Transformers, the dominant players in sequential data processing (like text) were Recurrent Neural Networks (RNNs) and their more sophisticated cousins, LSTMs (Long Short-Term Memory networks). These models processed information word by word, in a strict order, like reading a book one word at a time, remembering a little bit from the previous word to understand the current one.</p> <p>Imagine trying to understand a very long sentence: “The man who, despite his incredible talent and years of dedicated practice, ultimately failed to win the championship, decided to retire.” By the time an RNN gets to “decided to retire,” it might have largely forgotten “The man” at the beginning, struggling to link the two. This is the <strong>long-range dependency problem</strong>.</p> <p>Moreover, this sequential processing was a bottleneck. It was like a single-lane road: you couldn’t process multiple parts of the sentence at the same time. This made training very slow, especially on powerful parallel hardware like GPUs.</p> <p>We needed a new way. A way for models to see the <em>whole</em> sentence at once, to pick out the most important words, and to do it quickly.</p> <h3 id="enter-attention-the-core-idea">Enter Attention: The Core Idea</h3> <p>The core concept behind Transformers is <strong>Attention</strong>. It’s a beautifully intuitive idea: when you’re trying to understand a particular part of an input (say, a word in a sentence), you should pay more attention to the other parts of the input that are most relevant.</p> <p>Think about reading this blog post. When you read the word “Attention” in the previous paragraph, your brain instantly connects it to the earlier mention of “Transformers” and the idea of “understanding language.” You don’t just process “Attention” in isolation; you relate it to the entire context. That’s essentially what a Transformer does.</p> <p>Instead of trying to compress all past information into a single, ever-changing hidden state (like RNNs), Attention allows the model to <em>directly look back</em> at any part of the input sequence when processing a specific element.</p> <h3 id="self-attention-looking-within-yourself">Self-Attention: Looking Within Yourself</h3> <p>The real game-changer is <strong>Self-Attention</strong>. This isn’t just about paying attention to <em>other</em> inputs, but paying attention to <em>different parts of the same input sequence</em>.</p> <p>Let’s use our example sentence: “The animal didn’t cross the street because it was too tired.” When the model processes the word “it”, it needs to know what “it” refers to. Is it the “animal” or the “street”? A Self-Attention mechanism allows “it” to look at all other words in the sentence and decide which ones are most relevant. In this case, it should strongly attend to “animal” and less so to “street.”</p> <p>How does this magical “attending” happen? It boils down to three key concepts: <strong>Query (Q), Key (K), and Value (V)</strong>.</p> <p>Imagine you’re searching for a book in a library:</p> <ul> <li> <strong>Query (Q):</strong> This is your search query (e.g., “science fiction novels about space travel”). It represents the current word or information you’re focusing on.</li> <li> <strong>Keys (K):</strong> These are the labels or descriptions attached to all the books in the library (e.g., “genre: sci-fi, theme: space, author: Asimov”). Each word in the sentence generates a key.</li> <li> <strong>Values (V):</strong> These are the actual content of the books themselves. Each word in the sentence also generates a value.</li> </ul> <p>To find the most relevant books, you compare your <code class="language-plaintext highlighter-rouge">Query</code> to all the <code class="language-plaintext highlighter-rouge">Keys</code>. The better a <code class="language-plaintext highlighter-rouge">Key</code> matches your <code class="language-plaintext highlighter-rouge">Query</code>, the more “attention” you pay to that book’s <code class="language-plaintext highlighter-rouge">Value</code>.</p> <p>In Self-Attention, <em>every word in the input sequence acts as a Query, Key, and Value</em>. For each word, we calculate a score by taking its <code class="language-plaintext highlighter-rouge">Query</code> vector and doing a dot product with the <code class="language-plaintext highlighter-rouge">Key</code> vector of every other word (including itself). A higher dot product means higher similarity, indicating that the <code class="language-plaintext highlighter-rouge">Query</code> word should pay more attention to that <code class="language-plaintext highlighter-rouge">Key</code> word’s <code class="language-plaintext highlighter-rouge">Value</code>.</p> <p>The mathematical representation of this is surprisingly elegant:</p> \[\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V\] <p>Let’s break it down:</p> <ol> <li>$QK^T$: This is where each <code class="language-plaintext highlighter-rouge">Query</code> vector is multiplied by every <code class="language-plaintext highlighter-rouge">Key</code> vector. The result is a matrix of “attention scores” – how much each word (as a query) should attend to every other word (as a key).</li> <li>$\sqrt{d_k}$: We divide by the square root of the dimension of the key vectors. This is a scaling factor that helps stabilize training, preventing the dot products from becoming too large and pushing the softmax function into regions with tiny gradients.</li> <li>$\text{softmax}(\ldots)$: The softmax function converts these raw attention scores into probabilities, ensuring they sum to 1. This gives us the “attention weights” – how much emphasis each word places on every other word.</li> <li>$(\ldots)V$: Finally, we multiply these attention weights by the <code class="language-plaintext highlighter-rouge">Value</code> vectors. This creates a weighted sum of the <code class="language-plaintext highlighter-rouge">Value</code> vectors, effectively focusing on the most relevant information. This weighted sum becomes the new, context-aware representation for our <code class="language-plaintext highlighter-rouge">Query</code> word.</li> </ol> <p>The beauty? This entire operation can be done in parallel for all words in the sequence! No more sequential bottlenecks.</p> <h3 id="multi-head-attention-diverse-perspectives">Multi-Head Attention: Diverse Perspectives</h3> <p>Just as a single perspective might not be enough to fully understand a complex situation, a single Self-Attention mechanism might miss some nuances. This is where <strong>Multi-Head Attention</strong> comes in.</p> <p>Instead of having just one set of Q, K, and V matrices, we have multiple sets (e.g., 8 or 12 “heads”). Each head independently learns its own way of transforming the input words into Q, K, and V vectors, and thus, each head learns to focus on different aspects of the input.</p> <p>One head might learn to identify subject-verb relationships, another might focus on coreference (like “it” referring to “animal”), and yet another might capture semantic similarities. These different “perspectives” are then concatenated and linearly transformed, providing a richer, more comprehensive contextual understanding of each word. It’s like having a team of experts, each with their own specialty, analyzing the same problem from different angles.</p> <h3 id="positional-encoding-wheres-my-order">Positional Encoding: Where’s My Order?</h3> <p>Transformers, by their very design, process all words simultaneously. This means they inherently lose the information about the <em>order</em> of words in a sentence. “Dog bites man” means something very different from “Man bites dog,” but a vanilla Transformer wouldn’t know the difference.</p> <p>To fix this, we introduce <strong>Positional Encoding</strong>. Before feeding the word embeddings into the Transformer, we add a small vector to each word’s embedding that encodes its position in the sequence. These positional encodings are typically fixed patterns (often sine and cosine waves of different frequencies) that the model learns to interpret.</p> <p>Imagine a unique “fingerprint” added to each word based on its position. This allows the model to inject sequence order information without sacrificing the parallel processing power of Attention.</p> <h3 id="the-transformer-architecture-encoder-and-decoder">The Transformer Architecture: Encoder and Decoder</h3> <p>The original Transformer architecture consists of an <strong>Encoder</strong> and a <strong>Decoder</strong>, typically stacked multiple times (e.g., 6 encoder layers and 6 decoder layers).</p> <ul> <li> <strong>Encoder:</strong> The encoder’s job is to take an input sequence (like an English sentence) and transform it into a rich, abstract representation. Each encoder layer contains: <ol> <li>A Multi-Head Self-Attention mechanism.</li> <li>A position-wise Feed-Forward Network (basically a small neural network applied independently to each position).</li> <li>Crucially, <strong>Residual Connections</strong> (adding the input of a sub-layer to its output) and <strong>Layer Normalization</strong> are used throughout to aid training stability and speed.</li> </ol> </li> <li> <strong>Decoder:</strong> The decoder’s job is to take the encoder’s output and generate an output sequence (like a French translation). Each decoder layer has: <ol> <li>A <strong>Masked Multi-Head Self-Attention</strong> mechanism: This is vital! When generating a word, the decoder can only attend to words it has <em>already generated</em> (and the input). It can’t “peek” at future words. Masking achieves this by zeroing out connections to future positions.</li> <li>A Multi-Head <strong>Encoder-Decoder Attention</strong> (also called Cross-Attention): This is where the decoder <em>attends to the output of the encoder</em>. It helps the decoder focus on the most relevant parts of the <em>input</em> sequence when generating each word of the <em>output</em> sequence.</li> <li>Another position-wise Feed-Forward Network.</li> <li>Again, Residual Connections and Layer Normalization.</li> </ol> </li> </ul> <h3 id="the-unprecedented-impact">The Unprecedented Impact</h3> <p>The Transformer’s ability to process sequences in parallel, capture long-range dependencies efficiently, and leverage massive datasets for pre-training has revolutionized AI.</p> <ul> <li> <strong>BERT (Bidirectional Encoder Representations from Transformers):</strong> Pre-trained on huge amounts of text, BERT’s encoder can be fine-tuned for a multitude of NLP tasks like sentiment analysis, question answering, and named entity recognition, achieving state-of-the-art results.</li> <li> <strong>GPT (Generative Pre-trained Transformer) series (GPT-2, GPT-3, GPT-4):</strong> These models primarily use the <em>decoder-only</em> part of the Transformer, trained to predict the next word in a sequence. This simple task, scaled to billions of parameters and terabytes of text data, unleashed incredible text generation capabilities, leading to conversational AI like ChatGPT.</li> <li> <strong>T5, DALL-E, Vision Transformers (ViT):</strong> The Transformer architecture has proven so versatile that it’s no longer confined to text. T5 unifies various NLP tasks into a text-to-text format. DALL-E uses Transformers to generate images from text descriptions. ViTs apply the same principles to image processing, breaking images into patches and treating them like words in a sentence.</li> </ul> <h3 id="the-future-is-attending">The Future is Attending</h3> <p>Transformers aren’t without their limitations. They are incredibly computationally intensive to train due to their sheer size and the quadratic complexity of self-attention with respect to sequence length. Researchers are actively working on more efficient “sparse attention” mechanisms and other optimizations.</p> <p>However, the core idea of attention and the Transformer’s parallel processing capabilities have fundamentally reshaped our approach to AI, especially in handling complex, sequential data. It has brought us closer to truly understanding and generating human-like language and even bridging the gap between language and other modalities like vision.</p> <p>So, the next time you marvel at an AI writing an essay or holding a conversation, remember the ingenious mechanism at its heart: the Transformer, a testament to the power of simple, elegant ideas scaled to incredible proportions. It’s a journey from “reading word by word” to “attending to the whole picture” – and what a picture it’s painting for the future of AI!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>