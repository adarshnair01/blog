<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Don't Fool Yourself: Unmasking Your Model's True Performance with Cross-Validation | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/2025/dont-fool-yourself-unmasking-your-models-true-perf/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Don't Fool Yourself: Unmasking Your Model's True Performance with Cross-Validation</h1> <p class="post-meta"> Created on September 23, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/model-evaluation"> <i class="fa-solid fa-hashtag fa-sm"></i> Model Evaluation</a>   <a href="/blog/blog/tag/cross-validation"> <i class="fa-solid fa-hashtag fa-sm"></i> Cross-Validation</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/overfitting"> <i class="fa-solid fa-hashtag fa-sm"></i> Overfitting</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>So, you’ve just trained your first machine learning model. Maybe it’s a classifier predicting cat vs. dog, or a regressor estimating house prices. You run your evaluation metrics – accuracy, precision, RMSE – and <em>bam!</em> You see an amazing score. 95% accuracy! 0.05 RMSE! You feel like a data science wizard, ready to deploy your masterpiece to the world.</p> <p>Then, reality hits.</p> <p>You deploy your model, feed it new, unseen data, and suddenly those stellar numbers plummet. Your 95% accuracy is now 60%. Your perfect house price predictor is way off. What happened? Why did your model “lie” to you?</p> <p>This, my friends, is a classic rite of passage in the world of data science. It’s often a symptom of a nasty little problem called <strong>overfitting</strong>, and it highlights a fundamental challenge: <em>how do we truly know if our model is any good at generalizing to new, unseen data?</em></p> <h3 id="the-illusion-of-a-single-test-set">The Illusion of a Single Test Set</h3> <p>Let’s rewind. How did you get that initial, glorious accuracy score? Chances are, you split your dataset into two parts: a <strong>training set</strong> (what the model learns from) and a <strong>test set</strong> (what you evaluate the model on). This is a crucial first step, and much better than training and testing on the same data (which would <em>always</em> give you perfect scores, but tell you nothing about real-world performance).</p> <p>Imagine your data is a giant jigsaw puzzle. You use 80% of the pieces to figure out how they fit together (training), and then you try to assemble the remaining 20% to see if your puzzle-solving skills are good (testing).</p> <p>The problem with this approach, however, is that your performance metric – say, accuracy – is based on <em>just that one specific 20% slice</em> of data. What if that slice happened to be particularly “easy” for your model? What if, by sheer luck, it contained examples that your model just happened to be good at predicting, even if it wouldn’t fare well on a different 20% slice?</p> <p>Your single test set provides only a <em>single estimate</em> of your model’s performance. It’s like judging a student’s entire knowledge based on just one randomly selected question on a pop quiz. The student might ace that one question but flunk the rest of the syllabus. This single-point estimate can be quite <strong>variable</strong> and <strong>unreliable</strong>. It might make you overly optimistic or, conversely, overly pessimistic.</p> <p>We need something more robust, something that gives us a clearer, less biased picture of how our model <em>actually</em> generalizes.</p> <h3 id="enter-cross-validation-your-models-sanity-check">Enter Cross-Validation: Your Model’s Sanity Check</h3> <p>This is where <strong>Cross-Validation</strong> rides in like a superhero to save the day. At its heart, cross-validation is a technique to assess how the results of a statistical analysis (like training an ML model) will generalize to an independent dataset. It does this by repeatedly partitioning the original dataset into training and test sets, performing the analysis on each split, and then averaging the results.</p> <p>Think of it this way: instead of one pop quiz, you get 10 different mini-quizzes, each covering different parts of the material. By averaging your scores across all 10 mini-quizzes, you get a much more comprehensive and reliable assessment of your overall knowledge.</p> <p>The most common and widely used form of cross-validation is <strong>K-Fold Cross-Validation</strong>.</p> <h4 id="k-fold-cross-validation-the-workhorse">K-Fold Cross-Validation: The Workhorse</h4> <p>Here’s how K-Fold Cross-Validation works:</p> <ol> <li> <p><strong>Divide the Data:</strong> You first split your entire dataset into $K$ equal-sized “folds” or segments. Let’s say we choose $K=5$. This means your data is divided into 5 distinct chunks.</p> <p>Imagine your data as a loaf of bread, and you’ve sliced it into 5 equal pieces.</p> </li> <li> <strong>Iterate and Evaluate:</strong> The magic happens over $K$ iterations. In each iteration: <ul> <li> <strong>One fold</strong> is designated as the <strong>test set</strong> (or validation set).</li> <li>The <strong>remaining $K-1$ folds</strong> are combined to form the <strong>training set</strong>.</li> <li>Your model is then trained on this training set.</li> <li>The trained model is evaluated on the test set, and its performance metric (e.g., accuracy, RMSE) is recorded.</li> </ul> <p>So, for our $K=5$ example:</p> <ul> <li> <strong>Iteration 1:</strong> Fold 1 is test, Folds 2-5 are train. Calculate Accuracy$_1$.</li> <li> <strong>Iteration 2:</strong> Fold 2 is test, Folds 1, 3-5 are train. Calculate Accuracy$_2$.</li> <li> <strong>Iteration 3:</strong> Fold 3 is test, Folds 1-2, 4-5 are train. Calculate Accuracy$_3$.</li> <li> <strong>Iteration 4:</strong> Fold 4 is test, Folds 1-3, 5 are train. Calculate Accuracy$_4$.</li> <li> <strong>Iteration 5:</strong> Fold 5 is test, Folds 1-4 are train. Calculate Accuracy$_5$.</li> </ul> </li> <li> <p><strong>Average the Results:</strong> After all $K$ iterations are complete, you’ll have $K$ different performance scores (Accuracy$_1$, Accuracy$_2$, …, Accuracy$_K$). To get a final, robust estimate of your model’s performance, you simply average these scores:</p> <p>$\text{Average Accuracy} = \frac{1}{K} \sum_{i=1}^{K} \text{Accuracy}_i$</p> <p>This average accuracy is a much more reliable indicator of how well your model is likely to perform on truly unseen data. It essentially tells you: “On average, across different representative slices of my data, my model achieved this level of performance.”</p> </li> </ol> <h4 id="why-k-fold-is-a-game-changer">Why K-Fold is a Game Changer</h4> <ul> <li> <strong>Reduced Variance:</strong> By testing on multiple different subsets of the data, K-Fold CV smooths out the randomness of a single train-test split. Your final performance estimate is less sensitive to the particular way the data was split.</li> <li> <strong>More Data for Training &amp; Testing:</strong> Every data point gets to be in a test set exactly once, and it gets to be in a training set $K-1$ times. This makes more efficient use of your potentially limited data compared to a single split where a portion of data is <em>always</em> held out.</li> <li> <strong>Better Generalization Estimate:</strong> It gives you a much better sense of how your model will generalize to <em>any</em> new data, not just the specific test set you happened to pick initially. It helps you understand the <em>stability</em> of your model’s performance.</li> </ul> <h4 id="choosing-k-a-balancing-act">Choosing K: A Balancing Act</h4> <p>What’s the right value for $K$? It’s a trade-off:</p> <ul> <li> <strong>Small K (e.g., K=3 or K=5):</strong> <ul> <li> <strong>Pros:</strong> Faster to compute (fewer models to train).</li> <li> <strong>Cons:</strong> Higher bias in the performance estimate (each training set is smaller, so models might be underfit). Higher variance (test sets are larger, so performance might fluctuate more).</li> </ul> </li> <li> <strong>Large K (e.g., K=10 or K=N, where N is the number of data points):</strong> <ul> <li> <strong>Pros:</strong> Lower bias (training sets are larger, closer to the full dataset). More stable performance estimate.</li> <li> <strong>Cons:</strong> Slower to compute (more models to train, especially for <strong>Leave-One-Out Cross-Validation (LOOCV)</strong> where $K=N$).</li> </ul> </li> </ul> <p>Common choices are $K=5$ or $K=10$. They offer a good balance between computational cost and a reliable performance estimate.</p> <h3 id="beyond-basic-k-fold-specialized-strategies">Beyond Basic K-Fold: Specialized Strategies</h3> <p>While K-Fold is the most common, there are variations for specific scenarios:</p> <ul> <li> <strong>Stratified K-Fold:</strong> Imagine you have an imbalanced dataset, like trying to predict a rare disease (where healthy patients far outnumber sick ones). A simple K-Fold might, by chance, put all the sick patients into one test fold, leaving the other folds with none. Stratified K-Fold ensures that each fold maintains the same proportion of target classes as the overall dataset. This is <em>crucial</em> for imbalanced classification problems.</li> <li> <strong>Time Series Cross-Validation:</strong> For time-series data, you absolutely cannot “peek into the future.” You can’t train a model on data from 2023 and test it on data from 2022. Time series CV uses an “expanding window” or “rolling window” approach. You train on data up to a certain point in time and test on the <em>next</em> block of time, then expand your training window and repeat. This respects the temporal order of the data.</li> <li> <strong>Group K-Fold:</strong> If your data has natural groupings (e.g., multiple medical records from the same patient, or reviews from the same user), you don’t want to accidentally put some records from a patient in the training set and others from the <em>same patient</em> in the test set. This would lead to data leakage. Group K-Fold ensures that all data points belonging to a specific group stay together in either the training or test set.</li> </ul> <h3 id="when-to-use-cross-validation">When to Use Cross-Validation</h3> <p>Cross-validation isn’t just for getting a final accuracy score. It’s a powerful tool used throughout the machine learning workflow:</p> <ol> <li> <strong>Model Selection:</strong> When you’re comparing different algorithms (e.g., Logistic Regression vs. Support Vector Machine vs. Random Forest), cross-validation gives you a fair, robust way to see which one performs best on average.</li> <li> <strong>Hyperparameter Tuning:</strong> This is arguably where CV shines brightest. When you’re trying to find the optimal settings (hyperparameters) for your model (e.g., the <code class="language-plaintext highlighter-rouge">C</code> parameter in SVM, the <code class="language-plaintext highlighter-rouge">n_estimators</code> in a Random Forest), you can use techniques like Grid Search or Random Search <em>with cross-validation</em> to evaluate each combination of hyperparameters. This ensures you pick the parameters that lead to the most generalized performance, not just performance on one arbitrary test set.</li> <li> <strong>Robust Performance Estimation:</strong> As we discussed, it gives you a much more reliable estimate of your model’s true generalization ability before you deploy it.</li> </ol> <h3 id="important-gotchas-and-best-practices">Important Gotchas and Best Practices</h3> <ul> <li> <strong>Data Leakage Prevention:</strong> This is critical! Any data preprocessing steps that involve fitting to the data (like scaling with <code class="language-plaintext highlighter-rouge">StandardScaler</code>, imputation with <code class="language-plaintext highlighter-rouge">SimpleImputer</code>, or feature selection) <em>must be performed independently within each fold’s training set</em>. If you fit these transformers on your <em>entire dataset</em> before starting cross-validation, information from the test folds will “leak” into your training process, leading to overly optimistic results. Always wrap your preprocessing and model training within a <code class="language-plaintext highlighter-rouge">Pipeline</code> and then apply CV to the pipeline.</li> <li> <strong>Computational Cost:</strong> Cross-validation can be computationally expensive, especially with large datasets, complex models, or a high value of $K$. Be mindful of your resources and time.</li> <li> <strong>Always have an independent test set:</strong> Even after extensive cross-validation for model selection and hyperparameter tuning, it’s a good practice to have one final, truly independent “hold-out” test set that you use <em>only once</em> at the very end to confirm your model’s final performance. This set should never have been seen during any part of the training or cross-validation process.</li> </ul> <h3 id="conclusion-trusting-your-models">Conclusion: Trusting Your Models</h3> <p>Building a machine learning model is exciting, but trusting its performance is paramount. A single train-test split, while a good start, can paint an overly optimistic or pessimistic picture. Cross-validation, particularly K-Fold CV, is an indispensable tool in any data scientist’s arsenal.</p> <p>It’s more than just a technique; it’s a philosophy of robust evaluation. By embracing cross-validation, you move beyond the illusion of a single, potentially misleading score and gain a deeper, more reliable understanding of your model’s true capability to generalize. It empowers you to build models that don’t just perform well on your development machine but truly shine when faced with the unpredictable data of the real world.</p> <p>So next time you’re evaluating a model, don’t just split and pray. Cross-validate, understand the range of your model’s performance, and build with confidence!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>