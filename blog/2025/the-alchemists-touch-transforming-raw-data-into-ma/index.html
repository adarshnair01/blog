<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Alchemist's Touch: Transforming Raw Data into Machine Learning Gold with Feature Engineering | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/the-alchemists-touch-transforming-raw-data-into-ma/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Alchemist's Touch: Transforming Raw Data into Machine Learning Gold with Feature Engineering</h1> <p class="post-meta"> Created on October 08, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/feature-engineering"> <i class="fa-solid fa-hashtag fa-sm"></i> Feature Engineering</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/data-preprocessing"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Preprocessing</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>Welcome back to my journal, where I chronicle my journey through the fascinating world of Data Science and Machine Learning. Today, I want to talk about something that, in my opinion, is often undervalued but immensely powerful: <strong>Feature Engineering</strong>. If data is the fuel for our ML models, then feature engineering is the refining process that turns crude oil into high-octane rocket fuel.</p> <h3 id="what-even-is-feature-engineering">What Even <em>Is</em> Feature Engineering?</h3> <p>Imagine you’re trying to predict if a student will pass a test. You have raw data: their age, the date they took the test, and their past scores. Now, you could just feed this directly into a model. But what if you could give your model <em>more insightful</em> information?</p> <p>What if you calculated:</p> <ul> <li>The student’s average score over the last 3 tests?</li> <li>How many days it’s been since their last test?</li> <li>If the test was on a Monday (maybe students are groggier then?).</li> <li>The student’s age <em>squared</em> (perhaps older, more experienced students improve their learning rate exponentially?).</li> </ul> <p>These new, derived pieces of information are what we call <strong>features</strong>. And the entire process of creating these features from raw data, or transforming existing ones to make them more useful, is <strong>Feature Engineering</strong>.</p> <p>Think of yourself as a detective. You have a bunch of clues (raw data), but sometimes, the most crucial insights come from combining those clues, looking at them from different angles, or even inferring new information. That’s what feature engineering does for your machine learning models. It’s about giving your model the <em>best possible story</em> from the data.</p> <h3 id="why-bother-the-garbage-in-gold-out-principle">Why Bother? The “Garbage In, Gold Out” Principle</h3> <p>You might have heard the saying “Garbage In, Garbage Out” (GIGO) in computing. In machine learning, it’s particularly true. If your input features are poor, irrelevant, or poorly represented, even the most sophisticated deep learning model will struggle to perform well.</p> <p>Feature engineering turns GIGO into something closer to “Good Features In, Gold Out!” Here’s why it’s so critical:</p> <ol> <li> <strong>Improved Model Performance:</strong> This is the big one. Well-engineered features can significantly boost your model’s accuracy, precision, recall, or whatever metric you’re optimizing for. Sometimes, simple models with great features outperform complex models with raw, unprocessed features.</li> <li> <strong>Better Model Interpretability:</strong> When you create meaningful features, your model often becomes easier to understand. If you know “average score over last 3 tests” is a strong predictor, that’s a clear insight.</li> <li> <strong>Reduced Overfitting:</strong> By transforming features and making them more generalized, you can sometimes help your model focus on the actual patterns, rather than memorizing noise in the training data.</li> <li> <strong>Handling Non-Linearity:</strong> Many real-world relationships aren’t linear. Feature engineering, like creating polynomial features or interactions, allows linear models to capture these complex patterns.</li> <li> <strong>Dealing with Missing Data and Outliers:</strong> Engineering new features or transforming existing ones can help manage these common data quality issues.</li> </ol> <h3 id="the-art-and-science-of-feature-engineering">The Art and Science of Feature Engineering</h3> <p>I call it both an “art” and a “science” because it truly is.</p> <ul> <li> <strong>The Science:</strong> This involves understanding statistical distributions, mathematical transformations, and algorithms. It’s about applying proven techniques methodically.</li> <li> <strong>The Art:</strong> This comes from domain knowledge, creativity, intuition, and experience. It’s about looking at your data and asking, “What hidden relationships might exist? What information is crucial to this problem that isn’t immediately obvious?” If you’re predicting house prices, knowing that “number of bathrooms per square foot” might be a good feature comes from an understanding of real estate, not just statistics.</li> </ul> <p>Let’s dive into some common types of feature engineering techniques with examples.</p> <hr> <h3 id="1-numerical-features-refining-the-numbers"><strong>1. Numerical Features: Refining the Numbers</strong></h3> <p>Numerical data is the most straightforward, but often hides deep potential.</p> <h4 id="a-binning-discretization">a. Binning (Discretization)</h4> <p>Sometimes, precise numerical values aren’t as important as their range or category. For instance, age. Instead of using exact age, we might bin it into categories like “Child (0-12)”, “Teen (13-19)”, “Adult (20-65)”, “Senior (65+)”. This can help models capture non-linear relationships and reduce sensitivity to small variations.</p> <p>Example: <code class="language-plaintext highlighter-rouge">Age</code> $\rightarrow$ <code class="language-plaintext highlighter-rouge">Age_Group</code></p> <h4 id="b-transformations">b. Transformations</h4> <p>Numerical data can often be skewed (not normally distributed), which can hurt some models. Logarithmic or square root transformations can often normalize these distributions.</p> <ul> <li> <strong>Logarithmic Transform:</strong> Used for highly skewed positive data. $y = \log(x)$ or $y = \log(1+x)$ (to handle zeros). <ul> <li> <em>Why?</em> It compresses large values and expands small values, making the distribution more symmetrical. Imagine income data – most people earn less, a few earn a lot. Log transforming can make this distribution more normal.</li> </ul> </li> <li> <strong>Square Root Transform:</strong> Similar to log, but less aggressive. $y = \sqrt{x}$.</li> <li> <strong>Reciprocal Transform:</strong> $y = 1/x$. Useful for variables where smaller values mean “more” (e.g., time to complete a task, where less time is better).</li> </ul> <h4 id="c-scaling">c. Scaling</h4> <p>Many machine learning algorithms (like K-Nearest Neighbors, Support Vector Machines, neural networks) are sensitive to the scale of features. A feature with values from 0-1000 will dominate a feature with values from 0-1. Scaling puts them on an even playing field.</p> <ul> <li> <strong>Standardization (Z-score Normalization):</strong> Rescales data to have a mean ($\mu$) of 0 and a standard deviation ($\sigma$) of 1. \(z = \frac{x - \mu}{\sigma}\) <ul> <li> <em>Why?</em> Useful when your data follows a Gaussian (bell curve) distribution.</li> </ul> </li> <li> <strong>Min-Max Scaling (Normalization):</strong> Rescales data to a fixed range, usually 0 to 1. \(x' = \frac{x - \min(x)}{\max(x) - \min(x)}\) <ul> <li> <em>Why?</em> Good for algorithms that expect inputs in a specific range, like neural networks.</li> </ul> </li> </ul> <h4 id="d-polynomial-features">d. Polynomial Features</h4> <p>To capture non-linear relationships, you can create new features that are powers of existing features. If you have feature $X$, you can create $X^2$, $X^3$, etc. You can also create interaction terms like $X_1 \cdot X_2$.</p> <p>Example: If <code class="language-plaintext highlighter-rouge">Hours_Studied</code> and <code class="language-plaintext highlighter-rouge">IQ_Score</code> are features, you might create <code class="language-plaintext highlighter-rouge">Hours_Studied^2</code> or <code class="language-plaintext highlighter-rouge">Hours_Studied * IQ_Score</code>. This can help a linear model fit a quadratic or interactive relationship.</p> <hr> <h3 id="2-categorical-features-giving-context-to-categories"><strong>2. Categorical Features: Giving Context to Categories</strong></h3> <p>Categorical data represents types or groups (e.g., “red”, “blue”, “green” for color). Models don’t understand text directly, so we need to convert them to numbers.</p> <h4 id="a-one-hot-encoding">a. One-Hot Encoding</h4> <p>This is the most common method. For each unique category in a feature, a new binary (0 or 1) column is created.</p> <p>Example: <code class="language-plaintext highlighter-rouge">Color</code> = {“Red”, “Blue”, “Green”} $\rightarrow$ <code class="language-plaintext highlighter-rouge">Color_Red</code> (0/1), <code class="language-plaintext highlighter-rouge">Color_Blue</code> (0/1), <code class="language-plaintext highlighter-rouge">Color_Green</code> (0/1)</p> <ul> <li> <em>Why?</em> Prevents the model from incorrectly assuming an ordinal relationship (e.g., that “red” is “greater than” “blue” if you simply assigned 0, 1, 2).</li> <li> <em>Caveat:</em> Can lead to a high number of new features if you have many unique categories (high dimensionality).</li> </ul> <h4 id="b-label-encoding--ordinal-encoding">b. Label Encoding / Ordinal Encoding</h4> <p>If your categories have a natural order (e.g., “Small”, “Medium”, “Large”), you can assign numerical labels directly.</p> <p>Example: <code class="language-plaintext highlighter-rouge">Size</code> = {“Small”, “Medium”, “Large”} $\rightarrow$ <code class="language-plaintext highlighter-rouge">Size</code> = {0, 1, 2}</p> <ul> <li> <em>Why?</em> Saves space compared to One-Hot Encoding. Only use when there’s a clear order!</li> </ul> <h4 id="c-target-encoding-mean-encoding">c. Target Encoding (Mean Encoding)</h4> <p>This is a more advanced technique. Instead of creating new columns or arbitrary numbers, you replace a categorical value with the mean of the target variable for that category.</p> <p>Example: Predicting house price, <code class="language-plaintext highlighter-rouge">Neighborhood</code> is a categorical feature. Replace <code class="language-plaintext highlighter-rouge">Neighborhood A</code> with the average house price in <code class="language-plaintext highlighter-rouge">Neighborhood A</code>.</p> <ul> <li> <em>Why?</em> Can be very powerful as it directly embeds information about the target variable into the feature.</li> <li> <em>Caveat:</em> HIGH risk of data leakage! You must use cross-validation or careful split strategies to prevent using target information from the test set.</li> </ul> <hr> <h3 id="3-date-and-time-features-unearthing-temporal-patterns"><strong>3. Date and Time Features: Unearthing Temporal Patterns</strong></h3> <p>Dates and times are goldmines for features, as many phenomena are cyclical or time-dependent.</p> <p>From a <code class="language-plaintext highlighter-rouge">timestamp</code> or <code class="language-plaintext highlighter-rouge">datetime</code> column, you can extract:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">Year</code>, <code class="language-plaintext highlighter-rouge">Month</code>, <code class="language-plaintext highlighter-rouge">Day</code>, <code class="language-plaintext highlighter-rouge">Day_of_Week</code> (e.g., Monday=0, Sunday=6)</li> <li> <code class="language-plaintext highlighter-rouge">Hour</code>, <code class="language-plaintext highlighter-rouge">Minute</code>, <code class="language-plaintext highlighter-rouge">Second</code> </li> <li> <code class="language-plaintext highlighter-rouge">Week_of_Year</code>, <code class="language-plaintext highlighter-rouge">Quarter</code> </li> <li> <code class="language-plaintext highlighter-rouge">Is_Weekend</code>, <code class="language-plaintext highlighter-rouge">Is_Holiday</code> (requires a holiday calendar)</li> <li> <code class="language-plaintext highlighter-rouge">Time_Since_Last_Event</code> or <code class="language-plaintext highlighter-rouge">Time_Until_Next_Event</code> </li> <li> <strong>Cyclical Features:</strong> For features like <code class="language-plaintext highlighter-rouge">month</code> or <code class="language-plaintext highlighter-rouge">day_of_week</code>, simply treating them as linear numbers (1-12 for month) can be misleading. December (12) and January (1) are numerically far apart but temporally close. We can use sine and cosine transformations to capture this cyclical nature: \(x*{sin} = \sin(2\pi \cdot \text{value}/\text{max_value})\) \(x*{cos} = \cos(2\pi \cdot \text{value}/\text{max_value})\) For <code class="language-plaintext highlighter-rouge">month</code>, <code class="language-plaintext highlighter-rouge">max_value</code> would be 12. For <code class="language-plaintext highlighter-rouge">day_of_week</code>, <code class="language-plaintext highlighter-rouge">max_value</code> would be 7.</li> </ul> <p>Example: Predicting store sales. <code class="language-plaintext highlighter-rouge">Day_of_Week</code> and <code class="language-plaintext highlighter-rouge">Month</code> could be critical. Sales might peak on weekends or in specific months like December.</p> <hr> <h3 id="4-interaction-features-the-power-of-combination"><strong>4. Interaction Features: The Power of Combination</strong></h3> <p>Sometimes, the interaction between two features is more predictive than the features themselves.</p> <p>Example: <code class="language-plaintext highlighter-rouge">Age</code> and <code class="language-plaintext highlighter-rouge">Income</code>. Neither alone might tell you as much about purchasing power as <code class="language-plaintext highlighter-rouge">Age * Income</code> or <code class="language-plaintext highlighter-rouge">Income / Age</code>. Or for house prices, <code class="language-plaintext highlighter-rouge">Number_of_Rooms * Square_Footage</code>.</p> <h3 id="5-handling-missing-values-dont-let-gaps-stop-you"><strong>5. Handling Missing Values: Don’t Let Gaps Stop You</strong></h3> <p>Missing data is common. Simply dropping rows or columns can lose valuable information. Feature engineering includes strategies for imputation:</p> <ul> <li> <strong>Mean/Median/Mode Imputation:</strong> Replace missing values with the mean, median (robust to outliers), or mode (for categorical) of the respective column.</li> <li> <strong>Advanced Imputation:</strong> Using machine learning models (like K-Nearest Neighbors or regression) to predict missing values.</li> <li> <strong>Indicator Variable:</strong> Create a new binary feature <code class="language-plaintext highlighter-rouge">is_missing</code> (0/1) to explicitly tell the model that a value was originally missing. Sometimes the fact that data is missing is itself a predictive feature!</li> </ul> <hr> <h3 id="the-feature-engineering-workflow-my-personal-approach"><strong>The Feature Engineering Workflow (My Personal Approach)</strong></h3> <ol> <li> <strong>Understand Your Data and Domain:</strong> This is paramount. What does each column mean? What kind of problem are you solving? What business insights or real-world factors might influence the target? <em>This is where the ‘art’ begins.</em> </li> <li> <strong>Exploratory Data Analysis (EDA):</strong> Visualize distributions, correlations, outliers, and missing values. This step will often reveal potential areas for feature engineering.</li> <li> <strong>Brainstorm Features:</strong> Based on your understanding and EDA, think of new features that might be helpful. Don’t be afraid to be creative!</li> <li> <strong>Implement and Transform:</strong> Write code (Pandas in Python is your best friend here!) to create and transform your features.</li> <li> <strong>Evaluate:</strong> Train your model with the new features. Compare performance with a baseline model (without these features). Use feature importance techniques (e.g., from tree-based models) to see which new features are genuinely contributing.</li> <li> <strong>Iterate:</strong> Feature engineering is rarely a one-shot process. You’ll likely go back to step 1, refining features, adding new ones, or removing ineffective ones.</li> </ol> <h3 id="common-pitfalls-to-watch-out-for"><strong>Common Pitfalls to Watch Out For</strong></h3> <ul> <li> <strong>Data Leakage:</strong> This is the most dangerous trap. It occurs when your training data contains information about the target variable that would not be available in a real-world prediction scenario. Example: using future information, or target encoding without proper cross-validation.</li> <li> <strong>Over-engineering:</strong> Sometimes, too many complex features can lead to an overly complex model that overfits or is hard to interpret. Keep it simple where possible.</li> <li> <strong>Ignoring Domain Knowledge:</strong> Don’t just throw statistical methods at data blindly. The best features often come from someone who truly understands the problem domain.</li> <li> <strong>Not Scaling/Normalizing:</strong> For algorithms sensitive to scale, neglecting this step can significantly harm performance.</li> </ul> <h3 id="conclusion-beyond-the-algorithm"><strong>Conclusion: Beyond the Algorithm</strong></h3> <p>Feature engineering is a foundational skill in data science. It’s the difference between merely applying an algorithm and truly understanding and leveraging your data’s potential. It transforms you from a data processor into a data storyteller, enabling your models to “see” patterns they couldn’t before.</p> <p>It’s challenging, creative, and often the most rewarding part of building a machine learning model. So next time you’re faced with a dataset, don’t just jump to model training. Take a step back, put on your detective hat, and ask yourself: “What hidden gems can I unearth from this data? How can I make my features sing?”</p> <p>Happy feature engineering, and I’ll catch you in the next post!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>