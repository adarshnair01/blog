<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Unmasking BERT: How a Google Breakthrough Taught Computers to Understand Language Like Never Before | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/unmasking-bert-how-a-google-breakthrough-taught-co/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Unmasking BERT: How a Google Breakthrough Taught Computers to Understand Language Like Never Before</h1> <p class="post-meta"> Created on July 01, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> NLP</a>   <a href="/blog/blog/tag/bert"> <i class="fa-solid fa-hashtag fa-sm"></i> BERT</a>   <a href="/blog/blog/tag/transformers"> <i class="fa-solid fa-hashtag fa-sm"></i> Transformers</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>As a budding data scientist, one of the most magical areas for me has always been Natural Language Processing (NLP). The idea of teaching a machine to not just process words, but to <em>understand</em> them – their meaning, their context, their nuances – felt like trying to bottle lightning. For years, we struggled with models that could recognize patterns, but truly grasping the subtle dance of human language remained an elusive dream.</p> <p>Then, in 2018, Google dropped a bombshell: <strong>BERT</strong>.</p> <p>It wasn’t just another incremental improvement; it was a seismic shift. BERT, which stands for <strong>B</strong>idirectional <strong>E</strong>ncoder <strong>R</strong>epresentations from <strong>T</strong>ransformers, suddenly made computers surprisingly good at understanding context, answering questions, and even summarizing text. For me, it felt like we’d finally found the Rosetta Stone for machine-human communication.</p> <p>But what exactly <em>is</em> BERT, and why was it such a game-changer? Let’s unpack this marvel together.</p> <h3 id="the-problem-we-were-trying-to-solve-languages-elusive-context">The Problem We Were Trying to Solve: Language’s Elusive Context</h3> <p>Imagine the sentence: “The bank was so steep, I couldn’t climb it.” Now consider: “I went to the bank to deposit money.”</p> <p>The word “bank” has completely different meanings in these two sentences. As humans, we understand this effortlessly through context. For computers, this has historically been a huge challenge. Early NLP models, like those based on <strong>Recurrent Neural Networks (RNNs)</strong> and their sophisticated cousins, <strong>LSTMs (Long Short-Term Memory)</strong>, processed words sequentially, one after another.</p> <p>Think of it like reading a book one word at a time, only ever remembering what you just read. If you’re trying to understand “bank” in the second sentence, an LSTM might have seen “I,” “went,” “to,” “the,” and then “bank.” It’s building context from <em>left to right</em>.</p> <p>This unidirectional approach had a fundamental limitation: it couldn’t see the future. When processing “bank,” it didn’t know if “to deposit money” was coming up. This meant it could only ever build a <em>partial</em> understanding of a word’s meaning, limited by what it had <em>already</em> seen. This was a critical bottleneck for truly deep language understanding.</p> <h3 id="the-transformer-revolution-attention-is-all-you-need">The Transformer Revolution: Attention is All You Need</h3> <p>Before BERT, there was another groundbreaking paper from Google in 2017 titled “Attention Is All You Need.” This paper introduced the <strong>Transformer architecture</strong>, which completely changed how we thought about sequence processing.</p> <p>Instead of processing words sequentially, the Transformer introduced something called the <strong>attention mechanism</strong>. Imagine you’re reading a complex sentence. As a human, you don’t just process words linearly; your brain simultaneously considers how each word relates to <em>every other word</em> in the sentence to build meaning.</p> <p>The attention mechanism mimics this. It allows the model to weigh the importance of different words in the input sequence when encoding a particular word. So, when the Transformer processes “bank” in “I went to the bank to deposit money,” it can <em>attend</em> to “deposit money” at the same time it’s looking at “I went to the.” This is a massive leap!</p> <p>The core idea of attention can be simplified as calculating a “relevance score” between different words. For any given word, the model computes a score for how much it should “pay attention” to every other word in the sequence. These scores are then used to create a weighted sum of the other words’ representations, effectively focusing on the most relevant information. Mathematically, it involves matrix multiplications to produce “Query,” “Key,” and “Value” vectors for each word, then using these to calculate attention scores:</p> \[\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\] <p>Where $Q$ is the Query matrix, $K$ is the Key matrix, $V$ is the Value matrix, and $d_k$ is the dimension of the Key vectors (a scaling factor). Don’t get too bogged down in the math, the key takeaway is that it allows the model to dynamically decide <em>what to focus on</em> across the entire input sequence.</p> <h3 id="bert-the-bidirectional-master">BERT: The Bidirectional Master</h3> <p>The Transformer was powerful, but BERT took it to the next level by making it <em>bidirectional</em>. This means BERT processes language by looking at the words to its left <em>and</em> to its right <em>simultaneously</em> to understand the context of any given word.</p> <p>It’s like having super-vision: instead of just reading “bank” and knowing it came after “the,” BERT knows it’s nestled between “the” and “to deposit money” <em>at the same time</em>. This full-sentence context is what makes BERT so incredibly powerful.</p> <h3 id="how-does-bert-learn-the-pre-training-magic">How Does BERT Learn? The Pre-training Magic</h3> <p>BERT’s brilliance isn’t just in its architecture but also in its ingenious pre-training strategy. Instead of training it for a specific task (like sentiment analysis) from scratch, Google pre-trained BERT on a massive amount of text data – specifically, the entire English Wikipedia (2,500 million words) and the BookCorpus dataset (800 million words).</p> <p>This pre-training phase involved two clever, unsupervised tasks:</p> <ol> <li> <p><strong>Masked Language Model (MLM): Learning from Fill-in-the-Blanks</strong> Imagine taking a sentence and randomly hiding (masking) about 15% of its words. BERT’s task is then to predict those masked words based on the context of the <em>unmasked</em> words around them.</p> <p>For example, if BERT sees: “I went to the [MASK] to deposit money.” It must use “I went to the” and “to deposit money” to infer that the masked word is likely “bank.”</p> <p>This forces BERT to learn deep contextual relationships between words. It’s essentially a sophisticated “fill-in-the-blanks” game played on an astronomical scale. The objective function here could be thought of as maximizing the probability of predicting the correct masked tokens: \(\sum_{i \in \text{masked tokens}} \log P(\text{token}_i | \text{context})\) where $P(\text{token}_i | \text{context})$ is the probability BERT assigns to the correct word for a masked token, given the surrounding words.</p> </li> <li> <p><strong>Next Sentence Prediction (NSP): Understanding Relationships Between Sentences</strong> Language isn’t just about individual words; it’s also about how sentences relate to each other. BERT is trained on pairs of sentences, and for each pair, it has to predict whether the second sentence <em>actually follows</em> the first one in the original text, or if it’s a random sentence plucked from elsewhere.</p> <p>Example 1 (IsNext): “The cat sat on the mat. It purred contentedly.” (BERT should predict IS_NEXT) Example 2 (NotNext): “The cat sat on the mat. The sun rises in the east.” (BERT should predict NOT_NEXT)</p> <p>This task is crucial for understanding document-level relationships, which is vital for tasks like question answering and text summarization.</p> </li> </ol> <p>Through these two self-supervised tasks, BERT develops a profound understanding of language – its grammar, its semantics, and how words and sentences connect. It becomes a general-purpose “language brain.”</p> <h3 id="the-power-of-transfer-learning-fine-tuning-bert">The Power of Transfer Learning: Fine-tuning BERT</h3> <p>The beauty of BERT is that once it’s pre-trained, you don’t have to train it again for every new task. This is where <strong>transfer learning</strong> comes in. The pre-trained BERT model has already learned a vast amount of linguistic knowledge. You can then <em>fine-tune</em> it for specific downstream tasks with relatively small, task-specific datasets.</p> <p>For example, if you want to build a sentiment analysis model, you take the pre-trained BERT, add a small output layer on top, and train it for a few epochs on your labeled sentiment data. Because BERT already understands language so well, it picks up on the specific nuances of sentiment much faster and with far less data than training a model from scratch.</p> <p>This is analogous to a human learning to ride a bicycle. Once they’ve mastered the basics (balance, steering), learning to ride a specific type of bike (mountain bike, road bike) is much quicker than learning to ride from scratch.</p> <h3 id="bert-in-action-real-world-impact">BERT in Action: Real-World Impact</h3> <p>BERT’s impact on NLP has been immense. It achieved state-of-the-art results on 11 different NLP tasks when it was released. Here are some areas where BERT (and its many successors like RoBERTa, ALBERT, ELECTRA) shine:</p> <ul> <li> <strong>Question Answering:</strong> BERT can read a passage of text and pinpoint the exact answer to a question, even if the answer isn’t explicitly stated in one sentence. Google Search uses BERT to better understand complex queries.</li> <li> <strong>Sentiment Analysis:</strong> Determining if a piece of text expresses positive, negative, or neutral sentiment.</li> <li> <strong>Text Classification:</strong> Categorizing documents (e.g., spam detection, topic classification).</li> <li> <strong>Named Entity Recognition (NER):</strong> Identifying and classifying named entities in text (e.g., person names, organizations, locations).</li> <li> <strong>Machine Translation:</strong> While BERT itself isn’t a full translation system, its contextual understanding contributes to better translation models.</li> <li> <strong>Summarization:</strong> Generating concise summaries of longer texts.</li> </ul> <h3 id="my-takeaway-the-democratization-of-advanced-nlp">My Takeaway: The Democratization of Advanced NLP</h3> <p>For me, BERT wasn’t just a technical achievement; it was a democratizing force. Before BERT, achieving state-of-the-art results in NLP often required massive datasets and expert-level knowledge to design and train highly specialized models. BERT provided a powerful, pre-trained foundation that could be adapted to many tasks with less data and less effort.</p> <p>It allowed more researchers and practitioners, including those like me, to tackle complex language problems effectively. It opened the door to a new era of NLP, where understanding context is paramount, and transfer learning is the norm.</p> <p>The journey into NLP is continuous, with new models and techniques constantly emerging. But BERT stands as a monumental landmark, reminding us that with clever architecture and strategic training, we can teach machines to genuinely comprehend the rich, complex tapestry of human language. It truly unmasked the potential of deep learning for text, and its legacy continues to shape the future of how we interact with intelligent systems.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>