<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Decoding the Magic: A Deep Dive into Large Language Models (LLMs) | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/decoding-the-magic-a-deep-dive-into-large-language/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Decoding the Magic: A Deep Dive into Large Language Models (LLMs)</h1> <p class="post-meta"> Created on April 20, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/natural-language-processing"> <i class="fa-solid fa-hashtag fa-sm"></i> Natural Language Processing</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   <a href="/blog/blog/tag/transformers"> <i class="fa-solid fa-hashtag fa-sm"></i> Transformers</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hello there, fellow explorers of data and algorithms!</p> <p>Have you ever found yourself chatting with an AI, maybe getting it to draft an email, brainstorm ideas, or even explain a complex scientific concept? It feels almost like magic, doesn’t it? As someone who’s constantly fascinated by how machines learn to understand and generate human language, I’ve spent countless hours peeling back the layers of these incredible systems, often called Large Language Models (LLMs). Today, I want to share my journey into understanding these digital behemoths, taking you from the “wow factor” right down to the fundamental math that makes them tick.</p> <h3 id="the-genesis-what-exactly-is-a-large-language-model">The Genesis: What Exactly <em>Is</em> a Large Language Model?</h3> <p>At its heart, an LLM is a type of artificial intelligence designed to understand, generate, and manipulate human language. Think of it as an incredibly sophisticated prediction machine. Given a sequence of words, its primary job is to predict the <em>next most likely word</em>. That’s it! Sounds simple, right? But scale this simple task to billions of parameters and train it on vast swathes of the internet’s text data, and suddenly, this humble prediction engine can write Shakespearean sonnets, debug Python code, or summarize an entire research paper.</p> <p>The “Large” in LLM refers to two main things:</p> <ol> <li> <strong>Large amount of data:</strong> They are trained on truly colossal datasets, often comprising trillions of words from books, articles, websites, and more.</li> <li> <strong>Large number of parameters:</strong> These models boast billions, sometimes even trillions, of adjustable weights and biases, making them incredibly complex and capable of learning intricate patterns.</li> </ol> <p>But how do they learn these patterns? This is where we need to peek under the hood at the architectural marvel that powers them: the Transformer.</p> <h3 id="the-brain-behind-the-brilliance-the-transformer-architecture">The Brain Behind the Brilliance: The Transformer Architecture</h3> <p>Before 2017, most state-of-the-art language models relied on Recurrent Neural Networks (RNNs) or Long Short-Term Memory (LSTMs). These models processed text word by word, sequentially, trying to maintain a “memory” of previous words. This was slow and struggled with long-range dependencies (i.e., understanding how a word at the beginning of a long sentence relates to a word at the end).</p> <p>Then came the “Attention Is All You Need” paper, introducing the <strong>Transformer</strong> architecture. This was a game-changer. The core innovation? The <strong>self-attention mechanism</strong>.</p> <h4 id="understanding-self-attention-the-oracle-of-context">Understanding Self-Attention: The Oracle of Context</h4> <p>Imagine you’re reading this sentence: “The bank of the river was muddy.” And then this one: “I went to the bank to deposit money.” The word “bank” means different things in each sentence. A human instantly understands this context. Traditional RNNs struggled to maintain this nuanced understanding over long sentences.</p> <p>Self-attention allows the model to weigh the importance of other words in the input sequence when processing each word. For every word it processes, it asks: “Which other words in this sentence are most relevant to understanding <em>me</em>?”</p> <p>Here’s a simplified breakdown of how it works:</p> <p>For each word in the input sequence, we generate three different vectors:</p> <ol> <li> <strong>Query (Q) vector:</strong> What am I looking for? (Like asking a question)</li> <li> <strong>Key (K) vector:</strong> What do I have to offer? (Like an answer to a question, or a label)</li> <li> <strong>Value (V) vector:</strong> What information do I carry? (The actual content)</li> </ol> <p>These vectors are obtained by multiplying the word’s embedding (its numerical representation) by three different weight matrices ($W_Q, W_K, W_V$) that the model learns during training.</p> <p>$Q = X W_Q$ $K = X W_K$ $V = X W_V$</p> <p>Where $X$ is the input embedding for a given word.</p> <p>Now, for each word’s Query vector, we calculate its “attention score” with every other word’s Key vector using a dot product. This score tells us how much attention each other word should get.</p> <p>$Attention_Scores = Q K^T$</p> <p>These scores are then scaled down by the square root of the dimension of the key vectors, $\sqrt{d_k}$, to prevent exploding gradients. Finally, a <code class="language-plaintext highlighter-rouge">softmax</code> function is applied to these scaled scores, turning them into probabilities that sum up to 1, indicating the “weight” or “importance” of each word for the current word.</p> <p>$Attention_Weights = softmax(\frac{QK^T}{\sqrt{d_k}})$</p> <p>Finally, these attention weights are multiplied by the Value vectors and summed up. This produces an output vector for each word that is a weighted average of all Value vectors, where the weights reflect the importance of each word to the current word’s context.</p> <p>$Output = Attention_Weights V$</p> <p>Combining it all, the famous self-attention formula is:</p> <p>$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$</p> <p>This might look intimidating, but it’s fundamentally about calculating how much each word should “pay attention” to every other word in the sequence to build a richer representation.</p> <h4 id="multi-head-attention-multiple-perspectives">Multi-Head Attention: Multiple Perspectives</h4> <p>To make attention even more powerful, Transformers use <strong>Multi-Head Attention</strong>. Instead of just one set of $Q, K, V$ matrices, they use several independent sets (“heads”). Each head learns to focus on different aspects of relationships between words. For example, one head might focus on grammatical dependencies, while another might focus on semantic relationships. The outputs from all heads are then concatenated and linearly transformed to produce the final attention output. This allows the model to capture diverse contextual information simultaneously.</p> <h4 id="positional-encoding-order-matters">Positional Encoding: Order Matters!</h4> <p>The self-attention mechanism, as described, is permutation-invariant – it doesn’t care about the order of words, only their relationships. But word order is crucial in language! “Dog bites man” is very different from “Man bites dog.” To solve this, Transformers add <strong>positional encodings</strong> to the input embeddings. These are unique numerical vectors added to each word embedding based on its position in the sequence. It’s like giving each word a timestamp or an address, allowing the model to understand the sequence without sacrificing the parallel processing power of attention.</p> <p>$X_{pos} = X + PE$</p> <p>Where $PE$ is the positional encoding vector.</p> <h3 id="the-large-training-process-from-zero-to-genius">The “Large” Training Process: From Zero to Genius</h3> <p>Building an LLM involves two major phases:</p> <ol> <li> <p><strong>Pre-training (The Heavy Lifting):</strong> This is where the model learns the fundamental patterns of language. It’s fed an enormous amount of raw text and trained on a self-supervised task, most commonly <strong>causal language modeling</strong>. This means the model is given a sequence of words and tasked with predicting the <em>next word</em>. For example, given “The cat sat on the…”, it should predict “mat” (or “rug,” “floor,” etc.).</p> <p>The objective function it tries to minimize is typically the <strong>cross-entropy loss</strong>. For each predicted word $\hat{y}_i$ and the actual next word $y_i$, the loss function measures how “surprised” the model is:</p> <p>$L = -\sum_{i=1}^{N} y_i \log(\hat{y}_i)$</p> <p>Where $N$ is the vocabulary size, $y_i$ is 1 for the correct word and 0 otherwise (one-hot encoding), and $\hat{y}_i$ is the predicted probability for word $i$. Minimizing this loss iteratively, using techniques like gradient descent, allows the model to learn statistical relationships between words. This phase can take months on thousands of GPUs and cost millions of dollars.</p> </li> <li> <p><strong>Fine-tuning &amp; Alignment (The Polish):</strong> A pre-trained LLM is brilliant at predicting the next word, but it might not be great at <em>following instructions</em> or being truly <em>helpful and harmless</em>. This is where fine-tuning comes in.</p> <ul> <li> <strong>Instruction Fine-tuning:</strong> The model is trained on specific datasets of instruction-response pairs (e.g., “Summarize this article:” followed by a summary). This teaches the model to understand and respond to user prompts in a conversational or task-oriented manner.</li> <li> <strong>Reinforcement Learning from Human Feedback (RLHF):</strong> This is often the secret sauce that makes models like ChatGPT feel so polished. <ol> <li> <strong>Human Preference Data:</strong> Humans rank multiple model responses to a prompt from best to worst.</li> <li> <strong>Reward Model Training:</strong> A separate “reward model” is trained to predict these human preferences. It learns to score responses based on how helpful, honest, and harmless they are.</li> <li> <strong>Policy Optimization:</strong> The LLM’s parameters are then fine-tuned using a reinforcement learning algorithm (like Proximal Policy Optimization, PPO) to maximize the reward predicted by the reward model. Essentially, the LLM learns to generate responses that the reward model (which reflects human values) deems good.</li> </ol> </li> </ul> </li> </ol> <h3 id="emergent-abilities-more-than-the-sum-of-its-parts">Emergent Abilities: More Than the Sum of Its Parts</h3> <p>One of the most mind-blowing aspects of LLMs is the concept of “emergent abilities.” These are capabilities that are not explicitly programmed or obvious from the pre-training task but “emerge” spontaneously as the model scales up in size and data. Things like:</p> <ul> <li> <strong>In-context learning:</strong> Performing new tasks based on a few examples given in the prompt, without explicit fine-tuning.</li> <li> <strong>Reasoning:</strong> Solving math problems, logical puzzles, or generating code.</li> <li> <strong>Multilingualism:</strong> Translating between languages.</li> <li> <strong>Commonsense reasoning:</strong> Understanding implied meanings or real-world knowledge.</li> </ul> <p>It’s like reaching a critical mass where the complexity of the learned patterns allows for qualitatively new behaviors to appear. This is an active area of research and truly fascinating!</p> <h3 id="challenges-and-the-road-ahead">Challenges and the Road Ahead</h3> <p>Despite their incredible capabilities, LLMs are not without their limitations and challenges:</p> <ul> <li> <strong>Hallucinations:</strong> They can confidently generate factually incorrect information. Because they are prediction machines, they prioritize generating plausible-sounding text over factual accuracy.</li> <li> <strong>Bias:</strong> LLMs learn from the internet, which unfortunately contains human biases. These biases can be reflected in the model’s outputs, leading to unfair or harmful responses.</li> <li> <strong>Computational Cost:</strong> Training and running these models requires immense computational resources, making them expensive and energy-intensive.</li> <li> <strong>Lack of True Understanding:</strong> While LLMs can process and generate language proficiently, they don’t possess genuine understanding, consciousness, or common sense in the way humans do. They operate based on statistical patterns.</li> <li> <strong>Ethical Concerns:</strong> Issues like misuse, job displacement, and copyright infringement are ongoing debates.</li> </ul> <h3 id="my-personal-take--the-future">My Personal Take &amp; The Future</h3> <p>Working with LLMs feels like standing at the cusp of a new era. What started as curiosity about “next word prediction” has blossomed into exploring systems that can genuinely augment human creativity and productivity. The journey from understanding $QK^T$ to appreciating emergent abilities has been thrilling.</p> <p>Looking ahead, I believe we’ll see:</p> <ul> <li> <strong>More efficient models:</strong> Research into making LLMs smaller, faster, and less resource-intensive.</li> <li> <strong>Enhanced control and alignment:</strong> Better techniques to ensure models are helpful, harmless, and adhere to ethical guidelines.</li> <li> <strong>Multimodality:</strong> LLMs extending beyond text to understand and generate images, audio, and video.</li> <li> <strong>Specialized applications:</strong> Highly tuned LLMs for specific industries like medicine, law, or scientific research.</li> </ul> <p>The field is moving at lightning speed, and for anyone passionate about data science and machine learning, LLMs represent an incredibly rich and rewarding area of study. So, dive in, experiment, and don’t be afraid to ask “how does that work?” – because often, the most magical things have the most elegant explanations hidden beneath the surface.</p> <p>Thanks for joining me on this deep dive! Keep learning, keep building, and keep questioning.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>