<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Art of Learning by Doing: Demystifying Reinforcement Learning | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/2025/the-art-of-learning-by-doing-demystifying-reinforc/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Art of Learning by Doing: Demystifying Reinforcement Learning</h1> <p class="post-meta"> Created on May 22, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/reinforcement-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Reinforcement Learning</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/artificial-intelligence"> <i class="fa-solid fa-hashtag fa-sm"></i> Artificial Intelligence</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone! Today, I want to share something truly fascinating from the world of AI that has always captivated me: <strong>Reinforcement Learning (RL)</strong>. If you’ve ever marvelled at AlphaGo beating the world’s best Go players or dreamed of intelligent robots learning to perform complex tasks, then you’ve witnessed the power of RL in action. It’s a field that feels less like traditional programming and more like teaching an agent to discover optimal behaviour on its own, through pure experience.</p> <h3 id="what-is-reinforcement-learning-a-story-of-trial-and-error">What is Reinforcement Learning? A Story of Trial and Error</h3> <p>Think back to when you learned to ride a bike. No one gave you a perfect set of instructions for every pedal stroke, every turn, or every wobble. Instead, you tried, you fell (a ‘negative reward’), you adjusted, and eventually, you got the hang of it (a ‘positive reward’). You learned through direct interaction with your environment, guided by feedback.</p> <p>This is the core idea behind Reinforcement Learning. Unlike supervised learning, where models learn from labelled examples, or unsupervised learning, where they find patterns in unlabelled data, RL focuses on an <strong>agent</strong> learning to make sequences of decisions by interacting with an <strong>environment</strong> to achieve a specific <strong>goal</strong>. There’s no teacher providing the “right” answer for every situation; instead, the agent receives <strong>rewards</strong> (or penalties) for its actions, guiding it towards better strategies over time.</p> <p>It’s about learning the <em>policy</em> – a map from states to actions – that maximizes a numerical reward signal.</p> <h3 id="the-cast-of-characters-agent-environment-state-action-reward">The Cast of Characters: Agent, Environment, State, Action, Reward</h3> <p>To truly grasp RL, let’s meet the key players:</p> <ol> <li> <strong>The Agent:</strong> This is our learner or decision-maker. It’s the AI program we’re trying to train. In our bike analogy, you are the agent.</li> <li> <strong>The Environment:</strong> This is everything outside the agent that it interacts with. It could be a game board, a physical world for a robot, or even a stock market. For the cyclist, it’s the bike, the road, gravity, etc.</li> <li> <strong>State ($S$):</strong> At any given moment, the environment is in a particular state. This describes the current situation relevant to the agent. For a chess AI, the state is the current arrangement of pieces on the board. For our cyclist, it might be their speed, balance, and direction.</li> <li> <strong>Action ($A$):</strong> These are the choices the agent can make. In chess, moving a piece. For the cyclist, it’s steering, pedalling, braking.</li> <li> <strong>Reward ($R$):</strong> This is the feedback the environment gives the agent after an action. It’s a numerical signal that tells the agent how good or bad its last action was in achieving the goal. A positive reward encourages the action; a negative reward discourages it. Falling off the bike is a negative reward; staying upright and moving forward is a positive one.</li> </ol> <p>The cycle is continuous: the agent observes the current state, takes an action, the environment transitions to a new state, and the agent receives a reward. This loop forms the basis of all RL algorithms.</p> <h3 id="the-ultimate-goal-maximize-cumulative-reward">The Ultimate Goal: Maximize Cumulative Reward</h3> <p>While immediate rewards are important, an intelligent agent doesn’t just care about the next step; it cares about the <em>long-term outcome</em>. Imagine a chess player who only focuses on capturing a pawn (an immediate positive reward) but overlooks a checkmate opportunity for their opponent (a huge negative long-term outcome).</p> <p>Therefore, the agent’s objective is to <strong>maximize the total cumulative reward</strong> it receives over the long run. This is often represented by a discounted sum of future rewards:</p> <p>$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$</p> <p>Here:</p> <ul> <li>$G_t$ is the total discounted reward from time $t$.</li> <li>$R_{t+1}$ is the reward received at step $t+1$.</li> <li>$\gamma$ (gamma) is the <strong>discount factor</strong>, a value between 0 and 1. It determines the importance of future rewards. A $\gamma$ close to 0 means the agent is short-sighted, caring mostly about immediate rewards. A $\gamma$ close to 1 means it’s far-sighted, considering future rewards almost as important as immediate ones.</li> </ul> <h3 id="the-brains-of-the-operation-value-functions-and-policies">The Brains of the Operation: Value Functions and Policies</h3> <p>How does an agent figure out what to do to maximize this cumulative reward? This is where <strong>value functions</strong> and <strong>policies</strong> come in.</p> <ul> <li> <table> <tbody> <tr> <td> <strong>Policy ($\pi$):</strong> This is the agent’s strategy or “brain.” It maps observed states to actions. Essentially, it tells the agent, “If you are in <em>this</em> state, take <em>this</em> action.” A good policy is what we want our agent to learn. We often write it as $\pi(a</td> <td>s)$, the probability of taking action $a$ when in state $s$.</td> </tr> </tbody> </table> </li> <li> <strong>Value Function:</strong> A value function estimates “how good” a particular state or a particular action taken from a state is. It predicts the expected cumulative reward an agent can expect starting from that state (or taking that action from that state) and then following a certain policy. There are two main types: <ul> <li> <strong>State-Value Function ($V^\pi(s)$):</strong> This tells us the expected return (cumulative reward) if the agent starts in state $s$ and follows policy $\pi$ thereafter.</li> <li> <strong>Action-Value Function ($Q^\pi(s, a)$):</strong> This tells us the expected return if the agent starts in state $s$, takes action $a$, and then follows policy $\pi$ thereafter. This $Q$-value is often more useful because it directly helps the agent choose the best action: it can simply pick the action with the highest $Q$-value from its current state.</li> </ul> </li> </ul> <p>The ultimate goal of many RL algorithms is to find an <em>optimal policy</em> ($\pi^<em>$) and its corresponding *optimal value functions</em> ($V^<em>(s)$ and $Q^</em>(s, a)$) that achieve the maximum possible cumulative reward.</p> <h3 id="learning-the-strategy-q-learning">Learning the Strategy: Q-Learning</h3> <p>One of the most foundational and intuitive algorithms in RL is <strong>Q-Learning</strong>. It’s a “model-free” algorithm, meaning the agent doesn’t need to understand the environment’s internal mechanics (how states transition or rewards are given). It learns purely from experience. It’s also “off-policy,” meaning it can learn the value of an optimal policy while still exploring different actions.</p> <p>Q-Learning iteratively updates the $Q$-value for a given state-action pair based on the reward received and the estimated future rewards from the <em>next</em> state. The core update rule, derived from the <strong>Bellman Equation</strong>, looks like this:</p> <p>$Q(s, a) \leftarrow Q(s, a) + \alpha [R + \gamma \max_{a’} Q(s’, a’) - Q(s, a)]$</p> <p>Let’s break down this powerful equation:</p> <ul> <li>$Q(s, a)$: The current estimate of the $Q$-value for taking action $a$ in state $s$.</li> <li>$\alpha$ (alpha): The <strong>learning rate</strong> (between 0 and 1). It determines how much the new information overrides the old information. A high $\alpha$ means the agent learns quickly but might be unstable; a low $\alpha$ means slower but more stable learning.</li> <li>$R$: The immediate reward received after taking action $a$ from state $s$.</li> <li>$\gamma$: The <strong>discount factor</strong> we discussed earlier.</li> <li>$\max_{a’} Q(s’, a’)$: This is the crucial “future value” component. It represents the maximum expected future reward from the <em>new state</em> ($s’$) by taking the best possible action ($a’$) from there. This is how the agent looks ahead and learns to plan.</li> <li>$[R + \gamma \max_{a’} Q(s’, a’) - Q(s, a)]$: This entire bracketed term is the <strong>temporal difference (TD) error</strong>. It’s the difference between the <em>newly estimated</em> value (based on the immediate reward and the best future reward) and the <em>current estimate</em> of $Q(s,a)$. The agent adjusts its $Q(s,a)$ towards this new, more informed value.</li> </ul> <p>Over many iterations of exploring the environment and applying this update rule, the $Q$-values in a “Q-table” (a table mapping every state-action pair to its $Q$-value) converge to the optimal $Q$-values, guiding the agent to the optimal policy.</p> <h3 id="from-tables-to-neural-networks-deep-q-networks-dqn">From Tables to Neural Networks: Deep Q-Networks (DQN)</h3> <p>You might be thinking, “What if the number of states and actions is enormous, like in a complex video game or real-world robotics?” A simple Q-table would become impossibly large! This is where <strong>Deep Reinforcement Learning</strong> comes into play, combining RL with the power of deep neural networks.</p> <p><strong>Deep Q-Networks (DQN)</strong>, pioneered by Google DeepMind, use a neural network to <em>approximate</em> the $Q$-function, instead of storing it in a table. The state ($s$) is fed as input to the neural network, and the output layer produces the $Q$-values for all possible actions ($a$) in that state.</p> <p>This allows RL agents to tackle problems with incredibly vast state spaces, such as playing Atari games directly from pixel data. The network learns to extract relevant features from the raw input and estimate the optimal $Q$-values, making the agent truly scalable.</p> <h3 id="the-balancing-act-exploration-vs-exploitation">The Balancing Act: Exploration vs. Exploitation</h3> <p>One of the biggest challenges in RL is the <strong>exploration-exploitation dilemma</strong>.</p> <ul> <li> <strong>Exploitation:</strong> The agent uses its current knowledge (its learned $Q$-values) to choose the action it believes will yield the highest reward. It’s “exploiting” what it knows.</li> <li> <strong>Exploration:</strong> The agent tries new, unfamiliar actions that might lead to even greater rewards in the long run, even if they seem suboptimal now. It’s “exploring” the unknown.</li> </ul> <p>If an agent only exploits, it might get stuck in a locally optimal solution, never discovering better paths. If it only explores, it might never consolidate its learning. A common strategy to balance this is the <strong>$\epsilon$-greedy policy</strong>:</p> <ul> <li>With probability $\epsilon$ (epsilon), the agent chooses a random action (exploration).</li> <li>With probability $1 - \epsilon$, the agent chooses the action with the highest $Q$-value (exploitation).</li> </ul> <p>Typically, $\epsilon$ starts high and slowly decays over time, encouraging exploration early on and exploitation as the agent gains more knowledge.</p> <h3 id="real-world-magic-applications-of-rl">Real-World Magic: Applications of RL</h3> <p>Reinforcement Learning is not just an academic curiosity; it’s powering breakthroughs across various domains:</p> <ul> <li> <strong>Gaming:</strong> From AlphaGo to achieving superhuman performance in complex video games (Atari, StarCraft II, Dota 2).</li> <li> <strong>Robotics:</strong> Teaching robots to grasp objects, walk, or perform intricate tasks in unstructured environments.</li> <li> <strong>Autonomous Driving:</strong> Training self-driving cars to navigate traffic, make decisions, and avoid hazards.</li> <li> <strong>Finance:</strong> Optimizing trading strategies and portfolio management.</li> <li> <strong>Healthcare:</strong> Developing personalized treatment plans or drug discovery.</li> <li> <strong>Recommendation Systems:</strong> Personalizing content or product recommendations.</li> </ul> <h3 id="wrapping-up">Wrapping Up</h3> <p>Reinforcement Learning is a paradigm shift in how we approach AI, moving from explicitly programmed rules to agents that learn dynamically from experience. It mimics how humans and animals learn – through interaction, trial, error, and feedback.</p> <p>The journey from simple Q-tables to sophisticated Deep Q-Networks and beyond is a testament to the power of combining fundamental learning principles with modern deep learning architectures. While challenges like sparse rewards, sample efficiency, and safety remain, the field is rapidly advancing, promising an exciting future where AI agents can learn to solve increasingly complex problems with minimal human intervention.</p> <p>If this peek into RL has sparked your curiosity, I highly encourage you to dive deeper! Try implementing a simple Q-learning agent for a grid world problem, or explore the fascinating world of open-source RL frameworks like OpenAI Gym and Stable Baselines. The best way to understand this “learning by doing” paradigm is to do some learning yourself!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>