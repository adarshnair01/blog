<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Your Next Obsession: Unpacking the Magic of Recommender Systems | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/your-next-obsession-unpacking-the-magic-of-recomme/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Your Next Obsession: Unpacking the Magic of Recommender Systems</h1> <p class="post-meta"> Created on March 09, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/recommender-systems"> <i class="fa-solid fa-hashtag fa-sm"></i> Recommender Systems</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   <a href="/blog/blog/tag/portfolio"> <i class="fa-solid fa-hashtag fa-sm"></i> Portfolio</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>As a data science and machine learning enthusiast, one of the fields that consistently sparks my curiosity and admiration is the realm of Recommender Systems. Think about it: from the clothes you’re shown on an e-commerce site to the news articles popping up in your feed, these systems are quietly, yet profoundly, shaping our digital experiences. They are the unseen forces guiding our choices, often making life easier, richer, and sometimes, a little too tailored.</p> <p>In this deep dive, I want to take you on a journey through the fundamental ideas behind recommender systems. We’ll explore how they work, the different flavors they come in, and some of the cool challenges data scientists like us grapple with to make them smarter. Whether you’re a high school student just beginning to peek into the world of AI or a fellow tech enthusiast, I hope to make this topic both accessible and thought-provoking.</p> <h3 id="the-deluge-of-choice-why-we-need-recommendations">The Deluge of Choice: Why We Need Recommendations</h3> <p>Imagine walking into the world’s largest library, containing every book ever written, or browsing a music store with millions of songs. Exciting, right? But also incredibly overwhelming! This “information overload” is the very problem recommender systems were designed to solve.</p> <p>In our digital age, the sheer volume of available content – movies, music, products, news – is staggering. Without guidance, finding something you genuinely love becomes like searching for a needle in a haystack. Recommender systems act as our personalized digital sherpas, guiding us through this vast wilderness of options to discover content and products we’re most likely to enjoy, engage with, or purchase. They don’t just help us; they help platforms by boosting engagement, increasing sales, and enhancing user satisfaction. It’s a win-win!</p> <h3 id="the-two-pillars-content-based-vs-collaborative-filtering">The Two Pillars: Content-Based vs. Collaborative Filtering</h3> <p>At their core, most recommender systems fall into two main categories: <strong>Content-Based Filtering</strong> and <strong>Collaborative Filtering</strong>. While hybrid models often combine the best of both worlds, understanding these two foundational approaches is key.</p> <h4 id="1-content-based-filtering-you-liked-that-heres-more-of-that">1. Content-Based Filtering: “You liked that? Here’s more of that!”</h4> <p>This type of recommender system is probably the most intuitive. It works on the principle that if you liked something in the past, you’ll probably like similar things in the future. Think about it like a personal shopper who knows your taste inside out.</p> <p><strong>How it Works:</strong></p> <ol> <li> <strong>Item Features:</strong> Every item (a movie, a song, a book) is described by its characteristics or “features.” For a movie, these might be its genre (sci-fi, comedy), actors, director, keywords, release year, etc.</li> <li> <strong>User Profile:</strong> The system builds a “profile” of your preferences based on the features of items you’ve previously interacted with (liked, watched, purchased, rated highly). If you’ve watched many sci-fi thrillers starring Tom Cruise, your profile will reflect a strong preference for those features.</li> <li> <strong>Similarity:</strong> When it’s time to recommend, the system compares your user profile with the features of unrated items. It then suggests items that are most similar to your established preferences.</li> </ol> <p>Let’s illustrate with an example. Suppose you love the movie “Interstellar.” A content-based system would look at “Interstellar’s” features (sci-fi, space exploration, Christopher Nolan, Matthew McConaughey) and then recommend other movies that share many of these features, like “Inception” (also Nolan, complex plot) or “Arrival” (sci-fi, thought-provoking).</p> <p><strong>The Math Behind the Magic (Similarity):</strong></p> <p>To quantify “similarity,” we often represent items and user profiles as vectors in a multi-dimensional space. One common metric is <strong>Cosine Similarity</strong>. It measures the cosine of the angle between two vectors. If the vectors point in roughly the same direction (meaning the items are very similar), the cosine similarity will be close to 1. If they’re orthogonal (no similarity), it’s 0.</p> <p>For two vectors, A and B, representing an item or user profile:</p> <table> <tbody> <tr> <td>$cos(\theta) = \frac{A \cdot B}{</td> <td> </td> <td>A</td> <td> </td> <td>\cdot</td> <td> </td> <td>B</td> <td> </td> <td>} = \frac{\sum_{i=1}^{n} A_i B_i}{\sqrt{\sum_{i=1}^{n} A_i^2} \sqrt{\sum_{i=1}^{n} B_i^2}}$</td> </tr> </tbody> </table> <p>Where:</p> <ul> <li>$A \cdot B$ is the dot product of the vectors.</li> <li> <table> <tbody> <tr> <td>$</td> <td> </td> <td>A</td> <td> </td> <td>$ and $</td> <td> </td> <td>B</td> <td> </td> <td>$ are the magnitudes (lengths) of the vectors.</td> </tr> </tbody> </table> </li> </ul> <p><strong>Pros:</strong></p> <ul> <li> <strong>No Cold-Start for Items:</strong> Can recommend new items as long as they have features.</li> <li> <strong>Transparency:</strong> Easier to explain <em>why</em> an item was recommended.</li> <li> <strong>Personalization:</strong> Very good at recommending items closely aligned with a user’s known tastes.</li> </ul> <p><strong>Cons:</strong></p> <ul> <li> <strong>Limited Serendipity:</strong> Tends to recommend items very similar to what you already like, leading to an “echo chamber” effect. You might miss out on genuinely new things.</li> <li> <strong>Cold-Start for New Users:</strong> If a new user hasn’t interacted with enough items, the system can’t build an accurate profile.</li> <li> <strong>Feature Engineering:</strong> Requires detailed, structured data about items, which can be hard to get or define.</li> </ul> <h4 id="2-collaborative-filtering-people-like-you-liked-this-so-you-might-too">2. Collaborative Filtering: “People like you liked this, so you might too!”</h4> <p>Collaborative filtering takes a different, more social approach. Instead of relying on item features, it leverages the collective wisdom of the crowd. It assumes that if two users have similar tastes in the past, they will likely have similar tastes in the future.</p> <p><strong>How it Works:</strong></p> <p>Imagine a large table (a user-item interaction matrix) where rows are users, columns are items, and the cells contain ratings or interactions (e.g., 1 if watched, 0 if not).</p> <ol> <li> <strong>Find Similar Users (User-Based Collaborative Filtering):</strong> <ul> <li>The system identifies users who have rated or interacted with items similarly to you. For example, if you and User B both gave high ratings to “Dune” and “Blade Runner 2049,” you’re considered similar.</li> <li>Once similar users are found, the system recommends items that those similar users liked but you haven’t yet seen.</li> <li> <strong>Analogy:</strong> You ask your friend, “Hey, we like all the same movies! What have you watched recently that I might enjoy?”</li> </ul> </li> <li> <strong>Find Similar Items (Item-Based Collaborative Filtering):</strong> <ul> <li>This approach looks at items that are liked by similar users. If users who liked Movie X also tended to like Movie Y, then Movie Y is considered similar to Movie X.</li> <li>When you’ve watched Movie X, the system then recommends Movie Y.</li> <li> <strong>Analogy:</strong> You liked “The Martian.” The system notices that people who liked “The Martian” also tended to like “Gravity.” So, it recommends “Gravity” to you.</li> <li>This is often preferred in practice due to better scalability, as item similarity tends to be more stable than user similarity over time.</li> </ul> </li> </ol> <p><strong>The Math Behind the Magic (Similarity Again!):</strong></p> <p>Similar to content-based filtering, collaborative filtering also relies heavily on similarity measures. However, instead of comparing item features, we’re comparing user interaction patterns or item interaction patterns. <strong>Cosine Similarity</strong> is still popular, but <strong>Pearson Correlation</strong> is also frequently used, especially when dealing with explicit ratings (e.g., 1-5 stars) as it accounts for users’ different rating scales (some users might be generous raters, others very strict).</p> <p>Pearson Correlation for two users, u and v, across items they both rated:</p> <p>$P_{u,v} = \frac{\sum_{i \in I_{uv}} (R_{u,i} - \bar{R_u})(R_{v,i} - \bar{R_v})}{\sqrt{\sum_{i \in I_{uv}} (R_{u,i} - \bar{R_u})^2} \sqrt{\sum_{i \in I_{uv}} (R_{v,i} - \bar{R_v})^2}}$</p> <p>Where:</p> <ul> <li>$R_{u,i}$ is the rating user <em>u</em> gave to item <em>i</em>.</li> <li>$\bar{R_u}$ is the average rating given by user <em>u</em>.</li> <li>$I_{uv}$ is the set of items both user <em>u</em> and user <em>v</em> have rated.</li> </ul> <p><strong>Pros:</strong></p> <ul> <li> <strong>Serendipity:</strong> Can recommend items that are completely different from what a user has liked before, but which other similar users enjoyed. This can introduce users to new genres or artists.</li> <li> <strong>No Feature Engineering:</strong> Doesn’t require explicit item features; it learns patterns solely from user interactions.</li> <li> <strong>Handles Complexities:</strong> Can capture nuanced similarities that are hard to describe with explicit features.</li> </ul> <p><strong>Cons:</strong></p> <ul> <li> <strong>Cold-Start Problem (New Users/Items):</strong> New users have no interaction history, so the system can’t find similar users. New items have no interactions, so they can’t be recommended. This is a significant challenge.</li> <li> <strong>Sparsity:</strong> User-item matrices are often very sparse (most users have only interacted with a tiny fraction of all items), making it hard to find enough common interactions for accurate similarity calculations.</li> <li> <strong>Scalability:</strong> With millions of users and items, computing all pairwise similarities can be computationally intensive. Matrix factorization techniques (like Singular Value Decomposition, SVD) are often used to address this, by compressing the user-item matrix into a lower-dimensional representation.</li> </ul> <h3 id="the-best-of-both-worlds-hybrid-recommender-systems">The Best of Both Worlds: Hybrid Recommender Systems</h3> <p>Given the strengths and weaknesses of both content-based and collaborative filtering, it’s no surprise that many real-world systems, especially those at large companies like Netflix and Amazon, use <strong>Hybrid Recommender Systems</strong>. These models cleverly combine elements from both approaches to mitigate their individual shortcomings and boost overall recommendation quality.</p> <p><strong>Common Hybrid Strategies:</strong></p> <ol> <li> <strong>Weighted Hybrid:</strong> Combine scores from separate content-based and collaborative filtering models using a weighted sum.</li> <li> <strong>Feature Combination:</strong> Integrate content-based features directly into a collaborative filtering model (e.g., add movie genres as features for users in a matrix factorization model).</li> <li> <strong>Switching Hybrid:</strong> Switch between content-based and collaborative methods depending on the situation (e.g., use content-based for new users/items, then switch to collaborative once enough data is available).</li> <li> <strong>Ensemble Hybrid:</strong> Run multiple recommendation models independently and then combine their predictions.</li> </ol> <p>Hybrids often perform better because they can overcome cold-start issues, provide better serendipity than pure content-based models, and leverage more information for improved accuracy.</p> <h3 id="navigating-the-rough-seas-key-challenges">Navigating the Rough Seas: Key Challenges</h3> <p>Building effective recommender systems is far from trivial. Here are some of the fascinating challenges we face:</p> <ul> <li> <strong>Cold Start Problem:</strong> As mentioned, how do you recommend to a brand new user with no history, or a brand new item with no interactions? Strategies include recommending popular items, using demographic data, or relying on content features for new items.</li> <li> <strong>Sparsity:</strong> Most users only interact with a tiny fraction of available items. This leads to very sparse data, making it hard to find meaningful patterns. Matrix factorization methods (like SVD or Alternating Least Squares) are powerful tools to tackle this.</li> <li> <strong>Scalability:</strong> When dealing with millions of users and items, real-time recommendation generation requires highly optimized algorithms and distributed computing.</li> <li> <strong>Diversity and Serendipity:</strong> A system that only recommends items identical to your past preferences isn’t very exciting. We want recommendations that are relevant but also introduce us to new things. Balancing relevance with novelty and diversity is a constant challenge.</li> <li> <strong>Explainability:</strong> Can the system tell us <em>why</em> it recommended a particular item? This builds trust and helps users understand the recommendations.</li> <li> <strong>Shilling Attacks:</strong> Malicious users or competitors might try to manipulate the system by giving fake ratings to promote or demote certain items. Robust systems need ways to detect and mitigate these attacks.</li> <li> <strong>Bias:</strong> Recommender systems can inadvertently perpetuate or amplify existing biases present in the training data, leading to unfair or unrepresentative recommendations. Ensuring fairness and ethical AI is crucial.</li> </ul> <h3 id="the-impact-where-do-we-see-them">The Impact: Where Do We See Them?</h3> <p>Recommender systems are everywhere:</p> <ul> <li> <strong>E-commerce (Amazon, eBay):</strong> “Customers who bought this also bought…”</li> <li> <strong>Media Streaming (Netflix, Hulu):</strong> Personalized movie and TV show suggestions.</li> <li> <strong>Music Streaming (Spotify, Apple Music):</strong> Curated playlists and artist discovery.</li> <li> <strong>Social Media (Facebook, TikTok):</strong> Friend suggestions, content feed optimization.</li> <li> <strong>News Aggregators (Google News):</strong> Personalized news feeds.</li> <li> <strong>Job Boards (LinkedIn):</strong> Job recommendations.</li> </ul> <p>They are integral to how we discover and consume information in the digital age, making our online experiences more tailored and efficient.</p> <h3 id="my-thoughts-and-the-road-ahead">My Thoughts and the Road Ahead</h3> <p>Exploring recommender systems has been an incredibly rewarding part of my data science journey. It’s a field that perfectly blends theoretical machine learning concepts with tangible, real-world impact. The challenge of building systems that are not only accurate but also diverse, fair, and scalable is what truly excites me.</p> <p>As AI continues to evolve, recommender systems will only become more sophisticated, perhaps integrating even more advanced deep learning techniques, natural language processing for better understanding of content, and reinforcement learning to adapt in real-time to user feedback. The ethical implications, like preventing filter bubbles and ensuring data privacy, will also remain at the forefront of research and development.</p> <p>So, the next time you’re presented with a surprisingly perfect movie suggestion or a new song that instantly becomes a favorite, take a moment to appreciate the intricate algorithms working behind the scenes. They’re a testament to the power of data, mathematics, and machine learning to make our digital lives just a little bit more magical. And perhaps, they’ll inspire you to embark on your own journey into this captivating corner of AI!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>