<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Is Your Model Really That Good? Unveiling the Truth with Cross-Validation! | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/is-your-model-really-that-good-unveiling-the-truth/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Is Your Model Really That Good? Unveiling the Truth with Cross-Validation!</h1> <p class="post-meta"> Created on January 26, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/cross-validation"> <i class="fa-solid fa-hashtag fa-sm"></i> Cross-Validation</a>   <a href="/blog/blog/tag/model-evaluation"> <i class="fa-solid fa-hashtag fa-sm"></i> Model Evaluation</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/overfitting"> <i class="fa-solid fa-hashtag fa-sm"></i> Overfitting</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>If you’re anything like me, the thrill of building your first machine learning model is absolutely electrifying. You gather your data, choose an algorithm, train it, and then… <em>BAM!</em> A performance score! Maybe it’s 95% accuracy, or a super low error rate. You feel like a data wizard, ready to change the world.</p> <p>But then a nagging thought creeps in: Is it <em>really</em> that good? What if your model just got lucky? What if it’s like a student who aced a test because they <em>memorized</em> the answers, not because they <em>understood</em> the concepts? That’s where Cross-Validation comes in – it’s the trustworthy friend your machine learning models desperately need, a robust technique that helps us build models we can truly rely on.</p> <h3 id="the-elephant-in-the-room-overfitting-and-generalization">The Elephant in the Room: Overfitting and Generalization</h3> <p>Before we dive into Cross-Validation, let’s talk about the biggest challenge in machine learning: <strong>overfitting</strong>.</p> <p>Imagine you’re studying for a big exam. You have a textbook full of practice problems. If you just memorize the answers to <em>every single problem</em> in the textbook, you might ace any test that uses <em>those exact problems</em>. But if the actual exam has <em>new</em> problems, even if they cover the same concepts, you’d likely struggle because you didn’t truly <em>understand</em> the underlying material. You <em>overfit</em> to the practice problems.</p> <p>In machine learning, overfitting happens when your model learns the training data <em>too well</em>. It starts to memorize the noise and specific patterns in the training data, rather than learning the general underlying relationships. The result? Fantastic performance on the data it was trained on, but dismal performance on any new, unseen data.</p> <p>Our goal isn’t just to do well on the data we have; it’s to build a model that <strong>generalizes</strong> well. Generalization is the ability of a model to perform accurately on new, previously unseen data. It’s the ultimate measure of a model’s real-world usefulness.</p> <h3 id="our-first-step-the-train-test-split-a-good-start-but-not-enough">Our First Step: The Train-Test Split (A Good Start, But Not Enough)</h3> <p>To combat overfitting and get a realistic sense of generalization, the first technique we learn is the <strong>train-test split</strong>. The idea is simple:</p> <ol> <li>Take your entire dataset.</li> <li>Split it into two parts: a <strong>training set</strong> ($D_{train}$) and a <strong>test set</strong> ($D_{test}$). A common split is 70-80% for training and 20-30% for testing.</li> <li>You train your model <em>only</em> on the training set.</li> <li>Once the model is trained, you evaluate its performance <em>only</em> on the test set.</li> </ol> <p>This is a crucial step! By keeping the test set completely separate during training, we ensure that our model hasn’t “peaked” at the answers. The score on the test set gives us an estimate of how well our model might perform on new, unseen data.</p> <p>However, the train-test split has a subtle but significant limitation: <strong>the split is random!</strong> What if, purely by chance, our random split ends up with an “easy” test set? Or an “unrepresentative” one? Our single performance score might be misleading. One lucky split could make a mediocre model look amazing, or an unlucky split could make a good model look bad. We need a more robust way to estimate performance.</p> <h3 id="enter-cross-validation-the-robust-reality-check">Enter Cross-Validation: The Robust Reality Check</h3> <p>This is where Cross-Validation shines! Instead of just one random train-test split, Cross-Validation performs <strong>multiple splits and multiple evaluations</strong>, giving us a much more reliable and robust estimate of our model’s performance and its ability to generalize.</p> <p>Think back to our exam analogy. Instead of just one practice test, imagine you have a large pool of practice questions. With cross-validation, you’d:</p> <ol> <li>Take one set of questions for your first “practice test.”</li> <li>Study using all the <em>other</em> questions.</li> <li>Take your “practice test” and note your score.</li> <li>Then, pick a <em>different</em> set of questions for your second “practice test.”</li> <li>Study using all the <em>remaining</em> questions (which would be different from the previous study set).</li> <li>Take your second “practice test” and note your score.</li> <li>Repeat this process multiple times.</li> <li>Finally, you’d average all your practice test scores to get a much more reliable estimate of how well you truly understand the material.</li> </ol> <p>That, in a nutshell, is Cross-Validation!</p> <h4 id="the-star-of-the-show-k-fold-cross-validation">The Star of the Show: K-Fold Cross-Validation</h4> <p>The most popular and widely used form of cross-validation is <strong>K-Fold Cross-Validation</strong>. Here’s how it works:</p> <ol> <li> <p><strong>Divide into K Folds:</strong> Your entire dataset is randomly divided into $K$ equally sized “folds” or subsets. Let’s say $K=5$. Your data is split into 5 chunks.</p> </li> <li> <strong>Iterate K Times:</strong> You then perform $K$ rounds of training and testing: <ul> <li> <strong>Round 1:</strong> You use the <strong>first fold as your validation (test) set</strong>, and the remaining $K-1$ folds (folds 2, 3, 4, and 5) are combined to form your <strong>training set</strong>. You train your model on this training set and evaluate it on the first fold. You get a performance score ($S_1$).</li> <li> <strong>Round 2:</strong> You use the <strong>second fold as your validation set</strong>, and the remaining $K-1$ folds (folds 1, 3, 4, and 5) as your training set. Train and evaluate. Get score ($S_2$).</li> <li>…and so on, until…</li> <li> <strong>Round K:</strong> You use the <strong>K-th (fifth) fold as your validation set</strong>, and the remaining $K-1$ folds (folds 1, 2, 3, and 4) as your training set. Train and evaluate. Get score ($S_K$).</li> </ul> </li> <li> <p><strong>Average the Scores:</strong> After all $K$ rounds, you’ll have $K$ different performance scores ($S_1, S_2, …, S_K$). To get your final, robust estimate of your model’s performance, you simply calculate the average of these scores:</p> <p>$\bar{S} = \frac{1}{K} \sum_{i=1}^{K} S_i$</p> <p>You might also look at the standard deviation of these scores. A small standard deviation means your model’s performance is consistent across different folds, indicating higher reliability.</p> </li> </ol> <p><strong>Why $K$?</strong> The choice of $K$ is important. Common values are $K=5$ or $K=10$.</p> <ul> <li> <strong>Small $K$ (e.g., $K=2$):</strong> Each training fold is very large, which means the bias of the performance estimator will be low (the model is trained on a large amount of data, similar to the full dataset). However, each validation fold is also large, making the variance of the performance estimator high (the scores might fluctuate a lot depending on which fold is chosen for validation).</li> <li> <strong>Large $K$ (e.g., $K=N$, where $N$ is the number of data points, known as Leave-One-Out CV):</strong> Each training fold is small (N-1 data points), so the bias of the performance estimator might be higher (the model is trained on less data than the full dataset). However, each validation fold has only one data point, making the variance of the performance estimator lower (less fluctuation in scores). But it’s computationally very expensive.</li> </ul> <p>$K=5$ or $K=10$ often strikes a good balance between bias and variance, and computational cost.</p> <h3 id="why-k-fold-cross-validation-is-your-models-best-friend">Why K-Fold Cross-Validation is Your Model’s Best Friend</h3> <ol> <li> <strong>More Robust Performance Estimate:</strong> It significantly reduces the chances of getting a misleading performance score due to a single “lucky” or “unlucky” random split. You get a more stable and reliable measure of your model’s true generalization ability.</li> <li> <strong>Better Use of Data:</strong> Every single data point in your dataset gets to be in a test set exactly once, and it gets to be in a training set $K-1$ times. This maximizes the utility of your (often precious) data.</li> <li> <strong>Identifies Model Stability:</strong> By observing the variance (standard deviation) of the $K$ scores, you can get a sense of how stable your model’s performance is. If the scores vary wildly, your model might be unstable or highly sensitive to the specific training data.</li> <li> <strong>Crucial for Hyperparameter Tuning:</strong> Cross-validation is absolutely essential when you’re trying to find the best hyperparameters for your model (e.g., the <code class="language-plaintext highlighter-rouge">C</code> parameter in an SVM, or the number of trees in a Random Forest). Techniques like <code class="language-plaintext highlighter-rouge">GridSearchCV</code> or <code class="language-plaintext highlighter-rouge">RandomizedSearchCV</code> in Python’s Scikit-learn internally use cross-validation to find the best parameter combination that generalizes well.</li> </ol> <h3 id="other-types-of-cross-validation-briefly">Other Types of Cross-Validation (Briefly)</h3> <p>While K-Fold is the most common, here are a couple of others you might encounter:</p> <ul> <li> <strong>Leave-One-Out Cross-Validation (LOOCV):</strong> This is an extreme version of K-Fold where $K$ is equal to the total number of data points ($N$). Each time, one data point is used as the validation set, and the remaining $N-1$ points are used for training. It’s very thorough but computationally very expensive, especially for large datasets.</li> <li> <strong>Stratified K-Fold:</strong> When you have an imbalanced dataset (e.g., 95% of your data belongs to class A, and only 5% to class B), a regular K-Fold split might accidentally put all instances of class B into one fold, leading to biased evaluations. Stratified K-Fold ensures that each fold has roughly the same proportion of target classes as the original dataset, maintaining representativeness.</li> <li> <strong>Time Series Cross-Validation:</strong> For data that has a time component (like stock prices or weather forecasts), you <em>cannot</em> randomly shuffle the data. You must preserve the temporal order. Time series cross-validation usually involves training on data up to a certain point in time and validating on the subsequent time points, then progressively expanding the training window (e.g., “rolling origin” or “expanding window” methods). You can’t peek into the future!</li> </ul> <h3 id="when-to-use-cross-validation-almost-always">When to Use Cross-Validation (Almost Always!)</h3> <p>You should consider using cross-validation in nearly every machine learning project:</p> <ul> <li> <strong>When comparing different models:</strong> It provides a fairer, more reliable basis for comparison.</li> <li> <strong>During hyperparameter tuning:</strong> Essential for finding the best model configuration.</li> <li> <strong>When your dataset is small:</strong> It maximizes the use of your limited data for both training and testing.</li> <li> <strong>When you need a robust estimate of performance:</strong> To instill confidence in your model’s real-world predictions.</li> </ul> <h3 id="a-crucial-caveat-data-leakage">A Crucial Caveat: Data Leakage!</h3> <p>One of the biggest mistakes beginners (and even experienced practitioners) make with cross-validation is <strong>data leakage</strong>. This happens when information from your test set “leaks” into your training process, leading to an overly optimistic performance score.</p> <p>The most common culprit? Preprocessing steps like <strong>feature scaling (e.g., <code class="language-plaintext highlighter-rouge">StandardScaler</code>, <code class="language-plaintext highlighter-rouge">MinMaxScaler</code>)</strong> or <strong>feature selection</strong>. If you calculate scaling parameters (like the mean and standard deviation) on your <em>entire dataset</em> <em>before</em> performing cross-validation, then those parameters will have implicitly “seen” the test data.</p> <p><strong>The Golden Rule:</strong> Any data preprocessing step that learns from the data (like fitting a scaler or imputing missing values based on statistics) <strong>must be done <em>inside</em> the cross-validation loop</strong>, <em>after</em> the split into training and validation sets for each fold. This way, the preprocessing is always performed only on the current fold’s training data, preventing any information leakage from the validation set.</p> <h3 id="conclusion-build-models-you-can-trust">Conclusion: Build Models You Can Trust</h3> <p>Cross-validation isn’t just an advanced technique; it’s a fundamental pillar of responsible and robust machine learning. It moves us beyond a single, potentially misleading performance score to a more comprehensive and trustworthy understanding of how our models truly perform.</p> <p>By incorporating K-Fold cross-validation (or its variations) into your workflow, you’re not just getting a better performance estimate; you’re building models that are more likely to generalize well, models that you can deploy with confidence, and models that actually solve real-world problems.</p> <p>So, the next time you get that exciting initial accuracy score, pause for a moment. Ask yourself: Is my model really that good, or did it just get lucky? Then, go ahead and embrace cross-validation. Your models (and your stakeholders) will thank you for it!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>