<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Your Digital Genie: Unveiling the Magic and Math Behind Recommender Systems | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/your-digital-genie-unveiling-the-magic-and-math-be/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Your Digital Genie: Unveiling the Magic and Math Behind Recommender Systems</h1> <p class="post-meta"> Created on February 16, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/recommender-systems"> <i class="fa-solid fa-hashtag fa-sm"></i> Recommender Systems</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/collaborative-filtering"> <i class="fa-solid fa-hashtag fa-sm"></i> Collaborative Filtering</a>   <a href="/blog/blog/tag/content-based-filtering"> <i class="fa-solid fa-hashtag fa-sm"></i> Content-Based Filtering</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>As a data science enthusiast, there are few areas that capture my imagination quite like Recommender Systems. You interact with them hundreds of times a day, often without even realizing it. They’re the silent architects of your digital world, shaping what you see, what you buy, and what you consume. From Netflix suggesting your next binge-worthy show to Amazon nudging you towards that product you “might also like,” these systems are everywhere.</p> <p>But how do they work? Is it just magic, or is there some serious science and math behind those uncanny suggestions? Today, I want to take you on a journey to unravel the mysteries of Recommender Systems – how they’re built, the different flavors they come in, and the fascinating challenges data scientists like us face when building them.</p> <h3 id="the-problem-too-much-choice-too-little-time">The Problem: Too Much Choice, Too Little Time</h3> <p>Imagine walking into a massive library with millions of books. How do you find the one you’ll love? Or an online store with billions of products. How do you discover exactly what you need without getting overwhelmed? This is the core problem Recommender Systems try to solve: <strong>information overload</strong>.</p> <p>In an era where content and products are generated at an unprecedented rate, users need guides. They need intelligent systems that can filter through the noise and present them with items that are relevant, interesting, and valuable. And that, my friends, is where our digital genies come into play.</p> <p>A recommender system, at its heart, is an information filtering system that seeks to predict the “rating” or “preference” a user would give to an item. The goal is simple: connect users with items they’ll genuinely enjoy.</p> <p>Let’s dive into the two main categories of these systems:</p> <h3 id="1-content-based-filtering-the-if-you-like-this-youll-like-that-approach">1. Content-Based Filtering: The “If You Like This, You’ll Like That” Approach</h3> <p>Imagine you’re a big fan of sci-fi movies, particularly those with time travel and epic space battles. A content-based recommender system works a lot like a smart friend who knows your tastes. If you’ve loved <em>Interstellar</em> and <em>Arrival</em>, this system would look for other movies that share similar characteristics – say, a high sci-fi genre score, a focus on space exploration, or starring actors you’ve enjoyed before.</p> <p><strong>How it Works:</strong></p> <ol> <li> <strong>Item Profiles:</strong> Each item (e.g., movie, product, article) is described by its features. For a movie, these features could be genre (sci-fi, drama, comedy), actors, director, keywords, release year, etc. We represent these features as a vector.</li> <li> <strong>User Profiles:</strong> Your preferences are built from the items you’ve previously liked. If you’ve rated <em>Interstellar</em> highly, your user profile gains a stronger affinity for “sci-fi,” “space,” and “Matthew McConaughey.”</li> <li> <strong>Similarity Matching:</strong> The system then compares your user profile to all the available item profiles. The closer an item’s profile is to your user profile, the higher its recommendation score.</li> </ol> <p>A common way to measure this similarity is using <strong>Cosine Similarity</strong>. If we represent your user profile and an item’s profile as vectors in a multi-dimensional space, cosine similarity measures the cosine of the angle between them. A smaller angle (closer to 0) means higher similarity.</p> <p>Mathematically, for two vectors $\mathbf{A}$ and $\mathbf{B}$ (representing a user profile and an item profile):</p> \[cos(\theta) = \frac{\mathbf{A} \cdot \mathbf{B}}{||\mathbf{A}|| \cdot ||\mathbf{B}||} = \frac{\sum_{i=1}^n A_i B_i}{\sqrt{\sum_{i=1}^n A_i^2} \sqrt{\sum_{i=1}^n B_i^2}}\] <p>Where:</p> <ul> <li>$\mathbf{A} \cdot \mathbf{B}$ is the dot product of the vectors.</li> <li> <table> <tbody> <tr> <td>$</td> <td> </td> <td>\mathbf{A}</td> <td> </td> <td>$ and $</td> <td> </td> <td>\mathbf{B}</td> <td> </td> <td>$ are the magnitudes (Euclidean lengths) of the vectors.</td> </tr> </tbody> </table> </li> <li>$A_i$ and $B_i$ are the components of vectors $\mathbf{A}$ and $\mathbf{B}$ respectively.</li> </ul> <p><strong>Pros:</strong></p> <ul> <li> <strong>No “cold start” for items (if features exist):</strong> A new movie can be recommended if its features are known, even if no one has watched it yet.</li> <li> <strong>Handles niche tastes:</strong> If you have very specific preferences, the system can cater to them.</li> <li> <strong>Interpretability:</strong> It’s often easy to explain <em>why</em> an item was recommended (e.g., “because you liked other sci-fi movies”).</li> </ul> <p><strong>Cons:</strong></p> <ul> <li> <strong>Limited novelty:</strong> It mostly recommends items similar to what you already like, leading to a “filter bubble” and less serendipitous discoveries.</li> <li> <strong>Feature engineering overhead:</strong> Building good item profiles can be a lot of work.</li> <li> <strong>“Cold start” for new users:</strong> Until you’ve interacted with enough items, the system doesn’t know your preferences.</li> </ul> <h3 id="2-collaborative-filtering-the-people-like-you-also-like-this-approach">2. Collaborative Filtering: The “People Like You Also Like This” Approach</h3> <p>This is often considered the most powerful and widely used type of recommender system. Instead of focusing on item features, collaborative filtering (CF) looks at the collective behavior and preferences of users. It operates on the principle that if two users have similar tastes in the past, they will likely have similar tastes in the future.</p> <p>Think of it like this: your friend Alex has similar taste in music to you. If Alex discovers a new band you’ve never heard of and loves it, there’s a good chance you’ll love it too. CF takes this idea and scales it up to millions of users.</p> <p>There are two main sub-types of collaborative filtering:</p> <h4 id="a-user-based-collaborative-filtering-user-user-cf">A. User-Based Collaborative Filtering (User-User CF)</h4> <ol> <li> <strong>Find Similar Users:</strong> The system first identifies users who have similar rating patterns to you. It looks for “neighbors” who have rated many of the same items as you and given them similar ratings.</li> <li> <strong>Recommend Items:</strong> Once similar users are found, the system recommends items that those users liked but you haven’t yet seen or rated.</li> </ol> <p>To find similar users, we often use metrics like <strong>Pearson Correlation Coefficient</strong>. Unlike cosine similarity, Pearson correlation takes into account that different users might use different rating scales (e.g., one user might rate everything high, another might be more critical). It normalizes for these differences.</p> <p>For two users $u$ and $v$, their Pearson correlation $r_{u,v}$ is:</p> \[r_{u,v} = \frac{\sum_{i \in I_{uv}} (R_{u,i} - \bar{R_u})(R_{v,i} - \bar{R_v})}{\sqrt{\sum_{i \in I_{uv}} (R_{u,i} - \bar{R_u})^2} \sqrt{\sum_{i \in I_{uv}} (R_{v,i} - \bar{R_v})^2}}\] <p>Where:</p> <ul> <li>$I_{uv}$ is the set of items rated by both user $u$ and user $v$.</li> <li>$R_{u,i}$ is the rating of item $i$ by user $u$.</li> <li>$\bar{R_u}$ is the average rating given by user $u$.</li> </ul> <p><strong>Pros:</strong></p> <ul> <li> <strong>Serendipity:</strong> Can recommend items completely different from what a user has seen before, leading to new discoveries.</li> <li> <strong>No need for item features:</strong> Works purely on user-item interaction data.</li> <li> <strong>Handles complex items:</strong> Can recommend items where features are hard to define (e.g., abstract art, nuanced music).</li> </ul> <p><strong>Cons:</strong></p> <ul> <li> <strong>Scalability issues:</strong> Finding similar users among millions can be computationally very expensive.</li> <li> <strong>Sparsity:</strong> Most users only rate a tiny fraction of available items, making it hard to find enough common items to calculate meaningful similarities.</li> <li> <strong>Cold Start (for both users and items):</strong> New users have no ratings, so no similar users can be found. New items have no ratings, so they can’t be recommended.</li> </ul> <h4 id="b-item-based-collaborative-filtering-item-item-cf">B. Item-Based Collaborative Filtering (Item-Item CF)</h4> <p>This approach, popularized by Amazon, flips the script. Instead of finding similar <em>users</em>, it finds similar <em>items</em>.</p> <ol> <li> <strong>Find Similar Items:</strong> If you like movie A, the system looks for other movies (B, C, D) that are frequently liked by the <em>same users</em> who liked movie A.</li> <li> <strong>Recommend Items:</strong> If you’ve rated movie A highly, and movie B is very similar to movie A (based on other users’ preferences), then movie B is a good candidate for recommendation.</li> </ol> <p>Similarity here is often calculated between items (e.g., using Cosine Similarity on the columns of the user-item rating matrix). The key idea is that similar items are those that tend to be rated similarly by the same users.</p> <p><strong>Pros:</strong></p> <ul> <li> <strong>More stable recommendations:</strong> Item similarities tend to be more stable over time than user similarities (your taste changes less often than your social circle).</li> <li> <strong>Better scalability:</strong> Item similarity can often be pre-calculated offline.</li> <li><strong>Still provides serendipity.</strong></li> </ul> <p><strong>Cons:</strong></p> <ul> <li>Can be less diverse than user-based CF if items are very narrowly defined.</li> <li>Still suffers from sparsity and cold start for new items with no ratings.</li> </ul> <h4 id="c-matrix-factorization-latent-factor-models">C. Matrix Factorization (Latent Factor Models)</h4> <p>This is where things get really cool and a bit more mathematically sophisticated! Imagine there are some hidden, underlying characteristics (we call them “latent factors”) that explain why users like certain items. For example, a user might like “action,” “adventure,” and “fantasy” movies. These are latent factors.</p> <p>Matrix Factorization (MF) techniques, like <strong>Singular Value Decomposition (SVD)</strong> or methods used in algorithms like <strong>Alternating Least Squares (ALS)</strong> and <strong>Stochastic Gradient Descent (SGD) for FunkSVD</strong>, try to discover these latent factors.</p> <p><strong>How it Works (The intuition):</strong></p> <p>We have a massive, sparse User-Item interaction matrix $R$, where $R_{ui}$ is the rating user $u$ gave to item $i$. Most of this matrix is empty because users only interact with a tiny fraction of items.</p> <p>MF algorithms try to “factorize” this big sparse matrix $R$ into two smaller, dense matrices:</p> <ul> <li>A User-Factor matrix $P$ (of size $U \times K$), where $U$ is the number of users, and $K$ is the number of latent factors. Each row $p_u$ represents user $u$’s affinity for each of the $K$ latent factors.</li> <li>An Item-Factor matrix $Q$ (of size $I \times K$), where $I$ is the number of items, and $K$ is the number of latent factors. Each row $q_i$ represents item $i$’s degree to which it possesses each of the $K$ latent factors.</li> </ul> <p>By multiplying these two smaller matrices ($P \times Q^T$), we get an approximation of the original rating matrix $R’$. The magic is that $R’$ will be dense, filling in the missing ratings! The estimated rating for user $u$ on item $i$ would be $\hat{R}_{ui} = p_u^T q_i$.</p> <p>The goal is to find the matrices $P$ and $Q$ that minimize the error between the predicted ratings and the actual known ratings. This is typically done by minimizing a loss function (like Mean Squared Error), often with regularization terms to prevent overfitting:</p> \[\min_{P,Q} \sum_{(u,i) \in K} (r_{ui} - p_u^T q_i)^2 + \lambda(||P||_F^2 + ||Q||_F^2)\] <p>Where:</p> <ul> <li>$K$ is the set of known ratings.</li> <li>$r_{ui}$ is the actual rating user $u$ gave item $i$.</li> <li>$p_u^T q_i$ is the predicted rating.</li> <li>$\lambda$ is a regularization parameter to prevent overfitting.</li> <li> <table> <tbody> <tr> <td>$</td> <td> </td> <td>P</td> <td> </td> <td>_F^2$ and $</td> <td> </td> <td>Q</td> <td> </td> <td>_F^2$ are the Frobenius norms of matrices $P$ and $Q$, which penalize large factor values.</td> </tr> </tbody> </table> </li> </ul> <p><strong>Pros:</strong></p> <ul> <li> <strong>Handles sparsity very well:</strong> Can make accurate predictions even with very few known ratings.</li> <li> <strong>Discovers latent features:</strong> Can uncover hidden patterns and relationships between users and items that aren’t explicitly defined.</li> <li> <strong>Scalable:</strong> Algorithms like ALS are highly parallelizable.</li> <li>Often achieves higher accuracy than traditional CF methods.</li> </ul> <p><strong>Cons:</strong></p> <ul> <li> <strong>Interpretability:</strong> The latent factors themselves are often abstract and hard to interpret (e.g., “Factor 3” doesn’t cleanly map to “action movies”).</li> <li>Still suffers from the <strong>cold start problem</strong> for new users and new items, as they don’t have any ratings to build their factor vectors.</li> </ul> <h3 id="3-hybrid-recommender-systems-the-best-of-both-worlds">3. Hybrid Recommender Systems: The Best of Both Worlds</h3> <p>You might have noticed that both Content-Based and Collaborative Filtering have their strengths and weaknesses. So, what’s a data scientist to do? Combine them!</p> <p>Hybrid systems leverage the benefits of multiple approaches to overcome individual limitations. For instance, a hybrid system might:</p> <ul> <li>Use content-based filtering to make initial recommendations for new users (solving the cold start problem for users).</li> <li>Then use collaborative filtering once enough user interaction data is collected.</li> <li>Combine features from both content and collaborative models.</li> </ul> <p>Netflix famously uses a highly sophisticated hybrid system, blending various techniques to give you those eerily accurate suggestions.</p> <p><strong>Pros:</strong></p> <ul> <li><strong>Mitigates cold start problems.</strong></li> <li><strong>Increased accuracy and robustness.</strong></li> <li><strong>Offers more diverse and serendipitous recommendations.</strong></li> </ul> <p><strong>Cons:</strong></p> <ul> <li> <strong>Increased complexity</strong> in design, implementation, and maintenance.</li> </ul> <h3 id="key-challenges-in-building-recommender-systems">Key Challenges in Building Recommender Systems</h3> <p>While building these systems is incredibly rewarding, it’s far from easy. Here are some common hurdles:</p> <ol> <li> <strong>Cold Start Problem:</strong> This is the bane of all recommender systems. <ul> <li> <strong>New Users:</strong> If a user has no interaction history, how do you know what to recommend? (Solution: Ask for initial preferences, show popular items, use content-based for initial interaction).</li> <li> <strong>New Items:</strong> If a new product is added, how do you recommend it before anyone has bought/rated it? (Solution: Use content-based features, show to a diverse set of users, leverage explicit promotions).</li> </ul> </li> <li> <strong>Sparsity:</strong> Most users interact with only a tiny fraction of available items, leading to a very sparse user-item interaction matrix. This makes finding reliable patterns difficult.</li> <li> <strong>Scalability:</strong> Modern platforms have millions of users and items. Algorithms must be efficient enough to process this massive amount of data in real-time.</li> <li> <strong>Explainability:</strong> Users often want to know <em>why</em> an item was recommended. “Because similar users liked it” or “because of latent factor 7” isn’t always satisfying.</li> <li> <strong>Fairness and Bias:</strong> Recommenders can inadvertently reinforce biases present in historical data, leading to a lack of diversity or perpetuating stereotypes. They can also create “filter bubbles” where users are only exposed to information confirming their existing views.</li> <li> <strong>Shilling Attacks:</strong> Malicious actors might try to manipulate the system by creating fake profiles or ratings.</li> </ol> <h3 id="evaluating-recommender-systems-a-quick-glimpse">Evaluating Recommender Systems (A Quick Glimpse)</h3> <p>How do we know if a recommender system is “good”? We use various metrics:</p> <ul> <li> <strong>Accuracy Metrics:</strong> RMSE (Root Mean Squared Error) for explicit ratings, Precision, Recall, F1-score for implicit feedback (did the user interact with the recommended item?).</li> <li> <strong>Diversity &amp; Novelty:</strong> Does the system recommend a wide variety of items? Does it suggest new items the user wouldn’t typically find?</li> <li> <strong>Coverage:</strong> How many items can the system recommend?</li> </ul> <h3 id="the-future-is-bright-and-deep">The Future is Bright (and Deep!)</h3> <p>The field of Recommender Systems is constantly evolving. We’re seeing exciting advancements with Deep Learning, Reinforcement Learning, and contextual information (like time of day, location, current mood) being integrated to make recommendations even more personalized and dynamic.</p> <p>Next time you see a recommendation pop up on your screen, I hope you’ll look at it with new eyes. It’s not just a suggestion; it’s the result of intricate algorithms, vast amounts of data, and brilliant minds working to make your digital experience smoother and more delightful.</p> <p>This journey into Recommender Systems has truly deepened my appreciation for the power of data and machine learning. It’s a field where you can tangibly see the impact of your work on millions of people’s daily lives. If this sparked your interest, I highly encourage you to dive deeper – there’s a whole universe of algorithms and fascinating challenges waiting to be explored!</p> <p>Happy recommending!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>