<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Building AI with a Conscience: Your Blueprint for Ethical Machine Learning | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/building-ai-with-a-conscience-your-blueprint-for-e/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Building AI with a Conscience: Your Blueprint for Ethical Machine Learning</h1> <p class="post-meta"> Created on October 03, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/ai-ethics"> <i class="fa-solid fa-hashtag fa-sm"></i> AI Ethics</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/fairness"> <i class="fa-solid fa-hashtag fa-sm"></i> Fairness</a>   <a href="/blog/blog/tag/explainability"> <i class="fa-solid fa-hashtag fa-sm"></i> Explainability</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hello fellow data enthusiasts and future AI architects!</p> <p>If you’re anything like me, you’re probably captivated by the sheer power and potential of Artificial Intelligence. From recommending your next favorite song to powering self-driving cars, AI is no longer a futuristic fantasy; it’s woven into the very fabric of our daily lives. As someone diving deep into data science and machine learning, I find myself constantly amazed by what we can build. But lately, a question keeps echoing in my mind: <em>Just because we can build it, does that mean we should? And if we do, how do we ensure it’s built right?</em></p> <p>This isn’t just a philosophical debate for academics anymore. As builders, designers, and practitioners of AI, the responsibility falls squarely on our shoulders to ensure the systems we create are not only intelligent but also ethical. This topic, “Ethics in AI,” might sound daunting, but trust me, it’s one of the most crucial and fascinating areas you’ll explore. It’s about building AI with a conscience.</p> <h3 id="the-elephant-in-the-server-room-why-ai-ethics-matters-now">The Elephant in the Server Room: Why AI Ethics Matters NOW</h3> <p>You might be thinking, “Ethics? Isn’t that for philosophy class? I’m here to code!” And I get it. We’re often focused on optimizing models, boosting accuracy, and deploying at scale. But here’s the kicker: AI models learn from data. And data, whether we like it or not, reflects the biases, inequalities, and imperfections of the human world it comes from.</p> <p>Imagine an AI system designed to help doctors diagnose diseases, or one used by banks to approve loans, or even an algorithm that suggests candidates for jobs. If these systems are built on flawed data or with oversight that overlooks potential harms, the consequences can be profound. They can perpetuate discrimination, erode trust, and even endanger lives. This isn’t science fiction; it’s happening today.</p> <p>So, what exactly <em>is</em> AI ethics in practice? For me, it boils down to three core pillars: <strong>Fairness, Accountability, and Transparency (FAT)</strong>. Let’s peel back the layers on each.</p> <h3 id="pillar-1-fairness--unmasking-and-mitigating-bias">Pillar 1: Fairness – Unmasking and Mitigating Bias</h3> <p>This is perhaps the most talked-about aspect of AI ethics, and for good reason. AI models are essentially pattern recognition machines. If the data they are trained on contains historical biases, the model will learn and amplify those biases. It’s like teaching a student from a biased textbook – they’ll internalize those biases.</p> <p><strong>The Problem in Action:</strong></p> <ul> <li> <strong>Facial Recognition:</strong> Studies have shown that some facial recognition systems perform significantly worse on individuals with darker skin tones or women, leading to higher misidentification rates. This isn’t because the AI is inherently racist or sexist; it’s often due to an underrepresentation of these groups in the training datasets.</li> <li> <strong>Credit Scoring:</strong> An AI system might inadvertently deny loans to individuals from certain zip codes or backgrounds, not because of their creditworthiness, but due to historical lending patterns encoded in the data.</li> <li> <strong>Hiring Tools:</strong> Some AI recruitment tools have been found to discriminate against female candidates by favoring language more common in male-dominated resumes.</li> </ul> <p><strong>Thinking Technically About Fairness:</strong></p> <p>Fairness isn’t a single, easily quantifiable concept. It’s multi-faceted, and different definitions of fairness can even contradict each other. As data scientists, we need to understand these definitions to choose the appropriate one for a given context.</p> <p>Let’s consider a scenario where we’re building a model ($\hat{Y}$) to predict whether someone will default on a loan ($Y=1$ for default, $Y=0$ for no default). We have a sensitive attribute, $S$, which could represent a demographic group (e.g., $S=s_1$ for Group A, $S=s_2$ for Group B).</p> <ol> <li> <p><strong>Demographic Parity (Statistical Parity):</strong> This means the model’s positive prediction rate should be roughly equal across different sensitive groups. \(P(\hat{Y}=1 | S=s_1) \approx P(\hat{Y}=1 | S=s_2)\) In simpler terms, if our model predicts 10% of Group A will default, it should also predict 10% of Group B will default, regardless of their actual default rates. This sounds good, but it might lead to approving loans for less creditworthy individuals in one group to achieve parity, which isn’t always fair.</p> </li> <li> <strong>Equalized Odds:</strong> This is a stronger notion of fairness. It requires that the true positive rate (TPR) and false positive rate (FPR) of the model are equal across different sensitive groups. <ul> <li> <table> <tbody> <tr> <td> <strong>True Positive Rate (TPR):</strong> $P(\hat{Y}=1</td> <td>Y=1, S=s_1) \approx P(\hat{Y}=1</td> <td>Y=1, S=s_2)$ (The model correctly identifies defaults equally well in both groups).</td> </tr> </tbody> </table> </li> <li> <strong>False Positive Rate (FPR):</strong> $P(\hat{Y}=1 | Y=0, S=s_1) \approx P(\hat{Y}=1 | Y=0, S=s_2)$ (The model incorrectly predicts defaults equally poorly in both groups). This means if someone <em>actually</em> defaults, the model should be equally likely to flag them in Group A as in Group B. And if someone <em>doesn’t</em> default, the model should be equally likely to wrongly flag them in both groups. This is often preferred in high-stakes applications.</li> </ul> </li> <li> <strong>Predictive Parity (Positive Predictive Value Parity):</strong> This focuses on the accuracy of positive predictions. \(P(Y=1 | \hat{Y}=1, S=s_1) \approx P(Y=1 | \hat{Y}=1, S=s_2)\) Meaning, among those predicted to default, the proportion who <em>actually</em> default should be similar across groups.</li> </ol> <p><strong>The Challenge:</strong> It’s often mathematically impossible to satisfy all these fairness definitions simultaneously, especially when base rates (the actual proportion of defaults) differ between groups. This means we, as data scientists, must engage in careful decision-making, often involving domain experts and ethicists, to decide which form of fairness is most appropriate for a given application.</p> <p><strong>Mitigation Strategies:</strong></p> <ul> <li> <strong>Data Preprocessing:</strong> Cleaning and balancing datasets to reduce inherent biases (e.g., oversampling underrepresented groups, reweighting samples).</li> <li> <strong>In-processing Algorithms:</strong> Incorporating fairness constraints directly into the model training process.</li> <li> <strong>Post-processing Methods:</strong> Adjusting prediction thresholds after model training to achieve fairness criteria (e.g., lowering the threshold for a disadvantaged group to increase their acceptance rate).</li> </ul> <h3 id="pillar-2-accountability--whos-responsible-when-ai-goes-wrong">Pillar 2: Accountability – Who’s Responsible When AI Goes Wrong?</h3> <p>When a human makes a mistake, we know who to point to. But what happens when an AI system causes harm? An autonomous vehicle gets into an accident, an AI-powered medical diagnostic tool misdiagnoses a patient, or an algorithm unfairly denies someone a vital service. Who is accountable? The data scientist? The engineer? The company? The user?</p> <p>This isn’t an easy question, and legal and ethical frameworks are still evolving. But as AI practitioners, we have a vital role to play in establishing clear lines of responsibility.</p> <p><strong>Technical Contributions to Accountability:</strong></p> <ul> <li> <strong>Robust Monitoring:</strong> Implementing systems to continuously monitor AI performance in the real world, looking for unexpected behaviors or disparate impacts across groups.</li> <li> <strong>Versioning and Documentation:</strong> Meticulous tracking of model versions, training data, hyperparameters, and deployment decisions. If something goes wrong, we need to be able to trace it back.</li> <li> <strong>“Human-in-the-Loop” Systems:</strong> For critical applications, ensuring there’s always a human who can review, override, or intervene in AI decisions. The AI acts as an assistant, not an autonomous agent.</li> <li> <strong>Transparency (which we’ll cover next!):</strong> If we can understand <em>why</em> an AI made a decision, it becomes easier to assign accountability.</li> </ul> <p>Building accountability into AI means designing systems that are auditable, that leave a clear trail, and that have built-in safeguards for human oversight.</p> <h3 id="pillar-3-transparency--opening-the-black-box">Pillar 3: Transparency – Opening the “Black Box”</h3> <p>Many of the most powerful AI models, especially deep learning networks, are often referred to as “black boxes.” They can make incredibly accurate predictions, but it’s incredibly difficult to understand <em>how</em> they arrived at those predictions. If a loan application is rejected by an AI, the applicant has a right to know why. If an AI suggests a particular medical treatment, a doctor needs to understand the reasoning.</p> <p>This is where <strong>Explainable AI (XAI)</strong> comes into play. The goal of XAI is to make AI systems more understandable and interpretable to humans.</p> <p><strong>Why is Transparency so Hard (and Important)?</strong></p> <p>Imagine a linear regression model: $\hat{y} = w_0 + w_1x_1 + w_2x_2 + \dots + w_nx_n$. Here, the coefficients $w_i$ directly tell us the impact of each feature $x_i$ on the prediction $\hat{y}$. It’s highly transparent.</p> <p>Now, imagine a neural network with millions of parameters, multiple hidden layers, and complex non-linear activations. The decision-making process is distributed across these layers in a way that is incredibly difficult for a human to intuitively grasp. This lack of transparency can lead to:</p> <ul> <li> <strong>Lack of Trust:</strong> If we don’t understand it, we can’t fully trust it.</li> <li> <strong>Difficulty in Debugging:</strong> If an AI makes a wrong decision, how do we fix it if we don’t know why it went wrong?</li> <li> <strong>Hidden Biases:</strong> Opaque models can hide the propagation and amplification of biases.</li> </ul> <p><strong>Technical Approaches to Explainability:</strong></p> <p>XAI techniques aim to shed light on these black boxes. They can be broadly categorized:</p> <ul> <li> <strong>Local Explanations:</strong> Explaining <em>why</em> a specific prediction was made for a single instance. <ul> <li> <strong>LIME (Local Interpretable Model-agnostic Explanations):</strong> Works by perturbing a single data point and training a simple, interpretable model (like linear regression) on these perturbed samples around the original point. This local model then provides feature importance for that specific prediction.</li> <li> <strong>SHAP (SHapley Additive exPlanations):</strong> Based on game theory, SHAP values tell us how much each feature contributes to the prediction compared to the average prediction, distributing the “credit” among features fairly. It provides a consistent and theoretically sound way to explain predictions. For a given prediction $f(x)$, SHAP values $\phi_i$ sum up to the difference between the prediction and the average prediction: \(f(x) - E[f(x)] = \sum_{i=1}^M \phi_i(x)\) where $M$ is the number of features. Each $\phi_i(x)$ represents the contribution of feature $i$ to the prediction for input $x$.</li> </ul> </li> <li> <strong>Global Explanations:</strong> Understanding the overall behavior of the model. <ul> <li> <strong>Feature Importance:</strong> For tree-based models (e.g., Random Forests, Gradient Boosted Trees), we can often derive a global feature importance score based on how much each feature reduces impurity or error across all splits.</li> <li> <strong>Partial Dependence Plots (PDPs):</strong> Show the marginal effect of one or two features on the predicted outcome of a model, averaging over the values of all other features.</li> </ul> </li> </ul> <p>The trade-off between model complexity (and often, accuracy) and explainability is a common challenge. Sometimes, we might need to choose a slightly less accurate but more transparent model for critical applications.</p> <h3 id="beyond-the-fat-pillars-other-ethical-considerations">Beyond the FAT Pillars: Other Ethical Considerations</h3> <p>While Fairness, Accountability, and Transparency form a strong foundation, the ethical landscape of AI is vast. Other important considerations include:</p> <ul> <li> <strong>Privacy:</strong> How is user data collected, stored, and used? Ensuring data protection (e.g., GDPR compliance) and techniques like federated learning or differential privacy are crucial.</li> <li> <strong>Security:</strong> AI models are vulnerable to adversarial attacks, where subtle, imperceptible changes to input data can fool a model into making incorrect predictions. Building robust and secure AI is an ethical imperative.</li> <li> <strong>Environmental Impact:</strong> Training massive AI models requires enormous computational power, leading to significant energy consumption and carbon emissions. Responsible AI development also considers its ecological footprint.</li> <li> <strong>Human Autonomy and Control:</strong> Ensuring AI systems augment human capabilities rather than diminish human agency. Maintaining appropriate levels of human oversight and decision-making authority.</li> </ul> <h3 id="your-role-as-a-responsible-ai-builder">Your Role as a Responsible AI Builder</h3> <p>As aspiring data scientists and machine learning engineers, you are on the front lines of this technological revolution. You have a unique opportunity – and responsibility – to shape the future of AI.</p> <p>It’s not enough to just build models that are “good enough” or “accurate enough.” We must strive to build AI that is also <em>fair enough</em>, <em>accountable enough</em>, and <em>transparent enough</em>.</p> <p>This means:</p> <ul> <li> <strong>Questioning Data:</strong> Always critically examine your data for biases, incompleteness, and representation issues. “Garbage in, garbage out” has profound ethical implications.</li> <li> <strong>Understanding Impact:</strong> Think about the real-world consequences of your models. Who will be affected? How?</li> <li> <strong>Choosing Metrics Wisely:</strong> Beyond accuracy, recall, and precision, consider fairness metrics that align with the ethical goals of your project.</li> <li> <strong>Documenting Decisions:</strong> Keep a clear record of your design choices, fairness considerations, and any trade-offs made.</li> <li> <strong>Collaborating:</strong> Engage with ethicists, domain experts, and even end-users to gain diverse perspectives on potential ethical challenges.</li> <li> <strong>Continuous Learning:</strong> The field of AI ethics is rapidly evolving. Stay curious, read research, and participate in discussions.</li> </ul> <h3 id="the-future-we-build">The Future We Build</h3> <p>The journey into AI ethics is challenging, nuanced, and endlessly fascinating. It requires us to blend technical prowess with critical thinking, empathy, and a deep understanding of societal impact.</p> <p>As you embark on your data science and machine learning journey, remember that you’re not just writing code; you’re building systems that will influence lives, shape economies, and redefine societies. Let’s commit to building AI that is not only intelligent but also wise, just, and truly serves humanity.</p> <p>Let’s build AI with a conscience, together.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>