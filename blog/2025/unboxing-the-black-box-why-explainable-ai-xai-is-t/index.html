<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Unboxing the Black Box: Why Explainable AI (XAI) is the Future of Trustworthy Models | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/2025/unboxing-the-black-box-why-explainable-ai-xai-is-t/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Unboxing the Black Box: Why Explainable AI (XAI) is the Future of Trustworthy Models</h1> <p class="post-meta"> Created on April 12, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/explainable-ai"> <i class="fa-solid fa-hashtag fa-sm"></i> Explainable AI</a>   <a href="/blog/blog/tag/xai"> <i class="fa-solid fa-hashtag fa-sm"></i> XAI</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/interpretability"> <i class="fa-solid fa-hashtag fa-sm"></i> Interpretability</a>   <a href="/blog/blog/tag/ai-ethics"> <i class="fa-solid fa-hashtag fa-sm"></i> AI Ethics</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>Have you ever wondered how your favorite AI application, be it a recommendation engine, a medical diagnostic tool, or a credit risk predictor, actually arrives at its decisions? It’s easy to be amazed by the predictive power of these models, especially deep learning networks. They can identify complex patterns in data that humans might miss, achieving superhuman performance in specific tasks. But there’s a catch, a big one: often, these powerful models are <em>black boxes</em>.</p> <p>That’s right. You feed them data, they spit out a prediction, and you have no clear idea <em>why</em> they made that particular decision. Was it because of a specific feature? A combination of subtle interactions? Or perhaps, something entirely spurious that the model latched onto? In many critical domains, not knowing the “why” isn’t just inconvenient; it can be dangerous, unfair, or even illegal.</p> <p>This is where Explainable AI, or XAI, steps into the spotlight. It’s not just a buzzword; it’s a rapidly growing field dedicated to making AI systems understandable to humans. Think of it as giving our AI a voice, allowing it to articulate its reasoning.</p> <h3 id="the-problem-with-black-boxes-a-deep-dive">The Problem with Black Boxes: A Deep Dive</h3> <p>Let’s imagine a few scenarios where a black-box AI could cause serious issues:</p> <ol> <li> <strong>Healthcare:</strong> An AI model predicts a patient has a high risk of a certain disease. Great! But if the doctor doesn’t know <em>why</em> – was it a specific lab result, a demographic factor, or a family history? – they can’t verify the diagnosis, explain it to the patient, or even learn from the model’s “insights.” What if the model is biased against a certain demographic?</li> <li> <strong>Finance:</strong> A bank uses an AI to approve or deny loan applications. If your loan is denied, and the AI can’t explain why, how can you appeal the decision or improve your creditworthiness? This lack of transparency can lead to unfairness and erode trust.</li> <li> <strong>Autonomous Driving:</strong> A self-driving car makes a critical decision, like swerving or braking suddenly. If an accident occurs, how do we investigate the cause if the AI’s decision-making process is opaque?</li> <li> <strong>Debugging &amp; Improvement:</strong> If an AI model consistently makes wrong predictions in a specific scenario, how do you fix it if you don’t know which features or internal logic are at fault? It’s like trying to fix a broken engine without knowing what any of the parts do.</li> </ol> <p>These examples highlight the critical need for XAI. We’re not just building models that perform well; we’re building models that <em>make sense</em>.</p> <h3 id="what-exactly-is-explainable-ai-xai">What Exactly is Explainable AI (XAI)?</h3> <p>At its core, XAI is a set of techniques and methodologies that aim to make AI models more understandable, transparent, and interpretable. It’s about opening up that black box and shedding light on its internal workings.</p> <p><strong>Why do we need XAI?</strong> The reasons are multi-faceted:</p> <ul> <li> <strong>Trust:</strong> If we understand how an AI makes decisions, we’re more likely to trust it, especially in high-stakes situations.</li> <li> <strong>Accountability &amp; Ethics:</strong> XAI helps us identify and mitigate biases, ensuring fairness and preventing discrimination. It allows us to hold AI systems accountable for their actions.</li> <li> <strong>Debugging &amp; Improvement:</strong> Understanding why a model fails helps data scientists improve its performance, fix errors, and refine its design.</li> <li> <strong>Compliance:</strong> Regulations like GDPR’s “right to explanation” are pushing for greater transparency in automated decision-making.</li> <li> <strong>Learning &amp; Discovery:</strong> Sometimes, an AI model can uncover hidden patterns or relationships in data that provide new scientific insights. XAI helps us extract those insights.</li> </ul> <h3 id="key-concepts-in-xai">Key Concepts in XAI</h3> <p>Before we dive into specific techniques, let’s clarify a few important distinctions:</p> <ul> <li> <strong>Interpretability vs. Explainability:</strong> While often used interchangeably, “interpretability” generally refers to the degree to which a human can understand the cause and effect of a model’s prediction. “Explainability” refers to the techniques and methods used to achieve that interpretability.</li> <li> <strong>Local vs. Global Interpretability:</strong> <ul> <li> <strong>Local:</strong> Explaining a <em>single prediction</em> for a specific instance. “Why did <em>this</em> particular loan applicant get rejected?”</li> <li> <strong>Global:</strong> Explaining the <em>overall behavior</em> of the model across all predictions. “Which features are generally most important for loan approvals?”</li> </ul> </li> <li> <strong>Model-Agnostic vs. Model-Specific:</strong> <ul> <li> <strong>Model-Agnostic:</strong> Techniques that can be applied to <em>any</em> machine learning model (e.g., neural networks, random forests, SVMs). They treat the model as a black box and probe it from the outside.</li> <li> <strong>Model-Specific:</strong> Techniques designed for <em>particular types</em> of models (e.g., inspecting weights in a linear regression, analyzing decision paths in a decision tree).</li> </ul> </li> <li> <strong>Pre-hoc vs. Post-hoc Explanations:</strong> <ul> <li> <strong>Pre-hoc:</strong> Designing inherently interpretable models from the start (e.g., linear regression, decision trees).</li> <li> <strong>Post-hoc:</strong> Applying explanation techniques <em>after</em> a complex model has been trained. Most XAI research focuses here.</li> </ul> </li> </ul> <h3 id="popular-xai-techniques-opening-the-black-box">Popular XAI Techniques: Opening the Black Box</h3> <p>Let’s look at some of the most prominent XAI techniques that are helping us understand our models better.</p> <h4 id="1-lime-local-interpretable-model-agnostic-explanations">1. LIME (Local Interpretable Model-agnostic Explanations)</h4> <p>LIME is one of the most popular and intuitive post-hoc explanation techniques. It’s <strong>model-agnostic</strong> and provides <strong>local explanations</strong>.</p> <p><strong>How it works (the intuition):</strong> Imagine you want to understand why a complex model made a specific prediction for a particular data point (let’s call it $x$). LIME doesn’t try to understand the entire complex model. Instead, it creates a simplified, interpretable model (like a linear regression or a decision tree) that <em>approximates</em> the complex model’s behavior <em>only in the vicinity</em> of $x$.</p> <p>It does this by:</p> <ol> <li> <strong>Perturbing</strong> the original data point $x$ multiple times to create many slightly different, “neighboring” data points.</li> <li> <strong>Getting predictions</strong> from the black-box model for all these perturbed points.</li> <li> <strong>Weighting</strong> these perturbed points by their proximity to the original data point $x$.</li> <li> <strong>Training a simple, interpretable model</strong> (e.g., linear model $g$) on this weighted, local dataset.</li> </ol> <p>The mathematical intuition is to find an interpretable model $g \in \mathcal{G}$ that minimizes a locally weighted squared error: $L(f, g, \pi_x) = \sum_{z \in Z} \pi_x(z) (f(z) - g(z))^2 + \Omega(g)$ where:</p> <ul> <li>$f$ is the black-box model.</li> <li>$g$ is the interpretable model (e.g., linear regression).</li> <li>$\pi_x(z)$ is the proximity measure between the original instance $x$ and the perturbed instance $z$.</li> <li>$\Omega(g)$ is a complexity measure for $g$ (we want $g$ to be simple).</li> </ul> <p>LIME then presents the coefficients of this local linear model (or the rules of the local decision tree) as the explanation for the specific prediction of $x$. For image classification, this might mean highlighting super-pixels that contributed most to a classification. For text, it might highlight important words.</p> <h4 id="2-shap-shapley-additive-explanations">2. SHAP (SHapley Additive exPlanations)</h4> <p>SHAP is another powerful technique that unifies several other explanation methods. It provides both <strong>local</strong> and a form of <strong>global</strong> explanations and is <strong>model-agnostic</strong>.</p> <p><strong>How it works (the intuition):</strong> SHAP is based on cooperative game theory, specifically the concept of <strong>Shapley values</strong>. Imagine each feature in your dataset is a player in a game, and the “game” is predicting the outcome. The Shapley value for a feature represents its fair contribution to the prediction, averaged across all possible combinations (coalitions) of features.</p> <p>This means it answers: “How much does <em>this specific feature</em> contribute to the prediction compared to the average prediction, considering all possible ways this feature could have been included or excluded?”</p> <p>The general formula for the Shapley value for a feature $i$ is: $\phi_i = \sum_{S \subseteq N \setminus {i}} \frac{|S|!(|N|-|S|-1)!}{|N|!} [f_S(x_S \cup {x_i}) - f_S(x_S)]$ where:</p> <ul> <li>$N$ is the set of all features.</li> <li>$S$ is a subset of features not including $i$.</li> <li>$f_S(x_S)$ is the prediction of the model using only the features in set $S$.</li> <li>$f_S(x_S \cup {x_i})$ is the prediction of the model using features in set $S$ <em>plus</em> feature $i$.</li> </ul> <p>Don’t worry too much about the complex sum; the key idea is that SHAP values assign each feature an importance value for a specific prediction, such that the sum of all feature importance values equals the difference between the prediction and the average (or baseline) prediction.</p> <p>This allows us to visualize feature contributions for a single prediction and also aggregate these values across many predictions to get a global understanding of feature importance.</p> <h4 id="3-feature-importance-permutation-importance-gini-importance">3. Feature Importance (Permutation Importance, Gini Importance)</h4> <p>These are simpler, often <strong>global</strong> explanation methods.</p> <ul> <li> <p><strong>Permutation Importance:</strong> This is a <strong>model-agnostic</strong> technique. To find the importance of a feature, you randomly shuffle (permute) the values of that feature in the test set and see how much the model’s performance (e.g., accuracy, F1-score) decreases. A large drop indicates the feature is important. It’s intuitive and robust.</p> </li> <li> <p><strong>Gini Importance (for Tree-based models):</strong> This is a <strong>model-specific</strong> technique. In decision trees and random forests, Gini importance (or impurity reduction) measures how much each feature contributes to reducing impurity (making splits cleaner) across all trees in the forest. Features that lead to significant impurity reduction are considered more important.</p> </li> </ul> <h4 id="4-counterfactual-explanations">4. Counterfactual Explanations</h4> <p>These explanations answer the question: “What is the smallest change to the input that would flip the model’s prediction?”</p> <p>For example, if your loan was denied, a counterfactual explanation might be: “If your income was $5,000 higher, your loan would have been approved.” This is incredibly useful because it not only explains <em>why</em> a decision was made but also provides actionable advice on <em>how</em> to change the outcome. They are typically <strong>local</strong> and <strong>model-agnostic</strong>.</p> <h4 id="5-anchors">5. Anchors</h4> <p>Anchors are a technique for finding “rules” that sufficiently “anchor” a prediction. An Anchor explanation for a prediction $x$ is a set of conditions that are sufficient to guarantee the same prediction with high probability, regardless of the values of the other features.</p> <p>For example, for a fraud detection model, an Anchor might be: “If the transaction amount is over $10,000 <em>and</em> it’s from an unverified IP address, the transaction will be flagged as fraudulent with 99% certainty.” These are powerful because they provide robust, rule-based explanations.</p> <h3 id="the-benefits-of-embracing-xai">The Benefits of Embracing XAI</h3> <p>Integrating XAI into your data science workflow isn’t just a technical exercise; it’s a paradigm shift towards more responsible and effective AI development.</p> <ol> <li> <strong>Enhanced Trust and Adoption:</strong> When users, stakeholders, and even regulators understand how an AI system works, they are more likely to trust it and adopt its recommendations. This is critical for widespread AI integration into society.</li> <li> <strong>Fairness and Bias Detection:</strong> XAI techniques can expose discriminatory biases hidden within complex models. If an explanation consistently highlights irrelevant or protected attributes (like race or gender) as drivers for critical decisions (like loan approvals or medical diagnoses), it’s a clear red flag that the model is unfair and needs retraining or adjustment.</li> <li> <strong>Robustness and Reliability:</strong> By understanding the “why,” we can identify scenarios where our models might be brittle or unreliable. For example, if an image classification model for identifying a cat relies primarily on the background rather than the cat itself, we know it’s not robust and will fail in novel environments.</li> <li> <strong>Domain Expertise and Knowledge Discovery:</strong> Sometimes, XAI can surface unexpected correlations or feature importances that provide new insights into the problem domain, helping human experts learn from the AI.</li> <li> <strong>Regulatory Compliance:</strong> As mentioned with GDPR, the “right to explanation” is becoming a legal requirement in many jurisdictions, making XAI an indispensable tool for compliance.</li> </ol> <h3 id="challenges-and-the-road-ahead">Challenges and the Road Ahead</h3> <p>While XAI offers immense promise, it’s not without its challenges:</p> <ul> <li> <strong>Complexity of Explanation:</strong> Explaining a highly complex model (e.g., a massive deep neural network with billions of parameters) in a way that is both accurate and understandable to humans is still an active area of research. Sometimes, the explanation itself can be complex.</li> <li> <strong>Subjectivity of Interpretability:</strong> What constitutes a “good” explanation can vary widely depending on the audience (e.g., a data scientist needs more technical detail than a domain expert or a general user).</li> <li> <strong>Trade-off between Performance and Interpretability:</strong> Often, the most interpretable models (like linear regression) are not the most performant, and vice-versa. Finding the right balance is key.</li> <li> <strong>Scalability:</strong> Generating explanations for every single prediction, especially in real-time for high-throughput systems, can be computationally expensive.</li> <li> <strong>Human Factors:</strong> How do humans actually <em>perceive</em> and <em>use</em> explanations? Research in human-computer interaction is crucial here to ensure explanations are actionable and don’t lead to over-reliance or mistrust.</li> </ul> <p>The field of XAI is still evolving rapidly. Researchers are constantly developing new techniques, improving existing ones, and exploring how to best present explanations to diverse audiences. The goal isn’t necessarily to make every neural network fully transparent in a human-readable way, but rather to provide sufficient insight to ensure trust, fairness, and utility.</p> <h3 id="conclusion-building-responsible-ai-together">Conclusion: Building Responsible AI Together</h3> <p>As data scientists and machine learning engineers, we have a responsibility not just to build powerful models, but to build responsible ones. Explainable AI is a critical step in this journey. It empowers us to understand, debug, improve, and ultimately trust the intelligent systems we create.</p> <p>So, the next time you’re building a model, don’t just ask “how accurate is it?” Also ask, “how can I explain its decisions?” By embracing XAI, we move beyond merely predicting outcomes to truly understanding them, paving the way for a future where AI works <em>with</em> us, not just <em>for</em> us, in a transparent and trustworthy manner.</p> <p>Keep learning, keep questioning, and let’s keep unboxing those black boxes!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>