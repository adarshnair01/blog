<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Art and Science of Hyperparameter Tuning: My Quest for Smarter Models | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/the-art-and-science-of-hyperparameter-tuning-my-qu/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Art and Science of Hyperparameter Tuning: My Quest for Smarter Models</h1> <p class="post-meta"> Created on December 16, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/hyperparameter-tuning"> <i class="fa-solid fa-hashtag fa-sm"></i> Hyperparameter Tuning</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/model-optimization"> <i class="fa-solid fa-hashtag fa-sm"></i> Model Optimization</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hello fellow data adventurers!</p> <p>If you’re anything like me, you’ve probably spent countless hours building machine learning models. You gather data, preprocess it, choose an algorithm (say, a Random Forest or a Neural Network), train it, and then… you check the performance metrics. Sometimes, your model performs beautifully. Other times, it’s just “okay,” or even downright disappointing. You stare at the screen, wondering, “What am I missing? How can I make it <em>smarter</em>?”</p> <p>I remember those early days. My models felt like a mystery box. I’d tweak a number here, change a setting there, mostly based on intuition or a quick Google search. It felt less like science and more like… hopeful button-mashing. I quickly learned that this approach, while sometimes yielding lucky breaks, was unsustainable and inefficient. This frustration is what led me down the fascinating path of <strong>Hyperparameter Tuning</strong>.</p> <h3 id="whats-the-big-deal-a-chefs-analogy">What’s the Big Deal? A Chef’s Analogy</h3> <p>Imagine you’re a chef, and you’re trying to bake the perfect cake.</p> <ul> <li>Your <strong>ingredients</strong> (flour, sugar, eggs) are like your <strong>data</strong>.</li> <li>The <strong>recipe</strong> (how you mix them, in what order) is your <strong>machine learning algorithm</strong> (e.g., Logistic Regression, Support Vector Machine, Neural Network).</li> <li>The <strong>cake itself</strong> is your <strong>trained model</strong>.</li> </ul> <p>Now, here’s where it gets interesting:</p> <ul> <li> <p>The <em>proportions</em> of flour, sugar, and eggs you use are like your model’s <strong>parameters</strong>. These are learned <em>during</em> the baking process, based on the ingredients and the recipe. For instance, in a Linear Regression model, the <code class="language-plaintext highlighter-rouge">weights</code> and <code class="language-plaintext highlighter-rouge">biases</code> are parameters – they are learned from the data to best fit the relationship.</p> </li> <li> <p>But what about the <strong>oven temperature</strong>, the <strong>baking time</strong>, or even the <em>type</em> of oven you use? These are not learned from the ingredients; you decide them <em>before</em> you even start baking. These are your <strong>hyperparameters</strong>.</p> </li> </ul> <p>In machine learning, hyperparameters are the configuration variables of your learning algorithm itself, set <em>before</em> the training process begins. They dictate <em>how</em> your model learns. Picking the right hyperparameters is like setting the perfect oven temperature and baking time for your cake – it can be the difference between a golden masterpiece and a burnt, sad mess.</p> <h3 id="hyperparameters-vs-model-parameters-a-closer-look">Hyperparameters vs. Model Parameters: A Closer Look</h3> <p>Let’s cement this distinction, as it’s fundamental:</p> <ul> <li> <strong>Model Parameters</strong>: <ul> <li>Internal variables of the model.</li> <li>Learned automatically from the data during training.</li> <li>Define the model’s predictive function.</li> <li>Examples: Coefficients in a linear regression ($\beta_0, \beta_1, …$), weights and biases in a neural network.</li> </ul> </li> <li> <strong>Hyperparameters</strong>: <ul> <li>External configuration of the model.</li> <li>Set <em>manually</em> by the data scientist <em>before</em> training.</li> <li>Control the learning process itself.</li> <li>Examples: <ul> <li> <strong>Learning rate</strong> for gradient descent (<code class="language-plaintext highlighter-rouge">alpha</code>).</li> <li> <strong>Number of trees</strong> in a Random Forest (<code class="language-plaintext highlighter-rouge">n_estimators</code>).</li> <li> <strong>Depth</strong> of a decision tree (<code class="language-plaintext highlighter-rouge">max_depth</code>).</li> <li> <strong>Regularization strength</strong> (<code class="language-plaintext highlighter-rouge">C</code> in SVMs, <code class="language-plaintext highlighter-rouge">lambda</code> in Ridge/Lasso).</li> <li> <strong>Number of layers</strong> or <strong>neurons</strong> in a neural network.</li> <li> <strong>Kernel type</strong> in an SVM.</li> </ul> </li> </ul> </li> </ul> <p>The goal of hyperparameter tuning is to find the combination of hyperparameters that allows your model to perform optimally on unseen data. This usually means minimizing a <code class="language-plaintext highlighter-rouge">loss function</code> or maximizing a <code class="language-plaintext highlighter-rouge">performance metric</code> (like accuracy, precision, recall, F1-score) on a validation set.</p> <h3 id="the-manual-struggle-my-early-days">The Manual Struggle: My Early Days</h3> <p>My first approach to tuning was pure guesswork. I’d read an article recommending <code class="language-plaintext highlighter-rouge">learning_rate = 0.001</code> and <code class="language-plaintext highlighter-rouge">n_estimators = 100</code>, plug those in, train, evaluate, frown, then try <code class="language-plaintext highlighter-rouge">learning_rate = 0.01</code> and <code class="language-plaintext highlighter-rouge">n_estimators = 50</code>. This was like trying to bake a cake by randomly picking oven temperatures and times until it looked “right.” It was incredibly time-consuming, prone to human error, and rarely led to the <em>best</em> results. There had to be a better way!</p> <h3 id="the-quest-begins-systematic-approaches-to-finding-the-sweet-spot">The Quest Begins: Systematic Approaches to Finding the “Sweet Spot”</h3> <p>The good news is, smarter people than me have developed systematic ways to tackle this challenge. Before diving into the methods, let’s talk about a crucial concept: <strong>Cross-Validation</strong>.</p> <p>When we tune hyperparameters, we need to evaluate how well each set performs. If we evaluate on the training data, we risk <em>overfitting</em>. If we use our final test set, we contaminate it, making it no longer truly “unseen.” This is where <strong>cross-validation</strong> shines.</p> <p>The most common technique is <code class="language-plaintext highlighter-rouge">$k$-fold cross-validation</code>. We split our training data into <code class="language-plaintext highlighter-rouge">$k$</code> equal “folds.” For each set of hyperparameters, we train the model <code class="language-plaintext highlighter-rouge">$k$</code> times. Each time, one fold acts as the validation set, and the remaining <code class="language-plaintext highlighter-rouge">$k-1$</code> folds are used for training. The average performance across all <code class="language-plaintext highlighter-rouge">$k$</code> iterations gives us a robust estimate of how well those hyperparameters perform. This is paramount for reliable tuning.</p> <p>Now, let’s explore the tuning strategies:</p> <h4 id="1-grid-search-the-exhaustive-explorer">1. Grid Search: The Exhaustive Explorer</h4> <p>Imagine you have a few hyperparameters, each with a discrete set of values you want to try.</p> <ul> <li> <code class="language-plaintext highlighter-rouge">learning_rate</code>: <code class="language-plaintext highlighter-rouge">[0.001, 0.01, 0.1]</code> </li> <li> <code class="language-plaintext highlighter-rouge">n_estimators</code>: <code class="language-plaintext highlighter-rouge">[100, 200, 300]</code> </li> <li> <code class="language-plaintext highlighter-rouge">max_depth</code>: <code class="language-plaintext highlighter-rouge">[5, 10, 15]</code> </li> </ul> <p>Grid Search is like trying <em>every single possible combination</em> from these lists. It systematically builds a “grid” of hyperparameter values and trains/evaluates a model for each point on that grid.</p> <p><strong>How it works (conceptually):</strong></p> <ol> <li>Define a dictionary of hyperparameters and a list of values for each.</li> <li>The algorithm generates all possible combinations.</li> <li>For each combination: <ul> <li>Train a model using these hyperparameters.</li> <li>Evaluate its performance using cross-validation.</li> </ul> </li> <li>The combination yielding the best performance is selected.</li> </ol> <p><strong>Pros:</strong></p> <ul> <li>Simple to understand and implement (Scikit-learn’s <code class="language-plaintext highlighter-rouge">GridSearchCV</code> makes it a breeze).</li> <li>Guaranteed to find the best combination <em>within the defined grid</em>.</li> </ul> <p><strong>Cons:</strong></p> <ul> <li> <strong>Computationally expensive</strong>: The number of models to train grows exponentially with the number of hyperparameters and the number of values per hyperparameter. If you have <code class="language-plaintext highlighter-rouge">$d$</code> hyperparameters and try <code class="language-plaintext highlighter-rouge">$N$</code> values for each, you’ll train <code class="language-plaintext highlighter-rouge">$N^d$</code> models. <code class="language-plaintext highlighter-rouge">$O(N^d)$</code> complexity! This is often called the “curse of dimensionality.” If <code class="language-plaintext highlighter-rouge">$N=10$</code> and <code class="language-plaintext highlighter-rouge">$d=5$</code>, that’s <code class="language-plaintext highlighter-rouge">$10^5 = 100,000$</code> models!</li> <li>Can miss optimal values if they lie outside the chosen grid points.</li> </ul> <p>For small search spaces, Grid Search is a reliable workhorse. For larger, more complex models, it quickly becomes unfeasible.</p> <h4 id="2-random-search-the-efficient-explorer">2. Random Search: The Efficient Explorer</h4> <p>My moment of enlightenment came when I learned about Random Search. Instead of trying every combination, why not just pick combinations <em>randomly</em> from the defined hyperparameter distributions?</p> <p>The intuition behind Random Search is quite elegant: not all hyperparameters are equally important. Some have a much larger impact on performance than others. Grid Search spends equal time exploring all dimensions, which can be inefficient. Random Search, by sampling randomly, is more likely to explore a wider range of values for <em>each individual hyperparameter</em> within the same computational budget, often discovering better combinations faster than Grid Search, especially in high-dimensional spaces.</p> <p><strong>How it works (conceptually):</strong></p> <ol> <li>Define a hyperparameter space (e.g., a range for <code class="language-plaintext highlighter-rouge">learning_rate</code>, a set of categories for <code class="language-plaintext highlighter-rouge">kernel</code>).</li> <li>Define the number of iterations (<code class="language-plaintext highlighter-rouge">n_iter</code>).</li> <li>For <code class="language-plaintext highlighter-rouge">n_iter</code> times: <ul> <li>Randomly sample a combination of hyperparameters from the defined space.</li> <li>Train a model with these hyperparameters.</li> <li>Evaluate its performance using cross-validation.</li> </ul> </li> <li>Select the combination that performed best.</li> </ol> <p><strong>Pros:</strong></p> <ul> <li>Significantly more efficient than Grid Search in many cases, especially when only a few hyperparameters are truly important.</li> <li>Allows you to specify a fixed computational budget (number of iterations).</li> <li>Can sample from continuous distributions (e.g., a uniform distribution for <code class="language-plaintext highlighter-rouge">learning_rate</code> between <code class="language-plaintext highlighter-rouge">0.0001</code> and <code class="language-plaintext highlighter-rouge">1.0</code>).</li> </ul> <p><strong>Cons:</strong></p> <ul> <li>Still not guaranteed to find the <em>global</em> optimum, as it’s a random process.</li> <li>Requires thoughtful definition of the search space.</li> </ul> <p>Scikit-learn’s <code class="language-plaintext highlighter-rouge">RandomizedSearchCV</code> is an excellent tool for this. I often start with Random Search to quickly narrow down promising regions of the hyperparameter space.</p> <h4 id="3-bayesian-optimization-the-smart-learner">3. Bayesian Optimization: The Smart Learner</h4> <p>This is where things get really exciting, and a bit more advanced. Imagine you’re blindfolded and trying to find the highest point on a mountain. Grid Search would have you painstakingly check every spot on a predefined grid. Random Search would have you wander randomly, hoping to stumble upon the peak. Bayesian Optimization, however, is like having someone tell you the height after each step, and then using that information to decide the <em>next most promising step</em> to take.</p> <p>Bayesian Optimization builds a probabilistic model of the objective function (e.g., your model’s accuracy on the validation set) based on the hyperparameter combinations it has already tried and their resulting performance. This model, often called a <strong>surrogate model</strong> (commonly a Gaussian Process), estimates both the mean and uncertainty of the objective function across the entire hyperparameter space.</p> <p>It then uses an <strong>acquisition function</strong> to decide where to sample next. The acquisition function balances two things:</p> <ol> <li> <strong>Exploration</strong>: Trying hyperparameters in regions we haven’t explored much, where the uncertainty is high.</li> <li> <strong>Exploitation</strong>: Trying hyperparameters in regions that have historically shown good performance (low mean error, high mean accuracy).</li> </ol> <p>The most common acquisition function is <strong>Expected Improvement (EI)</strong>, which calculates the expected improvement over the best performance found so far, considering both the mean and variance of the surrogate model’s predictions.</p> <p><strong>How it works (conceptually):</strong></p> <ol> <li>Start with a few random hyperparameter evaluations (initial samples).</li> <li>Build a <strong>surrogate model</strong> (e.g., Gaussian Process) that approximates the true objective function using the results from previous evaluations.</li> <li>Use an <strong>acquisition function</strong> (e.g., Expected Improvement) to suggest the next hyperparameter combination to evaluate. This point is chosen because it’s estimated to be the most “promising” based on the surrogate model.</li> <li>Evaluate the model with these new hyperparameters.</li> <li>Add the new result to the history, update the surrogate model, and repeat from step 2 for a fixed number of iterations.</li> </ol> <p><strong>Pros:</strong></p> <ul> <li>Highly efficient: Can find better optima with significantly fewer evaluations than Grid or Random Search.</li> <li>Learns from past results to guide future searches.</li> <li>Well-suited for expensive-to-evaluate objective functions.</li> </ul> <p><strong>Cons:</strong></p> <ul> <li>More complex to implement than Grid/Random Search (though libraries like <code class="language-plaintext highlighter-rouge">scikit-optimize</code> (skopt), <code class="language-plaintext highlighter-rouge">Hyperopt</code>, <code class="language-plaintext highlighter-rouge">Optuna</code> make it much easier).</li> <li>Can be slow if the number of hyperparameters is very large, or if the surrogate model itself is complex to train.</li> <li>Sensitive to the choice of acquisition function and surrogate model.</li> </ul> <p>Bayesian Optimization has become my go-to for more complex problems or when computational resources are limited, but I need to squeeze out every bit of performance. It truly feels like a smart, guided quest rather than a blind one.</p> <h3 id="beyond-the-basics-what-else-is-out-there">Beyond the Basics: What Else is Out There?</h3> <p>While Grid, Random, and Bayesian Optimization cover most practical scenarios, it’s worth knowing there are other sophisticated techniques:</p> <ul> <li> <strong>Gradient-based Optimization</strong>: Applicable when hyperparameters are differentiable and the objective function can be optimized using gradient descent.</li> <li> <strong>Evolutionary Algorithms</strong>: Inspired by natural selection, these algorithms (like Genetic Algorithms) evolve a population of hyperparameter settings over generations.</li> <li> <strong>Hyperband/ASHA</strong>: Modern approaches designed for deep learning, focusing on early stopping of poor-performing configurations to speed up the search.</li> <li> <strong>Automated Machine Learning (AutoML)</strong>: An overarching field aiming to automate the entire ML pipeline, including hyperparameter tuning, model selection, and feature engineering. It promises to make ML more accessible and efficient.</li> </ul> <h3 id="practical-tips--my-takeaways">Practical Tips &amp; My Takeaways</h3> <p>My journey with hyperparameter tuning has taught me a few invaluable lessons:</p> <ol> <li> <strong>Start Simple</strong>: For initial exploration, Grid or Random Search are excellent. They are easy to use and often provide a good baseline.</li> <li> <strong>Define Sensible Ranges</strong>: Don’t guess wildly. Use domain knowledge, published papers, or previous experiments to define realistic and effective search spaces for your hyperparameters. For example, <code class="language-plaintext highlighter-rouge">learning_rate</code> usually falls between <code class="language-plaintext highlighter-rouge">1e-6</code> and <code class="language-plaintext highlighter-rouge">1.0</code>.</li> <li> <strong>Computational Awareness</strong>: Tuning can be very resource-intensive. Be mindful of how many models you’re training, especially with Grid Search. Distributed computing or cloud resources can be your best friend.</li> <li> <strong>Reproducibility</strong>: Always set random seeds (e.g., <code class="language-plaintext highlighter-rouge">random_state</code>) for both your model and your tuning process if using random components. This ensures your results can be replicated.</li> <li> <strong>Cross-Validation is Non-Negotiable</strong>: Never evaluate hyperparameter choices solely on your training data or your final test set. Cross-validation is your shield against overfitting during the tuning process.</li> <li> <strong>It’s an Iterative Process</strong>: You might start with a broad Random Search, identify promising regions, then perform a finer-grained Grid Search or Bayesian Optimization within those regions.</li> </ol> <h3 id="conclusion-embracing-the-tuners-mindset">Conclusion: Embracing the Tuner’s Mindset</h3> <p>Hyperparameter tuning is not just a technical step; it’s an art that requires understanding, patience, and strategic thinking. It’s the process that elevates your machine learning models from “good enough” to truly exceptional.</p> <p>My journey has transformed me from a hopeful button-masher into someone who approaches model optimization with systematic curiosity and powerful tools. It gives me a profound sense of control and understanding over my models.</p> <p>So, next time your model isn’t quite hitting the mark, don’t despair. Embrace the challenge of hyperparameter tuning. Dive into Grid Search, play with Random Search, and marvel at the intelligence of Bayesian Optimization. It’s a skill that will empower you to build smarter, more robust, and higher-performing machine learning systems. Happy tuning!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>