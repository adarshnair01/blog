<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> My Journey into Transformers: Unveiling the AI Architecture Behind ChatGPT | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/my-journey-into-transformers-unveiling-the-ai-arch/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">My Journey into Transformers: Unveiling the AI Architecture Behind ChatGPT</h1> <p class="post-meta"> Created on October 06, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> NLP</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/transformers"> <i class="fa-solid fa-hashtag fa-sm"></i> Transformers</a>   <a href="/blog/blog/tag/ai-architecture"> <i class="fa-solid fa-hashtag fa-sm"></i> AI Architecture</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>My first encounter with the world of Artificial Intelligence felt like stepping into a sci-fi movie. I remember marveling at how models could predict text or translate languages. But there was always a nagging question: how do they <em>really</em> understand the context, especially over long sentences?</p> <p>For a long time, the kings of sequential data processing were Recurrent Neural Networks (RNNs) and their more sophisticated cousins, LSTMs (Long Short-Term Memory networks). They processed information word by word, carrying a ‘memory’ of previous words. Imagine reading a book one word at a time, trying to remember the entire plot. It works, but it’s slow, and remembering details from the beginning of a very long book becomes incredibly difficult. This inability to capture long-range dependencies efficiently was a significant bottleneck.</p> <p>Then, in 2017, a paper dropped that changed everything: “Attention Is All You Need.” It introduced an entirely new architecture, the <strong>Transformer</strong>, that completely tossed out recurrence and convolutions, relying solely on a mechanism called <strong>attention</strong>. And just like that, the AI world pivoted. Today, Transformers are the backbone of almost every cutting-edge Natural Language Processing (NLP) model, from BERT to GPT-3 and, yes, even ChatGPT.</p> <p>So, let’s embark on this journey and unravel the genius of Transformers, piece by fascinating piece.</p> <h3 id="the-big-picture-encoder-decoder-architecture">The Big Picture: Encoder-Decoder Architecture</h3> <p>At its core, the original Transformer follows an <strong>Encoder-Decoder</strong> structure, much like many machine translation systems.</p> <ul> <li> <strong>Encoder:</strong> Takes an input sequence (e.g., an English sentence) and transforms it into a rich, contextual representation. Think of it as truly “understanding” the input.</li> <li> <strong>Decoder:</strong> Takes that contextual representation from the encoder and generates an output sequence (e.g., a French translation), one word at a time.</li> </ul> <p>Both the Encoder and Decoder are not single layers but rather <em>stacks</em> of identical blocks. The original paper used 6 identical encoders and 6 identical decoders stacked on top of each other.</p> <h3 id="the-foundation-embedding-and-positional-encoding">The Foundation: Embedding and Positional Encoding</h3> <p>Before any processing can begin, our words need to be converted into numbers that a neural network can understand. This is done via <strong>Word Embeddings</strong>, which convert each word into a dense vector (a list of numbers) representing its meaning. Words with similar meanings will have similar vectors.</p> <p>However, unlike RNNs, Transformers process all words in a sentence <em>simultaneously</em>. This means they lose the crucial information about word order. “Dog bites man” and “Man bites dog” have very different meanings! To compensate, Transformers introduce <strong>Positional Encoding</strong>.</p> <p>Each word embedding gets an additional vector added to it – the positional encoding – which carries information about the word’s position in the sequence. It’s like attaching a tiny GPS coordinate to each word. The original paper used sine and cosine functions of different frequencies to generate these unique position vectors:</p> <p>$PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{model}})$ $PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d_{model}})$</p> <p>Where:</p> <ul> <li>$pos$ is the word’s position in the sequence.</li> <li>$i$ is the dimension within the embedding vector.</li> <li>$d_{model}$ is the dimension of the embedding (and positional encoding) vectors.</li> </ul> <p>This clever approach allows the model to differentiate words based on their position, providing a sense of sequence without relying on recurrence.</p> <h3 id="the-encoder-diving-into-context">The Encoder: Diving into Context</h3> <p>Each encoder block consists of two main sub-layers:</p> <ol> <li><strong>Multi-Head Self-Attention Mechanism</strong></li> <li><strong>Position-wise Feed-Forward Network</strong></li> </ol> <p>Crucially, each sub-layer has a <strong>residual connection</strong> around it (adding the input of the sub-layer to its output) followed by <strong>layer normalization</strong>. Residual connections help with training deep networks by allowing gradients to flow more easily, while layer normalization stabilizes training and speeds convergence.</p> <h4 id="the-magic-of-self-attention">The Magic of Self-Attention</h4> <p>This is the beating heart of the Transformer. Self-attention allows a word to “look at” and “pay attention to” all other words in the input sequence to better understand its own meaning in context.</p> <p>Imagine the sentence: “The animal didn’t cross the street because it was too tired.” What does “it” refer to? An RNN would struggle, but self-attention can link “it” directly to “animal.”</p> <p>Here’s how it works: for each word, we create three vectors:</p> <ul> <li> <strong>Query (Q):</strong> What I’m looking for.</li> <li> <strong>Key (K):</strong> What I can offer.</li> <li> <strong>Value (V):</strong> My actual content.</li> </ul> <p>These are obtained by multiplying the word’s embedding (plus positional encoding) by three different weight matrices ($W^Q, W^K, W^V$) learned during training.</p> <p>To calculate the attention output for a word (let’s say “it”), its Query vector is dotted with the Key vectors of <em>all</em> other words (and itself) in the sentence. This dot product measures how relevant each other word is to “it.”</p> <p>Then, these scores are scaled down by $\sqrt{d_k}$ (where $d_k$ is the dimension of the Key vectors) to prevent very large values from pushing the softmax function into regions with tiny gradients. After scaling, a <strong>softmax</strong> function is applied, turning these raw scores into probabilities – indicating how much “attention” “it” should pay to each other word.</p> <p>Finally, these attention probabilities are multiplied by the Value vectors of all words and summed up. The result is a new, context-aware representation for “it.”</p> <p>Mathematically, for a set of queries $Q$, keys $K$, and values $V$: $Attention(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$</p> <p>This parallel computation, where each word simultaneously attends to all others, is a significant speedup over sequential RNNs.</p> <h4 id="multi-head-attention-seeing-from-different-angles">Multi-Head Attention: Seeing from Different Angles</h4> <p>If self-attention is like looking at a sentence, Multi-Head Attention is like putting on multiple pairs of glasses, each highlighting different aspects.</p> <p>Instead of performing one attention calculation, the input $Q, K, V$ are linearly projected $h$ times (e.g., 8 times in the original paper) into different, lower-dimensional representation spaces. Then, we perform $h$ independent attention calculations (the “heads”).</p> <p>$head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)$ $MultiHead(Q, K, V) = Concat(head_1, …, head_h)W^O$</p> <p>Each head can learn to focus on different types of relationships (e.g., one head might focus on syntactic relationships, another on semantic ones). The outputs from all these heads are then concatenated and linearly transformed into a single final vector. This enriches the model’s ability to capture diverse dependencies.</p> <h4 id="position-wise-feed-forward-networks">Position-wise Feed-Forward Networks</h4> <p>After the multi-head attention sub-layer, each position (word) in the sequence passes independently through an identical, simple fully connected feed-forward network. This network consists of two linear transformations with a ReLU activation in between:</p> <p>$FFN(x) = \text{max}(0, xW_1 + b_1)W_2 + b_2$</p> <p>This layer provides a point-wise, non-linear transformation that allows the model to process the attention-derived information further.</p> <h3 id="the-decoder-generating-output">The Decoder: Generating Output</h3> <p>The decoder stack is similar to the encoder but with a few crucial modifications, as its job is to <em>generate</em> a sequence. Each decoder block has three main sub-layers:</p> <ol> <li> <p><strong>Masked Multi-Head Self-Attention:</strong> This is like the encoder’s self-attention, but with a critical difference: <strong>masking</strong>. When generating a word, the decoder should only attend to the words it has <em>already generated</em> (and the input word itself). It cannot “cheat” by looking at future words in the target sequence. A mask is applied to the attention scores to block information from subsequent positions by setting their softmax input to negative infinity.</p> </li> <li> <p><strong>Encoder-Decoder Attention (Multi-Head Attention):</strong> This layer performs attention over the <em>output of the encoder stack</em>. Here, the Query comes from the <em>decoder’s previous layer</em>, while the Keys and Values come from the <em>encoder’s final output</em>. This allows the decoder to focus on relevant parts of the input sequence when generating each output word, providing the necessary contextual information from the source.</p> </li> <li> <p><strong>Position-wise Feed-Forward Network:</strong> Identical to the one in the encoder.</p> </li> </ol> <p>Again, residual connections and layer normalization are applied after each sub-layer.</p> <h3 id="the-output-layer">The Output Layer</h3> <p>The final output of the decoder stack is a vector for each position in the output sequence. This vector is fed into a final linear layer, which projects it into a much larger vector where each dimension corresponds to a word in the model’s vocabulary. Finally, a <strong>softmax</strong> function is applied to turn these scores into probabilities, and the word with the highest probability is selected as the output.</p> <h3 id="why-transformers-are-revolutionary">Why Transformers are Revolutionary</h3> <ol> <li> <strong>Parallelization:</strong> Unlike RNNs, which process sequentially, self-attention allows all words to be processed simultaneously. This dramatically speeds up training on modern hardware (GPUs).</li> <li> <strong>Long-Range Dependencies:</strong> Self-attention can directly connect any two words in a sentence, regardless of their distance. This solves the long-standing problem of capturing long-range dependencies that plagued RNNs.</li> <li> <strong>Transfer Learning Powerhouse:</strong> The Transformer architecture’s ability to learn rich, contextual representations of words has made it ideal for pre-training on massive text corpora (like BERT or GPT). These pre-trained models can then be fine-tuned for a variety of downstream NLP tasks with minimal data, leading to state-of-the-art performance across the board.</li> </ol> <h3 id="the-impact-and-beyond">The Impact and Beyond</h3> <p>The Transformer architecture didn’t just solve a few problems; it ignited a revolution. From Google Search to conversational AI like ChatGPT, image generation, and even protein folding prediction, Transformers (or variants of them) are at the heart of many of the most impressive AI achievements today.</p> <p>This journey into Transformers might seem complex, but understanding these foundational concepts unlocks a deeper appreciation for the intelligence we see in modern AI. The shift from sequential processing to an attention-driven, parallel paradigm truly changed the game, proving that sometimes, “Attention Is All You Need.”</p> <p>If you found this fascinating, I encourage you to dive deeper! Read the original “Attention Is All You Need” paper, explore implementations in PyTorch or TensorFlow, and experiment with pre-trained Transformer models. The world of AI is moving at lightning speed, and understanding its core mechanisms is the first step to building the future.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>