<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Support Vector Machines: Mastering the Art of Drawing the Best Line in Data | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/support-vector-machines-mastering-the-art-of-drawi/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Support Vector Machines: Mastering the Art of Drawing the Best Line in Data</h1> <p class="post-meta"> Created on January 18, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/svm"> <i class="fa-solid fa-hashtag fa-sm"></i> SVM</a>   <a href="/blog/blog/tag/classification"> <i class="fa-solid fa-hashtag fa-sm"></i> Classification</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/algorithms"> <i class="fa-solid fa-hashtag fa-sm"></i> Algorithms</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hello, fellow data explorers!</p> <p>Today, I want to share a journey into one of my favorite machine learning algorithms: <strong>Support Vector Machines (SVMs)</strong>. When I first encountered SVMs, I was immediately struck by their elegance and sheer power. They don’t just draw a line to separate data; they draw the <em>best possible</em> line, with a clever trick up their sleeve for when a line simply won’t do.</p> <p>Think of it like this: you’re trying to separate apples from oranges on a table. Most of the time, you can draw a clear line. But what if some apples are mixed with oranges, or what if the apples are in the middle and oranges are around them? SVMs have a sophisticated approach for all these scenarios.</p> <p>Let’s dive in!</p> <h3 id="the-core-idea-finding-the-best-separation">The Core Idea: Finding the Best Separation</h3> <p>At its heart, an SVM is a <strong>discriminative classifier</strong>. This means it tries to find a boundary (or a “hyperplane,” as we’ll call it) that separates data points belonging to different classes. Imagine you have a scatter plot of data points, some labeled ‘Class A’ (e.g., healthy cells) and others ‘Class B’ (e.g., cancerous cells). Your goal is to draw a line that best separates these two groups.</p> <p>Now, if the data is <em>linearly separable</em> (meaning you <em>can</em> draw a single straight line to separate them), you might think there are many such lines. And you’d be right! But which one is the <em>best</em>? This is where SVMs shine.</p> <h4 id="the-magic-of-the-maximum-margin-hyperplane">The Magic of the “Maximum Margin Hyperplane”</h4> <p>An SVM doesn’t just draw <em>any</em> line; it draws the line that maximizes the <strong>margin</strong> between the two classes. What’s a margin?</p> <p>Imagine that separating line is a road. The margin is the width of the empty space on either side of this road, up to the closest data points from each class. The data points that are closest to the hyperplane and essentially “define” this margin are called <strong>support vectors</strong>. They are the most crucial points in your dataset for determining the separation boundary.</p> <p>Why maximize this margin?</p> <ol> <li> <strong>Robustness:</strong> A wider margin means the classifier is more robust. If new, unseen data comes in that’s slightly different from your training data, it’s more likely to be classified correctly if there’s a wider “buffer zone.”</li> <li> <strong>Generalization:</strong> A wider margin generally leads to better generalization performance on unseen data. It prevents the model from being overly sensitive to individual data points.</li> </ol> <p>Let’s formalize this a little.</p> <h4 id="the-math-behind-the-margin">The Math Behind the Margin</h4> <p>In a 2-dimensional space, our separating boundary is a line. In a 3-dimensional space, it’s a plane. In higher dimensions (which our data often lives in), we call it a <strong>hyperplane</strong>.</p> <p>A hyperplane can be represented by the equation: \(w \cdot x + b = 0\) Where:</p> <ul> <li>$w$ is a vector perpendicular to the hyperplane (its “normal vector”).</li> <li>$x$ is a data point (a vector).</li> <li>$b$ is a scalar bias term.</li> </ul> <p>Our goal is to find $w$ and $b$ such that the hyperplane correctly classifies data points and maximizes the margin.</p> <p>Consider the support vectors. For the positive class (let’s say $y_i = +1$), the support vectors will lie on a parallel hyperplane defined by: \(w \cdot x_i + b = +1\) And for the negative class (let’s say $y_i = -1$), they will lie on a parallel hyperplane defined by: \(w \cdot x_i + b = -1\)</p> <table> <tbody> <tr> <td>The distance between these two parallel hyperplanes (which defines our margin) is $\frac{2}{</td> <td> </td> <td>w</td> <td> </td> <td>}$. To maximize this distance, we need to **minimize $</td> <td> </td> <td>w</td> <td> </td> <td>$**. For mathematical convenience (and because it makes the optimization problem convex), we usually minimize $\frac{1}{2}</td> <td> </td> <td>w</td> <td> </td> <td>^2$.</td> </tr> </tbody> </table> <p>So, the optimization problem for a <strong>Linear SVM (Hard Margin)</strong> looks like this:</p> <p>Minimize: \(\frac{1}{2} ||w||^2\) Subject to the constraints: \(y_i (w \cdot x_i + b) \ge 1 \quad \text{for all } i = 1, \dots, N\) This constraint ensures that every data point is on the correct side of its respective margin hyperplane. If $y_i = +1$, then $w \cdot x_i + b$ must be $\ge 1$. If $y_i = -1$, then $w \cdot x_i + b$ must be $\le -1$. Combining these with $y_i$ handles both cases efficiently.</p> <h3 id="dealing-with-real-world-imperfections-the-soft-margin-svm">Dealing with Real-World Imperfections: The Soft Margin SVM</h3> <p>The “hard margin” SVM we just discussed is beautiful, but it assumes your data is <em>perfectly</em> linearly separable. In the real world, this is rarely the case. Datasets often have noise, outliers, or overlapping classes. If we insist on a perfect separation, the hard margin SVM might not find a solution, or it might create a hyperplane that is overly sensitive to outliers, leading to poor generalization.</p> <p>This is where the <strong>Soft Margin SVM</strong> comes in. It introduces a bit of tolerance for misclassification or for points falling within the margin. It does this by introducing <strong>slack variables</strong> ($\xi_i$, pronounced “ksi”).</p> <p>Each $\xi_i \ge 0$ measures how much a data point $x_i$ violates the margin constraint:</p> <ul> <li>If $\xi_i = 0$, the point is correctly classified and outside the margin.</li> <li>If $0 &lt; \xi_i &lt; 1$, the point is correctly classified but lies within the margin.</li> <li>If $\xi_i \ge 1$, the point is misclassified.</li> </ul> <p>The optimization problem now becomes:</p> <p>Minimize: \(\frac{1}{2} ||w||^2 + C \sum\_{i=1}^{N} \xi_i\) Subject to the constraints: \(y_i (w \cdot x_i + b) \ge 1 - \xi_i \quad \text{for all } i = 1, \dots, N\) \(\xi_i \ge 0 \quad \text{for all } i = 1, \dots, N\)</p> <table> <tbody> <tr> <td>Here, $C$ is a crucial hyperparameter (a tuning knob for our model). It controls the trade-off between maximizing the margin (minimizing $</td> <td> </td> <td>w</td> <td> </td> <td>^2$) and minimizing the classification errors (minimizing $\sum \xi_i$).</td> </tr> </tbody> </table> <ul> <li>A <strong>small $C$</strong> allows for a larger margin but potentially more misclassifications (underfitting).</li> <li>A <strong>large $C$</strong> enforces a smaller margin to reduce misclassifications (potential overfitting).</li> </ul> <p>Choosing the right $C$ is often done through techniques like cross-validation.</p> <h3 id="beyond-the-line-the-kernel-trick-for-non-linear-data">Beyond the Line: The Kernel Trick for Non-Linear Data</h3> <p>This is arguably the most powerful and “magical” aspect of SVMs. What if your data isn’t even remotely linearly separable? Think of a dataset where positive examples form a circle in the middle, and negative examples are all around it. No straight line can separate them.</p> <p>Here’s the genius of the <strong>Kernel Trick</strong>: Instead of trying to find a linear boundary in the original low-dimensional space, we implicitly map our data into a much higher-dimensional feature space where it <em>becomes</em> linearly separable. Then, we find a hyperplane in that higher-dimensional space. When we project that hyperplane back down to our original space, it appears as a non-linear boundary!</p> <p>The “trick” part is that we don’t actually need to compute the coordinates of the data points in this high-dimensional space. We only need to calculate the <strong>dot product</strong> between pairs of data points in that higher dimension. A <strong>kernel function</strong> $K(x_i, x_j)$ is simply a function that computes this dot product for us in the original input space, without ever explicitly performing the mapping $\phi(x)$ to the higher-dimensional space.</p> \[K(x_i, x_j) = \phi(x_i) \cdot \phi(x_j)\] <p>This allows SVMs to find complex non-linear decision boundaries efficiently.</p> <p>Common Kernel Functions:</p> <ol> <li> <p><strong>Linear Kernel:</strong> This is the simplest, essentially the same as a linear SVM. \(K(x_i, x_j) = x_i \cdot x_j\) This is used when your data is (or is assumed to be) linearly separable.</p> </li> <li> <p><strong>Polynomial Kernel:</strong> Allows for curved decision boundaries. \(K(x_i, x_j) = (\gamma (x_i \cdot x_j) + r)^d\) Where $d$ is the degree of the polynomial, $\gamma$ is a scaling factor, and $r$ is a constant.</p> </li> <li> <p><strong>Radial Basis Function (RBF) / Gaussian Kernel:</strong> This is one of the most popular and powerful kernels. It can map data into an infinite-dimensional space and is very flexible for complex, non-linear patterns. \(K(x_i, x_j) = \exp(-\gamma ||x_i - x_j||^2)\) Here, $\gamma$ (gamma) is another crucial hyperparameter that defines the “reach” of a single training example. A small $\gamma$ means a large radius, and vice versa.</p> </li> </ol> <p>The choice of kernel and its associated hyperparameters (like $d$ for polynomial or $\gamma$ for RBF) is critical and often determined through experimentation and cross-validation.</p> <h3 id="advantages-of-svms">Advantages of SVMs</h3> <ul> <li> <strong>Effective in high-dimensional spaces:</strong> SVMs perform well even when the number of features is greater than the number of samples.</li> <li> <strong>Memory efficient:</strong> They only use a subset of training points (the support vectors) in the decision function, making them memory efficient.</li> <li> <strong>Versatile with kernels:</strong> Different kernel functions allow SVMs to handle a wide variety of datasets and decision boundary shapes.</li> <li> <strong>Robust to outliers (with soft margin):</strong> The $C$ parameter allows for a graceful handling of noisy data.</li> </ul> <h3 id="disadvantages-of-svms">Disadvantages of SVMs</h3> <ul> <li> <strong>Computationally intensive:</strong> Training can be slow on very large datasets, especially without a good optimization strategy.</li> <li> <strong>Sensitivity to feature scaling:</strong> SVMs are sensitive to the scaling of features. It’s often necessary to normalize or standardize your data before training an SVM.</li> <li> <strong>Choosing the right kernel and hyperparameters:</strong> This can be tricky and requires expertise and experimentation.</li> <li> <strong>Less intuitive probability estimates:</strong> Unlike logistic regression, SVMs don’t directly provide probability estimates for class membership, although methods exist to approximate them.</li> </ul> <h3 id="real-world-applications">Real-World Applications</h3> <p>SVMs are not just theoretical constructs; they are widely used in various domains:</p> <ul> <li> <strong>Image Classification:</strong> Identifying objects, faces, or even medical images.</li> <li> <strong>Text Classification:</strong> Spam detection, sentiment analysis, categorizing documents.</li> <li> <strong>Bioinformatics:</strong> Protein classification, gene expression analysis.</li> <li> <strong>Handwriting Recognition:</strong> Recognizing digits and characters.</li> </ul> <h3 id="my-personal-takeaway">My Personal Takeaway</h3> <p>Learning about SVMs felt like unlocking a new level of understanding in machine learning. It’s a testament to how elegant mathematical ideas can translate into incredibly powerful tools for solving real-world problems. The combination of clear geometric intuition (the margin), robust handling of imperfections (soft margin), and the sheer genius of the kernel trick makes SVMs a cornerstone algorithm in any data scientist’s toolkit.</p> <p>So, the next time you hear about classifying complex data, remember the Support Vector Machine, quietly working to draw the best possible line, or curve, or whatever boundary is needed to bring order to data’s beautiful chaos.</p> <p>Keep exploring, keep learning, and happy classifying!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>