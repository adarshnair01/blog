<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Beyond the Raw: Unlocking Model Power with Feature Engineering | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/beyond-the-raw-unlocking-model-power-with-feature/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Beyond the Raw: Unlocking Model Power with Feature Engineering</h1> <p class="post-meta"> Created on December 18, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/feature-engineering"> <i class="fa-solid fa-hashtag fa-sm"></i> Feature Engineering</a>   <a href="/blog/blog/tag/data-preprocessing"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Preprocessing</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="beyond-the-raw-unlocking-model-power-with-feature-engineering">Beyond the Raw: Unlocking Model Power with Feature Engineering</h2> <p>Hey everyone!</p> <p>As I delve deeper into the exciting realm of Data Science and Machine Learning, one concept keeps popping up as <em>absolutely critical</em> for building effective models: <strong>Feature Engineering</strong>. It’s often called an “art” as much as a “science,” and after wrestling with countless datasets, I totally get why. It’s where you roll up your sleeves and get truly creative with your data.</p> <p>Think of it this way: when you’re making a gourmet meal, you don’t just dump all the raw ingredients into a pot and hope for the best. You chop, dice, marinate, sauté, and season. Each step transforms the raw ingredients into something more palatable, more flavorful, and ultimately, a better dish.</p> <p>Feature Engineering is precisely that for your data. It’s the process of transforming raw data into features that better represent the underlying problem to predictive models, thereby improving model accuracy and often its interpretability.</p> <h3 id="why-bother-the-unsung-hero-of-model-performance">Why Bother? The Unsung Hero of Model Performance</h3> <p>You might be thinking, “Can’t the model just figure it out from the raw data?” Sometimes, yes, especially with deep learning models and massive datasets. But for most real-world scenarios, particularly with tabular data or when computational resources are limited, <strong>feature engineering is a game-changer.</strong></p> <p>Here’s why it’s such a big deal:</p> <ol> <li> <strong>Improved Model Performance:</strong> This is the big one. Well-engineered features provide your model with clearer signals, leading to higher accuracy, precision, recall, F1-score, or whatever metric you’re optimizing for. It’s like giving your model a super-powered magnifying glass instead of blurry spectacles.</li> <li> <strong>Better Interpretability:</strong> When you create features that directly capture meaningful aspects of the problem (e.g., “age group” instead of raw “age”), it often makes the model’s decisions easier to understand and explain.</li> <li> <strong>Reduced Overfitting:</strong> By focusing on the most relevant information and sometimes simplifying complex relationships, feature engineering can help models generalize better to unseen data.</li> <li> <strong>Handling Data Issues:</strong> It allows us to prepare and structure data for algorithms that have specific input requirements (e.g., numerical inputs for linear models). This includes dealing with categorical variables, missing values, and skewed distributions.</li> <li> <strong>Faster Training:</strong> A more concise and informative feature set can sometimes lead to faster convergence during model training.</li> </ol> <h3 id="the-how-a-toolkit-for-transformation">The “How”: A Toolkit for Transformation</h3> <p>Feature engineering isn’t a single technique; it’s a broad spectrum of methods. Let’s explore some common ones across different data types.</p> <h4 id="1-numerical-features-shaping-the-numbers">1. Numerical Features: Shaping the Numbers</h4> <p>Numerical data often needs a little massaging to be most effective.</p> <ul> <li> <strong>Scaling and Normalization:</strong> Many algorithms (like Gradient Descent-based models, SVMs, or K-Nearest Neighbors) are sensitive to the scale of input features. Features with larger ranges can disproportionately influence the model. <ul> <li> <strong>Min-Max Scaling:</strong> Scales features to a fixed range, usually $[0, 1]$. \(x' = \frac{x - x*{\min}}{x*{\max} - x\_{\min}}\) This is useful when you want to preserve the relative relationships between values within the feature.</li> <li> <strong>Standardization (Z-score normalization):</strong> Transforms data to have a mean of 0 and a standard deviation of 1. \(x' = \frac{x - \mu}{\sigma}\) This is often preferred when the data has outliers or when algorithms assume normally distributed data.</li> </ul> </li> <li> <p><strong>Discretization (Binning):</strong> Converting continuous numerical features into discrete categories or bins. For example, <code class="language-plaintext highlighter-rouge">Age</code> (continuous) can become <code class="language-plaintext highlighter-rouge">Age_Group</code> (e.g., ‘Child’, ‘Teen’, ‘Adult’, ‘Senior’). This can help to capture non-linear relationships or reduce the impact of small fluctuations.</p> </li> <li> <p><strong>Polynomial Features:</strong> Creating new features by raising existing features to a power (e.g., $x^2, x^3$) or creating interaction terms (e.g., $x_1 x_2$). This can capture non-linear relationships between features and the target variable. Imagine predicting house prices; <code class="language-plaintext highlighter-rouge">Lot_Size</code> might have a non-linear impact, or <code class="language-plaintext highlighter-rouge">Lot_Size * Number_of_Rooms</code> might be a powerful predictor.</p> </li> <li> <strong>Log Transformation:</strong> Applying a logarithmic function (e.g., $\log(x)$ or $\ln(x)$) to highly skewed numerical features. This can make distributions more symmetrical (closer to normal), which helps algorithms that assume normality and reduces the impact of extreme values.</li> </ul> <h4 id="2-categorical-features-making-sense-of-labels">2. Categorical Features: Making Sense of Labels</h4> <p>Categorical data (like <code class="language-plaintext highlighter-rouge">City</code>, <code class="language-plaintext highlighter-rouge">Gender</code>, <code class="language-plaintext highlighter-rouge">Product_Type</code>) needs special handling because models primarily work with numbers.</p> <ul> <li> <strong>One-Hot Encoding:</strong> This is one of the most common techniques. It converts each category value into a new binary (0 or 1) feature column. For example, if you have a <code class="language-plaintext highlighter-rouge">Color</code> feature with values ‘Red’, ‘Blue’, ‘Green’, it creates three new columns: <code class="language-plaintext highlighter-rouge">Color_Red</code>, <code class="language-plaintext highlighter-rouge">Color_Blue</code>, <code class="language-plaintext highlighter-rouge">Color_Green</code>. <ul> <li> <code class="language-plaintext highlighter-rouge">Color_Red</code> would be 1 if the original color was ‘Red’, 0 otherwise. This prevents the model from assuming an ordinal relationship between categories (e.g., ‘Red’ &gt; ‘Blue’).</li> </ul> </li> <li> <p><strong>Label Encoding:</strong> Assigns a unique integer to each category (e.g., ‘Red’: 0, ‘Blue’: 1, ‘Green’: 2). This is suitable for <em>ordinal</em> categorical data, where there’s an inherent order (e.g., ‘Small’, ‘Medium’, ‘Large’). If used on non-ordinal data, it can mislead the model into assuming an ordered relationship that doesn’t exist.</p> </li> <li> <strong>Target Encoding (Mean Encoding):</strong> A more advanced technique, especially useful for high-cardinality categorical features (features with many unique categories). It replaces each category with the mean of the target variable for that category. For example, for a <code class="language-plaintext highlighter-rouge">City</code> feature, ‘New York’ might be replaced by the average house price in New York. <em>Crucially, this must be done carefully to avoid data leakage (using information from the target variable that wouldn’t be available at prediction time).</em> </li> </ul> <h4 id="3-date-and-time-features-unearthing-temporal-patterns">3. Date and Time Features: Unearthing Temporal Patterns</h4> <p>Dates and times are a goldmine for features. Don’t just treat a timestamp as a string or a number; extract its rich context!</p> <ul> <li> <strong>Extracting Components:</strong> From a single timestamp, you can derive: <ul> <li> <code class="language-plaintext highlighter-rouge">Year</code>, <code class="language-plaintext highlighter-rouge">Month</code>, <code class="language-plaintext highlighter-rouge">Day</code>, <code class="language-plaintext highlighter-rouge">Day_of_Week</code>, <code class="language-plaintext highlighter-rouge">Day_of_Year</code>, <code class="language-plaintext highlighter-rouge">Hour</code>, <code class="language-plaintext highlighter-rouge">Minute</code>, <code class="language-plaintext highlighter-rouge">Second</code>.</li> <li> <code class="language-plaintext highlighter-rouge">Is_Weekend</code> (Boolean), <code class="language-plaintext highlighter-rouge">Is_Holiday</code> (Boolean).</li> <li> <code class="language-plaintext highlighter-rouge">Quarter_of_Year</code>.</li> </ul> </li> <li> <strong>Time Differences:</strong> Calculate the time elapsed since a specific event (e.g., <code class="language-plaintext highlighter-rouge">Days_Since_Last_Purchase</code>).</li> <li> <strong>Cyclical Features:</strong> For features like <code class="language-plaintext highlighter-rouge">Day_of_Year</code> or <code class="language-plaintext highlighter-rouge">Hour_of_Day</code>, using sine and cosine transformations can help the model understand their cyclical nature without imposing an artificial start/end. <ul> <li>$ \text{sin}(\text{2}\pi \times \text{day_of_year / 365}) $</li> <li>$ \text{cos}(\text{2}\pi \times \text{day_of_year / 365}) $</li> </ul> </li> </ul> <h4 id="4-text-features-from-words-to-vectors">4. Text Features: From Words to Vectors</h4> <p>When dealing with text data, we need to convert words into numerical representations.</p> <ul> <li> <strong>Bag-of-Words (BoW):</strong> Counts the frequency of each word in a document. It loses word order but captures word presence.</li> <li> <strong>TF-IDF (Term Frequency-Inverse Document Frequency):</strong> Weighs word counts by how rare they are across all documents. This helps to highlight important words that are specific to a document.</li> <li> <strong>Word Embeddings:</strong> More advanced techniques (like Word2Vec, GloVe, FastText) represent words as dense vectors in a continuous vector space, capturing semantic relationships.</li> </ul> <h4 id="5-domain-specific-features-the-power-of-expertise">5. Domain-Specific Features: The Power of Expertise</h4> <p>This is often where the “art” comes in. Leveraging domain knowledge – understanding the problem you’re trying to solve and the data you have – is invaluable.</p> <ul> <li> <strong>Ratios:</strong> Creating ratios like <code class="language-plaintext highlighter-rouge">Revenue_per_Customer</code> or <code class="language-plaintext highlighter-rouge">Profit_Margin</code> (from <code class="language-plaintext highlighter-rouge">Revenue</code> and <code class="language-plaintext highlighter-rouge">Cost</code>).</li> <li> <strong>Combinations:</strong> Combining existing features in meaningful ways. For a health dataset, calculating <code class="language-plaintext highlighter-rouge">BMI</code> from <code class="language-plaintext highlighter-rouge">Weight</code> and <code class="language-plaintext highlighter-rouge">Height</code> ($ \text{BMI} = \text{weight} / (\text{height})^2 $) is a classic example.</li> <li> <strong>Aggregations:</strong> For time-series or group-based data, calculating <code class="language-plaintext highlighter-rouge">mean</code>, <code class="language-plaintext highlighter-rouge">sum</code>, <code class="language-plaintext highlighter-rouge">min</code>, <code class="language-plaintext highlighter-rouge">max</code>, <code class="language-plaintext highlighter-rouge">count</code>, <code class="language-plaintext highlighter-rouge">std_dev</code> over specific windows or groups. For instance, <code class="language-plaintext highlighter-rouge">Average_Transactions_Last_7_Days</code>.</li> </ul> <h3 id="the-art-vs-the-science">The Art vs. The Science</h3> <p>Feature engineering is a blend:</p> <ul> <li> <strong>The Science:</strong> This involves statistical analysis, data visualization (histograms, scatter plots, correlation matrices), and using established techniques. We look for statistical relationships, distributions, and patterns. Tools like Pandas for data manipulation and Scikit-learn’s preprocessing modules are our scientific instruments.</li> <li> <strong>The Art:</strong> This is where creativity, intuition, and deep domain understanding shine. It’s about asking “what if?” and “what else could describe this?” It’s often an iterative, experimental process. You might try several feature ideas, evaluate their impact on your model, and refine. It’s the moment you stop just processing data and start thinking like the data itself.</li> </ul> <h3 id="practical-tips-and-best-practices">Practical Tips and Best Practices</h3> <ol> <li> <strong>Visualize, Visualize, Visualize!</strong> Before you engineer anything, look at your data. Histograms, scatter plots, box plots, and correlation matrices can reveal hidden patterns, outliers, and distributions that spark feature ideas.</li> <li> <strong>Start Simple:</strong> Don’t jump to complex feature interactions right away. Often, a few well-chosen simple features can provide a significant boost. Build iteratively.</li> <li> <strong>Leverage Domain Knowledge:</strong> Talk to experts in the field the data comes from. They often have insights into what truly drives the outcome. What questions would <em>they</em> ask?</li> <li> <strong>Avoid Data Leakage:</strong> This is crucial! Ensure that the features you create for your training data could also be created for new, unseen data at prediction time. For example, don’t use the target variable’s mean during feature creation <em>before</em> splitting your data into train/test sets, as this would bake future information into your training features.</li> <li> <strong>Iterate and Experiment:</strong> Feature engineering is rarely a one-shot deal. Try ideas, evaluate, and refine. Keep a log of what you tried and its impact.</li> <li> <strong>Don’t Forget Feature Selection:</strong> After engineering a plethora of features, you might end up with too many, some redundant or irrelevant. Feature selection techniques (e.g., recursive feature elimination, permutation importance) help you pick the best ones.</li> </ol> <h3 id="conclusion">Conclusion</h3> <p>Feature engineering is more than just a step in the data science pipeline; it’s a mindset. It’s about understanding your data deeply, thinking creatively, and transforming raw information into actionable insights for your models. While advanced algorithms and powerful computing get a lot of glory, a well-engineered feature set can often outperform complex models fed with raw, unoptimized data.</p> <p>So, the next time you’re faced with a dataset, don’t just jump straight to model training. Take a moment. Look at your data. Ask yourself: “What meaningful patterns can I extract? How can I help my model see what I see?” The answers to these questions are the beginning of powerful feature engineering, and they will undoubtedly elevate your data science projects from good to truly exceptional.</p> <p>Happy engineering!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>