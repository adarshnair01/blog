<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Unlocking Model Potential: The Art and Science of Feature Engineering | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/unlocking-model-potential-the-art-and-science-of-f/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Unlocking Model Potential: The Art and Science of Feature Engineering</h1> <p class="post-meta"> Created on June 07, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/feature-engineering"> <i class="fa-solid fa-hashtag fa-sm"></i> Feature Engineering</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   <a href="/blog/blog/tag/predictive-modeling"> <i class="fa-solid fa-hashtag fa-sm"></i> Predictive Modeling</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey there, fellow data enthusiast!</p> <p>Have you ever looked at a perfectly cooked meal and thought, “Wow, this chef is a genius”? Well, in the world of data science, we have our own kind of culinary magic. It’s not about transforming raw ingredients into a delicious dish, but about transforming raw data into something far more digestible and powerful for our machine learning models. We call this <strong>Feature Engineering</strong>.</p> <h3 id="my-aha-moment-with-raw-data">My Aha! Moment with Raw Data</h3> <p>I remember when I first started my journey into machine learning. I was so focused on learning fancy algorithms – linear regression, decision trees, neural networks – thinking that the “algorithm” was the be-all and end-all of model performance. I’d feed my model some raw CSV file, hit “train,” and then scratch my head when the results were, shall we say, less than stellar.</p> <p>It felt like trying to build a complex LEGO set by just dumping all the bricks on the floor and hoping they’d magically assemble themselves. My models were struggling, not because the algorithms were bad, but because the “ingredients” – my raw data – weren’t prepared in a way that made sense for them.</p> <p>That’s when I had my <em>aha!</em> moment. I realized that machine learning isn’t just about choosing the right recipe (algorithm); it’s equally, if not more, about preparing the ingredients in the best possible way. This, my friends, is the essence of Feature Engineering.</p> <h3 id="what-is-feature-engineering-really">What <em>Is</em> Feature Engineering, Really?</h3> <p>In simple terms, <strong>Feature Engineering is the process of using domain knowledge to create new features (variables) from existing raw data to help a machine learning model perform better.</strong></p> <p>Think of it this way: Imagine you’re trying to predict if a student will pass an exam. You have raw data like their <code class="language-plaintext highlighter-rouge">attendance_percentage</code>, <code class="language-plaintext highlighter-rouge">hours_studied_last_week</code>, and <code class="language-plaintext highlighter-rouge">date_of_birth</code>.</p> <p>Now, consider these “engineered” features:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">age_at_exam</code> (derived from <code class="language-plaintext highlighter-rouge">date_of_birth</code> and <code class="language-plaintext highlighter-rouge">exam_date</code>)</li> <li> <code class="language-plaintext highlighter-rouge">study_intensity</code> (e.g., <code class="language-plaintext highlighter-rouge">hours_studied_last_week</code> / <code class="language-plaintext highlighter-rouge">total_courses</code>)</li> <li> <code class="language-plaintext highlighter-rouge">attended_all_classes</code> (a binary flag based on <code class="language-plaintext highlighter-rouge">attendance_percentage</code>)</li> </ul> <p>Which set of features do you think will give your model a clearer picture? The engineered ones, right? They’re more directly related to the outcome you’re trying to predict. They give context and meaning to the raw numbers.</p> <p>The goal is to turn abstract data into concrete information that the model can readily understand and leverage. It’s about making your data’s story more compelling and easier for the model to follow.</p> <h3 id="why-does-it-matter-so-much-the-garbage-in-garbage-out-principle">Why Does It Matter So Much? The “Garbage In, Garbage Out” Principle</h3> <p>You’ve probably heard the phrase “Garbage In, Garbage Out.” In data science, this is incredibly accurate. Even the most sophisticated deep learning model will struggle if fed poor-quality or irrelevant features.</p> <p><strong>Here’s why Feature Engineering is paramount:</strong></p> <ol> <li> <strong>Improved Model Accuracy:</strong> Well-engineered features directly translate to better predictions. They allow the model to find patterns that were hidden in the raw data.</li> <li> <strong>Enhanced Model Interpretability:</strong> When you create features that are meaningful (e.g., <code class="language-plaintext highlighter-rouge">age_of_house</code> instead of <code class="language-plaintext highlighter-rouge">year_built</code>), it’s easier to understand <em>why</em> your model makes certain predictions.</li> <li> <strong>Reduced Overfitting:</strong> Sometimes, models can memorize the training data too well (overfitting). By creating more generalized and robust features, we can help the model learn the true underlying patterns rather than noise.</li> <li> <strong>Faster Training:</strong> A smaller, more relevant set of features can often lead to faster model training times without sacrificing performance.</li> <li> <strong>Unlocking Hidden Relationships:</strong> Raw features might have complex, non-linear relationships with the target variable. Engineered features can simplify these relationships, making them easier for the model to learn.</li> </ol> <h3 id="getting-our-hands-dirty-common-feature-engineering-techniques">Getting Our Hands Dirty: Common Feature Engineering Techniques</h3> <p>Let’s dive into some practical examples across different data types.</p> <h4 id="1-numerical-features-the-numbers-game">1. Numerical Features: The Numbers Game</h4> <p>Numerical data is often straightforward, but we can make it even better.</p> <ul> <li> <strong>Scaling (Normalization/Standardization):</strong> Many algorithms (like K-Nearest Neighbors, Support Vector Machines, Neural Networks) are sensitive to the scale of features. Features with larger ranges can dominate distance calculations. <ul> <li> <strong>Min-Max Scaling:</strong> Rescales a feature to a fixed range, usually between 0 and 1. $X_{scaled} = \frac{X - X_{min}}{X_{max} - X_{min}}$ <em>Example</em>: If ages range from 10 to 80, an age of 45 becomes $(45-10)/(80-10) = 35/70 = 0.5$.</li> <li> <strong>Standardization (Z-score Scaling):</strong> Rescales a feature to have a mean of 0 and a standard deviation of 1. $X_{scaled} = \frac{X - \mu}{\sigma}$ <em>Example</em>: If a feature has a mean ($\mu$) of 100 and standard deviation ($\sigma$) of 10, a value of 120 becomes $(120-100)/10 = 2$.</li> </ul> </li> <li> <strong>Binning (Discretization):</strong> Converting continuous numerical data into discrete categories (bins). This can help with noisy data or handle non-linear relationships. <ul> <li> <em>Example</em>: Instead of <code class="language-plaintext highlighter-rouge">age</code> (continuous), create <code class="language-plaintext highlighter-rouge">age_group</code> (e.g., “Child,” “Teen,” “Adult,” “Senior”).</li> <li> <em>Why</em>: Can make models more robust to small changes or outliers, and capture non-linearities.</li> </ul> </li> <li> <strong>Polynomial Features:</strong> Creating new features by raising existing features to a power. This helps capture non-linear relationships. <ul> <li> <em>Example</em>: If you have <code class="language-plaintext highlighter-rouge">square_footage</code>, you might create <code class="language-plaintext highlighter-rouge">square_footage^2</code> and <code class="language-plaintext highlighter-rouge">square_footage^3</code>. A linear model could then fit a curve: $y = \beta_0 + \beta_1 \cdot \text{sq_ft} + \beta_2 \cdot \text{sq_ft}^2$</li> <li> <em>Why</em>: Allows linear models to fit more complex, curved patterns in the data.</li> </ul> </li> <li> <strong>Interaction Features:</strong> Creating new features by multiplying two or more existing features. This captures how features might interact with each other. <ul> <li> <em>Example</em>: For predicting house prices, <code class="language-plaintext highlighter-rouge">square_footage * num_bedrooms</code> might be a good interaction feature, as it indicates space per room, which could be more informative than <code class="language-plaintext highlighter-rouge">square_footage</code> and <code class="language-plaintext highlighter-rouge">num_bedrooms</code> independently.</li> <li> <em>Why</em>: Represents a synergistic effect where the impact of one feature depends on the value of another. $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2$</li> </ul> </li> </ul> <h4 id="2-categorical-features-giving-labels-meaning">2. Categorical Features: Giving Labels Meaning</h4> <p>Categorical data represents types or groups (e.g., <code class="language-plaintext highlighter-rouge">color</code>, <code class="language-plaintext highlighter-rouge">city</code>, <code class="language-plaintext highlighter-rouge">education_level</code>). Models often can’t directly process text labels.</p> <ul> <li> <strong>One-Hot Encoding:</strong> Creates a new binary (0 or 1) column for each unique category. <ul> <li> <em>Example</em>: If <code class="language-plaintext highlighter-rouge">color</code> has values “Red”, “Blue”, “Green”, it becomes three new columns: <code class="language-plaintext highlighter-rouge">is_Red</code>, <code class="language-plaintext highlighter-rouge">is_Blue</code>, <code class="language-plaintext highlighter-rouge">is_Green</code>. If the original <code class="language-plaintext highlighter-rouge">color</code> was “Red”, then <code class="language-plaintext highlighter-rouge">is_Red</code>=1, <code class="language-plaintext highlighter-rouge">is_Blue</code>=0, <code class="language-plaintext highlighter-rouge">is_Green</code>=0.</li> <li> <em>Why</em>: Avoids implying any order or relationship between categories where none exists. Essential for most ML models.</li> </ul> </li> <li> <strong>Label Encoding:</strong> Assigns a unique integer to each category. <ul> <li> <em>Example</em>: “Red” -&gt; 0, “Blue” -&gt; 1, “Green” -&gt; 2.</li> <li> <em>Why</em>: Simpler, reduces dimensionality. <em>Caution</em>: Only use if there’s an inherent ordinal relationship between categories (e.g., “Small” &lt; “Medium” &lt; “Large”), otherwise, it can mislead models into thinking there’s an ordering where none exists (e.g., 0 &lt; 1 &lt; 2 implying Red &lt; Blue &lt; Green).</li> </ul> </li> <li> <strong>Target Encoding (Mean Encoding):</strong> Replaces each category with the average value of the target variable for that category. <ul> <li> <em>Example</em>: If you’re predicting house prices, and <code class="language-plaintext highlighter-rouge">neighborhood</code> is a categorical feature, you might replace “Downtown” with the average house price in Downtown.</li> <li> <em>Why</em>: Captures the direct relationship between the category and the target, potentially reducing dimensionality and improving model performance. <em>Caution</em>: Prone to overfitting without proper cross-validation.</li> </ul> </li> </ul> <h4 id="3-date-and-time-features-unearthing-temporal-patterns">3. Date and Time Features: Unearthing Temporal Patterns</h4> <p>Dates and times are a goldmine for features, but models can’t directly interpret <code class="language-plaintext highlighter-rouge">2023-10-27 14:30:00</code>.</p> <ul> <li> <strong>Extracting Components:</strong> Break down datetime into its constituent parts: <ul> <li> <code class="language-plaintext highlighter-rouge">year</code>, <code class="language-plaintext highlighter-rouge">month</code>, <code class="language-plaintext highlighter-rouge">day</code>, <code class="language-plaintext highlighter-rouge">day_of_week</code>, <code class="language-plaintext highlighter-rouge">hour</code>, <code class="language-plaintext highlighter-rouge">minute</code>, <code class="language-plaintext highlighter-rouge">second</code>.</li> <li> <code class="language-plaintext highlighter-rouge">is_weekend</code> (binary), <code class="language-plaintext highlighter-rouge">is_holiday</code> (binary).</li> <li> <em>Example</em>: From <code class="language-plaintext highlighter-rouge">2023-10-27 14:30:00</code>, extract <code class="language-plaintext highlighter-rouge">year=2023</code>, <code class="language-plaintext highlighter-rouge">month=10</code>, <code class="language-plaintext highlighter-rouge">day=27</code>, <code class="language-plaintext highlighter-rouge">day_of_week=5</code> (Friday), <code class="language-plaintext highlighter-rouge">hour=14</code>, <code class="language-plaintext highlighter-rouge">minute=30</code>.</li> </ul> </li> <li> <strong>Cyclic Features:</strong> For periodic data (like hour of day, day of year), direct numerical values can mislead models (e.g., hour 23 is closer to hour 0 than hour 10). Use sine and cosine transformations. <ul> <li><code class="language-plaintext highlighter-rouge">hour_sin = sin(2 * \pi * hour / 24)</code></li> <li><code class="language-plaintext highlighter-rouge">hour_cos = cos(2 * \pi * hour / 24)</code></li> <li> <em>Why</em>: These transformations represent the cyclical nature, where the beginning and end of a cycle are close.</li> </ul> </li> <li> <strong>Time Differences:</strong> Calculate the duration between two events. <ul> <li> <em>Example</em>: <code class="language-plaintext highlighter-rouge">time_since_last_purchase</code>, <code class="language-plaintext highlighter-rouge">age_of_user_account</code>.</li> </ul> </li> </ul> <h4 id="4-text-features-a-quick-glimpse">4. Text Features (A Quick Glimpse)</h4> <p>Text data (like reviews or descriptions) needs special handling.</p> <ul> <li> <strong>Bag-of-Words (BoW):</strong> Counts the occurrences of words in a document.</li> <li> <strong>TF-IDF (Term Frequency-Inverse Document Frequency):</strong> Weighs word counts by how common they are across all documents, giving more importance to rare, distinctive words.</li> <li> <strong>Word Embeddings (Advanced):</strong> Represents words as dense vectors in a continuous space, capturing semantic relationships. (e.g., Word2Vec, GloVe).</li> </ul> <h4 id="5-domain-specific-features-the-art-of-human-insight">5. Domain-Specific Features: The Art of Human Insight</h4> <p>This is where true mastery comes in. Beyond generic techniques, using your understanding of the problem space to create features is often the most impactful.</p> <ul> <li> <em>Example</em>: In healthcare, <code class="language-plaintext highlighter-rouge">BMI = weight / (height^2)</code> is a powerful feature, combining two raw measurements.</li> <li> <em>Example</em>: In finance, <code class="language-plaintext highlighter-rouge">debt_to_income_ratio</code>.</li> <li> <em>Example</em>: In e-commerce, <code class="language-plaintext highlighter-rouge">average_items_per_order</code>.</li> </ul> <p>These features don’t just emerge from data; they’re <em>invented</em> by someone who understands what the data represents in the real world.</p> <h3 id="the-art-and-science-its-not-just-about-recipes">The Art and Science: It’s Not Just About Recipes</h3> <p>While there are many techniques, Feature Engineering isn’t just a rigid set of rules or algorithms. It’s truly a blend of art and science:</p> <ul> <li> <strong>The Science:</strong> Understanding the mathematical properties of different transformations, knowing which techniques are suitable for which data types and models.</li> <li> <strong>The Art:</strong> This involves creativity, intuition, and deep domain knowledge. It’s about asking “What if I combine these two features?” or “What hidden information might be encoded in this timestamp?” It’s an iterative process of exploration, hypothesis, creation, and validation. You might try several things, discard what doesn’t work, and refine what does.</li> </ul> <p>It often begins with thorough <strong>Exploratory Data Analysis (EDA)</strong>. By visualizing and summarizing your data, you start to uncover patterns, anomalies, and relationships that hint at useful features waiting to be born.</p> <h3 id="automated-feature-engineering-a-helping-hand">Automated Feature Engineering: A Helping Hand</h3> <p>For complex datasets, generating features manually can be time-consuming. Tools like <code class="language-plaintext highlighter-rouge">Featuretools</code> can automatically generate a large number of candidate features using techniques like “deep feature synthesis.” While powerful, these tools still benefit from human guidance and domain expertise to select the most meaningful features. They’re a great assistant, but not a replacement for your brain!</p> <h3 id="common-pitfalls-to-avoid">Common Pitfalls to Avoid</h3> <ul> <li> <strong>Data Leakage:</strong> This is a big one! Accidentally using information from the target variable (or future data) to create features. For example, if you’re predicting loan default, don’t use a feature that only becomes available <em>after</em> the loan defaults. Always engineer features only from data that would be available at the time of prediction.</li> <li> <strong>Over-Engineering:</strong> Creating too many features, especially highly correlated ones, can make your model slow, prone to overfitting, and difficult to interpret. Simplicity often wins.</li> <li> <strong>Ignoring Domain Knowledge:</strong> The biggest mistake you can make is treating Feature Engineering as a purely technical task without consulting experts or understanding the real-world context of your data.</li> </ul> <h3 id="conclusion-your-models-best-friend">Conclusion: Your Model’s Best Friend</h3> <p>Feature Engineering is a cornerstone of successful machine learning projects. It’s the critical bridge between raw, messy data and a model that truly understands the underlying patterns. It empowers your models to make smarter, more accurate predictions, and it’s where much of the real “magic” of data science happens.</p> <p>So, next time you’re building a model, don’t just feed it raw data. Take the time to understand your data, brainstorm potential relationships, and thoughtfully engineer features. Experiment, iterate, and observe how your model transforms from a struggling student into a top performer. This journey of transforming data is one of the most rewarding aspects of being a data scientist, and it’s a skill that will set you apart.</p> <p>Happy Feature Engineering!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>