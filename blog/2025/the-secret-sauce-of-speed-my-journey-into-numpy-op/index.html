<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Secret Sauce of Speed: My Journey into NumPy Optimization | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/the-secret-sauce-of-speed-my-journey-into-numpy-op/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Secret Sauce of Speed: My Journey into NumPy Optimization</h1> <p class="post-meta"> Created on December 08, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/numpy"> <i class="fa-solid fa-hashtag fa-sm"></i> NumPy</a>   <a href="/blog/blog/tag/optimization"> <i class="fa-solid fa-hashtag fa-sm"></i> Optimization</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/performance"> <i class="fa-solid fa-hashtag fa-sm"></i> Performance</a>   <a href="/blog/blog/tag/python"> <i class="fa-solid fa-hashtag fa-sm"></i> Python</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>Remember that feeling when you first started diving into the world of data science? The thrill of exploring datasets, building models, and uncovering insights? It’s like being an intrepid explorer in a vast digital jungle! But then, a subtle frustration often creeps in. You’re working with a massive dataset, and your beautiful Python code, which seemed so quick with smaller samples, suddenly grinds to a halt. Your machine’s fan spins up, and you’re left staring at a loading spinner, wondering if you did something wrong.</p> <p>I’ve been there. Many times. It’s like trying to move a mountain of data with a teaspoon. You know there <em>must</em> be a better way. And guess what? There is! For most of us in data science and machine learning, the unsung hero behind our numerical operations is often <strong>NumPy</strong>. And today, I want to share my journey into unlocking its true power through optimization techniques. It’s not just about writing code that <em>works</em>, but writing code that <em>flies</em>.</p> <h2 id="the-heart-of-the-matter-why-is-numpy-so-fast-and-how-can-we-make-it-faster">The Heart of the Matter: Why is NumPy So Fast (and How Can We Make it Faster)?</h2> <p>Before we dive into the “how,” let’s quickly touch on the “why.” You might be thinking, “Isn’t Python just Python? How can one library be so much faster?” The magic of NumPy lies in its foundation: it’s largely written in highly optimized C and Fortran. When you perform an operation on a NumPy array, you’re not executing slow, element-by-element Python loops. Instead, you’re delegating the heavy lifting to pre-compiled, super-efficient code.</p> <p>Think of it like this: If you had to add a million numbers, would you rather do it manually, one by one, with a pen and paper (Python loops)? Or would you use a calculator (NumPy’s C/Fortran backend) that can process entire chunks of numbers at once, incredibly fast? The answer is obvious!</p> <p>Our goal with NumPy optimization isn’t to reinvent the wheel, but to ensure we’re always using that super-fast calculator effectively, and not accidentally falling back to pen-and-paper methods.</p> <p>Let’s explore the key strategies that have made a massive difference in my own projects:</p> <h3 id="1-embrace-vectorization-your-new-best-friend">1. Embrace Vectorization: Your New Best Friend</h3> <p>If there’s one takeaway from this post, it’s this: <strong>vectorization is king!</strong> This is the fundamental principle behind NumPy’s speed. It means performing operations on entire arrays (or parts of arrays) at once, rather than iterating over individual elements using Python <code class="language-plaintext highlighter-rouge">for</code> loops.</p> <p>Let’s look at a classic example: adding two lists of numbers.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">time</span>

<span class="n">size</span> <span class="o">=</span> <span class="mi">10</span><span class="o">**</span><span class="mi">6</span> <span class="c1"># One million elements
</span>
<span class="c1"># --- Python List Approach ---
</span><span class="n">list_a</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">size</span><span class="p">))</span>
<span class="n">list_b</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">size</span><span class="p">))</span>

<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="n">result_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">list_a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">list_b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">size</span><span class="p">)]</span>
<span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Python loop time: </span><span class="si">{</span><span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s"> seconds</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># --- NumPy Vectorized Approach ---
</span><span class="n">np_a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
<span class="n">np_b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>

<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="n">result_np</span> <span class="o">=</span> <span class="n">np_a</span> <span class="o">+</span> <span class="n">np_b</span> <span class="c1"># This is vectorization!
</span><span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">NumPy vectorized time: </span><span class="si">{</span><span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s"> seconds</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Output (approximate, varies by machine):</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Python loop time: 0.1000 seconds
NumPy vectorized time: 0.0030 seconds
</code></pre></div></div> <p>That’s a <strong>30x speedup</strong> for a simple operation on a million elements! Imagine this difference across billions of operations in a complex model.</p> <p>The beauty is that most common mathematical operations (<code class="language-plaintext highlighter-rouge">+</code>, <code class="language-plaintext highlighter-rouge">-</code>, <code class="language-plaintext highlighter-rouge">*</code>, <code class="language-plaintext highlighter-rouge">/</code>, <code class="language-plaintext highlighter-rouge">**</code>) are inherently vectorized when used with NumPy arrays.</p> <p><strong>Beyond basic arithmetic, explore these vectorized NumPy functions:</strong></p> <ul> <li> <code class="language-plaintext highlighter-rouge">np.sum()</code>, <code class="language-plaintext highlighter-rouge">np.mean()</code>, <code class="language-plaintext highlighter-rouge">np.std()</code>, <code class="language-plaintext highlighter-rouge">np.max()</code>, <code class="language-plaintext highlighter-rouge">np.min()</code> for aggregations.</li> <li> <code class="language-plaintext highlighter-rouge">np.sqrt()</code>, <code class="language-plaintext highlighter-rouge">np.log()</code>, <code class="language-plaintext highlighter-rouge">np.exp()</code>, <code class="language-plaintext highlighter-rouge">np.sin()</code> for element-wise mathematical functions.</li> <li> <code class="language-plaintext highlighter-rouge">np.dot()</code> or <code class="language-plaintext highlighter-rouge">@</code> for matrix multiplication (a cornerstone of linear algebra and machine learning!).</li> <li> <code class="language-plaintext highlighter-rouge">np.where()</code> for conditional logic, replacing <code class="language-plaintext highlighter-rouge">if/else</code> in loops. <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="c1"># Instead of:
# new_list = [x * 2 if x &gt; 5 else x for x in my_list]
</span>    
<span class="c1"># Do this:
</span><span class="n">my_array</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">7</span><span class="p">])</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">my_array</span> <span class="o">&gt;</span> <span class="mi">5</span><span class="p">,</span> <span class="n">my_array</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">my_array</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span> <span class="c1"># Output: [ 1 12  3 16  2 14]
</span></code></pre></div> </div> </li> </ul> <p><strong>My advice:</strong> Every time you find yourself writing a <code class="language-plaintext highlighter-rouge">for</code> loop that iterates over a NumPy array or performs element-wise operations, stop and ask: “Can this be vectorized?” More often than not, the answer is yes!</p> <h3 id="2-understanding-broadcasting-the-implicit-magic">2. Understanding Broadcasting: The Implicit Magic</h3> <p>Broadcasting is a powerful mechanism that allows NumPy to perform operations on arrays of different shapes and sizes, usually without explicitly creating multiple copies of the smaller array. It’s essentially NumPy “stretching” the smaller array to match the larger one’s dimensions, making operations possible that would otherwise require explicit looping or memory-intensive replication.</p> <p>Imagine you have a large grid of numbers (a 2D array) and you want to add a constant value to every number, or add a single row to every row in the grid. Broadcasting handles this elegantly.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Adding a scalar to an array
</span><span class="n">matrix</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                   <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span>
                   <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]])</span>
<span class="n">scalar</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">result_scalar</span> <span class="o">=</span> <span class="n">matrix</span> <span class="o">+</span> <span class="n">scalar</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Matrix + Scalar:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">result_scalar</span><span class="p">)</span>

<span class="c1"># Adding a 1D array (vector) to a 2D array
</span><span class="n">vector</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="c1"># This vector has shape (3,)
</span><span class="n">result_vector</span> <span class="o">=</span> <span class="n">matrix</span> <span class="o">+</span> <span class="n">vector</span> <span class="c1"># Adds [1,0,-1] to each row
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Matrix + Vector:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">result_vector</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Output:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Matrix + Scalar:
 [[11 12 13]
  [14 15 16]
  [17 18 19]]

Matrix + Vector:
 [[ 2  2  2]
  [ 5  5  5]
  [ 8  8  8]]
</code></pre></div></div> <p>The rules of broadcasting can seem a bit quirky at first, but the general idea is:</p> <ol> <li>If dimensions don’t match, one array might be “stretched” to match the other.</li> <li>If any dimension is 1, it can be stretched to match the other array’s dimension.</li> <li>Dimensions are compared from the trailing (rightmost) dimension.</li> </ol> <p>Broadcasting is a huge performance booster because it often avoids creating large temporary arrays in memory, making your code not only faster but also more memory-efficient.</p> <h3 id="3-minimize-unnecessary-copies-be-mindful-of-memory">3. Minimize Unnecessary Copies: Be Mindful of Memory</h3> <p>When you perform operations in NumPy, it’s crucial to understand when a new array is created versus when you’re just getting a “view” of an existing array. Creating unnecessary copies can quickly consume memory and slow down your computations, especially with massive datasets.</p> <p>Consider these scenarios:</p> <ul> <li> <strong>Slicing often returns views:</strong> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="n">original_array</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Original: </span><span class="si">{</span><span class="n">original_array</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># [0 1 2 3 4 5 6 7 8 9]
</span>
<span class="c1"># This is a VIEW of original_array
</span><span class="n">sliced_view</span> <span class="o">=</span> <span class="n">original_array</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">5</span><span class="p">]</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">View: </span><span class="si">{</span><span class="n">sliced_view</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>    <span class="c1"># [2 3 4]
</span>
<span class="c1"># If you modify the view, the original also changes!
</span><span class="n">sliced_view</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">99</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Original after view modification: </span><span class="si">{</span><span class="n">original_array</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># [ 0  1 99  3  4  5  6  7  8  9]
</span></code></pre></div> </div> <p>Views are great for efficiency, but be careful! If you need an independent copy, use <code class="language-plaintext highlighter-rouge">.copy()</code>:</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="n">independent_copy</span> <span class="o">=</span> <span class="n">original_array</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">5</span><span class="p">].</span><span class="nf">copy</span><span class="p">()</span>
<span class="n">independent_copy</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">77</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Original after copy modification: </span><span class="si">{</span><span class="n">original_array</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># Still [ 0  1 99  3  4  5  6  7  8  9]
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Independent copy: </span><span class="si">{</span><span class="n">independent_copy</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>             <span class="c1"># [77  3  4]
</span></code></pre></div> </div> </li> <li> <strong>In-place operations (<code class="language-plaintext highlighter-rouge">+=</code>, <code class="language-plaintext highlighter-rouge">*=</code>) vs. re-assignment (<code class="language-plaintext highlighter-rouge">=</code>)</strong>: <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">x before: </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># [0 1 2]
</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">1</span> <span class="c1"># Creates a NEW array, then re-assigns x to it
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">x after x = x + 1: </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># [1 2 3]
</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">y before: </span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># [0 1 2]
</span><span class="n">y</span> <span class="o">+=</span> <span class="mi">1</span> <span class="c1"># Often performs the operation IN-PLACE, modifying y directly
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">y after y += 1: </span><span class="si">{</span><span class="n">y</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># [1 2 3]
</span></code></pre></div> </div> <p>While the result is the same, <code class="language-plaintext highlighter-rouge">y += 1</code> can be more memory efficient by avoiding the creation of an intermediate array. It’s not <em>always</em> in-place (NumPy sometimes needs to create a new array for certain operations), but it’s a good habit to prefer it when possible.</p> </li> </ul> <p>By being mindful of when copies are created, you can significantly reduce memory footprint and improve performance, especially when chaining multiple operations.</p> <h3 id="4-choose-your-data-types-wisely-smaller-is-often-better">4. Choose Your Data Types Wisely: Smaller is Often Better</h3> <p>NumPy allows you to specify the data type (<code class="language-plaintext highlighter-rouge">dtype</code>) of elements in an array (e.g., <code class="language-plaintext highlighter-rouge">np.int8</code>, <code class="language-plaintext highlighter-rouge">np.float32</code>, <code class="language-plaintext highlighter-rouge">np.float64</code>). The default is often <code class="language-plaintext highlighter-rouge">float64</code> for floating-point numbers and <code class="language-plaintext highlighter-rouge">int64</code> for integers. While these provide high precision and range, they also consume more memory.</p> <p>For instance, a <code class="language-plaintext highlighter-rouge">float64</code> (double-precision float) uses 8 bytes per element, while <code class="language-plaintext highlighter-rouge">float32</code> (single-precision float) uses only 4 bytes. For a million elements, that’s 8MB vs. 4MB!</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">sys</span>

<span class="n">large_data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">10</span><span class="o">**</span><span class="mi">6</span><span class="p">)</span> <span class="c1"># Defaults to float64
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Size of float64 array: </span><span class="si">{</span><span class="n">large_data</span><span class="p">.</span><span class="n">nbytes</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> MB</span><span class="sh">"</span><span class="p">)</span>

<span class="n">smaller_data</span> <span class="o">=</span> <span class="n">large_data</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Size of float32 array: </span><span class="si">{</span><span class="n">smaller_data</span><span class="p">.</span><span class="n">nbytes</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> MB</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># For integers
</span><span class="n">int_data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">10</span><span class="o">**</span><span class="mi">6</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">int64</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Size of int64 array: </span><span class="si">{</span><span class="n">int_data</span><span class="p">.</span><span class="n">nbytes</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> MB</span><span class="sh">"</span><span class="p">)</span>

<span class="n">int_data_small</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">10</span><span class="o">**</span><span class="mi">6</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">int16</span><span class="p">)</span> <span class="c1"># Values up to ~32,000
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Size of int16 array: </span><span class="si">{</span><span class="n">int_data_small</span><span class="p">.</span><span class="n">nbytes</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> MB</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Output (approximate):</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Size of float64 array: 7.63 MB
Size of float32 array: 3.81 MB
Size of int64 array: 7.63 MB
Size of int16 array: 1.91 MB
</code></pre></div></div> <p><strong>Why does this matter beyond memory?</strong></p> <ul> <li> <strong>Cache Performance:</strong> Smaller data types mean more elements can fit into your CPU’s cache. When data is in the cache, the CPU can access it much faster than retrieving it from main memory.</li> <li> <strong>Memory Bandwidth:</strong> Less data to move means faster data transfer between memory and CPU.</li> </ul> <p>If your data doesn’t require the full precision of <code class="language-plaintext highlighter-rouge">float64</code> (e.g., image pixel values, simple counts), switching to <code class="language-plaintext highlighter-rouge">float32</code>, <code class="language-plaintext highlighter-rouge">int32</code>, <code class="language-plaintext highlighter-rouge">int16</code>, or even <code class="language-plaintext highlighter-rouge">int8</code> can yield significant performance gains, especially for memory-bound operations. Always consider the range and precision needs of your data before blindly using defaults.</p> <h3 id="5-contiguous-memory-layout-the-order-in-the-rows">5. Contiguous Memory Layout: The Order in the Rows</h3> <p>This one might sound a bit more technical, but it boils down to how your array’s data is actually stored in your computer’s memory. NumPy arrays can be stored in two primary orders:</p> <ul> <li> <strong>C-order (row-major):</strong> Elements of a row are contiguous in memory. This is the default for NumPy and how C/Python typically store multi-dimensional arrays.</li> <li> <strong>Fortran-order (column-major):</strong> Elements of a column are contiguous in memory.</li> </ul> <p>Imagine a book on a shelf. If you read the book row-by-row, it’s efficient if the words of each line are next to each other. If you had to jump to different pages for each word in a line, it would be incredibly slow!</p> <p>Similarly, if your NumPy array is stored in C-order (row-major) and you’re performing operations that primarily access data row-wise (like summing across rows <code class="language-plaintext highlighter-rouge">axis=1</code>), it will be faster because the CPU can load contiguous blocks of data into its cache. If you then perform column-wise operations on this C-order array (like summing across columns <code class="language-plaintext highlighter-rouge">axis=0</code>), the CPU might have to jump around in memory, leading to more cache misses and slower performance.</p> <p>You can check the memory layout using <code class="language-plaintext highlighter-rouge">arr.flags</code>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">matrix</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">matrix</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Is C-contiguous? </span><span class="si">{</span><span class="n">matrix</span><span class="p">.</span><span class="n">flags</span><span class="p">[</span><span class="sh">'</span><span class="s">C_CONTIGUOUS</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Is F-contiguous? </span><span class="si">{</span><span class="n">matrix</span><span class="p">.</span><span class="n">flags</span><span class="p">[</span><span class="sh">'</span><span class="s">F_CONTIGUOUS</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># If you need to change order for specific operations:
</span><span class="n">matrix_f_order</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">asfortranarray</span><span class="p">(</span><span class="n">matrix</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\n</span><span class="s">F-order matrix flags:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Is C-contiguous? </span><span class="si">{</span><span class="n">matrix_f_order</span><span class="p">.</span><span class="n">flags</span><span class="p">[</span><span class="sh">'</span><span class="s">C_CONTIGUOUS</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Is F-contiguous? </span><span class="si">{</span><span class="n">matrix_f_order</span><span class="p">.</span><span class="n">flags</span><span class="p">[</span><span class="sh">'</span><span class="s">F_CONTIGUOUS</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><strong>When does this matter?</strong> For most common operations, NumPy handles this efficiently. However, if you’re writing custom code that iterates over large multi-dimensional arrays (which you should probably vectorize anyway!) or interfacing with other libraries that expect a specific memory layout, understanding this can prevent unexpected slowdowns. In general, try to perform operations in the natural order of your array’s memory layout.</p> <h3 id="6-leverage-blaslapack-the-deep-optimization-within">6. Leverage BLAS/LAPACK: The Deep Optimization Within</h3> <p>This isn’t an optimization <em>you</em> directly implement, but it’s important to know about. For heavy-duty linear algebra operations (like matrix multiplication, eigenvalue decomposition, solving linear systems), NumPy often doesn’t do the math itself. Instead, it delegates these tasks to highly optimized external libraries like <strong>BLAS (Basic Linear Algebra Subprograms)</strong> and <strong>LAPACK (Linear Algebra Package)</strong>.</p> <p>These libraries are written in Fortran and C, are meticulously optimized for specific hardware architectures, and often use multi-threading. This is the “secret sauce” that makes <code class="language-plaintext highlighter-rouge">np.dot()</code> or <code class="language-plaintext highlighter-rouge">@</code> operator incredibly fast for large matrices.</p> <p>Ensuring your NumPy installation is linked to an optimized BLAS library (like OpenBLAS, MKL - Intel Math Kernel Library, or Apple’s Accelerate Framework) can provide significant performance boosts for linear algebra-intensive tasks common in machine learning. Many data science environments (like Anaconda) come pre-configured with MKL for this reason.</p> <h2 id="practical-tips-for-your-optimization-journey">Practical Tips for Your Optimization Journey</h2> <ol> <li> <strong>Benchmark, Benchmark, Benchmark!</strong> Don’t guess where your bottlenecks are. Use tools to measure. <ul> <li>In Jupyter notebooks or IPython, <code class="language-plaintext highlighter-rouge">%timeit</code> is your best friend for quick timings of single lines or small blocks of code. <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="n">arr</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">10</span><span class="o">**</span><span class="mi">6</span><span class="p">)</span>
<span class="o">%</span><span class="n">timeit</span> <span class="n">arr</span><span class="o">**</span><span class="mi">2</span>
<span class="o">%</span><span class="n">timeit</span> <span class="n">np</span><span class="p">.</span><span class="nf">square</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span> <span class="c1"># Often marginally faster as it's a direct C function call
</span></code></pre></div> </div> </li> <li>For more complex functions, Python’s built-in <code class="language-plaintext highlighter-rouge">cProfile</code> module can help you identify which lines of code are taking the most time.</li> </ul> </li> <li> <p><strong>Profile Before Optimizing:</strong> It’s tempting to optimize everything, but focus your efforts where they matter most. A small part of your code often accounts for most of the execution time. Find that part, optimize it, and then re-profile.</p> </li> <li> <p><strong>Read the Docs (and Examples):</strong> The NumPy documentation is incredibly rich. For any function, check its documentation for performance tips, <code class="language-plaintext highlighter-rouge">dtype</code> parameters, and alternative vectorized approaches.</p> </li> <li> <strong>Start Simple, Scale Up:</strong> When prototyping, don’t worry excessively about micro-optimizations. Get your logic working. Once you have a working solution, then test it on larger datasets and optimize the bottlenecks.</li> </ol> <h2 id="conclusion-unleashing-your-inner-speed-demon">Conclusion: Unleashing Your Inner Speed Demon</h2> <p>NumPy optimization isn’t just about making your code faster; it’s about fundamentally understanding how numerical operations are executed at a deeper level. It’s about respecting computational resources, writing cleaner and more efficient code, and ultimately, tackling larger, more complex data science challenges with confidence.</p> <p>My journey from frustrated waits to lightning-fast computations has been incredibly rewarding. By embracing vectorization, understanding broadcasting, being mindful of memory, and choosing appropriate data types, you transform from a casual coder into a high-performance data artisan.</p> <p>So, the next time your script feels sluggish, remember these techniques. Go forth, optimize, and unleash the true power of NumPy in your data science and machine learning projects! Your CPU (and your patience) will thank you.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>