<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Art of Creation: Unraveling Diffusion Models, From Noise to Brilliance | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/the-art-of-creation-unraveling-diffusion-models-fr/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Art of Creation: Unraveling Diffusion Models, From Noise to Brilliance</h1> <p class="post-meta"> Created on July 18, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/generative-ai"> <i class="fa-solid fa-hashtag fa-sm"></i> Generative AI</a>   <a href="/blog/blog/tag/diffusion-models"> <i class="fa-solid fa-hashtag fa-sm"></i> Diffusion Models</a>   <a href="/blog/blog/tag/ai-explained"> <i class="fa-solid fa-hashtag fa-sm"></i> AI Explained</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>My journey into the world of Artificial Intelligence has always been fueled by a fascination with creation. How do we build systems that don’t just recognize patterns, but generate entirely new ones? For a long time, Generative Adversarial Networks (GANs) dominated this space, creating impressive, often photo-realistic images. But then, a new contender emerged, quietly at first, and then with a spectacular burst of creative power: Diffusion Models.</p> <p>If you’ve marvelled at the breathtaking images from DALL-E 2, Midjourney, or Stable Diffusion, you’ve witnessed Diffusion Models in action. These models are not just a step forward; they feel like a paradigm shift, enabling AI to craft visuals and other data modalities with unprecedented fidelity and diversity.</p> <p>Today, I want to take you on a deep dive into the heart of Diffusion Models. We’ll explore the elegant simplicity behind their operation, peek at the underlying math, and understand why they’ve become the darling of generative AI. Don’t worry if you’re not a math wizard; my goal is to make this accessible, much like explaining a fascinating magic trick by revealing its clever mechanics.</p> <h3 id="the-alchemists-secret-what-exactly-are-diffusion-models">The Alchemist’s Secret: What Exactly Are Diffusion Models?</h3> <p>At their core, Diffusion Models are a type of <em>generative model</em>. Their purpose, much like GANs, is to learn the underlying distribution of a dataset and then generate new samples that resemble the original data. But how they achieve this is fundamentally different.</p> <p>Think of it like this: Imagine you have a beautiful, pristine photograph. Now, imagine a meticulous process where you slowly, gradually, add a tiny bit of static or “noise” to this photo. You do it again, and again, over many small steps, until the original photo is completely obscured, lost in a sea of pure, random noise.</p> <p>Now, here’s the magic: What if you could reverse that process? What if you could learn to <em>un-noise</em> the image, step by step, recovering the original photo (or something very similar) from pure static? That, in a nutshell, is what Diffusion Models learn to do.</p> <p>They consist of two main parts:</p> <ol> <li> <strong>Forward Diffusion Process (The “Noising” Journey):</strong> A fixed, predetermined process where we gradually add Gaussian noise to an input image over several timesteps, eventually transforming it into pure noise.</li> <li> <strong>Reverse Diffusion Process (The “Denoising” Miracle):</strong> A learned process where a neural network attempts to reverse the forward process, gradually removing noise to transform pure noise back into a clean, meaningful data sample.</li> </ol> <p>Let’s unpack these two fascinating stages.</p> <h3 id="phase-1-the-forward-diffusion-process--embracing-the-chaos">Phase 1: The Forward Diffusion Process – Embracing the Chaos</h3> <p>In the forward diffusion process, we start with a clean image, let’s call it $\mathbf{x}<em>0$. Over a series of $T$ timesteps, we progressively add a small amount of Gaussian noise to it. Each step transforms $\mathbf{x}</em>{t-1}$ into $\mathbf{x}_t$.</p> <p>Mathematically, this process is simple and beautiful. At each step $t$, we sample $\mathbf{x}<em>t$ from a conditional distribution that depends only on the previous step $\mathbf{x}</em>{t-1}$:</p> <table> <tbody> <tr> <td>$q(\mathbf{x}_t</td> <td>\mathbf{x}<em>{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \beta_t}\mathbf{x}</em>{t-1}, \beta_t \mathbf{I})$</td> </tr> </tbody> </table> <p>Let’s break that down:</p> <ul> <li>$\mathcal{N}$ represents a Gaussian (normal) distribution.</li> <li>$\mathbf{x}_t$ is the image at timestep $t$.</li> <li>$\sqrt{1 - \beta_t}$ determines how much of the previous image $\mathbf{x}_{t-1}$ we retain (it’s less than 1, so the image gradually fades).</li> <li>$\beta_t \mathbf{I}$ is the variance of the Gaussian noise added. $\beta_t$ is a pre-defined “variance schedule” – a sequence of small values (e.g., from 0.0001 to 0.02) that dictates how much noise is added at each step. This schedule can be linear, cosine, etc., but it’s <em>fixed</em> and not learned.</li> <li>$\mathbf{I}$ is the identity matrix, meaning the noise is isotropic (same variance in all directions).</li> </ul> <p>The cool part is that because this is a Markov chain (meaning $\mathbf{x}<em>t$ only depends on $\mathbf{x}</em>{t-1}$), we can derive a direct way to sample $\mathbf{x}<em>t$ from the _original</em> image $\mathbf{x}_0$ in a single step! This is a powerful trick that simplifies training:</p> <table> <tbody> <tr> <td>$q(\mathbf{x}_t</td> <td>\mathbf{x}_0) = \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t}\mathbf{x}_0, (1 - \bar{\alpha}_t)\mathbf{I})$</td> </tr> </tbody> </table> <p>Here, $\alpha_t = 1 - \beta_t$, and $\bar{\alpha}<em>t = \prod</em>{s=1}^t \alpha_s$. This equation tells us that any noisy image $\mathbf{x}_t$ can be expressed as a combination of the original image $\mathbf{x}_0$ and some pure Gaussian noise $\epsilon$:</p> <p>$\mathbf{x}_t = \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\epsilon$</p> <p>where $\epsilon \sim \mathcal{N}(0, \mathbf{I})$.</p> <p>Why is this direct sampling important? Because during training, instead of incrementally adding noise step-by-step to get to $\mathbf{x}_t$, we can directly generate $\mathbf{x}_t$ from $\mathbf{x}_0$ and a randomly sampled noise $\epsilon$. This allows our model to learn across various noise levels efficiently.</p> <h3 id="phase-2-the-reverse-diffusion-process--sculpting-from-static">Phase 2: The Reverse Diffusion Process – Sculpting from Static</h3> <table> <tbody> <tr> <td>Now for the truly ingenious part: the reverse process. Our goal is to train a neural network to predict $p_\theta(\mathbf{x}_{t-1}</td> <td>\mathbf{x}_t)$, essentially reversing each step of noise addition. If we can accurately do this, we can start with pure noise $\mathbf{x}_T$ and iteratively transform it back into a meaningful image $\mathbf{x}_0$.</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>However, directly modeling $p_\theta(\mathbf{x}_{t-1}</td> <td>\mathbf{x}<em>t)$ is incredibly complex. The brilliant insight of Diffusion Models (specifically Denoising Diffusion Probabilistic Models, or DDPMs) is to realize that if we knew the original data $\mathbf{x}_0$, we could perfectly determine the reverse step’s distribution, $q(\mathbf{x}</em>{t-1}</td> <td>\mathbf{x}_t, \mathbf{x}_0)$. This true posterior is also a Gaussian distribution.</td> </tr> </tbody> </table> <p>The problem is, during inference, we <em>don’t</em> know $\mathbf{x}<em>0$. So, what if our neural network, which we’ll call $\epsilon</em>\theta$, could predict the <em>noise component</em> $\epsilon$ that was added to $\mathbf{x}_0$ to get $\mathbf{x}_t$?</p> <p>Remember our equation from the forward process: $\mathbf{x}<em>t = \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\epsilon$. If our network $\epsilon</em>\theta(\mathbf{x}<em>t, t)$ can predict $\epsilon$, we can then rearrange this equation to _estimate</em> $\mathbf{x}_0$:</p> <p>$\hat{\mathbf{x}}<em>0 = \frac{\mathbf{x}_t - \sqrt{1 - \bar{\alpha}_t}\epsilon</em>\theta(\mathbf{x}_t, t)}{\sqrt{\bar{\alpha}_t}}$</p> <p>With this estimated $\hat{\mathbf{x}}<em>0$, we can then approximate the mean of our reverse Gaussian distribution, allowing us to sample $\mathbf{x}</em>{t-1}$ from $\mathbf{x}_t$.</p> <h3 id="training-the-denoising-genius-the-u-net">Training the Denoising Genius (The U-Net)</h3> <p>So, how do we train $\epsilon_\theta$? We use a neural network, commonly a U-Net architecture (known for its effectiveness in image-to-image tasks like segmentation), because it’s excellent at processing spatial information and preserving detail across different scales. The U-Net takes the noisy image $\mathbf{x}<em>t$ and the current timestep $t$ as input, and it outputs its prediction of the noise, $\epsilon</em>\theta(\mathbf{x}_t, t)$.</p> <p>The training objective is surprisingly simple: we want the network’s predicted noise to be as close as possible to the <em>actual</em> noise $\epsilon$ that was added.</p> <table> <tbody> <tr> <td>$L(\theta) = \mathbb{E}_{t, \mathbf{x}_0, \epsilon} \left[</td> <td> </td> <td>\epsilon - \epsilon_\theta(\sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\epsilon, t)</td> <td> </td> <td>^2 \right]$</td> </tr> </tbody> </table> <p>Let’s unpack the training loop for a single batch:</p> <ol> <li> <strong>Sample a real image $\mathbf{x}_0$</strong> from your dataset.</li> <li> <strong>Sample a random timestep $t$</strong> (e.g., between 1 and $T$).</li> <li> <strong>Sample pure Gaussian noise $\epsilon$</strong> from $\mathcal{N}(0, \mathbf{I})$.</li> <li> <strong>Calculate the noisy image $\mathbf{x}_t$</strong> using the forward diffusion equation: $\mathbf{x}_t = \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\epsilon$.</li> <li> <strong>Pass $\mathbf{x}<em>t$ and $t$ into the U-Net $\epsilon</em>\theta$</strong> to get its predicted noise, $\epsilon_\theta(\mathbf{x}_t, t)$.</li> <li> <table> <tbody> <tr> <td> <strong>Calculate the loss</strong> (Mean Squared Error) between the predicted noise and the actual noise: $</td> <td> </td> <td>\epsilon - \epsilon_\theta(\mathbf{x}_t, t)</td> <td> </td> <td>^2$.</td> </tr> </tbody> </table> </li> <li> <strong>Update the U-Net’s weights</strong> using gradient descent to minimize this loss.</li> </ol> <p>This process is repeated millions of times. The U-Net learns to identify and predict the noise component for various levels of noise (different timesteps $t$).</p> <h3 id="generating-new-data-the-iterative-creation">Generating New Data: The Iterative Creation</h3> <p>Once our $\epsilon_\theta$ network is trained, generating new data is like watching a reverse time-lapse. We start with a completely random Gaussian noise image $\mathbf{x}_T$. Then, for $t = T, T-1, \dots, 1$:</p> <ol> <li> <strong>Use the trained $\epsilon_\theta(\mathbf{x}_t, t)$</strong> to predict the noise in the current image $\mathbf{x}_t$.</li> <li> <table> <tbody> <tr> <td> <strong>Apply a denoising step</strong> using a slightly more complex formula that leverages the predicted noise to estimate the mean and variance of $p_\theta(\mathbf{x}_{t-1}</td> <td>\mathbf{x}<em>t)$, and then sample $\mathbf{x}</em>{t-1}$.</td> </tr> </tbody> </table> <ul> <li>A simplified interpretation: $\mathbf{x}_{t-1}$ is derived from $\mathbf{x}_t$ by subtracting the predicted noise component and adding a small amount of new noise (which mimics the uncertainty in the reverse step).</li> <li>The actual equation often looks like: $\mathbf{x}<em>{t-1} = \frac{1}{\sqrt{\alpha_t}} (\mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}}\epsilon</em>\theta(\mathbf{x}_t, t)) + \sigma_t \mathbf{z}$ where $\mathbf{z} \sim \mathcal{N}(0, \mathbf{I})$.</li> </ul> </li> </ol> <p>After $T$ steps, we are left with $\mathbf{x}_0$, a brand new image that the model “dreamed up” from pure static! It’s an iterative refinement process, much like an artist refining a clay sculpture from a blob.</p> <h3 id="why-diffusion-models-are-dominating-generative-ai">Why Diffusion Models are Dominating Generative AI</h3> <p>My fascination with Diffusion Models comes from their incredible performance and inherent stability, which addresses many of the challenges faced by previous generative models like GANs.</p> <ul> <li> <strong>Unparalleled Image Quality:</strong> They consistently produce state-of-the-art, high-resolution, and visually stunning images that often surpass human perception.</li> <li> <strong>Training Stability:</strong> Unlike GANs, which often suffer from mode collapse (where the generator gets stuck producing only a few types of outputs) and difficult training dynamics, Diffusion Models have a well-defined, stable loss function. This makes them much easier to train reliably.</li> <li> <strong>Diverse Sample Generation:</strong> They excel at capturing the full diversity of the training data. Because the generative process starts from random noise, it naturally explores the entire data manifold.</li> <li> <strong>Controllability:</strong> Diffusion Models are remarkably amenable to <em>conditional generation</em>. By incorporating text embeddings (like CLIP) or other conditioning information into the U-Net, we can guide the denoising process to generate images that match specific descriptions, styles, or even other images. This is the core of models like Stable Diffusion.</li> <li> <strong>Beyond Images:</strong> While I’ve focused on images, Diffusion Models are incredibly versatile. They are being applied to generate audio, video, 3D assets, chemical structures for drug discovery, and more.</li> </ul> <h3 id="challenges-and-the-road-ahead">Challenges and the Road Ahead</h3> <p>Despite their brilliance, Diffusion Models aren’t without their quirks:</p> <ul> <li> <strong>Inference Speed:</strong> The iterative nature of sampling means they can be slower than GANs during generation, often requiring hundreds or thousands of steps. However, research into techniques like DDIM (Denoising Diffusion Implicit Models) and latent diffusion (like Stable Diffusion, which denoises in a lower-dimensional latent space) has significantly sped up this process.</li> <li> <strong>Computational Cost:</strong> Training these models, especially on vast datasets like LAION-5B, requires substantial computational resources.</li> </ul> <p>The field is evolving at a breakneck pace. We’re seeing innovations in faster sampling, more efficient architectures, and applications in increasingly complex domains. It’s truly an exciting time to be involved in generative AI.</p> <h3 id="my-thoughts-from-noise-to-brilliance">My Thoughts: From Noise to Brilliance</h3> <p>For me, understanding Diffusion Models has been a revelation. It highlights an elegant principle: sometimes, the most complex and beautiful creations can emerge from simple, repetitive processes. The idea of “un-noising” the world, one tiny step at a time, resonates deeply. It’s a reminder that even in chaos, there’s an inherent structure waiting to be unveiled.</p> <p>As Data Scientists and Machine Learning Engineers, comprehending these models opens up incredible avenues for creativity and problem-solving. Whether it’s crafting hyper-realistic product images, generating synthetic data for privacy-preserving AI, or even designing new molecules, Diffusion Models are a powerful tool in our growing arsenal.</p> <p>So, the next time you see an AI-generated image that takes your breath away, remember the quiet, iterative journey from pure static to stunning brilliance. It’s not magic, but it’s certainly close enough to inspire wonder. I encourage you to dive deeper, perhaps by trying out a Diffusion Model yourself or exploring the foundational papers. The future of creation is here, and it’s delightfully noisy.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>