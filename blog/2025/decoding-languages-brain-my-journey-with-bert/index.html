<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Decoding Language's Brain: My Journey with BERT | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/decoding-languages-brain-my-journey-with-bert/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Decoding Language's Brain: My Journey with BERT</h1> <p class="post-meta"> Created on October 12, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> NLP</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/transformers"> <i class="fa-solid fa-hashtag fa-sm"></i> Transformers</a>   <a href="/blog/blog/tag/bert"> <i class="fa-solid fa-hashtag fa-sm"></i> BERT</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="decoding-languages-brain-my-journey-with-bert">Decoding Language’s Brain: My Journey with BERT</h2> <p>Hey everyone!</p> <p>If you’re anything like me, you’ve probably spent countless hours wondering about one of the most fascinating challenges in artificial intelligence: teaching machines to truly <em>understand</em> human language. It’s not just about recognizing words; it’s about context, nuance, sarcasm, and all the subtle cues we humans pick up effortlessly. For a long time, this felt like an insurmountable hurdle. Then came BERT.</p> <p>For me, encountering BERT (Bidirectional Encoder Representations from Transformers) wasn’t just learning about another AI model; it felt like peeking into the future of how machines would interact with and interpret our world. It’s a breakthrough that made me truly excited about what’s possible in Data Science and Machine Learning. And today, I want to share that excitement with you, demystifying this incredible piece of technology.</p> <h3 id="the-problem-before-bert-when-words-lived-in-silos">The Problem Before BERT: When Words Lived in Silos</h3> <p>Imagine trying to understand a language by looking up each word in a dictionary, one by one, without ever considering how they fit together in a sentence. That’s a bit like what early machine learning models faced.</p> <p>Before BERT, Natural Language Processing (NLP) had come a long way, but it still grappled with fundamental challenges:</p> <ol> <li> <p><strong>Context (or lack thereof):</strong> Models like Word2Vec and GloVe were revolutionary. They learned “word embeddings,” numerical representations of words where words with similar meanings were close together in a multi-dimensional space. For instance, “king” and “queen” would be close. But here’s the catch: the word “bank” would have <em>one</em> representation, regardless of whether you meant a “river bank” or a “financial bank.” The context was lost.</p> </li> <li> <p><strong>Unidirectional Thinking:</strong> Recurrent Neural Networks (RNNs) and their advanced cousins, LSTMs (Long Short-Term Memory networks), tried to solve context by reading sentences sequentially, one word after another. They could build a state based on previous words. However, they typically read either from left-to-right or right-to-left. They couldn’t easily consider context from <em>both</em> sides simultaneously. This meant they couldn’t grasp how a word like “bass” (the fish or the instrument) was influenced by words <em>before</em> it <em>and</em> <em>after</em> it in the same pass. Plus, sequential processing was slow and hard to parallelize during training.</p> </li> </ol> <p>This is where BERT, with its innovative use of the Transformer architecture, stepped in and changed the game forever.</p> <h3 id="the-transformer-berts-powerful-backbone">The Transformer: BERT’s Powerful Backbone</h3> <p>BERT didn’t invent the Transformer; that credit goes to a team at Google in their groundbreaking 2017 paper “Attention Is All You Need.” But BERT brilliantly adapted it. The core idea of the Transformer is radical: <strong>it completely removed recurrence</strong> from sequence processing. No more left-to-right or right-to-left sequential reading!</p> <p>How does it achieve this? Through a mechanism called <strong>Self-Attention</strong>.</p> <h4 id="self-attention-the-looking-around-mechanism">Self-Attention: The “Looking Around” Mechanism</h4> <p>Imagine you’re reading the sentence: “The animal didn’t cross the street because it was too tired.” When you read “it,” you instantly know “it” refers to “the animal.” Your brain does this by “paying attention” to relevant words in the sentence. Self-attention allows the model to do precisely that.</p> <p>For each word in a sentence, self-attention calculates how much “focus” it should place on every other word in that same sentence. This allows it to weigh the importance of other words when processing a particular word, effectively building a rich contextual understanding.</p> <p>Mathematically, it involves three key vectors derived from each word’s embedding:</p> <ul> <li> <strong>Query ($Q$):</strong> What am I looking for? (Like asking a question)</li> <li> <strong>Key ($K$):</strong> What do I have? (Like an index in a database)</li> <li> <strong>Value ($V$):</strong> What information do I pass on if the Query matches? (Like the actual data)</li> </ul> <p>The attention score for a word (Query) with respect to other words (Keys) is computed by taking the dot product of $Q$ and $K^T$, then scaling it down (to prevent large values from dominating the softmax function), and finally applying a softmax function to get a probability distribution. This distribution tells us how much attention to pay. These weights are then multiplied by the Value vectors and summed up.</p> \[Attention(Q, K, V) = softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V\] <p>Here, $d_k$ is the dimension of the key vectors, used for scaling. This formula, while looking complex, essentially says: “For each word (Query), compare it to all other words (Keys), get a relevance score, normalize those scores, and then use them to combine the information (Values) from all other words.”</p> <p>This isn’t done just once; the Transformer uses <strong>Multi-Head Attention</strong>. This means it performs several independent attention calculations in parallel (each with its own set of $Q, K, V$ matrices), allowing the model to focus on different aspects of relationships within the sentence simultaneously. One “head” might focus on grammatical dependencies, another on semantic relatedness.</p> <h4 id="positional-encoding-keeping-order-in-chaos">Positional Encoding: Keeping Order in Chaos</h4> <p>Since the Transformer processes all words in parallel, it loses the sequential order information that RNNs inherently had. To reintroduce this, a clever trick called <strong>Positional Encoding</strong> is used. We add a unique vector to each word embedding based on its position in the sentence. These vectors are often generated using sine and cosine functions, allowing the model to implicitly learn relative positions. So, while “dog bites man” and “man bites dog” might have the same words, their positional encodings would make their representations distinct.</p> <h3 id="berts-innovation-learning-from-unlabeled-text">BERT’s Innovation: Learning from Unlabeled Text</h3> <p>The Transformer was a beast, but BERT took it to another level by defining <em>how</em> to train such a powerful model on vast amounts of raw, unlabeled text (like Wikipedia or books). This pre-training phase is what makes BERT so versatile.</p> <p>BERT’s genius lies in its two unique pre-training tasks:</p> <ol> <li> <p><strong>Masked Language Modeling (MLM):</strong> This is like a “fill-in-the-blanks” game. During training, BERT randomly masks (hides) about 15% of the words in a sentence and then tries to predict those masked words.</p> <p>For example, given “The man went to the [MASK] store,” BERT has to predict “grocery.” Why is this brilliant? Because to accurately predict the masked word, BERT <em>must</em> understand the context from <em>both</em> the left and the right sides of the masked word simultaneously. This is the “bidirectional” aspect of BERT that was missing in previous models. It forces the model to truly grasp the relationships between words across an entire sentence.</p> </li> <li> <p><strong>Next Sentence Prediction (NSP):</strong> This task teaches BERT about relationships between entire sentences. Given two sentences, BERT has to predict if the second sentence logically follows the first one in the original text.</p> <ul> <li> <strong>Example 1 (IsNext):</strong> Sentence A: “The quick brown fox jumped over the lazy dog.” Sentence B: “It landed softly on the grass.” (Label: IsNext)</li> <li> <strong>Example 2 (NotNext):</strong> Sentence A: “The quick brown fox jumped over the lazy dog.” Sentence B: “The sun is a star.” (Label: NotNext)</li> </ul> <p>This helps BERT understand discourse coherence and learn representations useful for tasks like question answering or natural language inference.</p> </li> </ol> <h4 id="berts-input-representation">BERT’s Input Representation</h4> <p>To handle these tasks, BERT requires a special input format:</p> <ul> <li> <strong>Token Embeddings:</strong> Each word (or sub-word, using WordPiece tokenization for handling rare words) is converted into a numerical vector.</li> <li> <strong>Segment Embeddings:</strong> A unique embedding is added to each token to indicate whether it belongs to “Sentence A” or “Sentence B” (crucial for NSP).</li> <li> <strong>Positional Embeddings:</strong> As discussed, these encode the position of each token in the sequence.</li> <li> <strong>Special Tokens:</strong> <ul> <li> <code class="language-plaintext highlighter-rouge">[CLS]</code>: A special classification token at the beginning of every input. Its final hidden state is used as the aggregate representation of the entire input for classification tasks.</li> <li> <code class="language-plaintext highlighter-rouge">[SEP]</code>: A separator token used to distinguish between sentences.</li> </ul> </li> </ul> <p>All these embeddings are summed together to form the final input representation that flows into the Transformer blocks.</p> <h3 id="fine-tuning-bert-making-it-work-for-you">Fine-Tuning BERT: Making it Work for You</h3> <p>The beauty of BERT is that once it’s pre-trained on a massive corpus (like all of Wikipedia and BookCorpus), it has learned a deep, generalized understanding of language. We don’t need to train it from scratch for every new task. This is called <strong>transfer learning</strong>.</p> <p>To use BERT for a specific task (like sentiment analysis, spam detection, or question answering), we simply add a small, task-specific output layer on top of the pre-trained BERT model. Then, we “fine-tune” the entire model (or sometimes just the new layer) on our specific, smaller dataset. Because BERT has already learned so much about language structure and semantics, it often requires much less data and achieves far superior results compared to training a model from scratch.</p> <p>This paradigm has democratized NLP, allowing researchers and practitioners to achieve state-of-the-art results without needing immense computational resources or vast domain-specific datasets for initial training.</p> <h3 id="why-bert-was-a-game-changer">Why BERT Was a Game Changer</h3> <p>BERT’s impact on NLP cannot be overstated:</p> <ul> <li> <strong>State-of-the-Art Performance:</strong> It shattered previous benchmarks on numerous NLP tasks, including GLUE and SQuAD (question answering).</li> <li> <strong>True Bidirectional Context:</strong> Finally, models could understand words based on their full surrounding context, leading to richer and more accurate representations.</li> <li> <strong>Transfer Learning Powerhouse:</strong> It solidified the pre-train/fine-tune paradigm, making advanced NLP accessible.</li> <li> <strong>Parallelization:</strong> The Transformer architecture allowed for much faster training on GPUs/TPUs compared to sequential RNNs.</li> </ul> <p>Of course, BERT isn’t without its considerations. It’s computationally intensive to train from scratch (though using pre-trained versions mitigates this), and like all large language models, it can inherit biases present in its training data. Research continues to evolve, with models like RoBERTa, ALBERT, Longformer, and GPT-3 building upon or diverging from BERT’s foundations, pushing the boundaries further.</p> <h3 id="my-takeaway-and-your-next-step">My Takeaway and Your Next Step</h3> <p>For me, BERT represented a monumental leap – a moment where machines truly started to “get” language in a way we hadn’t seen before. It transformed what was possible in areas from search engines and chatbots to content summarization and language translation.</p> <p>If you’re interested in diving deeper, I highly recommend exploring the original “Attention Is All You Need” paper and then the “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding” paper. You can also play around with pre-trained BERT models using libraries like Hugging Face’s Transformers, which makes implementation surprisingly straightforward.</p> <p>The world of NLP is moving incredibly fast, and BERT was a major catalyst. Understanding its core principles is not just understanding a model; it’s understanding a pivotal moment in the history of AI.</p> <p>Keep learning, keep exploring, and who knows what language puzzles you’ll solve next!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>