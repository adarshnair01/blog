<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Your Digital Psychic: How Recommender Systems Read Your Mind (and What's Under the Hood) | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/your-digital-psychic-how-recommender-systems-read/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Your Digital Psychic: How Recommender Systems Read Your Mind (and What's Under the Hood)</h1> <p class="post-meta"> Created on February 20, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/recommender-systems"> <i class="fa-solid fa-hashtag fa-sm"></i> Recommender Systems</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/collaborative-filtering"> <i class="fa-solid fa-hashtag fa-sm"></i> Collaborative Filtering</a>   <a href="/blog/blog/tag/matrix-factorization"> <i class="fa-solid fa-hashtag fa-sm"></i> Matrix Factorization</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone! Have you ever had that uncanny feeling while browsing online, like the website knew exactly what you were thinking? For me, it happens almost daily. Whether it’s Spotify introducing me to my next favorite band, YouTube suggesting a tutorial for a niche skill I just picked up, or an e-commerce site popping up with an item I didn’t even realize I needed – it feels like magic. But as a budding data scientist, I know it’s not magic; it’s the ingenious work of <strong>Recommender Systems</strong>.</p> <p>These systems are at the heart of our personalized digital world, and diving into how they work has been one of the most rewarding parts of my machine learning journey. They’re more than just fancy algorithms; they’re bridges connecting you to relevant information, products, and entertainment, enhancing user experience and driving significant business value. In this post, I want to take you on a tour behind the scenes, exploring the core ideas and some of the cool math that makes this “digital psychic” possible.</p> <h3 id="the-grand-vision-why-do-we-need-recommender-systems">The Grand Vision: Why Do We Need Recommender Systems?</h3> <p>Imagine a world without recommendations. You’d have to sift through millions of songs, movies, or products to find what you like. It’s overwhelming! Recommender systems solve this “information overload” problem by:</p> <ol> <li> <strong>Personalization:</strong> Tailoring content to individual tastes.</li> <li> <strong>Discovery:</strong> Helping users find new things they might love but wouldn’t have found otherwise.</li> <li> <strong>Engagement:</strong> Keeping users hooked and active on platforms.</li> <li> <strong>Business Value:</strong> Increasing sales, subscriptions, and ad revenue.</li> </ol> <p>At their core, recommender systems aim to predict a user’s preference for an item. Let’s explore the two main philosophical approaches to achieving this.</p> <h3 id="approach-1-content-based-filtering--the-you-like-x-so-youll-like-more-x-method">Approach 1: Content-Based Filtering – The “You Like X, So You’ll Like More X” Method</h3> <p>Think of Content-Based Filtering as a sophisticated librarian who knows your taste in books. If you tell them you love sci-fi novels about space travel and artificial intelligence, they’ll recommend other books that share those specific characteristics.</p> <p><strong>How it Works:</strong></p> <ol> <li> <strong>Item Representation:</strong> Each item (e.g., a movie, a song, a product) is described by its features. For a movie, these could be genre (sci-fi, action), actors, director, keywords, release year, etc. We can represent these features as a vector.</li> <li> <strong>User Profile:</strong> We build a profile for each user based on the features of items they have liked in the past. If you’ve watched many sci-fi movies, your user profile will have a strong “sci-fi” component. This profile is essentially an aggregate (average or weighted average) of the vectors of items you’ve enjoyed.</li> <li> <strong>Similarity:</strong> When recommending, the system compares your user profile vector to the item vectors of unrated items. It then suggests items whose features are most similar to your past preferences.</li> </ol> <table> <tbody> <tr> <td>Let’s get a little mathematical. Suppose we represent an item $i$ as a vector of features $x_i \in \mathbb{R}^d$. A user $u$’s profile could be represented as $p_u = \frac{1}{</td> <td>R_u</td> <td>} \sum_{i \in R_u} x_i$, where $R_u$ is the set of items user $u$ has rated positively.</td> </tr> </tbody> </table> <p>To find similar items, we often use <strong>Cosine Similarity</strong>. For two vectors $A$ and $B$, their cosine similarity is:</p> <table> <tbody> <tr> <td>$similarity(A, B) = \frac{A \cdot B}{</td> <td> </td> <td>A</td> <td> </td> <td>\cdot</td> <td> </td> <td>B</td> <td> </td> <td>}$</td> </tr> </tbody> </table> <p>This formula measures the cosine of the angle between two vectors. A higher cosine (closer to 1) means the vectors point in roughly the same direction, indicating higher similarity. So, we’d recommend items $j$ for which $similarity(p_u, x_j)$ is highest.</p> <p><strong>Pros:</strong></p> <ul> <li> <strong>Explainable:</strong> We can tell the user <em>why</em> an item was recommended (e.g., “Because you liked other sci-fi movies”).</li> <li> <strong>No Cold Start for New Items:</strong> If an item has features, it can be recommended immediately, even if no one has rated it yet.</li> <li> <strong>User Independence:</strong> Recommendations for one user aren’t affected by other users’ tastes.</li> </ul> <p><strong>Cons:</strong></p> <ul> <li> <strong>Limited Serendipity:</strong> It tends to recommend items similar to what you already like, potentially trapping you in a “filter bubble.”</li> <li> <strong>Feature Engineering:</strong> Requires detailed metadata about items, which can be hard to get or maintain.</li> <li> <strong>Cold Start for New Users:</strong> If a user hasn’t rated anything, we can’t build a profile for them.</li> </ul> <h3 id="approach-2-collaborative-filtering--the-people-like-you-like-this-method">Approach 2: Collaborative Filtering – The “People Like You, Like This” Method</h3> <p>This approach is my personal favorite because it taps into the collective wisdom of the crowd. Instead of relying on item features, Collaborative Filtering (CF) finds users or items that are “similar” purely based on their interaction patterns (e.g., ratings, purchases).</p> <p>Imagine you’re at a book club. Someone recommends a book, and your friend jumps in, “Oh, I loved that one too! And if you liked that, you should definitely check out X.” Your friend isn’t telling you about the features of X; they’re leveraging their shared taste with you.</p> <p>There are two main types of Collaborative Filtering:</p> <h4 id="21-user-based-collaborative-filtering-user-user-cf">2.1. User-Based Collaborative Filtering (User-User CF)</h4> <ul> <li> <strong>Idea:</strong> Find users who are similar to you, then recommend items that <em>they</em> liked but <em>you</em> haven’t seen yet.</li> <li> <strong>Process:</strong> <ol> <li> <strong>Find Similar Users:</strong> Identify a group of users who have rated items similarly to you.</li> <li> <strong>Aggregate Preferences:</strong> Take items these similar users liked (and you haven’t) and recommend the ones with the highest collective preference.</li> </ol> </li> </ul> <p>To quantify user similarity, we can again use metrics like Pearson Correlation or Cosine Similarity on their rating vectors. Let $R_{u,i}$ be user $u$’s rating for item $i$. We build a user-item matrix where rows are users and columns are items.</p> <p>The predicted rating for user $u$ on item $i$ can be calculated as a weighted average of the ratings given by similar users $v$:</p> <table> <tbody> <tr> <td>$P_{u,i} = \bar{R}<em>u + \frac{\sum</em>{v \in N(u;k)} sim(u,v) \cdot (R_{v,i} - \bar{R}<em>v)}{\sum</em>{v \in N(u;k)}</td> <td>sim(u,v)</td> <td>}$</td> </tr> </tbody> </table> <p>Here:</p> <ul> <li>$N(u;k)$ is the set of $k$ most similar users to user $u$.</li> <li>$sim(u,v)$ is the similarity between user $u$ and user $v$.</li> <li>$\bar{R}_u$ is the average rating given by user $u$.</li> <li>$(R_{v,i} - \bar{R}_v)$ is the deviation of user $v$’s rating for item $i$ from user $v$’s average rating (to normalize for users who generally rate high or low).</li> </ul> <h4 id="22-item-based-collaborative-filtering-item-item-cf">2.2. Item-Based Collaborative Filtering (Item-Item CF)</h4> <ul> <li> <strong>Idea:</strong> If you liked item A, find other items that are similar to item A based on how <em>other users</em> have rated them. Then recommend those similar items.</li> <li> <strong>Process:</strong> <ol> <li> <strong>Find Similar Items:</strong> For an item you’ve liked (say, a movie), identify other movies that are frequently liked or rated highly by the same set of users.</li> <li> <strong>Recommend:</strong> Suggest items similar to your positively rated ones.</li> </ol> </li> </ul> <p>This approach often works better in practice than User-User CF because item similarity tends to be more stable over time than user preferences. The similarity between two items $i$ and $j$ can be calculated using Cosine Similarity on the columns of the user-item matrix (representing item rating vectors).</p> <p>The predicted rating for user $u$ on item $i$ is calculated as:</p> <table> <tbody> <tr> <td>$P_{u,i} = \frac{\sum_{j \in I_u} sim(i,j) \cdot R_{u,j}}{\sum_{j \in I_u}</td> <td>sim(i,j)</td> <td>}$</td> </tr> </tbody> </table> <p>Here:</p> <ul> <li>$I_u$ is the set of items that user $u$ has already rated.</li> <li>$sim(i,j)$ is the similarity between item $i$ and item $j$.</li> </ul> <p><strong>Pros of Collaborative Filtering:</strong></p> <ul> <li> <strong>Serendipity:</strong> Can recommend items you’d never discover through content-based methods.</li> <li> <strong>No Feature Engineering:</strong> Doesn’t require item metadata.</li> <li> <strong>Adapts to Trends:</strong> Automatically picks up on new trends in user preferences.</li> </ul> <p><strong>Cons:</strong></p> <ul> <li> <strong>Cold Start for New Users/Items:</strong> If a user hasn’t rated anything, or an item is brand new, there’s no data for CF. This is a huge challenge!</li> <li> <strong>Sparsity:</strong> Most user-item matrices are extremely sparse (users have rated only a tiny fraction of all available items), making similarity calculations difficult.</li> <li> <strong>Scalability:</strong> As the number of users and items grows, finding similar users/items can become computationally expensive.</li> </ul> <h3 id="tackling-the-challenges-sparsity-and-scalability-with-matrix-factorization">Tackling the Challenges: Sparsity and Scalability with Matrix Factorization</h3> <p>The sparsity problem in CF is significant. Imagine a Netflix user-movie matrix: with millions of users and thousands of movies, most entries would be empty. Traditional CF struggles with this. This is where <strong>Matrix Factorization</strong> comes to the rescue!</p> <p><strong>The Core Idea:</strong> Instead of directly working with the sparse user-item matrix, we try to decompose it into two lower-dimensional matrices:</p> <ol> <li>A <strong>user-feature matrix</strong> where each user is represented by a set of “latent factors” or “features.”</li> <li>An <strong>item-feature matrix</strong> where each item is also represented by the same set of latent factors.</li> </ol> <p>These “latent factors” are not explicit features like “genre” but rather abstract numerical representations that capture underlying preferences and characteristics. Think of them as hidden dimensions of taste.</p> <p>Mathematically, if our original user-item interaction matrix is $R$ (with dimensions $M \times N$, where $M$ is the number of users and $N$ is the number of items), we want to approximate it as the product of two much smaller matrices, $P$ and $Q^T$:</p> <p>$R \approx P Q^T$</p> <p>Where:</p> <ul> <li>$P$ is an $M \times K$ matrix (user-latent factor matrix). Each row $p_u$ represents user $u$’s preferences for $K$ latent factors.</li> <li>$Q$ is an $N \times K$ matrix (item-latent factor matrix). Each row $q_i$ represents item $i$’s characteristics for the same $K$ latent factors. $Q^T$ is its transpose ($K \times N$).</li> </ul> <p>The predicted rating for user $u$ on item $i$, $\hat{R}_{u,i}$, is simply the dot product of their respective latent factor vectors:</p> <p>$\hat{R}<em>{u,i} = p_u^T q_i = \sum</em>{k=1}^{K} p_{u,k} q_{i,k}$</p> <p>We learn the values in $P$ and $Q$ by minimizing the error between our predicted ratings and the actual known ratings. A common objective function to minimize is the <strong>Root Mean Squared Error (RMSE)</strong>, often with regularization to prevent overfitting:</p> <table> <tbody> <tr> <td>$\min_{P,Q} \sum_{(u,i) \in K} (R_{u,i} - p_u^T q_i)^2 + \lambda (</td> <td> </td> <td>P</td> <td> </td> <td>_F^2 +</td> <td> </td> <td>Q</td> <td> </td> <td>_F^2)$</td> </tr> </tbody> </table> <p>Here:</p> <ul> <li>$K$ is the set of all known ratings.</li> <li> <table> <tbody> <tr> <td>$</td> <td> </td> <td>P</td> <td> </td> <td>_F^2$ and $</td> <td> </td> <td>Q</td> <td> </td> <td>_F^2$ are Frobenius norms, which act as regularization terms to keep the latent factors from becoming too large and prevent overfitting.</td> </tr> </tbody> </table> </li> <li>$\lambda$ is the regularization parameter.</li> </ul> <p>Popular techniques for matrix factorization include Singular Value Decomposition (SVD) (though direct SVD on sparse matrices can be tricky), Alternating Least Squares (ALS), and a variant of SVD often attributed to Simon Funk, which uses Stochastic Gradient Descent (SGD) to find the latent factors. These methods are powerful because they can infer meaningful relationships even from sparse data, and the resulting low-dimensional representations are much more efficient for prediction.</p> <h3 id="hybrid-recommender-systems-the-best-of-both-worlds">Hybrid Recommender Systems: The Best of Both Worlds</h3> <p>Given the strengths and weaknesses of Content-Based and Collaborative Filtering, real-world systems often combine them into <strong>Hybrid Recommender Systems</strong>.</p> <ul> <li>They can use content-based methods to address the cold-start problem for new users/items (if features are available).</li> <li>They can use collaborative methods for serendipitous recommendations and to adapt to trending tastes.</li> <li>Many sophisticated hybrid models exist, including those that blend predictions, incorporate features into matrix factorization, or use deep learning.</li> </ul> <h3 id="evaluating-our-digital-psychic">Evaluating Our Digital Psychic</h3> <p>How do we know if our recommender system is doing a good job? We need metrics!</p> <ol> <li> <strong>Offline Metrics:</strong> <ul> <li> <strong>RMSE (Root Mean Squared Error):</strong> For rating prediction tasks, measures the average magnitude of the errors in predicting numerical ratings. Lower is better.</li> <li> <strong>Precision@k and Recall@k:</strong> For item recommendation, these measure how many of the top <em>k</em> recommendations are actually relevant, and how many relevant items were captured in the top <em>k</em>.</li> <li> <strong>MAP (Mean Average Precision):</strong> A ranking-aware metric.</li> </ul> </li> <li> <strong>Online Metrics (A/B Testing):</strong> <ul> <li> <strong>Click-Through Rate (CTR):</strong> How often users click on recommendations.</li> <li> <strong>Conversion Rate:</strong> How often users purchase/subscribe after a recommendation.</li> <li> <strong>Time Spent:</strong> How much time users spend engaging with recommended content.</li> </ul> </li> </ol> <p>Ultimately, the goal isn’t just to be accurate but to drive user engagement and satisfaction.</p> <h3 id="beyond-the-basics-the-evolving-landscape">Beyond the Basics: The Evolving Landscape</h3> <p>The field of recommender systems is constantly evolving. We’re seeing exciting advancements with:</p> <ul> <li> <strong>Deep Learning:</strong> Neural networks are being used to learn complex user and item representations, capture sequential user behavior, and handle implicit feedback more effectively. Think of techniques like Word2Vec applied to items (item2vec), or recurrent neural networks (RNNs) for session-based recommendations.</li> <li> <strong>Context-Aware Recommendations:</strong> Incorporating external factors like time of day, location, or even the user’s current mood.</li> <li> <strong>Fairness and Transparency:</strong> Ensuring recommendations are not biased, promote diversity, and are explainable to users.</li> </ul> <h3 id="my-takeaway">My Takeaway</h3> <p>Building a truly effective recommender system is a blend of art and science. It requires a deep understanding of data, algorithms, and human psychology. From the elegant simplicity of Content-Based Filtering to the collective intelligence of Collaborative Filtering and the dimensionality reduction magic of Matrix Factorization, each technique offers a unique lens through which to predict preferences.</p> <p>The next time Netflix suggests a movie or Amazon shows you a product, pause for a moment. Appreciate the intricate dance of data and algorithms happening behind the scenes, all designed to make your digital life a little more intuitive, a little more personalized, and maybe, just a little more magical. It’s a field brimming with fascinating challenges and endless opportunities – and I, for one, can’t wait to see what new “magic” we’ll uncover next!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>