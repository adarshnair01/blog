<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Art of Not Memorizing: How Regularization Teaches Models to Truly Learn | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/the-art-of-not-memorizing-how-regularization-teach/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Art of Not Memorizing: How Regularization Teaches Models to Truly Learn</h1> <p class="post-meta"> Created on June 23, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/regularization"> <i class="fa-solid fa-hashtag fa-sm"></i> Regularization</a>   <a href="/blog/blog/tag/overfitting"> <i class="fa-solid fa-hashtag fa-sm"></i> Overfitting</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/model-training"> <i class="fa-solid fa-hashtag fa-sm"></i> Model Training</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>Welcome back to my portfolio journal. Today, I want to dive into a concept that, once you grasp it, will fundamentally change how you think about building robust machine learning models: <strong>Regularization</strong>.</p> <p>Imagine you’re preparing for a big exam. You have two ways to study:</p> <ol> <li> <strong>Memorize every single answer from past exams.</strong> You know <em>exactly</em> what to write for those specific questions. But if the teacher changes even one word, or asks a slightly different question on the same topic, you’re lost.</li> <li> <strong>Understand the underlying concepts.</strong> You spend time grasping the principles, solving various problem types, and connecting ideas. When the exam comes, no matter how the question is phrased, you can apply your knowledge.</li> </ol> <p>Which student do you think will perform better on an unseen, challenging exam? Clearly, student number two.</p> <p>In the world of machine learning, our models often behave like student number one. They can get incredibly good at predicting outcomes for the data they’ve <em>already seen</em> during training. This phenomenon is called <strong>overfitting</strong>, and it’s one of the biggest challenges we face.</p> <h3 id="the-problem-overfitting--when-models-memorize-not-learn">The Problem: Overfitting – When Models Memorize, Not Learn</h3> <p>What exactly is overfitting? Let’s say you’re trying to build a model to predict house prices based on features like size, number of bedrooms, and location. You train your model on a dataset of houses.</p> <p>An overfit model would essentially “memorize” the prices of every house in your training data, including all the quirks and random noise. It creates an incredibly complex, jagged function that perfectly passes through every single data point.</p> <p>Visually, imagine fitting a curve to some data points. If you use a simple linear line, it might not capture all the nuances. But if you use a ridiculously complex polynomial that wiggles and turns to hit <em>every single point</em>, including outliers and measurement errors, that’s overfitting.</p> <p><strong>Why is this bad?</strong> Because when you give this overfit model <em>new</em> data – houses it hasn’t seen before – it performs terribly. It’s so focused on the specific details of the training data that it fails to generalize to new, slightly different examples. It hasn’t learned the <em>true underlying patterns</em>; it just learned to recite the training examples.</p> <p>This is a critical flaw because the whole point of machine learning is to make accurate predictions on <em>unseen data</em>.</p> <h3 id="the-solution-regularization--our-models-strict-but-fair-teacher">The Solution: Regularization – Our Model’s Strict but Fair Teacher</h3> <p>This is where regularization swoops in like a strict but fair teacher. Its job is to prevent the model from becoming too complex and memorizing the training data. It encourages the model to find simpler, more generalizable patterns.</p> <p>How does it do this? By adding a <strong>penalty</strong> to the model’s cost function (also known as the loss function).</p> <p>Remember, during training, our model tries to minimize its cost function, which usually measures how wrong its predictions are. The goal is: lower cost = better model.</p> <p>Regularization modifies this cost function:</p> \[\text{New Cost Function} = \text{Original Cost Function} + \text{Regularization Penalty}\] <p>This regularization penalty discourages the model from using very large coefficients (weights) for its features. Think of it this way: large weights often correspond to complex models that are trying too hard to fit every little detail. By penalizing these large weights, we effectively nudge the model towards simpler solutions.</p> <p>Let’s look at the two most common types: L1 and L2 Regularization.</p> <h4 id="1-l2-regularization-ridge-regression">1. L2 Regularization (Ridge Regression)</h4> <p>L2 regularization adds a penalty proportional to the <strong>square</strong> of the magnitude of the coefficients.</p> <p>The cost function with L2 regularization looks like this:</p> \[J(\theta) = \frac{1}{2m}\sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda \sum_{j=1}^n \theta_j^2\] <p>Let’s break that down:</p> <ul> <li>The first part, $\frac{1}{2m}\sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2$, is our standard <strong>Mean Squared Error (MSE)</strong> cost function for linear regression. It measures how far off our predictions $h_\theta(x^{(i)})$ are from the actual values $y^{(i)}$.</li> <li>The second part, $\lambda \sum_{j=1}^n \theta_j^2$, is the <strong>L2 regularization penalty</strong>. <ul> <li>$\theta_j$ represents the different coefficients (weights) of our model’s features.</li> <li>$\theta_j^2$ means we’re squaring each coefficient.</li> <li>$\sum_{j=1}^n$ means we sum up these squared coefficients for all $n$ features.</li> <li>$\lambda$ (lambda) is a crucial hyperparameter called the <strong>regularization strength</strong>.</li> </ul> </li> </ul> <p><strong>What does L2 do?</strong> By adding $\sum \theta_j^2$ to the cost, the model now has a dual objective: minimize prediction errors AND keep the coefficients small. If a coefficient tries to grow very large to perfectly fit some noise, the penalty term will shoot up, making the overall cost higher. This forces the model to choose smaller coefficients, effectively “shrinking” them towards zero. It rarely makes them <em>exactly</em> zero, but it keeps them contained.</p> <p>Think of L2 as a gentle nudge. It tells the model, “Hey, try to explain the data, but don’t get too excited about any single feature. Keep your explanations concise.”</p> <h4 id="2-l1-regularization-lasso-regression">2. L1 Regularization (Lasso Regression)</h4> <p>L1 regularization adds a penalty proportional to the <strong>absolute value</strong> of the magnitude of the coefficients.</p> <p>The cost function with L1 regularization looks like this:</p> \[J(\theta) = \frac{1}{2m}\sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda \sum_{j=1}^n |\theta_j|\] <table> <tbody> <tr> <td>The only difference here is that we use $</td> <td>\theta_j</td> <td>$ (absolute value) instead of $\theta_j^2$ (square).</td> </tr> </tbody> </table> <p><strong>What does L1 do?</strong> Like L2, L1 also shrinks coefficients. However, due to the nature of the absolute value function, L1 has a unique property: it tends to shrink some coefficients <strong>all the way to zero</strong>.</p> <p><strong>Why is this powerful?</strong> If a feature’s coefficient becomes zero, it means that feature is completely excluded from the model. This effectively performs <strong>feature selection</strong>! L1 regularization can help identify and eliminate irrelevant features, leading to simpler and more interpretable models.</p> <p>Think of L1 as a strict editor. It tells the model, “Explain the data, but be ruthless. If a feature isn’t absolutely essential, cut it out.”</p> <h3 id="the-mighty-lambda-lambda-parameter-tuning-the-discipline">The Mighty $\lambda$ (Lambda) Parameter: Tuning the Discipline</h3> <p>Both L1 and L2 regularization have this parameter $\lambda$. It’s a hyperparameter that <em>we</em> (the data scientists) have to choose.</p> <ul> <li> <strong>If $\lambda$ is 0:</strong> There’s no regularization penalty. The model is free to overfit.</li> <li> <strong>If $\lambda$ is very small:</strong> There’s a small penalty. The model can still be somewhat complex.</li> <li> <strong>If $\lambda$ is very large:</strong> The penalty dominates the cost function. The model is heavily restricted, and coefficients will be forced to be very small (or zero for L1). This can lead to <strong>underfitting</strong>, where the model is too simple to capture the underlying patterns in the data (like using a straight line for highly curved data).</li> <li> <strong>The “just right” $\lambda$</strong>: This is the sweet spot! We typically find the optimal $\lambda$ using techniques like cross-validation, where we test different $\lambda$ values and pick the one that gives the best performance on validation data (data the model hasn’t seen during training, but isn’t our final test set).</li> </ul> <h3 id="a-glimpse-into-the-geometry-why-l1-zeros-out">A Glimpse into the Geometry (Why L1 Zeros Out)</h3> <p>For those curious, the difference in behavior between L1 and L2 can be intuitively understood through their geometric interpretations.</p> <p>Imagine our model has only two coefficients, $\theta_1$ and $\theta_2$. The original cost function (without regularization) forms contours (like a bowl shape) in a 2D plane. The regularization penalty adds a “constraint region” where the coefficients are allowed to exist.</p> <ul> <li>For L2 regularization ($\theta_1^2 + \theta_2^2 \le C$), this constraint region is a <strong>circle</strong>.</li> <li> <table> <tbody> <tr> <td>For L1 regularization ($</td> <td>\theta_1</td> <td>+</td> <td>\theta_2</td> <td>\le C$), this constraint region is a <strong>diamond shape</strong> (a square rotated by 45 degrees).</td> </tr> </tbody> </table> </li> </ul> <p>The optimal coefficients are found where the cost function’s contours first “touch” the boundary of this constraint region. Because the L1 diamond has “corners” on the axes (where one of the coefficients is zero), the cost function contours are much more likely to touch at these corners, forcing one or more coefficients to exactly zero. The L2 circle, being smooth, typically results in coefficients being shrunk but rarely exactly zero.</p> <h3 id="beyond-l1-and-l2-other-regularization-techniques">Beyond L1 and L2: Other Regularization Techniques</h3> <p>While L1 and L2 are fundamental for linear models, regularization is a broad concept, and many other techniques achieve similar goals for different model types:</p> <ul> <li> <strong>Dropout:</strong> In neural networks, randomly “dropping out” (deactivating) a percentage of neurons during training prevents them from co-adapting too much, forcing the network to learn more robust features.</li> <li> <strong>Early Stopping:</strong> Simply stopping the training process when the model’s performance on a validation set starts to degrade, even if its performance on the training set is still improving. This catches the model before it starts to overfit.</li> <li> <strong>Data Augmentation:</strong> Creating more training data by transforming existing data (e.g., rotating, flipping images) effectively teaches the model to be more robust to variations.</li> </ul> <h3 id="conclusion-building-smarter-more-reliable-models">Conclusion: Building Smarter, More Reliable Models</h3> <p>Regularization is not just a fancy mathematical trick; it’s a cornerstone of building reliable, generalizable machine learning models. It’s the mechanism by which we teach our models to truly <em>learn</em> the underlying patterns in the data, rather than just memorizing noise and specific examples.</p> <p>By understanding and applying techniques like L1 and L2 regularization, you empower your models to perform robustly on unseen data, which is the ultimate goal of any predictive system. It’s about building models that are not just smart, but truly wise.</p> <p>Next time you train a model, remember the student who understood the concepts, not just memorized the answers. Regularization helps your models become that student.</p> <p>Happy modeling!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>