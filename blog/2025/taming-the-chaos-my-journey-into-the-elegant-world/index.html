<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Taming the Chaos: My Journey into the Elegant World of Kalman Filters | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/taming-the-chaos-my-journey-into-the-elegant-world/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Taming the Chaos: My Journey into the Elegant World of Kalman Filters</h1> <p class="post-meta"> Created on May 18, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/kalman-filter"> <i class="fa-solid fa-hashtag fa-sm"></i> Kalman Filter</a>   <a href="/blog/blog/tag/state-estimation"> <i class="fa-solid fa-hashtag fa-sm"></i> State Estimation</a>   <a href="/blog/blog/tag/sensor-fusion"> <i class="fa-solid fa-hashtag fa-sm"></i> Sensor Fusion</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="taming-the-chaos-my-journey-into-the-elegant-world-of-kalman-filters">Taming the Chaos: My Journey into the Elegant World of Kalman Filters</h2> <p>Have you ever looked at a flickering stock price chart, tried to pinpoint a drone’s exact location from a wobbly video feed, or perhaps just fumbled with a weather forecast that seems to change its mind every hour? In the world of data, noise and uncertainty are constants. We make predictions, we take measurements, and almost always, both are a little bit <em>off</em>.</p> <p>This challenge haunted me for a while, especially as I delved deeper into real-world data science problems. How do we make sense of a system whose true state is hidden, obscured by the very data we collect? Then, I stumbled upon a truly elegant solution, a marvel of applied mathematics and engineering: the <strong>Kalman Filter</strong>.</p> <p>It felt like discovering a secret superpower, a way to peer through the static and discern the underlying reality. Today, I want to share that journey with you. We’ll explore the core idea behind Kalman Filters, peek at the math that makes them tick, and see why they’re indispensable in everything from your smartphone’s GPS to the navigation systems of spacecraft.</p> <h3 id="the-problem-when-everything-is-a-little-bit-wrong">The Problem: When Everything is a Little Bit Wrong</h3> <p>Imagine you’re trying to track a small robot moving across a floor. You have two sources of information:</p> <ol> <li> <strong>Your Robot’s Internal Model:</strong> Based on its motor commands, you <em>predict</em> where the robot <em>should</em> be. But motors aren’t perfect; friction, slippage, and tiny delays mean this prediction will drift over time.</li> <li> <strong>A Sensor Measurement:</strong> Maybe a camera sees the robot or a simple ultrasonic sensor gives a distance. This measurement is also noisy – the camera might have pixels blurring, the sensor could be inaccurate.</li> </ol> <p>So, you have a pretty good <em>guess</em> from your model, and a noisy <em>observation</em> from your sensor. Which one do you trust more? How do you combine them optimally to get the <em>best possible estimate</em> of the robot’s true position? This, in a nutshell, is the problem the Kalman Filter solves.</p> <h3 id="the-core-idea-a-predict-update-cycle">The Core Idea: A Predict-Update Cycle</h3> <p>At its heart, the Kalman Filter is a recursive two-step process:</p> <ol> <li> <strong>Predict:</strong> Based on the system’s previous estimated state and a model of how the system evolves (e.g., the robot’s movement commands), it predicts the current state. Importantly, it also predicts the <em>uncertainty</em> of this prediction.</li> <li> <strong>Update (or Correct):</strong> When a new measurement comes in, the filter combines this noisy measurement with its prediction. It doesn’t just average them; it weights them based on their respective uncertainties. If the measurement is very accurate and the prediction is uncertain, it leans heavily on the measurement. If the measurement is noisy and the prediction is solid, it trusts the prediction more. This weighted combination then becomes the new, refined estimate of the system’s state, along with its updated (and hopefully reduced) uncertainty.</li> </ol> <p>This cycle repeats indefinitely. With each new measurement, the Kalman Filter refines its understanding of the system’s true state, continuously battling noise and uncertainty.</p> <h3 id="the-magic-behind-the-curtain-the-kalman-gain">The Magic Behind the Curtain: The Kalman Gain</h3> <p>The secret sauce to optimally combining predictions and measurements is something called the <strong>Kalman Gain</strong>. Think of it as a “trust factor.”</p> <ul> <li>If your measurement system is very precise (low noise), the Kalman Gain will be high, meaning you’ll trust the measurement more and update your state estimate significantly towards it.</li> <li>If your prediction model is very accurate (low uncertainty), the Kalman Gain will be low, meaning you’ll trust your prediction more and only slightly adjust it based on the new measurement.</li> </ul> <p>This gain dynamically adjusts at each step, ensuring the optimal balance between the two sources of information. It’s what makes the Kalman Filter so powerful – it’s always learning which source to trust more.</p> <h3 id="a-glimpse-into-the-math-dont-worry-its-elegant">A Glimpse into the Math (Don’t worry, it’s elegant!)</h3> <p>The Kalman Filter operates on a <strong>state vector</strong>, $x_k$, which encapsulates everything we want to know about the system at time $k$. For our robot, $x_k$ might include its position $(x, y)$ and velocity $(\dot{x}, \dot{y})$. It also maintains a <strong>covariance matrix</strong>, $P_k$, which describes the uncertainty or “spread” of our state estimate.</p> <p>Here are the key equations, broken down into the two steps:</p> <h4 id="1-prediction-step-time-update">1. Prediction Step (Time Update)</h4> <p>First, we project the state and its uncertainty forward in time.</p> <ul> <li> <strong>Project the state estimate:</strong> $\hat{x}<em>k^- = A \hat{x}</em>{k-1} + B u_k$ <ul> <li>$\hat{x}<em>k^-$: The _a priori</em> (predicted) state estimate at time $k$.</li> <li>$\hat{x}<em>{k-1}$: The _a posteriori</em> (updated) state estimate from the previous time step $(k-1)$.</li> <li>$A$: The state transition matrix. It describes how the state evolves from $k-1$ to $k$ <em>without</em> any external influence. For our robot, this might encode simple physics (new position = old position + velocity * time).</li> <li>$B$: The control input matrix.</li> <li>$u_k$: The control vector (e.g., motor commands given to the robot).</li> <li>This equation essentially says: “Our best guess for the new state is where it was, plus how it moved based on our model and control.”</li> </ul> </li> <li> <strong>Project the error covariance:</strong> $P_k^- = A P_{k-1} A^T + Q$ <ul> <li>$P_k^-$: The <em>a priori</em> error covariance matrix. This is the uncertainty associated with our predicted state.</li> <li>$P_{k-1}$: The <em>a posteriori</em> error covariance matrix from the previous step.</li> <li>$Q$: The process noise covariance matrix. This accounts for uncertainty in our system’s model itself (e.g., the robot’s motors are not perfectly precise).</li> <li>This equation says: “Our uncertainty increases because our prediction model isn’t perfect, and because our previous estimate wasn’t perfectly certain either.”</li> </ul> </li> </ul> <h4 id="2-update-step-measurement-update">2. Update Step (Measurement Update)</h4> <p>When a new measurement arrives, we use it to refine our prediction.</p> <ul> <li> <strong>Calculate the Kalman Gain:</strong> $K_k = P_k^- H^T (H P_k^- H^T + R)^{-1}$ <ul> <li>$K_k$: The Kalman Gain, the “trust factor” we discussed earlier.</li> <li>$H$: The observation matrix. It relates the state vector to the measurement vector. For example, if our state has position $(x,y)$ but our sensor only measures $x$, then $H$ would map $(x,y)$ to $x$.</li> <li>$R$: The measurement noise covariance matrix. This represents the uncertainty/noise in our sensor measurements.</li> <li>This equation calculates how much we should trust the new measurement versus our prediction. If $R$ is small (accurate sensor), $K_k$ will be large. If $P_k^-$ is small (accurate prediction), $K_k$ will be small.</li> </ul> </li> <li> <strong>Update the state estimate:</strong> $\hat{x}_k = \hat{x}_k^- + K_k (z_k - H \hat{x}_k^-)$ <ul> <li>$\hat{x}<em>k$: The _a posteriori</em> (updated) state estimate at time $k$. This is our final, best estimate.</li> <li>$z_k$: The actual measurement vector received at time $k$.</li> <li>$(z_k - H \hat{x}<em>k^-)$: This is the <strong>measurement residual</strong> or <strong>innovation</strong>. It’s the difference between the actual measurement and what we _expected</em> to measure based on our prediction.</li> <li>This equation says: “Our new best estimate is our prediction, plus a correction term. This correction is the difference between our measurement and our expectation, weighted by how much we trust the measurement (Kalman Gain).”</li> </ul> </li> <li> <strong>Update the error covariance:</strong> $P_k = (I - K_k H) P_k^-$ <ul> <li>$P_k$: The <em>a posteriori</em> error covariance matrix. This represents the uncertainty in our new, refined state estimate. It should ideally be <em>smaller</em> than $P_k^-$.</li> <li>$I$: The identity matrix.</li> <li>This equation says: “Our uncertainty has now decreased because we’ve incorporated a new piece of information (the measurement).”</li> </ul> </li> </ul> <p>And then, the cycle repeats! $\hat{x}<em>k$ and $P_k$ become $\hat{x}</em>{k-1}$ and $P_{k-1}$ for the next iteration.</p> <h3 id="an-intuitive-example-tracking-a-car">An Intuitive Example: Tracking a Car</h3> <p>Let’s simplify. Imagine tracking a car moving along a straight road.</p> <ul> <li> <strong>State:</strong> We want to know its position ($p$) and velocity ($v$). So, $x_k = \begin{bmatrix} p_k \ v_k \end{bmatrix}$.</li> <li> <strong>Model ($A, B, Q$):</strong> We predict its new position using $p_k = p_{k-1} + v_{k-1} \Delta t$, and its new velocity is roughly the old velocity (constant velocity model). We add some process noise ($Q$) because the driver might accelerate/decelerate slightly.</li> <li> <strong>Measurement ($z_k, H, R$):</strong> We get noisy GPS readings for position only. So $z_k = p_k^{GPS}$, and $H = \begin{bmatrix} 1 &amp; 0 \end{bmatrix}$ (to extract position from the state vector). The GPS itself has measurement noise ($R$).</li> </ul> <p>The Kalman Filter would:</p> <ol> <li> <strong>Predict:</strong> Based on the last known position and velocity, guess where the car <em>should</em> be now. And increase its uncertainty.</li> <li> <strong>Measure:</strong> Get a noisy GPS reading.</li> <li> <strong>Update:</strong> Compare the GPS reading to its prediction. If the GPS is very different, but historically reliable (small $R$), it adjusts its predicted position and velocity significantly. If the GPS is known to be very noisy (large $R$), it might only make a small adjustment, relying more on its internal model. Its uncertainty then shrinks.</li> </ol> <p>This continuous process allows the filter to give a far smoother and more accurate estimate of the car’s true position and velocity than either the raw GPS or the prediction model alone could provide.</p> <h3 id="why-is-it-so-powerful">Why is it so Powerful?</h3> <ol> <li> <strong>Optimal Estimation:</strong> For linear systems with Gaussian noise, the Kalman Filter provides the <em>optimal</em> estimate in the least-squares sense. This is a huge theoretical guarantee!</li> <li> <strong>Robust to Noise:</strong> It’s designed to handle noisy data from multiple sources gracefully.</li> <li> <strong>Real-time Processing:</strong> Its recursive nature makes it ideal for real-time applications, as it only needs the previous state, not the entire history.</li> <li> <strong>Handles Missing Data:</strong> If a measurement is missed, you simply skip the update step and proceed with only the prediction. Your uncertainty will just grow more.</li> <li> <strong>Versatility:</strong> From aerospace engineering to finance, its applications are incredibly broad.</li> </ol> <h3 id="limitations-and-beyond">Limitations and Beyond</h3> <p>The standard Kalman Filter, as described, assumes:</p> <ol> <li> <strong>Linear System Dynamics:</strong> The state transition ($A$) and observation ($H$) matrices must be linear.</li> <li> <strong>Gaussian Noise:</strong> Both process noise ($Q$) and measurement noise ($R$) are assumed to be Gaussian.</li> </ol> <p>What happens if your system is non-linear? For example, a robot moving in a circle, or a sensor measuring distance in a non-linear way? That’s where <strong>Extended Kalman Filters (EKF)</strong> and <strong>Unscented Kalman Filters (UKF)</strong> come in.</p> <ul> <li> <strong>EKF:</strong> Linearizes the non-linear functions around the current operating point using Jacobians (derivatives). It’s widely used but can be tricky to implement and sometimes diverge.</li> <li> <strong>UKF:</strong> Uses a deterministic sampling approach (sigma points) to capture the distribution of the non-linear transformation more accurately without explicit linearization. Often more robust than EKF.</li> </ul> <p>These extensions show the incredible adaptability of the core Kalman idea.</p> <h3 id="real-world-impact">Real-World Impact</h3> <p>The Kalman Filter is not just a theoretical construct; it’s a workhorse in countless critical systems:</p> <ul> <li> <strong>GPS Receivers:</strong> Fusing satellite signals, inertial measurements, and barometer readings to give you precise location data.</li> <li> <strong>Aerospace &amp; Defense:</strong> Navigation for aircraft, missiles, satellites, and spacecraft (it was crucial for the Apollo missions!).</li> <li> <strong>Robotics:</strong> Autonomous navigation for drones, self-driving cars, and industrial robots (simultaneously estimating its own position and mapping its environment - Simultaneous Localization and Mapping or SLAM).</li> <li> <strong>Finance:</strong> Estimating volatility, predicting stock prices, and managing portfolios.</li> <li> <strong>Weather Forecasting:</strong> Combining atmospheric models with real-time sensor data.</li> <li> <strong>Medical Imaging:</strong> Improving image reconstruction and tracking biological processes.</li> </ul> <h3 id="conclusion-embracing-uncertainty">Conclusion: Embracing Uncertainty</h3> <p>My journey into Kalman Filters taught me that we don’t always need perfect data to make accurate decisions. Instead, by understanding and modeling the inherent uncertainties in our predictions and measurements, we can fuse disparate pieces of imperfect information into a surprisingly robust and precise estimate of reality.</p> <p>It’s a testament to the power of combining statistical thinking with dynamic system modeling. For anyone diving into data science or machine learning, especially in areas involving time-series data, sensor fusion, or control systems, understanding the Kalman Filter isn’t just an advantage – it’s a fundamental key to taming the chaos and truly seeing through the noise.</p> <p>So, the next time your phone tells you the precise turn to make, or a drone elegantly navigates a complex environment, remember the elegant dance of the Kalman Filter, quietly working behind the scenes, turning uncertainty into clarity. What hidden states will <em>you</em> try to unveil?</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>