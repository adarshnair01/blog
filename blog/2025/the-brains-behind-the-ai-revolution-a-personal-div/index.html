<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Brains Behind the AI Revolution: A Personal Dive into Deep Learning | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/the-brains-behind-the-ai-revolution-a-personal-div/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Brains Behind the AI Revolution: A Personal Dive into Deep Learning</h1> <p class="post-meta"> Created on April 19, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/artificial-intelligence"> <i class="fa-solid fa-hashtag fa-sm"></i> Artificial Intelligence</a>   <a href="/blog/blog/tag/neural-networks"> <i class="fa-solid fa-hashtag fa-sm"></i> Neural Networks</a>   <a href="/blog/blog/tag/ai-explained"> <i class="fa-solid fa-hashtag fa-sm"></i> AI Explained</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>My journey into data science began with a simple question: How do computers learn? This curiosity quickly led me down a rabbit hole, past the traditional algorithms of Machine Learning, and straight into the captivating world of Deep Learning. It’s a field that, at first glance, feels almost like science fiction – machines developing their own “intuition” from mountains of data. But as I peeled back the layers, I discovered a beautiful blend of mathematics, biology, and computational power that’s both elegant and incredibly effective.</p> <p>Today, I want to share a piece of that journey with you, demystifying Deep Learning and perhaps sparking your own interest in the brains behind the AI revolution.</p> <h3 id="what-even-is-deep-learning-a-nesting-doll-analogy">What Even <em>Is</em> Deep Learning? A Nesting Doll Analogy</h3> <p>Let’s start broad.</p> <ul> <li> <strong>Artificial Intelligence (AI)</strong> is the grand vision: making machines intelligent, capable of mimicking human cognitive functions. Think of it as the largest nesting doll.</li> <li> <strong>Machine Learning (ML)</strong> is a subset of AI. It’s about giving computers the ability to learn from data <em>without</em> being explicitly programmed for every single task. Instead of writing rules for every possible scenario (e.g., “if image has pixels X, Y, Z, it’s a cat”), we feed it examples of cats and non-cats, and it learns to figure out the patterns itself. This is the middle nesting doll.</li> <li> <strong>Deep Learning (DL)</strong> is a specialized subset of Machine Learning. It uses <strong>Artificial Neural Networks (ANNs)</strong> with many layers (hence “deep”) to learn complex patterns and representations from data. This is our smallest, most intricate nesting doll, and the one we’re cracking open today.</li> </ul> <p>So, while all Deep Learning is Machine Learning, and all Machine Learning is AI, the reverse isn’t true. Deep Learning excels where traditional ML often struggles: with vast amounts of unstructured data like images, audio, and text.</p> <p>Why the sudden explosion of Deep Learning’s popularity? Three main ingredients stirred together in the last decade:</p> <ol> <li> <strong>Big Data:</strong> We now generate and collect enormous datasets, providing ample ‘experience’ for these hungry networks.</li> <li> <strong>Computational Power:</strong> Modern GPUs (Graphics Processing Units), originally designed for video games, turned out to be perfect for the parallel computations needed to train deep networks.</li> <li> <strong>Improved Algorithms &amp; Architectures:</strong> Smarter ways to design and train these networks emerged, making them more stable and effective.</li> </ol> <h3 id="the-spark-of-inspiration-our-own-brains">The Spark of Inspiration: Our Own Brains</h3> <p>The core idea behind Deep Learning is surprisingly old, dating back to the 1940s: simulate the human brain. Our brains are made of billions of interconnected neurons, tiny processing units that fire electrical signals.</p> <p>An artificial neural network attempts to mimic this structure, albeit in a highly simplified way. Let’s look at the basic building block: <strong>the artificial neuron</strong>, or <strong>perceptron</strong>.</p> <p>Imagine a single neuron. It receives signals from other neurons, processes them, and then decides whether to fire its own signal onwards. In an artificial neuron:</p> <ul> <li>It receives several <strong>inputs</strong> ($x_1, x_2, \ldots, x_n$).</li> <li>Each input is multiplied by a <strong>weight</strong> ($w_1, w_2, \ldots, w_n$), representing the strength of the connection, just like synapses in a biological brain.</li> <li>These weighted inputs are summed up, and a <strong>bias</strong> term ($b$) is added. This bias acts like an additional input that always has a value of 1, allowing the neuron to shift its activation threshold. $z = \sum_{i=1}^{n} w_i x_i + b$</li> <li>Finally, this sum ($z$) passes through an <strong>activation function</strong> ($\sigma$), which decides whether the neuron “fires” and what output it sends. Common activation functions include the Sigmoid (squashes output between 0 and 1), ReLU (Rectified Linear Unit, outputs 0 if input is negative, input itself if positive), and Tanh. $a = \sigma(z)$</li> </ul> <p>This output ($a$) then becomes an input for other neurons.</p> <h3 id="from-single-neurons-to-deep-networks">From Single Neurons to “Deep” Networks</h3> <p>A single perceptron is quite limited. The “deep” in Deep Learning comes from stacking many of these artificial neurons into multiple layers. We typically have:</p> <ul> <li>An <strong>Input Layer</strong>: This is where your data (e.g., pixels of an image, words in a sentence) enters the network.</li> <li>One or more <strong>Hidden Layers</strong>: These are the “thinking” layers where the magic happens. Each neuron in a hidden layer takes inputs from the previous layer, performs its calculation, and passes its output to the next layer. The more hidden layers, the “deeper” the network.</li> <li>An <strong>Output Layer</strong>: This layer produces the final result (e.g., “cat” or “dog” for an image, a predicted stock price, a translated word).</li> </ul> <p>The power of deep networks comes from their ability to learn <strong>hierarchical feature representations</strong>. Imagine an image of a cat:</p> <ul> <li>The first hidden layer might learn to detect very simple features like edges, lines, and corners.</li> <li>The second layer might combine these edges to recognize slightly more complex shapes like ears, eyes, or whiskers.</li> <li>Subsequent layers combine these features to recognize parts of a face, and finally, the output layer identifies the entire animal as a “cat.”</li> </ul> <p>It’s like building understanding brick by brick, from simple components to complex concepts, all learned automatically from the data. This hierarchical learning is why Deep Learning is so good at tasks like image recognition, which stumped earlier AI approaches.</p> <h3 id="how-do-they-learn-the-magic-of-backpropagation">How Do They Learn? The Magic of Backpropagation</h3> <p>This is where things get really interesting. If we just randomly assign weights and biases, the network will produce garbage. The “learning” part means adjusting these weights and biases so that the network gets better at its task over time. This is primarily done using a process called <strong>backpropagation</strong> and an optimization algorithm called <strong>gradient descent</strong>.</p> <ol> <li> <p><strong>The Loss Function (Measuring “Badness”):</strong> First, we need a way to quantify how “wrong” our network’s predictions are. This is the job of the <strong>loss function</strong> (or cost function). For example, if we’re predicting a numerical value, we might use <strong>Mean Squared Error (MSE)</strong>: $L = \frac{1}{m} \sum_{j=1}^{m} (y_j - \hat{y}_j)^2$ Here, $y_j$ is the actual correct value, $\hat{y}_j$ is the network’s prediction, and $m$ is the number of examples. The goal is to minimize this loss. If we’re classifying between categories, we might use <strong>Cross-Entropy Loss</strong>.</p> </li> <li> <p><strong>Gradient Descent (Finding the “Bottom of the Hill”):</strong> Imagine the loss function as a mountainous landscape, and we want to find the lowest valley. The weights and biases are our coordinates on this landscape. <strong>Gradient Descent</strong> is like taking small steps downhill. The “gradient” tells us the direction of the steepest ascent (uphill). So, to go downhill, we move in the <em>opposite</em> direction of the gradient. For each weight ($w$) and bias ($b$) in the network, we update it by subtracting a fraction of its gradient with respect to the loss: $w_{new} = w_{old} - \alpha \frac{\partial L}{\partial w}$ $b_{new} = b_{old} - \alpha \frac{\partial L}{\partial b}$ Here, $\alpha$ is the <strong>learning rate</strong>, a crucial hyperparameter that determines how big our steps are. Too large, and we might overshoot the valley; too small, and we might take forever to get there.</p> </li> <li> <p><strong>Backpropagation (The Credit Assignment Problem):</strong> Calculating those gradients ($\frac{\partial L}{\partial w}$) for every single weight in a deep network is computationally complex. This is where <strong>backpropagation</strong> shines. It’s an ingenious algorithm that efficiently calculates the gradients of the loss function with respect to every weight and bias in the network, working backward from the output layer to the input layer.</p> <p>Think of it this way: The output layer makes a prediction and feels the “pain” of being wrong (the loss). Backpropagation tells the <em>previous</em> layer how much each of its neurons contributed to that pain. That layer then adjusts its weights and passes on its “blame” to the layer before it, and so on, all the way back to the beginning. It’s like a feedback loop that allows the network to distribute credit or blame for its errors among all its constituent parts.</p> </li> </ol> <p>This iterative process of <strong>forward pass</strong> (making a prediction), <strong>calculating loss</strong>, and <strong>backward pass</strong> (adjusting weights via backpropagation and gradient descent) is how deep learning models learn to perform complex tasks with remarkable accuracy.</p> <h3 id="diving-deeper-specialized-architectures">Diving Deeper: Specialized Architectures</h3> <p>While the feedforward neural network (which we’ve discussed) is foundational, Deep Learning boasts several specialized architectures tailored for different types of data and tasks:</p> <ul> <li> <p><strong>Convolutional Neural Networks (CNNs):</strong> These are the workhorses for <strong>image and video data</strong>. Instead of connecting every neuron to every pixel, CNNs use “convolutional filters” to scan images, detecting local patterns like edges, textures, and shapes. They exploit the spatial relationships within an image. Think of how our eyes process small parts of a scene before combining them into a full picture. Famous for powering facial recognition, medical image analysis, and self-driving cars.</p> </li> <li> <p><strong>Recurrent Neural Networks (RNNs):</strong> Built for <strong>sequential data</strong> like text, audio, and time series. Unlike feedforward networks, RNNs have loops that allow information to persist from one step to the next, giving them a form of “memory.” This is vital for understanding context. Imagine reading a sentence: to understand the current word, you need to remember the previous ones. While basic RNNs struggle with long-term dependencies (the vanishing/exploding gradient problem), their more advanced siblings, <strong>Long Short-Term Memory (LSTM)</strong> networks and <strong>Gated Recurrent Units (GRUs)</strong>, largely solved these issues.</p> </li> <li> <p><strong>Transformers:</strong> The current state-of-the-art for <strong>natural language processing (NLP)</strong> and increasingly for other domains. They revolutionized sequence modeling by introducing the “attention mechanism,” allowing the network to weigh the importance of different parts of the input sequence when making a prediction. This lets them understand relationships between words regardless of their position, unlike RNNs that process sequentially. Think of models like GPT-3, ChatGPT, and BERT – they’re all built on the Transformer architecture.</p> </li> </ul> <h3 id="the-road-ahead-challenges-and-ethical-considerations">The Road Ahead: Challenges and Ethical Considerations</h3> <p>Despite their incredible power, Deep Learning models aren’t magic. They come with their own set of challenges:</p> <ul> <li> <strong>Data Hunger:</strong> They typically require enormous amounts of labeled data to train effectively.</li> <li> <strong>Computational Cost:</strong> Training large models can take days or weeks on powerful hardware.</li> <li> <strong>“Black Box” Problem:</strong> It can be hard to interpret <em>why</em> a deep neural network made a particular decision, leading to concerns in high-stakes applications like medicine or law.</li> <li> <strong>Hyperparameter Tuning:</strong> Choosing the right number of layers, neurons, learning rate, and other parameters often requires extensive experimentation.</li> <li> <strong>Bias and Fairness:</strong> If the training data contains biases (e.g., underrepresenting certain demographics), the model will learn and perpetuate those biases, leading to unfair or discriminatory outcomes. This is a critical ethical challenge facing the AI community.</li> </ul> <h3 id="my-continuing-journey-into-the-deep">My Continuing Journey into the Deep</h3> <p>Exploring Deep Learning has been an incredibly rewarding experience. From the elegant simplicity of a single perceptron to the staggering complexity of a Transformer, it’s a testament to human ingenuity and our endless quest to understand intelligence itself.</p> <p>It’s a field that is constantly evolving, with new architectures and techniques emerging regularly. The ability of these models to learn, adapt, and discover intricate patterns from raw data is not just fascinating; it’s profoundly impactful. Deep Learning is shaping the world around us in ways we’re only beginning to understand, from accelerating scientific discovery to creating new forms of art.</p> <p>If you’ve ever felt a spark of curiosity about how AI truly works, I encourage you to dive deeper. There are countless online resources, courses, and communities waiting to help you start your own journey. The future of AI is being written right now, and understanding Deep Learning is an essential chapter in that story.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>