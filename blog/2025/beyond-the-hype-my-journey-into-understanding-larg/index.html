<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Beyond the Hype: My Journey Into Understanding Large Language Models | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/beyond-the-hype-my-journey-into-understanding-larg/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Beyond the Hype: My Journey Into Understanding Large Language Models</h1> <p class="post-meta"> Created on July 11, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/natural-language-processing"> <i class="fa-solid fa-hashtag fa-sm"></i> Natural Language Processing</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   <a href="/blog/blog/tag/transformers"> <i class="fa-solid fa-hashtag fa-sm"></i> Transformers</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hello fellow explorers of the digital frontier!</p> <p>If you’re anything like me, your social media feeds, news outlets, and even casual conversations have been dominated by “AI” lately. Specifically, Large Language Models (LLMs) like ChatGPT have burst onto the scene, dazzling us with their ability to write essays, debug code, and even generate creative stories on demand. It’s easy to dismiss them as magic, or perhaps a complex parlor trick. But as a budding Data Scientist and MLE, I’ve always been driven by a desire to understand <em>how</em> things work, not just <em>that</em> they work. So, I embarked on a personal quest to demystify LLMs, and I’d love to share what I’ve discovered with you.</p> <p>This isn’t just about buzzwords; it’s about understanding a fundamental shift in how we interact with information and technology. Let’s dive deep, from the basic building blocks to the cutting-edge techniques that power these incredible systems.</p> <h3 id="whats-in-a-name-large-language-and-model">What’s in a Name? “Large,” “Language,” and “Model”</h3> <p>Before we dissect the inner workings, let’s break down the name itself: <strong>Large Language Models</strong>. Each word is significant.</p> <ol> <li> <p><strong>Large</strong>: This isn’t just a casual adjective; it’s a defining characteristic. We’re talking about models with billions, even trillions, of parameters. Think of parameters as the “knobs” or “switches” a model can adjust during training to learn patterns. The human brain has an estimated 86 billion neurons, each with thousands of connections. While not a direct comparison, the sheer scale of LLMs suggests a level of complexity that allows for incredibly nuanced understanding and generation of information. This “largeness” also implies massive datasets (terabytes of text) and immense computational power (think thousands of high-end GPUs training for months).</p> </li> <li> <p><strong>Language</strong>: This is what they’re designed for: understanding, generating, and manipulating human language. But how do computers “understand” words? It starts with <strong>tokens</strong>. LLMs don’t process words directly; they break text into smaller units called tokens (which can be words, parts of words, or punctuation marks). Each token is then converted into a numerical representation called an <strong>embedding</strong>.</p> <p>Imagine mapping every word in a dictionary to a point in a high-dimensional space. Words with similar meanings (e.g., “king” and “queen”) would be closer together in this space than dissimilar words (e.g., “king” and “banana”). These embeddings are dense vectors, typically hundreds or thousands of dimensions long. For a given token, say “cat,” its embedding $E_{cat}$ might be a vector like $E_{cat} \in \mathbb{R}^d$, where $d$ is the dimensionality of the embedding space. These embeddings are crucial because they allow the model to grasp semantic relationships and context.</p> </li> <li> <p><strong>Model</strong>: This refers to the architecture and algorithms that process these language embeddings. At its core, an LLM is a complex mathematical function that takes an input sequence of tokens (and their embeddings) and predicts the most likely next token, or a sequence of tokens. The most dominant architecture that enabled the LLM revolution is the <strong>Transformer</strong>.</p> </li> </ol> <h3 id="the-heart-of-an-llm-the-transformer-architecture">The Heart of an LLM: The Transformer Architecture</h3> <p>Before the Transformer, recurrent neural networks (RNNs) and long short-term memory (LSTMs) were the go-to for sequential data like text. They processed words one by one, maintaining a “memory” of previous words. The problem? They struggled with long-range dependencies; by the time they got to the end of a long sentence, they might have “forgotten” the beginning.</p> <p>In 2017, Google Brain published “Attention Is All You Need,” introducing the Transformer. This paper changed everything, largely due to a mechanism called <strong>Self-Attention</strong>.</p> <h4 id="self-attention-understanding-context">Self-Attention: Understanding Context</h4> <p>Imagine you’re reading the sentence: “The bank of the river was overgrown with moss, so I sat on the bank of my savings account.” As a human, you effortlessly understand that “bank” means two different things in that sentence. Traditional RNNs might struggle with this because they process sequentially without a strong mechanism to weigh the importance of other words for the current word’s meaning.</p> <p>Self-attention allows each word in a sequence to “look” at every other word in the same sequence and decide how much attention to pay to them. It does this by computing three vectors for each token’s embedding: a <strong>Query</strong> ($Q$), a <strong>Key</strong> ($K$), and a <strong>Value</strong> ($V$). These are derived by multiplying the token’s embedding by three different weight matrices ($W_Q, W_K, W_V$) that the model learns during training.</p> <p>Here’s the simplified idea:</p> <ol> <li>For each word, it generates a <strong>Query</strong> (what am I looking for?).</li> <li>It compares this Query to the <strong>Keys</strong> of all other words (what do other words have to offer?).</li> <li>The similarity of a Query to a Key determines an <strong>attention score</strong>.</li> <li>These scores are normalized (using a softmax function) to get <strong>attention weights</strong> – telling us how much attention each other word deserves.</li> <li>Finally, these attention weights are used to compute a weighted sum of the <strong>Values</strong> of all words. This sum becomes the new, context-rich representation for the original word.</li> </ol> <p>The core math for scaled dot-product attention looks like this:</p> \[Attention(Q, K, V) = softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V\] <p>Where:</p> <ul> <li>$Q$ is the Query matrix (stacked queries for all words).</li> <li>$K$ is the Key matrix (stacked keys for all words).</li> <li>$V$ is the Value matrix (stacked values for all words).</li> <li>$d_k$ is the dimension of the keys, used for scaling to prevent vanishing gradients.</li> </ul> <p>This mechanism allows the model to process all words in a sentence simultaneously, vastly improving efficiency and ability to capture long-range dependencies.</p> <h4 id="multi-head-attention-positional-encoding-and-more">Multi-Head Attention, Positional Encoding, and More</h4> <ul> <li> <strong>Multi-Head Attention</strong>: Instead of just one set of $Q, K, V$ matrices, Transformers use multiple “attention heads.” Each head learns to focus on different aspects of relationships between words (e.g., one head might focus on grammatical dependencies, another on semantic relationships). The results from these heads are concatenated and transformed.</li> <li> <strong>Positional Encoding</strong>: Since self-attention processes words in parallel without inherent order, we need to tell the model about word positions. Positional encodings are vectors added to the word embeddings, providing information about their absolute and relative positions in the sequence. This ensures “cat bites dog” is different from “dog bites cat.”</li> <li> <strong>Feed-Forward Networks</strong>: After attention, each token’s representation passes through a simple, position-wise feed-forward neural network, adding non-linearity to the model’s capacity.</li> <li> <strong>Residual Connections and Layer Normalization</strong>: These techniques help stabilize the training of deep networks by allowing gradients to flow more easily and ensuring consistent signal distribution.</li> </ul> <p>Most modern LLMs, like GPT-3/4, are <strong>decoder-only Transformers</strong>. This means they focus on generating output text token by token, leveraging an attention mechanism that only looks at previous tokens (masked attention), which is perfect for predicting the next word in a sequence.</p> <h3 id="learning-to-speak-pre-training">Learning to Speak: Pre-training</h3> <p>How do these models acquire their vast knowledge of language? Through a process called <strong>pre-training</strong>. This is an unsupervised learning phase where the model is exposed to an enormous amount of raw text data—think billions of web pages, books, articles, code, and more.</p> <p>The primary task during pre-training is <strong>Next Token Prediction (NTP)</strong>. Given a sequence of tokens, the model’s goal is to predict the very next token. For example, if the input is “The quick brown fox”, the model tries to predict “jumps”. It learns to do this by minimizing a <strong>cross-entropy loss function</strong>:</p> \[L = -\sum_{i=1}^{V} y_i \log(\hat{y}_i)\] <p>Where $V$ is the vocabulary size, $y_i$ is 1 for the correct next token and 0 otherwise (one-hot encoding), and $\hat{y}_i$ is the model’s predicted probability for token $i$. This loss guides the model to make better predictions.</p> <p>By predicting the next word over and over again on colossal datasets, the LLM develops an incredible statistical understanding of language: grammar, syntax, facts, common sense, and even subtle nuances. It’s like reading every book in existence and constantly being tested on what word comes next in any sentence.</p> <h3 id="refining-the-art-fine-tuning-and-alignment">Refining the Art: Fine-tuning and Alignment</h3> <p>While pre-training gives the LLM its core language capabilities, the resulting model might still be clunky, prone to generating gibberish, or even harmful content. This is where <strong>fine-tuning</strong> comes in.</p> <ol> <li> <p><strong>Supervised Fine-Tuning (SFT) / Instruction Tuning</strong>: After pre-training, the model is further trained on smaller, high-quality datasets of prompt-response pairs. These are human-curated examples demonstrating how the model should behave (e.g., “Summarize this article:” followed by a human-written summary). This teaches the model to follow instructions and generate helpful responses.</p> </li> <li> <p><strong>Reinforcement Learning from Human Feedback (RLHF)</strong>: This is the “secret sauce” behind models like ChatGPT, aligning them with human values and preferences. It’s a three-step process:</p> <ul> <li> <strong>Collect Demonstration Data and SFT the model</strong>: The initial model is fine-tuned with human-written prompt-response examples.</li> <li> <strong>Train a Reward Model (RM)</strong>: Human annotators are asked to rank multiple responses generated by the LLM for a given prompt (e.g., “Which response is better: A, B, C, or D?”). This preference data is used to train a separate smaller neural network, the Reward Model, which learns to predict human preferences. It effectively acts as an automated judge.</li> <li> <strong>Optimize the LLM with the Reward Model</strong>: The LLM is then fine-tuned using a reinforcement learning algorithm (often Proximal Policy Optimization, PPO). The LLM generates responses, the Reward Model assigns a score (reward) to each, and the LLM learns to generate responses that maximize this reward, thereby aligning itself with human preferences for helpfulness, harmlessness, and honesty.</li> </ul> </li> </ol> <p>This careful alignment process transforms a raw language predictor into a helpful assistant.</p> <h3 id="beyond-the-basics-scaling-laws-and-emergent-abilities">Beyond the Basics: Scaling Laws and Emergent Abilities</h3> <p>One of the fascinating discoveries in LLM research is the existence of <strong>scaling laws</strong>. Roughly speaking, as you increase the number of parameters in the model, the amount of training data, and the computational budget, the performance of the model improves predictably. This has driven the trend towards ever-larger models.</p> <p>Even more intriguing are <strong>emergent abilities</strong>. These are capabilities that aren’t present in smaller models but “emerge” seemingly spontaneously when models reach a certain scale. Examples include in-context learning (performing tasks from examples without explicit fine-tuning), multi-step reasoning, and following complex instructions. It’s like individual components suddenly working together to form something greater than the sum of their parts.</p> <h3 id="applications--impact-what-can-they-do">Applications &amp; Impact: What Can They Do?</h3> <p>The capabilities of LLMs are truly transformative:</p> <ul> <li> <strong>Content Generation</strong>: Writing articles, marketing copy, poetry, scripts, or even full novels.</li> <li> <strong>Customer Service &amp; Personal Assistants</strong>: Powering chatbots that can answer complex queries, schedule appointments, and provide support.</li> <li> <strong>Code Generation &amp; Debugging</strong>: Assisting developers by writing code snippets, explaining code, and identifying errors.</li> <li> <strong>Information Retrieval &amp; Summarization</strong>: Quickly extracting key information from vast amounts of text or summarizing long documents.</li> <li> <strong>Translation</strong>: Breaking down language barriers.</li> <li> <strong>Creative Exploration</strong>: Brainstorming ideas, generating different perspectives, or even composing music.</li> </ul> <h3 id="the-road-ahead-challenges-and-ethical-considerations">The Road Ahead: Challenges and Ethical Considerations</h3> <p>Despite their immense power, LLMs are far from perfect and pose significant challenges:</p> <ul> <li> <strong>Hallucinations</strong>: They can generate factually incorrect information with high confidence, “making things up” because they are predicting the most statistically probable next token, not necessarily the truth.</li> <li> <strong>Bias</strong>: LLMs learn from the data they’re trained on. If that data contains biases (e.g., gender, racial, cultural), the model will reflect and even amplify those biases.</li> <li> <strong>Computational Cost</strong>: Training and running LLMs consume enormous amounts of energy, raising environmental concerns.</li> <li> <strong>Interpretability</strong>: They are “black boxes.” It’s incredibly difficult to understand <em>why</em> an LLM makes a particular decision or generates a specific output.</li> <li> <strong>Misinformation &amp; Malicious Use</strong>: The ability to generate convincing fake content, from deepfakes to spam, raises concerns about the spread of misinformation and potential for malicious use.</li> <li> <strong>Ethical Implications</strong>: Questions around copyright, job displacement, and the nature of intelligence itself are becoming increasingly relevant.</li> </ul> <h3 id="conclusion">Conclusion</h3> <p>Our journey through the world of Large Language Models reveals a fascinating blend of elegant mathematics, intricate architecture, and vast computational power. From the humble token embedding to the sophisticated dance of self-attention, and finally, the meticulous process of pre-training and alignment, LLMs represent a pinnacle of modern AI engineering.</p> <p>They are not magic, but rather powerful statistical engines that have learned the intricate patterns of human language to an unprecedented degree. Understanding their underlying mechanisms is not just intellectually satisfying; it equips us to better harness their potential, mitigate their risks, and navigate the exciting, yet challenging, future they are helping to create.</p> <p>The field is evolving at lightning speed. What we’ve covered today is merely a snapshot. I encourage you to keep exploring, keep questioning, and perhaps even build your own small language model someday. The future of AI is truly in our hands!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>