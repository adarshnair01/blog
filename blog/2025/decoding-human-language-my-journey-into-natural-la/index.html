<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Decoding Human Language: My Journey into Natural Language Processing | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/2025/decoding-human-language-my-journey-into-natural-la/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Decoding Human Language: My Journey into Natural Language Processing</h1> <p class="post-meta"> Created on May 15, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/natural-language-processing"> <i class="fa-solid fa-hashtag fa-sm"></i> Natural Language Processing</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/text-analytics"> <i class="fa-solid fa-hashtag fa-sm"></i> Text Analytics</a>   <a href="/blog/blog/tag/artificial-intelligence"> <i class="fa-solid fa-hashtag fa-sm"></i> Artificial Intelligence</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>From the moment we’re born, language surrounds us. It’s how we express thoughts, share ideas, and connect with the world. We inherently understand nuance, context, and even sarcasm. But imagine trying to teach a computer all of that – a machine that fundamentally only understands 0s and 1s. This seemingly impossible challenge is the core of <strong>Natural Language Processing (NLP)</strong>, and it’s a field that has utterly captivated me.</p> <p>My journey into data science began with a fascination for making sense of complex information, but it was NLP that truly felt like peering into the future. It’s the magic behind virtual assistants like Siri and Alexa, the brains of Google Translate, and the silent guardian sifting through your emails for spam. It’s about bridging the colossal gap between human communication – fluid, messy, and infinitely complex – and the rigid logic of computers.</p> <h3 id="why-is-language-so-hard-for-computers">Why Is Language So Hard for Computers?</h3> <p>Before we dive into <em>how</em> computers learn language, let’s appreciate <em>why</em> it’s such a monumental task. As humans, we take our linguistic abilities for granted. But consider these challenges:</p> <ol> <li> <strong>Ambiguity:</strong> “I saw a man with a telescope.” Was the man holding the telescope, or was I using a telescope to see a man? Or perhaps I was a man who owned a telescope and saw something else? Context is everything!</li> <li> <strong>Synonymy &amp; Polysemy:</strong> The same word can have multiple meanings (<em>bank</em> - river bank vs. financial bank), and different words can have the same meaning (<em>car</em>, <em>automobile</em>, <em>vehicle</em>).</li> <li> <strong>Sarcasm &amp; Irony:</strong> “Oh, what <em>brilliant</em> weather we’re having!” (said during a torrential downpour). A computer struggles immensely with this.</li> <li> <strong>Evolving Language:</strong> New words, slang, and phrases emerge constantly. Remember “yeet” or “rizz”?</li> <li> <strong>Grammar &amp; Syntax:</strong> While we have rules, there are countless exceptions, and sentence structures vary wildly.</li> </ol> <p>These inherent complexities mean that a simple dictionary lookup isn’t enough. We need sophisticated methods to empower machines to truly “understand.”</p> <h3 id="the-nlp-toolkit-from-basic-prep-to-deep-insights">The NLP Toolkit: From Basic Prep to Deep Insights</h3> <p>My first foray into NLP felt like learning the alphabet of text data. Before any fancy algorithms, text needs to be cleaned and structured. Think of it as preparing raw ingredients before cooking a gourmet meal.</p> <h4 id="1-tokenization-breaking-it-down">1. Tokenization: Breaking It Down</h4> <p>The first step is usually <strong>tokenization</strong> – splitting a stream of text into smaller units called “tokens.” These can be words, punctuation marks, or even sub-word units.</p> <p><em>Example:</em> “Hello, world! How are you?” Tokens: <code class="language-plaintext highlighter-rouge">["Hello", ",", "world", "!", "How", "are", "you", "?"]</code></p> <h4 id="2-stop-word-removal-filtering-the-noise">2. Stop Word Removal: Filtering the Noise</h4> <p>Common words like “a,” “an,” “the,” “is,” “are” provide little semantic value for many tasks (like sentiment analysis or topic modeling). Removing these <strong>stop words</strong> helps focus on the more meaningful terms.</p> <p><em>Example (after stop word removal):</em> Original: “The quick brown fox jumps over the lazy dog.” Processed: <code class="language-plaintext highlighter-rouge">["quick", "brown", "fox", "jumps", "lazy", "dog"]</code></p> <h4 id="3-stemming--lemmatization-getting-to-the-root">3. Stemming &amp; Lemmatization: Getting to the Root</h4> <p>Words often appear in different forms (e.g., “run,” “running,” “runs,” “ran”). To treat them as the same underlying concept, we reduce them to a base form.</p> <ul> <li> <strong>Stemming:</strong> A crude heuristic process that chops off suffixes. It’s fast but can produce non-dictionary words. <ul> <li> <code class="language-plaintext highlighter-rouge">running</code> -&gt; <code class="language-plaintext highlighter-rouge">run</code> </li> <li> <code class="language-plaintext highlighter-rouge">connection</code> -&gt; <code class="language-plaintext highlighter-rouge">connect</code> </li> <li> <code class="language-plaintext highlighter-rouge">universal</code> -&gt; <code class="language-plaintext highlighter-rouge">univers</code> (oops!)</li> </ul> </li> <li> <strong>Lemmatization:</strong> A more sophisticated, dictionary-based process that returns the actual base form (lemma) of a word, considering its context and Part-of-Speech (POS). <ul> <li> <code class="language-plaintext highlighter-rouge">running</code> -&gt; <code class="language-plaintext highlighter-rouge">run</code> </li> <li> <code class="language-plaintext highlighter-rouge">better</code> -&gt; <code class="language-plaintext highlighter-rouge">good</code> </li> <li> <code class="language-plaintext highlighter-rouge">are</code> -&gt; <code class="language-plaintext highlighter-rouge">be</code> </li> </ul> </li> </ul> <p>Lemmatization is generally preferred when accuracy is paramount.</p> <h4 id="4-part-of-speech-pos-tagging-understanding-roles">4. Part-of-Speech (POS) Tagging: Understanding Roles</h4> <p><strong>POS tagging</strong> assigns a grammatical category (noun, verb, adjective, adverb, etc.) to each word. This helps in understanding the syntactic structure of a sentence.</p> <p><em>Example:</em> “The (DT) quick (JJ) brown (JJ) fox (NN) jumps (VBZ) over (IN) the (DT) lazy (JJ) dog (NN).” <em>(DT: Determiner, JJ: Adjective, NN: Noun, VBZ: Verb (3rd person singular present), IN: Preposition)</em></p> <h4 id="5-named-entity-recognition-ner-spotting-key-information">5. Named Entity Recognition (NER): Spotting Key Information</h4> <p><strong>NER</strong> is about identifying and classifying “named entities” into predefined categories like person names, organizations, locations, dates, etc. It’s incredibly useful for information extraction.</p> <p><em>Example:</em> “<strong>Apple</strong> (ORG) acquired <strong>X Company</strong> (ORG) in <strong>California</strong> (LOC) last <strong>Tuesday</strong> (DATE).”</p> <p>These initial steps are crucial. They transform raw, unstructured text into a more digestible format for machine learning models.</p> <h3 id="from-rules-to-learning-the-rise-of-machine-learning">From Rules to Learning: The Rise of Machine Learning</h3> <p>Early NLP systems were often rule-based, relying on meticulously crafted grammars and lexicons. While precise, these systems were brittle, hard to scale, and couldn’t adapt to new language variations. This is where machine learning swept in, ushering in an era of statistical NLP.</p> <p>The core idea? Instead of explicitly programming rules, we feed the computer vast amounts of text data and let it <em>learn</em> patterns. But computers don’t understand words directly; they need numbers. So, we had to figure out how to represent text numerically.</p> <h4 id="1-bag-of-words-bow-a-simple-start">1. Bag-of-Words (BoW): A Simple Start</h4> <p>One of the simplest ways is the <strong>Bag-of-Words (BoW)</strong> model. Imagine each document as a “bag” of words, where the order doesn’t matter, only the frequency of each word. We create a vocabulary of all unique words in our entire collection of documents (corpus), and then each document is represented as a vector showing how many times each word from the vocabulary appears in it.</p> <p><em>Example:</em> Document 1: “The cat sat on the mat.” Document 2: “The dog ate the cat.” Vocabulary: <code class="language-plaintext highlighter-rouge">{"the":0, "cat":1, "sat":2, "on":3, "mat":4, "dog":5, "ate":6}</code></p> <p>Vector for Doc 1: <code class="language-plaintext highlighter-rouge">[2, 1, 1, 1, 1, 0, 0]</code> (counts for “the”, “cat”, “sat”, “on”, “mat”, “dog”, “ate”) Vector for Doc 2: <code class="language-plaintext highlighter-rouge">[2, 1, 0, 0, 0, 1, 1]</code></p> <p>While simple, BoW loses all information about word order and context, which, as we discussed, is vital for human language.</p> <h4 id="2-tf-idf-weighing-importance">2. TF-IDF: Weighing Importance</h4> <p>To improve upon BoW, the <strong>TF-IDF (Term Frequency-Inverse Document Frequency)</strong> technique emerged. It not only counts word occurrences but also gives more weight to words that are important in a specific document <em>and</em> are relatively rare across the entire corpus. This helps identify truly significant terms.</p> <p>The calculation involves two parts:</p> <ul> <li> <strong>Term Frequency (TF):</strong> How often a term <code class="language-plaintext highlighter-rouge">t</code> appears in a document <code class="language-plaintext highlighter-rouge">d</code>. $TF(t,d) = \frac{\text{Number of times term t appears in document d}}{\text{Total number of terms in document d}}$</li> <li> <strong>Inverse Document Frequency (IDF):</strong> A measure of how rare or common a term <code class="language-plaintext highlighter-rouge">t</code> is across all documents <code class="language-plaintext highlighter-rouge">D</code> in the corpus. $IDF(t,D) = \log \frac{\text{Total number of documents D}}{\text{Number of documents with term t}}$ (The log helps to dampen the effect of very large differences).</li> </ul> <p>Finally, we multiply them: $TF-IDF(t,d,D) = TF(t,d) \times IDF(t,D)$</p> <p>A high TF-IDF score indicates a term is highly relevant to a specific document. This numeric representation allowed us to apply powerful machine learning algorithms like Naive Bayes or Support Vector Machines for tasks like spam detection or sentiment classification.</p> <h3 id="the-deep-learning-revolution-understanding-meaning-and-context">The Deep Learning Revolution: Understanding Meaning and Context</h3> <p>While statistical methods were a huge leap forward, they still struggled with capturing the true semantic meaning and intricate relationships between words. The “bag” approach inherently ignored sequence. This is where <strong>deep learning</strong> changed everything.</p> <h4 id="1-word-embeddings-words-as-vectors-of-meaning">1. Word Embeddings: Words as Vectors of Meaning</h4> <p>Instead of simple counts, deep learning introduced <strong>word embeddings</strong>. These are dense, low-dimensional vectors where each word is mapped to a point in a multi-dimensional space. The magic? Words with similar meanings are located close to each other in this space.</p> <p>Models like <strong>Word2Vec</strong> and <strong>GloVe</strong> learn these embeddings by predicting context or co-occurrence. This means that if “king” is near “queen” and “man” is near “woman,” then the vector arithmetic often holds true:</p> <p>$vector(“king”) - vector(“man”) + vector(“woman”) \approx vector(“queen”)$</p> <p>This ability to capture semantic relationships was groundbreaking. Suddenly, computers had a rudimentary understanding of word meaning and analogy, far beyond just frequency.</p> <h4 id="2-recurrent-neural-networks-rnns-and-lstms-remembering-the-past">2. Recurrent Neural Networks (RNNs) and LSTMs: Remembering the Past</h4> <p>To handle sequences (like sentences), <strong>Recurrent Neural Networks (RNNs)</strong> were developed. Unlike traditional neural networks, RNNs have loops that allow information to persist, acting as a “memory” of previous words. This made them suitable for tasks like machine translation or text generation.</p> <p>However, basic RNNs struggled with long sequences, often forgetting information from early parts of a text (the “vanishing gradient” problem). This led to the creation of <strong>Long Short-Term Memory (LSTM)</strong> networks, a special type of RNN designed to remember information for much longer periods. LSTMs became the workhorses for many sequential NLP tasks.</p> <h4 id="3-transformers-the-game-changer">3. Transformers: The Game Changer</h4> <p>While LSTMs were powerful, they processed information sequentially, making them slow for very long texts and difficult to parallelize. Then came the <strong>Transformer architecture</strong> in 2017 with the paper “Attention Is All You Need.”</p> <p>The key innovation of Transformers is the <strong>attention mechanism</strong>. Instead of processing words one by one, attention allows the model to weigh the importance of different words in the input sequence when processing any single word. For example, when processing the word “its” in “The animal didn’t cross the street because its legs were injured,” the attention mechanism helps the model realize “its” refers to “animal.”</p> <p>Transformers can process all words in parallel, making them much faster and better at capturing long-range dependencies. This architecture powers the most advanced NLP models today:</p> <ul> <li> <strong>BERT (Bidirectional Encoder Representations from Transformers):</strong> Google’s model that understands context from both left-to-right and right-to-left. It’s fantastic for understanding existing text.</li> <li> <strong>GPT (Generative Pre-trained Transformer):</strong> Developed by OpenAI, these models are exceptional at generating human-like text, translating, summarizing, and answering questions. GPT-3 and GPT-4 are the large language models you’ve likely heard of.</li> </ul> <p>These models, trained on gargantuan amounts of text data from the internet, can perform a wide array of NLP tasks with astonishing accuracy, often surpassing human performance on specific benchmarks.</p> <h3 id="real-world-magic-where-nlp-shines">Real-World Magic: Where NLP Shines</h3> <p>The applications of NLP are vast and growing every day:</p> <ul> <li> <strong>Machine Translation:</strong> Seamlessly translating languages, connecting people across borders (Google Translate, DeepL).</li> <li> <strong>Sentiment Analysis:</strong> Understanding the emotional tone of text – crucial for customer feedback, social media monitoring, and market research.</li> <li> <strong>Chatbots &amp; Virtual Assistants:</strong> The conversational AI that powers customer service, smart home devices, and personal assistants (Siri, Alexa, Google Assistant).</li> <li> <strong>Spam Detection:</strong> Filtering unwanted emails by identifying suspicious language patterns.</li> <li> <strong>Text Summarization:</strong> Condensing lengthy documents or articles into concise summaries.</li> <li> <strong>Information Extraction:</strong> Automatically pulling out specific data (e.g., dates, names, product details) from unstructured text, useful in legal or medical fields.</li> <li> <strong>Code Autocompletion &amp; Generation:</strong> Helping programmers write code faster and even generating entire functions.</li> </ul> <h3 id="the-future-and-ethical-considerations">The Future and Ethical Considerations</h3> <p>NLP is one of the most dynamic and exciting fields in AI. We’re seeing models become incredibly sophisticated, capable of nuanced conversation, creative writing, and complex problem-solving. The line between human-generated and machine-generated text is blurring, and the potential for new applications is limitless.</p> <p>However, with great power comes great responsibility. The models learn from the data they’re trained on. If that data contains biases (e.g., gender, racial, or cultural), the models will reflect and even amplify those biases. This can lead to unfair or discriminatory outcomes in critical applications. Issues like misinformation, the generation of “deepfakes” in text, and privacy concerns also demand our careful attention.</p> <p>As we push the boundaries of what machines can understand and generate, it’s paramount that we, as developers and users, consider the ethical implications and strive to build AI that is fair, transparent, and beneficial to all of humanity.</p> <h3 id="my-ongoing-adventure">My Ongoing Adventure</h3> <p>My journey into NLP has been a thrilling ride, a continuous learning process that bridges linguistics, computer science, and mathematics. It’s a field that constantly surprises with its rapid advancements and the sheer ingenuity of the solutions developed. From the humble beginnings of counting words to the profound capabilities of understanding context through attention mechanisms, NLP has truly transformed how we interact with technology and understand the vast ocean of text data around us.</p> <p>If you’re curious about data science or AI, I strongly encourage you to dip your toes into NLP. Start with some Python libraries like NLTK or spaCy, experiment with sentiment analysis, or try building a simple chatbot. The tools are more accessible than ever, and the impact you can make is immense. The language of machines is evolving, and it’s an exciting time to be part of the conversation.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>