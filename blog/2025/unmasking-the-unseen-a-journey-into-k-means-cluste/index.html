<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Unmasking the Unseen: A Journey into K-Means Clustering | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2025/unmasking-the-unseen-a-journey-into-k-means-cluste/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Unmasking the Unseen: A Journey into K-Means Clustering</h1> <p class="post-meta"> Created on January 29, 2025 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/k-means"> <i class="fa-solid fa-hashtag fa-sm"></i> K-Means</a>   <a href="/blog/blog/tag/unsupervised-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Unsupervised Learning</a>   <a href="/blog/blog/tag/clustering"> <i class="fa-solid fa-hashtag fa-sm"></i> Clustering</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>Welcome back to my corner of the data science universe. Today, I’m absolutely thrilled to pull back the curtain on an algorithm that, for me, truly embodies the “magic” of machine learning: <strong>K-Means Clustering</strong>.</p> <p>Imagine you’ve just inherited a massive, unlabeled box of LEGO bricks. You want to organize them, but there are no instructions, no color-coded compartments, nothing. Your goal is simply to group similar bricks together. You might start by picking a few random bricks to represent your “main types,” then sort all the other bricks into piles based on which “type” they’re most like. Once you’ve done that, you might look at your piles and realize, “Okay, maybe <em>this</em> brick is a better representative for this pile than the one I picked initially.” So, you pick new representatives, and then re-sort all the bricks again. You repeat this process until your piles feel “right” and stable.</p> <p>Believe it or not, you’ve just conceptualized K-Means Clustering! It’s an unsupervised learning algorithm that does exactly this, but with data points instead of LEGOs. It’s about finding inherent groups within data without any prior labels or categories. And trust me, it’s one of the most fundamental and widely used tools in a data scientist’s toolkit.</p> <h3 id="whats-the-big-idea-behind-k-means">What’s the Big Idea Behind K-Means?</h3> <p>At its heart, K-Means aims to partition <code class="language-plaintext highlighter-rouge">n</code> data points into <code class="language-plaintext highlighter-rouge">k</code> distinct, non-overlapping subgroups, or <strong>clusters</strong>. The ‘K’ in K-Means literally stands for the number of clusters we want to find. The goal? To make the data points within each cluster as similar to each other as possible, while making data points in different clusters as dissimilar as possible.</p> <p>Think about it:</p> <ul> <li> <strong>Customer Segmentation:</strong> Grouping customers by purchasing behavior to tailor marketing strategies.</li> <li> <strong>Document Classification:</strong> Automatically sorting articles into topics like “sports,” “politics,” or “technology.”</li> <li> <strong>Image Compression:</strong> Reducing the number of colors in an image by grouping similar pixel colors.</li> </ul> <p>The applications are everywhere, and they all start with this simple, yet powerful, idea of grouping.</p> <h3 id="the-k-means-algorithm-a-four-step-dance">The K-Means Algorithm: A Four-Step Dance</h3> <p>Let’s break down the mechanics. The algorithm is iterative, meaning it repeats a set of steps until a certain condition is met (usually when the clusters stabilize).</p> <h4 id="step-1-initialization--choose-your-k-and-drop-your-centroids">Step 1: Initialization – Choose Your <code class="language-plaintext highlighter-rouge">k</code> and Drop Your Centroids</h4> <p>The first, and perhaps most crucial, decision is to pick <code class="language-plaintext highlighter-rouge">k</code>, the number of clusters you want to find. This often requires some domain knowledge or a bit of experimentation (we’ll talk about how later).</p> <p>Once <code class="language-plaintext highlighter-rouge">k</code> is chosen, the algorithm randomly selects <code class="language-plaintext highlighter-rouge">k</code> data points from your dataset to serve as the initial <strong>centroids</strong> (the “representatives” of your LEGO piles). These centroids are essentially the center points of your yet-to-be-formed clusters.</p> <h4 id="step-2-assignment--every-point-finds-its-home">Step 2: Assignment – Every Point Finds Its Home</h4> <p>Now, for every single data point in your dataset, we calculate its distance to <em>each</em> of the <code class="language-plaintext highlighter-rouge">k</code> centroids. The most common way to measure this “distance” is using <strong>Euclidean distance</strong>. For two points, $x = (x_1, x_2, \dots, x_n)$ and $y = (y_1, y_2, \dots, y_n)$, the Euclidean distance is:</p> <p>$d(x,y) = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}$</p> <p>In simpler terms, it’s the straight-line distance between two points in space.</p> <p>Once we’ve calculated all these distances, each data point is assigned to the cluster whose centroid it is <em>closest</em> to. This forms our initial <code class="language-plaintext highlighter-rouge">k</code> clusters.</p> <h4 id="step-3-update--redefine-your-centers">Step 3: Update – Redefine Your Centers</h4> <p>With all data points now assigned to a cluster, the initial, randomly placed centroids probably aren’t the best representation of their respective groups. So, for each cluster, we recalculate its centroid. The new centroid is simply the <strong>mean</strong> (average) of all the data points currently assigned to that cluster.</p> <table> <tbody> <tr> <td>If $C_j$ is the set of data points in cluster $j$, and $</td> <td>C_j</td> <td>$ is the number of points in cluster $j$, the new centroid $\mu_j$ is calculated as:</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$\mu_j = \frac{1}{</td> <td>C_j</td> <td>} \sum_{x \in C_j} x$</td> </tr> </tbody> </table> <p>This step moves the centroids to the true “center of gravity” of their current clusters, making them better representatives.</p> <h4 id="step-4-iterate--repeat-until-stable">Step 4: Iterate – Repeat Until Stable</h4> <p>Steps 2 and 3 are repeated.</p> <ul> <li>Data points are re-assigned to the <em>newest</em> closest centroids.</li> <li>Centroids are recalculated based on their <em>new</em> assigned points.</li> </ul> <p>This iterative process continues until the centroids no longer move significantly, or until a maximum number of iterations is reached. When the centroids stop changing positions, it means the clusters have stabilized, and our algorithm has converged.</p> <h3 id="a-mental-walkthrough-imagine-the-data-dancing">A Mental Walkthrough: Imagine the Data Dancing</h3> <p>Let’s visualize this. Picture a scatter plot of data points on a graph.</p> <ol> <li> <strong>Start:</strong> We pick <code class="language-plaintext highlighter-rouge">k</code> arbitrary points as initial centroids (maybe two red crosses, two blue crosses).</li> <li> <strong>Assignment:</strong> Every data point “looks” at the crosses and decides, “I’m closer to the red cross!” or “I’m closer to the blue cross!” It then changes its color to match its closest cross. Suddenly, your plot has red points and blue points.</li> <li> <strong>Update:</strong> Now, the red cross looks at all the red points, and moves itself to their average location. The blue cross does the same.</li> <li> <strong>Repeat:</strong> With the crosses moved, some points might now find themselves closer to the <em>other</em> cross. They switch colors! Then the crosses move again…</li> <li> <strong>Converge:</strong> This “dancing” of points and crosses continues until no point wants to switch its color, and no cross wants to move. You’re left with clear, distinct clusters.</li> </ol> <p>Pretty neat, right?</p> <h3 id="the-cost-of-clustering-what-k-means-tries-to-minimize">The “Cost” of Clustering: What K-Means Tries to Minimize</h3> <p>Behind the scenes, K-Means isn’t just randomly moving centroids. It’s trying to optimize a specific objective. This objective is usually defined by the <strong>Within-Cluster Sum of Squares (WCSS)</strong>, also known as <strong>Inertia</strong>.</p> <p>WCSS measures the sum of the squared distances between each data point and the centroid of the cluster it belongs to.</p> <p>$WCSS = \sum_{j=1}^{k} \sum_{x \in C_j} |x - \mu_j|^2$</p> <p>Here:</p> <ul> <li>$k$ is the number of clusters.</li> <li>$C_j$ is the $j$-th cluster.</li> <li>$x$ is a data point in cluster $C_j$.</li> <li>$\mu_j$ is the centroid of cluster $C_j$.</li> <li>$|x - \mu_j|^2$ is the squared Euclidean distance between point $x$ and centroid $\mu_j$.</li> </ul> <p>The goal of the K-Means algorithm is to <strong>minimize this WCSS</strong>. By minimizing it, we’re essentially making the clusters as compact and “tight” as possible, ensuring that points within a cluster are very close to their centroid.</p> <p>It’s important to note that K-Means uses a greedy approach, and due to its dependence on initial centroid placement, it might converge to a <strong>local optimum</strong> rather than the global optimum. This is why running the algorithm multiple times with different random initializations (often controlled by a <code class="language-plaintext highlighter-rouge">n_init</code> parameter in libraries like scikit-learn) is a common practice, and the run with the lowest WCSS is typically chosen.</p> <h3 id="the-million-dollar-question-how-do-we-choose-k">The Million-Dollar Question: How Do We Choose <code class="language-plaintext highlighter-rouge">k</code>?</h3> <p>This is often the trickiest part of K-Means. How do you know if you should group your LEGOs into 3 piles or 7? Here are a couple of popular methods:</p> <h4 id="the-elbow-method">The Elbow Method</h4> <p>This is probably the most common heuristic.</p> <ol> <li>Run K-Means for a range of <code class="language-plaintext highlighter-rouge">k</code> values (e.g., from 1 to 10).</li> <li>For each <code class="language-plaintext highlighter-rouge">k</code>, calculate the WCSS (Inertia).</li> <li>Plot the WCSS values against the number of clusters <code class="language-plaintext highlighter-rouge">k</code>.</li> </ol> <p>What you’re looking for is an “elbow” in the graph. As you increase <code class="language-plaintext highlighter-rouge">k</code>, WCSS will generally decrease (because adding more clusters will always reduce the distance of points to their closest centroid). However, at some point, adding more clusters provides diminishing returns, and the rate of decrease in WCSS will slow down dramatically – this is your “elbow.” It suggests that after this <code class="language-plaintext highlighter-rouge">k</code>, you’re just splitting existing, well-formed clusters, rather than finding genuinely new ones.</p> <p>It’s called the elbow method because the plot often looks like an arm, and the optimal <code class="language-plaintext highlighter-rouge">k</code> is at the bend of the elbow.</p> <h4 id="other-methods-briefly">Other Methods (Briefly):</h4> <ul> <li> <strong>Silhouette Score:</strong> Measures how similar a data point is to its own cluster compared to other clusters. A higher silhouette score indicates better-defined clusters.</li> <li> <strong>Domain Knowledge:</strong> Sometimes, the problem itself dictates <code class="language-plaintext highlighter-rouge">k</code>. If you know you want to segment customers into “low,” “medium,” and “high” value, <code class="language-plaintext highlighter-rouge">k=3</code> might be a sensible starting point.</li> </ul> <h3 id="k-means-strengths-and-weaknesses">K-Means: Strengths and Weaknesses</h3> <p>No algorithm is perfect, and K-Means is no exception.</p> <h4 id="strengths">Strengths:</h4> <ul> <li> <strong>Simplicity:</strong> Easy to understand and implement.</li> <li> <strong>Speed:</strong> Relatively fast, especially for large datasets, because it only computes distances to centroids and updates means.</li> <li> <strong>Scalability:</strong> Performs well on large datasets.</li> <li> <strong>Interpretability:</strong> Clusters are easy to interpret, as they are defined by their mean.</li> </ul> <h4 id="weaknesses">Weaknesses:</h4> <ul> <li> <strong>Need to Specify <code class="language-plaintext highlighter-rouge">k</code>:</strong> As we discussed, choosing <code class="language-plaintext highlighter-rouge">k</code> can be arbitrary and challenging.</li> <li> <strong>Sensitive to Initial Centroids:</strong> Different initial placements can lead to different final clusterings (local optima).</li> <li> <strong>Assumes Spherical Clusters:</strong> K-Means works best when clusters are roughly spherical and similarly sized. It struggles with clusters of irregular shapes or varying densities.</li> <li> <strong>Sensitive to Outliers:</strong> Outliers can drastically shift centroid positions, skewing cluster formation.</li> <li> <strong>Feature Scaling Matters:</strong> Features with larger ranges will have a greater impact on distance calculations, so data scaling (e.g., standardization) is crucial.</li> </ul> <h3 id="real-world-scenarios-where-k-means-shines">Real-World Scenarios Where K-Means Shines</h3> <p>Let’s ground this with a few more quick examples:</p> <ul> <li> <strong>Retail:</strong> Segmenting customers into “value-conscious,” “brand loyal,” “impulse buyers” based on transaction data. This helps in targeted advertising.</li> <li> <strong>Healthcare:</strong> Grouping patients with similar symptoms or disease progression for personalized treatment plans or drug discovery.</li> <li> <strong>Geospatial Analysis:</strong> Identifying areas with similar demographic profiles or environmental conditions.</li> <li> <strong>Anomaly Detection:</strong> If a data point doesn’t fit well into any cluster (it’s far from all centroids), it might be an anomaly or outlier worth investigating.</li> </ul> <h3 id="wrapping-up-our-journey">Wrapping Up Our Journey</h3> <p>K-Means Clustering is a cornerstone algorithm in the unsupervised learning paradigm. It’s elegantly simple, yet incredibly powerful for discovering hidden structures and patterns within unlabeled data. While it has its limitations, understanding its mechanics, its objective function, and how to evaluate its results equips you with a formidable tool for a vast array of data science problems.</p> <p>So, the next time you encounter a pile of unorganized data, don’t despair! Remember our LEGO analogy, think of K-Means, and embark on your own journey to unmask the unseen patterns.</p> <p>Keep exploring, keep learning, and keep building!</p> <p>Until next time, [Your Name/Portfolio Name]</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>