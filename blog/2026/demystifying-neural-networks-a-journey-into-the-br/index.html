<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Demystifying Neural Networks: A Journey into the Brains of AI | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2026/demystifying-neural-networks-a-journey-into-the-br/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Demystifying Neural Networks: A Journey into the Brains of AI</h1> <p class="post-meta"> Created on January 28, 2026 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2026"> <i class="fa-solid fa-calendar fa-sm"></i> 2026 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/neural-networks"> <i class="fa-solid fa-hashtag fa-sm"></i> Neural Networks</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey there, fellow explorers of the digital frontier!</p> <p>If you’ve been anywhere near the world of technology lately, you’ve undoubtedly heard the buzzwords: Artificial Intelligence, Machine Learning, Deep Learning. These terms are often thrown around, sometimes interchangeably, but at the heart of many of the most impressive AI achievements lies a singular, powerful concept: the Neural Network.</p> <p>I remember my first encounter with neural networks. It felt like trying to understand an alien language – full of Greek letters, mysterious activation functions, and the daunting concept of “backpropagation.” But as I peeled back the layers, I discovered not an alien but a beautifully elegant system, surprisingly intuitive once you grasp its core components.</p> <p>So, let’s embark on this journey together. Forget the intimidation, and let’s unravel the “brains” of modern AI, one neuron at a time.</p> <h3 id="the-spark-of-inspiration-our-own-brain">The Spark of Inspiration: Our Own Brain</h3> <p>Before we dive into the artificial, let’s briefly look at the biological. Our brains are astonishing networks of billions of interconnected neurons. Each biological neuron receives signals through its <strong>dendrites</strong>, processes them in its <strong>soma</strong> (cell body), and if the combined signal is strong enough, it “fires” an electrical impulse down its <strong>axon</strong> to other neurons. This constant dance of signals is how we think, feel, learn, and experience the world.</p> <p>Pretty mind-blowing, right? Scientists and mathematicians, inspired by this biological marvel, sought to create a simplified, mathematical model of this fundamental unit. And thus, the Artificial Neuron was born.</p> <h3 id="the-artificial-neuron-the-humble-building-block">The Artificial Neuron: The Humble Building Block</h3> <p>Imagine our artificial neuron as a tiny decision-making unit. It receives several inputs, weighs their importance, sums them up, and then decides whether to “fire” an output based on a specific rule.</p> <p>Let’s break down its components:</p> <ol> <li> <p><strong>Inputs ($x_1, x_2, \ldots, x_n$):</strong> These are numerical values representing data. For example, if we’re trying to predict if a house will sell, inputs could be its size, number of bedrooms, or age.</p> </li> <li> <p><strong>Weights ($w_1, w_2, \ldots, w_n$):</strong> Each input $x_i$ is multiplied by a corresponding weight $w_i$. Think of weights as the neuron’s way of understanding the <em>importance</em> of each input. A larger weight means that input has a stronger influence on the neuron’s decision. Initially, these weights are random, but they are what the neural network learns to adjust over time.</p> </li> <li> <p><strong>Weighted Sum:</strong> The neuron first calculates a weighted sum of its inputs. This is essentially saying, “Let’s see the combined effect of all these weighted inputs.” \(z = \sum_{i=1}^{n} x_i w_i\) Or, if you prefer the more compact vector notation: $z = \mathbf{x} \cdot \mathbf{w}$.</p> </li> <li> <p><strong>Bias ($b$):</strong> After the weighted sum, we add a bias term, $b$. The bias is like an adjustable threshold or an “offset.” It allows the neuron to activate even if all inputs are zero, or makes it harder for the neuron to activate regardless of the inputs. It gives the neuron more flexibility to fit the data. \(z = \sum_{i=1}^{n} x_i w_i + b\)</p> </li> <li> <p><strong>Activation Function ($f$):</strong> This is where things get interesting and non-linear. The result of the weighted sum plus bias ($z$) is fed into an <strong>activation function</strong>. This function decides whether the neuron should “fire” and what value it should output. Without activation functions, stacking multiple layers of neurons would just be equivalent to a single linear transformation, severely limiting the network’s ability to learn complex patterns.</p> <ul> <li> <p><strong>Why non-linear?</strong> Imagine trying to separate data points that form a circle using only straight lines. You can’t! Non-linear activation functions allow our networks to model and learn highly complex, non-linear relationships in data.</p> </li> <li> <p><strong>Common Activation Functions:</strong></p> <ul> <li> <strong>Sigmoid:</strong> $f(z) = \frac{1}{1 + e^{-z}}$. Squashes values between 0 and 1, often used in output layers for binary classification (e.g., probability of something being true).</li> <li> <strong>ReLU (Rectified Linear Unit):</strong> $f(z) = \max(0, z)$. Simply outputs $z$ if $z$ is positive, and 0 otherwise. It’s incredibly popular in hidden layers due to its computational efficiency and ability to mitigate certain training issues.</li> <li> <strong>Tanh (Hyperbolic Tangent):</strong> $f(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$. Squashes values between -1 and 1.</li> </ul> </li> </ul> <p>The final output of a single artificial neuron is: \(\hat{y} = f \left( \sum_{i=1}^{n} x_i w_i + b \right)\)</p> </li> </ol> <p>This $\hat{y}$ is the neuron’s prediction or contribution to the next layer.</p> <h3 id="from-neuron-to-network-building-the-brain">From Neuron to Network: Building the Brain</h3> <p>A single artificial neuron, while foundational, is quite limited in what it can learn. The real power emerges when we connect many of these neurons together, forming layers, creating a <strong>Neural Network</strong>.</p> <p>A typical neural network, specifically a <strong>Feedforward Neural Network</strong> (also known as a Multi-Layer Perceptron or MLP), consists of at least three types of layers:</p> <ol> <li> <p><strong>Input Layer:</strong> This layer doesn’t perform any computation. It simply passes the raw data (our $x_1, \ldots, x_n$) into the network. Each input feature corresponds to one neuron in this layer.</p> </li> <li> <p><strong>Hidden Layers:</strong> These are the “brains” of the network. Between the input and output layers, there can be one or many hidden layers. Each neuron in a hidden layer receives inputs from <em>all</em> neurons in the previous layer, applies its weights, bias, and activation function, and then passes its output to <em>all</em> neurons in the next layer. These layers learn to extract increasingly complex features and patterns from the data. The more hidden layers, the “deeper” the network. This is where “Deep Learning” gets its name!</p> </li> <li> <p><strong>Output Layer:</strong> This layer produces the network’s final prediction. The number of neurons in the output layer depends on the task. For binary classification (e.g., spam or not spam), you might have one neuron with a sigmoid activation. For multi-class classification (e.g., classifying images into 10 categories), you might have 10 neurons, often using a <strong>softmax</strong> activation function to output probabilities for each class. For regression (e.g., predicting house prices), it might be a single neuron with a linear activation.</p> </li> </ol> <p>The flow of information in such a network is always in one direction: from the input layer, through the hidden layers, and finally to the output layer. This is why it’s called “feedforward.”</p> <h3 id="how-do-they-learn-the-magic-of-training">How Do They Learn? The Magic of Training</h3> <p>Okay, so we have inputs, weights, biases, and activation functions. But how does a neural network actually <em>learn</em>? How do those initially random weights and biases become so finely tuned that the network can recognize cats in pictures or translate languages?</p> <p>This is where the concept of <strong>training</strong> comes in, and it’s perhaps the most crucial part of understanding neural networks.</p> <p>The core idea is simple:</p> <ol> <li> <strong>Make a prediction:</strong> Given an input, the network processes it and makes an output prediction ($\hat{y}$).</li> <li> <strong>Calculate the error:</strong> Compare this prediction with the actual correct answer ($y$). The difference is the “error.”</li> <li> <strong>Adjust to reduce error:</strong> Based on this error, the network intelligently adjusts its internal weights and biases to make a better prediction next time.</li> </ol> <p>Let’s dive deeper into steps 2 and 3:</p> <h4 id="1-the-loss-function-quantifying-wrongness">1. The Loss Function: Quantifying “Wrongness”</h4> <p>To calculate the error, we use a <strong>Loss Function</strong> (also called a Cost Function or Error Function). This mathematical function quantifies how far off our network’s prediction ($\hat{y}$) is from the true value ($y$). Our ultimate goal during training is to minimize this loss.</p> <ul> <li> <strong>Mean Squared Error (MSE):</strong> A common loss function for regression tasks (predicting continuous values). \(L(\hat{y}, y) = (\hat{y} - y)^2\) This simply calculates the squared difference between the predicted and actual values. We square it to ensure positive values and penalize larger errors more heavily.</li> <li> <strong>Cross-Entropy Loss:</strong> Often used for classification tasks, it measures the difference between two probability distributions (our predicted probabilities vs. the true probabilities).</li> </ul> <h4 id="2-gradient-descent-finding-the-path-downhill">2. Gradient Descent: Finding the Path Downhill</h4> <p>Once we have a measure of error (the loss), how do we adjust the weights and biases to reduce it? This is where <strong>optimization algorithms</strong> come into play, and the most fundamental one is <strong>Gradient Descent</strong>.</p> <p>Imagine you’re blindfolded on a mountain, and your goal is to reach the lowest point (the minimum loss). What do you do? You feel the slope around you and take a small step in the direction that goes downhill the steepest. You repeat this process until you can’t go downhill anymore.</p> <p>In our analogy:</p> <ul> <li> <strong>The Mountain:</strong> Represents the loss function, where different combinations of weights and biases result in different levels of loss.</li> <li> <strong>Your Position:</strong> The current values of our network’s weights and biases.</li> <li> <strong>The Slope:</strong> This is the <strong>gradient</strong> – a vector of partial derivatives ($\frac{\partial L}{\partial w_i}$ and $\frac{\partial L}{\partial b_i}$). A partial derivative tells us how much the loss changes with respect to a tiny change in a single weight or bias.</li> </ul> <p>Gradient Descent updates each weight and bias in the network by moving it in the direction opposite to its gradient (i.e., downhill). For a given weight $w$: \(w_{\text{new}} = w_{\text{old}} - \alpha \frac{\partial L}{\partial w_{\text{old}}}\) And similarly for biases.</p> <ul> <li> <strong>Learning Rate ($\alpha$):</strong> This is a crucial parameter that determines the size of each “step” we take down the mountain. <ul> <li>A small learning rate means tiny steps, leading to slow convergence but potentially finding a more precise minimum.</li> <li>A large learning rate means big steps, which can lead to faster convergence but risks overshooting the minimum or even diverging.</li> </ul> </li> </ul> <h4 id="3-backpropagation-the-efficient-error-distributor">3. Backpropagation: The Efficient Error Distributor</h4> <p>Calculating the gradient for every single weight and bias in a deep network with potentially millions of parameters seems computationally daunting. This is where <strong>Backpropagation</strong> comes to the rescue.</p> <p>Invented in the 1980s (though with earlier roots), backpropagation is an algorithm that efficiently computes the gradients of the loss function with respect to <em>every</em> weight and bias in the network. It does this by applying the <strong>chain rule</strong> from calculus.</p> <p>Here’s the intuition:</p> <ol> <li> <strong>Forward Pass:</strong> An input travels from the input layer, through the hidden layers, to the output layer, generating a prediction ($\hat{y}$).</li> <li> <strong>Calculate Output Error:</strong> The loss function compares $\hat{y}$ with the true $y$, giving us the error at the output layer.</li> <li> <strong>Backward Pass (Backpropagation):</strong> The error from the output layer is then “propagated” backward through the network. The chain rule allows us to determine how much each individual weight and bias in the preceding layers contributed to that final error. It’s like tracing the responsibility for a mistake back through an assembly line to each worker. Each neuron in the hidden layers gets a “share” of the blame (or credit) for the final error, allowing its weights and bias to be adjusted proportionally.</li> </ol> <p>This backward flow of error information is what makes training deep neural networks feasible. Without backpropagation, the computational cost would be astronomical.</p> <h3 id="a-glimpse-of-power-what-can-they-do">A Glimpse of Power: What Can They Do?</h3> <p>So, why go through all this trouble? Because neural networks, especially deep ones, are incredibly powerful universal function approximators. Given enough data and computational resources, they can learn to approximate virtually any complex function. This translates into astonishing capabilities:</p> <ul> <li> <strong>Image Recognition:</strong> Identifying objects, faces, and scenes in images (e.g., Google Photos, self-driving cars).</li> <li> <strong>Natural Language Processing (NLP):</strong> Understanding, generating, and translating human language (e.g., ChatGPT, Google Translate, spam filters).</li> <li> <strong>Speech Recognition:</strong> Converting spoken words into text (e.g., Siri, Alexa).</li> <li> <strong>Recommendation Systems:</strong> Suggesting movies, products, or music (e.g., Netflix, Amazon, Spotify).</li> <li> <strong>Game Playing:</strong> Beating human champions in complex games like Go (AlphaGo).</li> <li> <strong>Drug Discovery and Material Science:</strong> Accelerating research in scientific fields.</li> </ul> <h3 id="the-road-ahead-understanding-and-beyond">The Road Ahead: Understanding and Beyond</h3> <p>We’ve covered a lot: the humble neuron, the architecture of a network, and the elegant dance of loss functions, gradient descent, and backpropagation that allows these networks to learn. This foundational understanding is crucial for anyone venturing into the world of AI and Machine Learning.</p> <p>Neural networks are not perfect “brains”; they are mathematical models. They are data-hungry, computationally intensive, and sometimes behave like “black boxes” where it’s hard to interpret <em>why</em> they made a certain decision. But their ability to learn intricate patterns and solve problems previously thought intractable is nothing short of revolutionary.</p> <p>My hope is that this journey has demystified neural networks for you, transforming them from an intimidating alien concept into an accessible, albeit complex, marvel of computational intelligence. The field of AI is dynamic and ever-evolving, and armed with this foundational knowledge, you’re well-equipped to dive deeper, experiment, and perhaps even contribute to the next generation of intelligent systems.</p> <p>Keep learning, keep building, and keep pushing the boundaries of what’s possible!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>