<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> From Scatter Plots to Serious Predictions: Mastering Linear Regression | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2026/from-scatter-plots-to-serious-predictions-masterin/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">From Scatter Plots to Serious Predictions: Mastering Linear Regression</h1> <p class="post-meta"> Created on January 11, 2026 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2026"> <i class="fa-solid fa-calendar fa-sm"></i> 2026 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/linear-regression"> <i class="fa-solid fa-hashtag fa-sm"></i> Linear Regression</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/statistics"> <i class="fa-solid fa-hashtag fa-sm"></i> Statistics</a>   <a href="/blog/blog/tag/predictive-modeling"> <i class="fa-solid fa-hashtag fa-sm"></i> Predictive Modeling</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey there, fellow data explorer!</p> <p>Have you ever looked at a bunch of numbers and just <em>felt</em> like there was a pattern hidden within them, a story waiting to be told? Maybe you’ve seen graphs showing how ice cream sales spike when temperatures rise, or how longer study hours generally lead to better exam scores. For me, these observations always sparked a burning question: can we <em>quantify</em> these relationships? Can we not just observe them, but actually <em>predict</em> future outcomes based on them?</p> <p>This curiosity is what first drew me into the world of Data Science and Machine Learning. And at the very heart of making sense of such patterns, at least for beginners, lies an elegant and incredibly powerful technique: <strong>Linear Regression</strong>. It’s often called the “Hello World” of predictive modeling, and for good reason. It’s simple enough to grasp quickly, yet forms the backbone of countless real-world applications.</p> <p>So, buckle up! In this post, I want to share my journey of understanding Linear Regression – what it is, how it works, and why it’s such a vital tool in our data science toolkit. We’ll dive into the intuition, the math, and even a few of the caveats, all while keeping it as engaging as a detective story.</p> <hr> <h3 id="what-exactly-is-linear-regression-the-core-idea">What Exactly Is Linear Regression? The Core Idea</h3> <p>Imagine you have a scatter plot. Each point on this plot represents a pair of values – say, the number of hours a student studied for an exam ($X$) and the score they achieved ($Y$). As you look at these points, you might notice a general trend: as $X$ increases, $Y$ tends to increase as well. It’s not perfect, but there’s a discernible upward drift.</p> <p>Linear Regression, at its heart, is about finding the <strong>“line of best fit”</strong> through these data points. This line isn’t just any line we arbitrarily draw; it’s a mathematically determined line that best summarizes the relationship between our input variable (or <em>feature</em>, $X$) and our output variable (or <em>target</em>, $Y$). Once we have this line, we can use it to make predictions. If a new student tells us they studied for a certain number of hours, we can use our line to predict their potential exam score.</p> <p>Think of it as trying to draw a straight path through a bustling market. You want your path to be as close as possible to most of the stalls, minimizing how far you have to stray from the main thoroughfare.</p> <hr> <h3 id="the-math-behind-the-magic-our-familiar-straight-line">The Math Behind the Magic: Our Familiar Straight Line</h3> <p>You’ve likely encountered the equation for a straight line in high school math:</p> <p>$y = mx + b$</p> <p>Where:</p> <ul> <li>$y$ is the value on the vertical axis.</li> <li>$m$ is the slope of the line (how steep it is).</li> <li>$x$ is the value on the horizontal axis.</li> <li>$b$ is the y-intercept (where the line crosses the y-axis).</li> </ul> <p>In the world of Linear Regression, we use slightly different notation, which might look intimidating at first, but it’s the exact same concept!</p> <p>$\hat{y} = \beta_0 + \beta_1x$</p> <p>Let’s break down these new symbols:</p> <ul> <li>$\hat{y}$ (pronounced “y-hat”) represents our <strong>predicted value</strong>. It’s our best guess for $Y$ given a certain $X$. We use a hat to distinguish it from the actual observed $Y$ values.</li> <li>$x$ is still our input feature (e.g., hours studied).</li> <li>$\beta_0$ (beta-nought or beta-zero) is our <strong>y-intercept</strong>. It tells us the predicted value of $Y$ when $X$ is zero. In some contexts, like predicting house prices, an intercept of zero might not make practical sense, but mathematically, it’s where our line starts on the Y-axis.</li> <li>$\beta_1$ (beta-one) is our <strong>slope coefficient</strong>. It tells us how much $\hat{y}$ is expected to change for every one-unit increase in $x$. If $\beta_1$ is 0.5, it means for every additional hour studied, the score is predicted to increase by 0.5 points.</li> </ul> <p>Our mission with Linear Regression is to find the <em>optimal</em> values for $\beta_0$ and $\beta_1$ that define the “best fit” line for our specific dataset. But what exactly does “best fit” mean?</p> <hr> <h3 id="defining-best-fit-the-cost-of-being-wrong">Defining “Best Fit”: The Cost of Being Wrong</h3> <p>When we draw a line through our data points, most points won’t fall exactly <em>on</em> the line. There will always be some difference between the actual observed value of $Y$ and the value $\hat{y}$ predicted by our line. This difference is called the <strong>error</strong> or <strong>residual</strong>.</p> <p>For each data point $i$, the residual $e_i$ is: $e_i = y_i - \hat{y}_i$</p> <p>Where $y_i$ is the actual value and $\hat{y}_i$ is the predicted value for that specific point.</p> <p>Now, if we simply summed up all these residuals, some would be positive (where our line predicted too low) and some would be negative (where our line predicted too high). These positive and negative errors would cancel each other out, leading to a sum close to zero even for a terrible line!</p> <p>To truly measure how “wrong” our line is, we need to penalize <em>all</em> errors, regardless of direction. The most common way to do this in Linear Regression is to <strong>square each residual</strong> before summing them up. This gives us the <strong>Sum of Squared Residuals (SSR)</strong>:</p> <p>$SSR = \sum_{i=1}^n (y_i - \hat{y}_i)^2$</p> <p>Why square them?</p> <ol> <li> <strong>Eliminates negatives:</strong> Squaring a negative number makes it positive, so all errors contribute positively to our total “wrongness.”</li> <li> <strong>Penalizes larger errors more:</strong> A residual of 2, when squared, becomes 4. A residual of 10, when squared, becomes 100. This means larger errors (points farther from our line) have a much greater impact on the total sum, effectively “pulling” our line towards them to minimize those big deviations.</li> </ol> <p>This SSR is at the heart of our <strong>Cost Function</strong>. In Linear Regression, we typically use the <strong>Mean Squared Error (MSE)</strong> as our cost function, which is simply the average of the squared residuals:</p> <p>$J(\beta_0, \beta_1) = \frac{1}{n} \sum_{i=1}^n (y_i - (\beta_0 + \beta_1x_i))^2$</p> <p>Here, $J(\beta_0, \beta_1)$ represents our cost function, which depends on the values of $\beta_0$ and $\beta_1$ we choose. Our ultimate goal is to find the values of $\beta_0$ and $\beta_1$ that <strong>minimize this cost function</strong>. This minimized MSE corresponds to the “best fit” line!</p> <hr> <h3 id="minimizing-the-cost-finding-the-sweet-spot">Minimizing the Cost: Finding the Sweet Spot</h3> <p>So, we have a way to measure how good (or bad) our line is. Now, how do we actually find the $\beta_0$ and $\beta_1$ that give us the <em>lowest</em> possible MSE?</p> <p>Imagine our cost function as a 3D bowl, where the x-axis is $\beta_1$, the y-axis is $\beta_0$, and the z-axis represents the MSE. We want to find the very bottom of that bowl.</p> <p>There are two primary ways to find this minimum:</p> <ol> <li> <p><strong>The Normal Equation (Closed-Form Solution):</strong> For simple Linear Regression (and even Multiple Linear Regression), there’s a direct mathematical formula derived using calculus that can give us the optimal $\beta_0$ and $\beta_1$ in one go. You just plug in your data, and out come the coefficients. This is incredibly efficient for smaller datasets. The derivation involves setting the partial derivatives of the cost function with respect to $\beta_0$ and $\beta_1$ to zero and solving for them. While powerful, this method can become computationally expensive for very large datasets (millions of data points and features) because it involves matrix inversions.</p> </li> <li> <p><strong>Gradient Descent (Iterative Solution):</strong> This is where machine learning truly begins to shine. Gradient Descent is an iterative optimization algorithm. Think of it like this: you’re blindfolded and trying to find the lowest point in a valley. You can’t see the bottom, but you can feel the slope of the ground right where you’re standing. If it slopes down to your left, you take a small step left. If it slopes forward, you take a small step forward. You keep doing this, taking small steps in the direction of the steepest descent, until you feel no more slope – you’re at the bottom!</p> <p>In mathematical terms, Gradient Descent works by:</p> <ul> <li>Starting with arbitrary (often random) values for $\beta_0$ and $\beta_1$.</li> <li>Calculating the <em>gradient</em> (the slope) of the cost function at the current $\beta_0, \beta_1$ point. This gradient tells us the direction of steepest <em>ascent</em>.</li> <li>Updating $\beta_0$ and $\beta_1$ by taking a small step in the opposite direction of the gradient (downhill). The size of this step is controlled by a parameter called the <strong>learning rate</strong>.</li> <li>Repeating these steps many times until the changes in $\beta_0$ and $\beta_1$ become very small, indicating we’ve reached (or are very close to) the minimum of the cost function.</li> </ul> </li> </ol> <p>Gradient Descent is incredibly versatile and forms the basis for optimizing many, many more complex machine learning models beyond Linear Regression.</p> <hr> <h3 id="beyond-simple-multiple-linear-regression">Beyond Simple: Multiple Linear Regression</h3> <p>While our example used just one input feature ($X$ for hours studied), most real-world scenarios are far more complex. What if we wanted to predict exam scores based on hours studied <em>and</em> previous GPA <em>and</em> attendance?</p> <p>This is where <strong>Multiple Linear Regression</strong> comes in. Instead of just one $x$, we have multiple $x$ variables ($x_1, x_2, …, x_n$). The equation expands to:</p> <p>$\hat{y} = \beta_0 + \beta_1x_1 + \beta_2x_2 + … + \beta_nx_n$</p> <p>Here:</p> <ul> <li>$x_1, x_2, …, x_n$ are our different input features.</li> <li>$\beta_1, \beta_2, …, \beta_n$ are their respective coefficients, each telling us the expected change in $\hat{y}$ for a one-unit increase in that specific feature, <em>holding all other features constant</em>.</li> </ul> <p>Geometrically, instead of fitting a 2D line, we’re now fitting a “hyperplane” in higher dimensions. But the core principle remains the same: we’re still minimizing the Mean Squared Error to find the optimal $\beta$ coefficients.</p> <hr> <h3 id="the-fine-print-assumptions-of-linear-regression">The Fine Print: Assumptions of Linear Regression</h3> <p>While powerful, Linear Regression isn’t a magic bullet. For its results to be reliable and interpretable, certain assumptions about the data and the error term should ideally hold true:</p> <ol> <li> <strong>Linearity:</strong> The relationship between $X$ and $Y$ must truly be linear. If it’s curved, a straight line won’t capture the pattern well.</li> <li> <strong>Independence of Observations:</strong> Each data point should be independent of the others. One observation should not influence another.</li> <li> <strong>Homoscedasticity:</strong> The variance of the residuals should be constant across all levels of $X$. In simple terms, the spread of the points around the line should be roughly the same along the entire length of the line. We don’t want a “fan” shape where errors get much larger for higher $X$ values.</li> <li> <strong>Normality of Residuals:</strong> The residuals should be approximately normally distributed. This is important for statistical inference (like calculating confidence intervals), but less critical for just making predictions.</li> <li> <strong>No Multicollinearity (for Multiple Linear Regression):</strong> Input features ($x_i$) should not be highly correlated with each other. If they are, it becomes hard for the model to distinguish the individual impact of each feature.</li> </ol> <p>Violating these assumptions doesn’t necessarily mean your model is useless, but it <em>does</em> mean you should interpret its results with caution and consider alternative approaches or data transformations.</p> <hr> <h3 id="when-to-use-it-when-to-be-careful-limitations">When to Use It, When to Be Careful (Limitations)</h3> <p><strong>Use Linear Regression when:</strong></p> <ul> <li>You suspect a linear relationship between your variables.</li> <li>You need a simple, interpretable model.</li> <li>You want to understand the <em>strength</em> and <em>direction</em> of the relationship between variables (e.g., “how much does an extra hour of study increase the score?”).</li> <li>You’re working with smaller to moderately sized datasets.</li> </ul> <p><strong>Be careful or consider alternatives when:</strong></p> <ul> <li>The relationship is clearly non-linear (e.g., exponential growth, saturation curves). While you can transform variables to make them linear, sometimes other models are more natural.</li> <li>Your data contains many outliers, as Linear Regression is sensitive to them (due to squaring errors).</li> <li>Your goal is very complex pattern recognition in high-dimensional, non-linear data (e.g., image recognition, natural language processing). Here, more advanced algorithms like neural networks, support vector machines, or tree-based models often perform better.</li> </ul> <hr> <h3 id="a-final-thought-experiment">A Final Thought Experiment</h3> <p>Imagine you’re trying to predict the growth rate of a plant based on the amount of sunlight it receives. You plant 10 seeds, give them varying amounts of light (from 1 hour to 10 hours a day), and measure their growth after a month.</p> <p>Plotting this data, you’d likely see a roughly upward trend. Some plants might grow a bit more than others for the same amount of light, maybe due to differences in soil or genes (these are our residuals!).</p> <p>If you used Linear Regression, the model would calculate the optimal $\beta_0$ (expected growth with zero sunlight – perhaps negative, representing decay) and $\beta_1$ (how much additional growth to expect for each extra hour of sunlight). This line then becomes your predictive tool. If a new plant receives 7 hours of sunlight, you’d use your line to estimate its growth.</p> <p>It’s a beautiful simplification of a complex world, allowing us to make informed guesses and understand underlying relationships.</p> <hr> <h3 id="conclusion-your-first-step-into-predictive-power">Conclusion: Your First Step into Predictive Power</h3> <p>Linear Regression is more than just a line on a graph; it’s a fundamental concept that underpins much of statistical modeling and machine learning. It was one of the first algorithms I truly grappled with, and understanding its mechanics – from the equation of a line to the concept of minimizing a cost function – felt like unlocking a secret door to data-driven insights.</p> <p>It teaches us the importance of:</p> <ul> <li> <strong>Defining our objective:</strong> Minimizing error.</li> <li> <strong>Quantifying error:</strong> Using techniques like Mean Squared Error.</li> <li> <strong>Optimization:</strong> Iteratively finding the best parameters (Gradient Descent) or directly solving for them (Normal Equation).</li> </ul> <p>So, the next time you see a scatter plot, don’t just see a jumble of points. See the potential for a story, a trend, and perhaps, a beautifully fitted line ready to make its next prediction. This is just the beginning of your journey into the vast and exciting world of predictive modeling. Keep exploring, keep asking questions, and keep fitting those lines!</p> <p>What other foundational algorithms would you like to explore next? Let me know in the comments!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>