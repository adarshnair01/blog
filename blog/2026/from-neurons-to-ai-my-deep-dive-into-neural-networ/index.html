<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> From Neurons to AI: My Deep Dive into Neural Networks | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2026/from-neurons-to-ai-my-deep-dive-into-neural-networ/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">From Neurons to AI: My Deep Dive into Neural Networks</h1> <p class="post-meta"> Created on January 10, 2026 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2026"> <i class="fa-solid fa-calendar fa-sm"></i> 2026 </a>   ·   <a href="/blog/blog/tag/neural-networks"> <i class="fa-solid fa-hashtag fa-sm"></i> Neural Networks</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="from-neurons-to-ai-my-deep-dive-into-neural-networks">From Neurons to AI: My Deep Dive into Neural Networks</h2> <p>Remember those sci-fi movies where computers could <em>think</em> and <em>learn</em>? For a long time, that felt like pure fantasy. But today, we live in a world where AI is translating languages, powering self-driving cars, recognizing faces, and even generating art. The magic behind much of this incredible progress? <strong>Neural Networks.</strong></p> <p>When I first encountered Neural Networks, I admit, the name sounded intimidating. It conjured images of complex brain surgery or esoteric computer science. But as I peeled back the layers (pun intended!), I discovered an elegant, powerful, and surprisingly intuitive framework that mimics, in a very simplified way, how our own brains process information. I want to share that journey with you, making this powerful concept accessible, whether you’re a curious high school student or an aspiring data scientist.</p> <h3 id="the-spark-inspiration-from-biology">The Spark: Inspiration from Biology</h3> <p>Our journey begins, as many great scientific stories do, with an observation of nature. The human brain, with its billions of interconnected neurons, is the ultimate learning machine. Each neuron receives signals, processes them, and then, if the combined signal is strong enough, fires off its own signal to other neurons. This constant dance of electrical impulses allows us to learn, adapt, and experience the world.</p> <p>In the 1940s, scientists Warren McCulloch and Walter Pitts proposed a simplified model of a biological neuron – the <strong>artificial neuron</strong> or <strong>perceptron</strong>. Their idea was to create a mathematical function that could simulate this input-process-output behavior.</p> <h3 id="building-block-1-the-artificial-neuron-perceptron">Building Block 1: The Artificial Neuron (Perceptron)</h3> <p>Imagine a single, tiny decision-maker. That’s essentially what an artificial neuron is. Let’s break down its components:</p> <ol> <li> <strong>Inputs ($x_1, x_2, …, x_n$):</strong> These are pieces of information, like features from a dataset (e.g., pixel values of an image, or a person’s age and income).</li> <li> <strong>Weights ($w_1, w_2, …, w_n$):</strong> Each input is assigned a ‘weight’. Think of a weight as indicating the <em>importance</em> or <em>strength</em> of that particular input. A higher weight means that input has a greater influence on the neuron’s decision.</li> <li> <strong>Bias ($b$):</strong> This is an additional adjustable parameter that shifts the activation function. It allows a neuron to activate even if all inputs are zero, or conversely, prevent activation even with strong inputs. It’s like a neuron’s predisposition to fire.</li> <li> <strong>Summation Function:</strong> The neuron first calculates a weighted sum of its inputs, adds the bias, and produces an intermediate value, often denoted as $z$. $z = \sum_{i=1}^{n} w_i x_i + b$ If you’re familiar with linear algebra, this is simply a dot product of the input vector and the weight vector, plus the bias.</li> <li> <p><strong>Activation Function ($\sigma$):</strong> This is the crucial non-linear step. After computing $z$, the neuron passes it through an activation function. This function decides whether the neuron ‘fires’ (activates) or not, and how strongly. Without non-linear activation functions, a neural network would just be a fancy linear regression model, unable to learn complex patterns.</p> <p>Common activation functions include:</p> <ul> <li> <strong>Sigmoid:</strong> $\sigma(z) = \frac{1}{1 + e^{-z}}$. This squashes any input value into a range between 0 and 1, making it useful for probability-like outputs.</li> <li> <strong>ReLU (Rectified Linear Unit):</strong> $\sigma(z) = \max(0, z)$. This is simpler and very popular in deep learning; it outputs $z$ if $z$ is positive, and 0 otherwise. It introduces non-linearity without complex calculations.</li> </ul> <p>The final output of a single neuron, $a$, is therefore: $a = \sigma(z) = \sigma\left(\sum_{i=1}^{n} w_i x_i + b\right)$</p> </li> </ol> <p>So, a single artificial neuron takes multiple inputs, assigns importance to each (weights), adds a predisposition (bias), sums them up, and then decides whether to “fire” and how strongly, based on its activation function.</p> <h3 id="building-block-2-connecting-neurons-into-a-network">Building Block 2: Connecting Neurons into a Network</h3> <p>A single neuron, while interesting, isn’t very powerful on its own. The real magic happens when you connect many of them together, forming <strong>layers</strong>.</p> <p>Imagine these neurons arranged in distinct layers:</p> <ol> <li> <strong>Input Layer:</strong> These neurons don’t perform any computation; they simply receive the raw data (our $x_i$’s) and pass them on to the next layer.</li> <li> <strong>Hidden Layers:</strong> These are the computational workhorses. Each neuron in a hidden layer takes inputs from the previous layer, performs its weighted sum and activation, and passes its output to the neurons in the <em>next</em> layer. A network can have one, two, or many hidden layers. The more hidden layers, the “deeper” the network.</li> <li> <strong>Output Layer:</strong> The final layer of neurons. Their outputs represent the network’s prediction (e.g., a probability of an image being a cat, or a predicted stock price).</li> </ol> <p>This flow of information from the input layer, through the hidden layers, and finally to the output layer is called <strong>feedforward propagation</strong>. It’s how the network makes a prediction given some input data.</p> <p>Each connection between neurons has its own weight, and each neuron has its own bias. The sheer number of these adjustable parameters is what allows neural networks to learn incredibly complex patterns and relationships in data.</p> <h3 id="the-heart-of-learning-how-neural-networks-train">The Heart of Learning: How Neural Networks Train</h3> <p>Here’s where it gets really exciting. How do these weights and biases get set to the “right” values? This is the <strong>training process</strong>, and it’s what allows a neural network to learn from data.</p> <p>The training process is an iterative dance between making a prediction, evaluating its error, and adjusting the parameters to reduce that error.</p> <ol> <li> <p><strong>The Loss Function (Measuring Error):</strong> First, we need a way to quantify “how wrong” our network’s predictions are. This is the job of the <strong>loss function</strong> (or cost function). It takes the network’s output ($\hat{y}$) and compares it to the true, desired output ($y$) for a given input. The higher the loss, the worse the prediction.</p> <p>A common loss function for regression tasks is the <strong>Mean Squared Error (MSE)</strong>: $L = \frac{1}{m} \sum_{j=1}^{m} (y_j - \hat{y}_j)^2$ where $m$ is the number of training examples. (Sometimes, for computational convenience with calculus, it’s $\frac{1}{2}$ instead of $\frac{1}{m}$ or just $\frac{1}{2}$ for a single example.)</p> <p>Our goal is to <em>minimize</em> this loss function.</p> </li> <li> <p><strong>Optimization: Gradient Descent (Finding the Best Path):</strong> Imagine you’re blindfolded on a bumpy mountain, and you want to reach the lowest point (the minimum loss). What would you do? You’d feel the slope around you and take a small step downhill. That’s essentially what <strong>Gradient Descent</strong> does.</p> <p>The “slope” in our mathematical mountain (the loss function landscape) is given by the <strong>gradient</strong>. The gradient is a vector of partial derivatives, telling us the direction of the <em>steepest ascent</em>. Since we want to go <em>downhill</em> (minimize loss), we move in the opposite direction of the gradient.</p> <p>For each weight ($w$) and bias ($b$) in our network, we calculate how much a tiny change in that parameter would affect the loss. This is $\frac{\partial L}{\partial w}$ (the partial derivative of the Loss with respect to weight $w$).</p> <p>The update rule for a weight would look something like this: $w_{new} = w_{old} - \alpha \frac{\partial L}{\partial w}$ $b_{new} = b_{old} - \alpha \frac{\partial L}{\partial b}$</p> <p>Here, $\alpha$ is the <strong>learning rate</strong>, a crucial hyperparameter. It determines the size of the steps we take down the mountain. A large learning rate might overshoot the minimum; a small one might take too long to get there.</p> </li> <li> <p><strong>Backpropagation (The Magic of Error Attribution):</strong> Calculating the gradient for <em>all</em> weights and biases in a multi-layered network seems like a daunting task. How do we know how much a weight in an early layer contributed to an error in the output? This is where <strong>Backpropagation</strong> comes in, arguably the most important algorithm in neural networks.</p> <p>Backpropagation is essentially the application of the chain rule from calculus. It efficiently calculates the gradients of the loss function with respect to <em>every single weight and bias</em> in the network, starting from the output layer and working backward through the hidden layers.</p> <p>Think of it this way: the network makes a prediction, and we calculate the error at the output. Backpropagation then “propagates” this error backward through the network, assigning a “blame” or responsibility to each weight and bias for the overall error. It tells us precisely how to adjust each parameter to reduce the error.</p> <p>This cycle of <strong>Feedforward Propagation</strong> (making a prediction) $\rightarrow$ <strong>Calculating Loss</strong> $\rightarrow$ <strong>Backpropagation</strong> (calculating gradients) $\rightarrow$ <strong>Gradient Descent</strong> (updating weights) is repeated thousands, sometimes millions, of times using vast amounts of training data. Each complete pass through the entire dataset is called an <strong>epoch</strong>. Over these epochs, the network’s weights and biases are iteratively refined, allowing it to learn increasingly accurate patterns.</p> </li> </ol> <h3 id="the-deep-in-deep-learning">The “Deep” in Deep Learning</h3> <p>When you hear the term “Deep Learning,” it refers to neural networks with <strong>multiple hidden layers</strong>. While a single-layer network can learn simple patterns, adding more layers allows the network to learn hierarchical representations of data.</p> <p>For example, in image recognition:</p> <ul> <li>The first hidden layer might learn to detect simple edges and corners.</li> <li>The second layer might combine these edges to recognize basic shapes (circles, squares).</li> <li>Subsequent layers might combine shapes to identify parts of objects (eyes, wheels).</li> <li>The final layers combine these parts to recognize complete objects (faces, cars).</li> </ul> <p>This hierarchical learning capability is what gives deep neural networks their incredible power, especially with complex, high-dimensional data like images, audio, and text. Different architectures, like Convolutional Neural Networks (CNNs) for images or Recurrent Neural Networks (RNNs) for sequences, build upon these core principles, specializing for particular data types.</p> <h3 id="why-are-neural-networks-so-powerful">Why Are Neural Networks So Powerful?</h3> <ol> <li> <strong>Universal Function Approximators:</strong> Mathematically, a neural network with at least one hidden layer and non-linear activation functions can approximate <em>any</em> continuous function. This means they can theoretically learn any relationship between inputs and outputs, given enough data and computation.</li> <li> <strong>Feature Learning:</strong> Unlike traditional machine learning algorithms where you often have to manually design “features” from raw data, deep neural networks can learn these features <em>automatically</em> during training. This is a huge advantage, especially for complex data where hand-crafting features is difficult or impossible.</li> <li> <strong>Scalability:</strong> With vast datasets and powerful computational resources (like GPUs), neural networks can continue to improve their performance, making them ideal for the “big data” era.</li> </ol> <h3 id="acknowledging-the-challenges">Acknowledging the Challenges</h3> <p>While powerful, neural networks aren’t without their considerations:</p> <ul> <li> <strong>Data Hungry:</strong> They typically require large amounts of labeled data to train effectively.</li> <li> <strong>Computationally Intensive:</strong> Training deep networks can demand significant processing power and time.</li> <li> <strong>“Black Box” Problem:</strong> Understanding <em>why</em> a complex deep neural network makes a particular prediction can be challenging, making interpretability an active area of research.</li> <li> <strong>Overfitting:</strong> A network might learn the training data too well, memorizing noise rather than generalizable patterns, leading to poor performance on new, unseen data. Techniques like regularization and early stopping are used to combat this.</li> </ul> <h3 id="my-reflection-the-journey-continues">My Reflection: The Journey Continues</h3> <p>My journey into neural networks has been incredibly rewarding. What started as an abstract concept has become a tangible tool for solving real-world problems. Understanding the core mechanics – the humble neuron, the layers, the iterative dance of backpropagation and gradient descent – provides a solid foundation for exploring the vast landscape of deep learning.</p> <p>The beauty of neural networks lies in their ability to learn intricate patterns from raw data, transforming our relationship with technology. From predicting the next word you type to diagnosing diseases, their impact is undeniable and still rapidly evolving.</p> <p>I encourage you to continue exploring. Play with online neural network playgrounds, try building a simple one in Python with libraries like TensorFlow or PyTorch, or delve deeper into specific architectures. The more you explore, the more you’ll appreciate the elegant simplicity and profound power hidden within these digital brains. The future of AI is being built on these principles, and by understanding them, you’re better equipped to be a part of it.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>