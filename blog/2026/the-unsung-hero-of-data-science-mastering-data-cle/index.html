<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Unsung Hero of Data Science: Mastering Data Cleaning Strategies | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2026/the-unsung-hero-of-data-science-mastering-data-cle/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Unsung Hero of Data Science: Mastering Data Cleaning Strategies</h1> <p class="post-meta"> Created on January 14, 2026 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2026"> <i class="fa-solid fa-calendar fa-sm"></i> 2026 </a>   ·   <a href="/blog/blog/tag/data-cleaning"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Cleaning</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/data-preprocessing"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Preprocessing</a>   <a href="/blog/blog/tag/python"> <i class="fa-solid fa-hashtag fa-sm"></i> Python</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>If you’re anything like I was when I first dove into the exciting world of data science and machine learning, you probably imagined spending most of your time building sophisticated models, tuning hyper-parameters, and marveling at predictive power. I envisioned myself a digital wizard, conjuring insights from complex algorithms.</p> <p>Then, I met real-world data.</p> <p>The truth is, before any of that magic can happen, there’s a vital, often gritty, and sometimes frustrating, but ultimately rewarding, phase: <strong>data cleaning</strong>. It’s the unsung hero, the silent architect, the meticulous librarian of the data science workflow. You’ve probably heard the statistic: data scientists spend 70-80% of their time on data preparation and cleaning. I can confirm, it’s absolutely true!</p> <p>This blog post isn’t just a technical guide; it’s a peek into my own journey and the strategies I’ve cultivated to tame the wild beast that is raw data. Whether you’re a high school student just starting to code with Pandas or an aspiring MLE, understanding these fundamentals is non-negotiable.</p> <h2 id="why-is-data-cleaning-so-crucial-the-garbage-in-garbage-out-principle">Why is Data Cleaning So Crucial? The “Garbage In, Garbage Out” Principle</h2> <p>Imagine trying to bake a perfect cake with rotten eggs, stale flour, and moldy sugar. No matter how skilled the baker, the result will be inedible. The same goes for data.</p> <p>The principle is simple: <strong>Garbage In, Garbage Out (GIGO)</strong>.</p> <ol> <li> <strong>Model Performance</strong>: Machine learning models learn patterns from the data they’re trained on. If that data is noisy, inconsistent, or riddled with errors, the model will learn those errors. This leads to inaccurate predictions, poor generalizations, and a model that performs terribly in the real world. A biased dataset will produce a biased model.</li> <li> <strong>Misleading Insights</strong>: Beyond models, data-driven decisions rely on accurate insights. Dirty data can lead to skewed statistics, incorrect trends, and flawed business strategies. You might see patterns that don’t exist, or miss crucial ones that do.</li> <li> <strong>Data Integrity &amp; Reliability</strong>: Clean data builds trust. When your data is reliable, you can confidently use it for analysis, reporting, and model deployment, knowing your conclusions are sound.</li> </ol> <p>So, how do we tackle this beast? Let’s dive into the strategies that have become my go-to toolkit.</p> <h2 id="the-data-cleaning-toolkit-my-essential-strategies">The Data Cleaning Toolkit: My Essential Strategies</h2> <h3 id="1-understanding-your-data-the-first-commandment">1. Understanding Your Data: The First Commandment</h3> <p>Before I even think about writing lines of cleaning code, I spend a significant amount of time just <em>looking</em> at the data. This exploratory data analysis (EDA) phase is like getting to know a new friend – you ask questions, observe their habits, and try to understand their quirks.</p> <ul> <li> <strong><code class="language-plaintext highlighter-rouge">df.info()</code></strong>: Provides a concise summary of the DataFrame, including the number of non-null values and data types for each column. This immediately flags missing data and incorrect types.</li> <li> <strong><code class="language-plaintext highlighter-rouge">df.describe()</code></strong>: Generates descriptive statistics (mean, std, min, max, quartiles) for numerical columns. This helps spot unusual ranges, potential outliers, and skewed distributions.</li> <li> <strong><code class="language-plaintext highlighter-rouge">df.head()</code> / <code class="language-plaintext highlighter-rouge">df.sample()</code></strong>: A quick glance at the first few rows or a random sample can reveal obvious formatting issues, inconsistent entries, or unexpected values.</li> <li> <strong><code class="language-plaintext highlighter-rouge">df.value_counts()</code></strong>: Invaluable for categorical columns. It shows unique values and their frequencies, immediately highlighting inconsistencies like typos (“New York”, “new york”, “NYC”).</li> <li> <strong>Visualizations</strong>: Histograms reveal distributions, box plots are fantastic for visualizing outliers and spread, and scatter plots help identify relationships and potential errors between variables.</li> <li> <strong>Domain Knowledge</strong>: This is often overlooked! Talk to the people who collected the data, the subject matter experts. They can provide invaluable context on what “normal” data looks like, what certain values mean, and common data entry errors. My rule: always consult the domain experts if available!</li> </ul> <h3 id="2-handling-missing-values-filling-the-gaps">2. Handling Missing Values: Filling the Gaps</h3> <p>Missing data is arguably the most common and frustrating problem. It’s like having holes in your puzzle – how do you complete the picture?</p> <h4 id="identification">Identification</h4> <p>The first step is always to identify <em>where</em> and <em>how much</em> data is missing. In Python, <code class="language-plaintext highlighter-rouge">df.isnull().sum()</code> gives you a count of missing values per column. Libraries like <code class="language-plaintext highlighter-rouge">missingno</code> can create powerful visualizations of missing data patterns.</p> <h4 id="strategies-for-imputation-filling-missing-values">Strategies for Imputation (Filling Missing Values)</h4> <p>This is often a tricky balance, like being a detective trying to infer what’s missing without introducing false information.</p> <ul> <li> <strong>Deletion (Row-wise or Column-wise)</strong>: <ul> <li> <strong>Row-wise (<code class="language-plaintext highlighter-rouge">df.dropna(axis=0)</code>):</strong> If only a small percentage of rows have missing values, or if the missingness is completely random and not indicative of an underlying pattern, you might drop those rows. <em>Caution</em>: If you have a lot of missing data spread across many rows, this can lead to significant data loss.</li> <li> <strong>Column-wise (<code class="language-plaintext highlighter-rouge">df.dropna(axis=1)</code>):</strong> If a column has an overwhelming number of missing values (e.g., 70-80% or more), it might be better to drop the entire column, as it provides little useful information.</li> </ul> </li> <li> <strong>Imputation for Numerical Data</strong>: <ul> <li> <strong>Mean Imputation</strong>: Replace missing values with the mean of the column. This is suitable for normally distributed data but can distort relationships if there are outliers. <ul> <li>The mean is calculated as: $\mu = \frac{1}{N} \sum_{i=1}^{N} x_i$, where $N$ is the number of non-missing observations and $x_i$ are the observed values.</li> </ul> </li> <li> <strong>Median Imputation</strong>: Replace missing values with the median of the column. This is more robust to outliers and skewed distributions than the mean.</li> <li> <strong>Mode Imputation</strong>: Replace missing values with the most frequent value. This is typically used for categorical data but can also be applied to numerical data with distinct peaks.</li> <li> <strong>Forward/Backward Fill (<code class="language-plaintext highlighter-rouge">ffill</code>/<code class="language-plaintext highlighter-rouge">bfill</code>)</strong>: Useful for time-series data, where you might carry forward the last observed value or carry backward the next observed value.</li> </ul> </li> <li> <strong>Imputation for Categorical Data</strong>: <ul> <li> <strong>Mode Imputation</strong>: The most common approach, replacing missing values with the most frequent category.</li> <li> <strong>“Unknown” Category</strong>: Sometimes, the fact that a value is missing is itself informative. Creating a new category like “Unknown” or “Not Provided” can preserve this information.</li> </ul> </li> <li> <strong>Advanced Imputation Techniques</strong>: <ul> <li> <strong>K-Nearest Neighbors (KNN) Imputation</strong>: Uses the values of the k-nearest neighbors to impute missing values. It’s more sophisticated but computationally intensive.</li> <li> <strong>Regression Imputation</strong>: Predicts missing values using other features in the dataset, treating the column with missing values as the target variable for a regression model.</li> </ul> </li> </ul> <p>The choice of imputation strategy largely depends on the nature of your data, the percentage of missing values, and the domain context.</p> <h3 id="3-tackling-outliers-the-anomaly-detectives">3. Tackling Outliers: The Anomaly Detectives</h3> <p>Outliers are data points that significantly deviate from other observations. They can be genuine extreme values, or they can be errors (e.g., a data entry mistake like “2000” for age instead of “20”).</p> <h4 id="identification-1">Identification</h4> <ul> <li> <strong>Visual Methods</strong>: Box plots are excellent for quickly visualizing outliers. Scatter plots can also reveal unusual data points.</li> <li> <strong>Statistical Methods</strong>: <ul> <li> <strong>Z-score</strong>: Measures how many standard deviations an element is from the mean. A common threshold is a Z-score absolute value greater than 2 or 3. <ul> <li>$Z = \frac{x - \mu}{\sigma}$, where $x$ is the data point, $\mu$ is the mean, and $\sigma$ is the standard deviation.</li> </ul> </li> <li> <strong>Interquartile Range (IQR)</strong>: This is my preferred method for skewed distributions. <ul> <li>$IQR = Q_3 - Q_1$, where $Q_1$ is the 25th percentile and $Q_3$ is the 75th percentile.</li> <li>Outliers are typically defined as values less than $Q_1 - 1.5 \times IQR$ or greater than $Q_3 + 1.5 \times IQR$.</li> </ul> </li> </ul> </li> </ul> <h4 id="strategies-for-handling-outliers">Strategies for Handling Outliers</h4> <p>Outliers aren’t always bad; sometimes they tell a story (e.g., a record-breaking sales day), but other times they’re just noise that can skew your model.</p> <ul> <li> <strong>Deletion</strong>: If an outlier is clearly a data entry error and you can’t correct it, or if it’s an extreme value that will severely distort your model, you might remove it. <em>Caution</em>: Deletion should be done carefully, as you might lose valuable information.</li> <li> <strong>Transformation</strong>: Applying mathematical transformations like log transformation ($\log(x)$) or square root transformation ($\sqrt{x}$) can reduce the impact of extreme values and make the data more normally distributed. This is especially useful for highly skewed data.</li> <li> <strong>Capping / Winsorization</strong>: Instead of deleting outliers, you can cap them. This means replacing values above an upper threshold (e.g., 99th percentile) with the threshold value, and values below a lower threshold (e.g., 1st percentile) with that threshold value. This limits their impact without removing them entirely.</li> <li> <strong>Binning</strong>: Grouping numerical data into bins can also reduce the impact of outliers by treating a range of values as a single category.</li> <li> <strong>Treat as a Separate Class</strong>: In some anomaly detection scenarios, outliers are exactly what you want to find, so you might model them specifically.</li> </ul> <h3 id="4-correcting-inconsistent-data-bringing-order-to-chaos">4. Correcting Inconsistent Data: Bringing Order to Chaos</h3> <p>This is where I often feel like a digital librarian, organizing and standardizing everything. Inconsistent data refers to variations in data entry, formatting, or units that should logically be the same.</p> <ul> <li> <strong>Data Types</strong>: Always ensure your columns have the correct data types. Numbers should be numerical (<code class="language-plaintext highlighter-rouge">int</code>, <code class="language-plaintext highlighter-rouge">float</code>), dates should be <code class="language-plaintext highlighter-rouge">datetime</code> objects, and text should be <code class="language-plaintext highlighter-rouge">object</code> or <code class="language-plaintext highlighter-rouge">string</code>. Incorrect types can prevent calculations or cause errors in models. <ul> <li><code class="language-plaintext highlighter-rouge">df['column'] = pd.to_numeric(df['column'], errors='coerce')</code></li> <li><code class="language-plaintext highlighter-rouge">df['date_column'] = pd.to_datetime(df['date_column'], errors='coerce')</code></li> </ul> </li> <li> <strong>Categorical Inconsistencies</strong>: <ul> <li> <strong>Typos and Variations</strong>: “New York”, “new york”, “NYC”, “NY” should all be standardized to one representation. <ul> <li>Use <code class="language-plaintext highlighter-rouge">.str.lower()</code> or <code class="language-plaintext highlighter-rouge">.str.upper()</code> to standardize case.</li> <li>Use <code class="language-plaintext highlighter-rouge">.str.strip()</code> to remove leading/trailing whitespace.</li> <li>Use <code class="language-plaintext highlighter-rouge">.replace()</code> or mapping dictionaries to unify variations.</li> <li>For more complex cases, libraries like <code class="language-plaintext highlighter-rouge">fuzzywuzzy</code> can perform fuzzy string matching to identify and correct similar-sounding entries.</li> </ul> </li> <li> <strong>Units</strong>: Ensure all numerical values in a column are in the same unit (e.g., all temperatures in Celsius, all weights in kilograms). If not, convert them.</li> </ul> </li> <li> <strong>Formatting Issues</strong>: <ul> <li> <strong>Dates</strong>: Dates can come in many formats (“MM/DD/YYYY”, “DD-MM-YY”, “YYYY-MM-DD”). Standardize them to a single format.</li> <li> <strong>Text</strong>: Remove special characters, HTML tags, or unwanted punctuation if they are not relevant to your analysis. Regular expressions (using Python’s <code class="language-plaintext highlighter-rouge">re</code> module) are incredibly powerful here.</li> </ul> </li> </ul> <h3 id="5-removing-duplicate-records-the-unique-identifier">5. Removing Duplicate Records: The Unique Identifier</h3> <p>Duplicate records occur when the same entry appears multiple times in your dataset. This can happen due to data entry errors, merging datasets, or issues in data extraction.</p> <h4 id="identification-2">Identification</h4> <ul> <li> <code class="language-plaintext highlighter-rouge">df.duplicated().sum()</code> will tell you how many rows are exact duplicates.</li> <li> <code class="language-plaintext highlighter-rouge">df.duplicated(subset=['column1', 'column2']).sum()</code> helps identify duplicates based on specific columns (e.g., if you consider a customer record duplicate if the ‘name’ and ‘email’ match).</li> </ul> <h4 id="strategies">Strategies</h4> <ul> <li> <strong>Deletion</strong>: <code class="language-plaintext highlighter-rouge">df.drop_duplicates()</code> is your friend. <ul> <li> <code class="language-plaintext highlighter-rouge">df.drop_duplicates(inplace=True)</code> removes exact duplicate rows.</li> <li> <code class="language-plaintext highlighter-rouge">df.drop_duplicates(subset=['column1', 'column2'], keep='first', inplace=True)</code> allows you to define what constitutes a duplicate and decide whether to keep the ‘first’ or ‘last’ occurrence.</li> </ul> </li> </ul> <p>Duplicates can bias models by overrepresenting certain observations or inflating counts, so removing them is a standard cleaning step.</p> <h3 id="6-feature-engineering-and-selection-a-quick-nod">6. Feature Engineering and Selection (A Quick Nod)</h3> <p>While not strictly “cleaning,” this phase is often intertwined. As you clean, you might realize some columns are entirely irrelevant to your problem and can be dropped (<code class="language-plaintext highlighter-rouge">df.drop(['col_name'], axis=1)</code>). Conversely, cleaning often reveals opportunities to create new, more informative features from existing ones. For example, extracting day, month, and year from a datetime column, or creating interaction terms ($X_1 \times X_2$).</p> <h2 id="the-iterative-nature-of-cleaning-its-a-journey-not-a-destination">The Iterative Nature of Cleaning: It’s a Journey, Not a Destination</h2> <p>Here’s one of the most important lessons I’ve learned: <strong>Data cleaning is rarely a one-shot process.</strong></p> <p>You clean, you explore, you model, and then you discover new problems. A model might perform poorly, leading you to re-examine the data more closely and uncover issues you missed initially. It’s an iterative loop.</p> <ul> <li> <strong>Document Everything</strong>: Keep a clear record of every cleaning step. Which columns did you drop? How did you handle missing values? What outliers did you remove? This is crucial for reproducibility and for understanding the impact of your cleaning decisions.</li> <li> <strong>Version Control</strong>: If your cleaning process is complex, treat your data like code. Use tools like Git to version control your cleaning scripts and even your cleaned datasets.</li> </ul> <h2 id="embrace-the-mess-for-therein-lies-the-clarity">Embrace the Mess, For Therein Lies the Clarity</h2> <p>Data cleaning might not have the glamour of deep learning or the intrigue of complex algorithms, but it is the bedrock upon which all successful data science projects are built. It’s a craft, an art, and a science that demands patience, attention to detail, and a healthy dose of skepticism about the data you’re working with.</p> <p>By mastering these strategies, you’re not just preparing data; you’re developing a critical mindset that will serve you well in any data-driven endeavor. So, roll up your sleeves, fire up your Jupyter notebooks, and embrace the mess – because finding clarity within chaos is one of the most rewarding aspects of a data scientist’s journey.</p> <p>Happy Cleaning!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>