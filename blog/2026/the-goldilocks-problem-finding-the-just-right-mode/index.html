<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Goldilocks Problem: Finding the 'Just Right' Model in Machine Learning | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2026/the-goldilocks-problem-finding-the-just-right-mode/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Goldilocks Problem: Finding the 'Just Right' Model in Machine Learning</h1> <p class="post-meta"> Created on January 22, 2026 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2026"> <i class="fa-solid fa-calendar fa-sm"></i> 2026 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/overfitting"> <i class="fa-solid fa-hashtag fa-sm"></i> Overfitting</a>   <a href="/blog/blog/tag/underfitting"> <i class="fa-solid fa-hashtag fa-sm"></i> Underfitting</a>   <a href="/blog/blog/tag/bias-variance-tradeoff"> <i class="fa-solid fa-hashtag fa-sm"></i> Bias-Variance Tradeoff</a>   <a href="/blog/blog/tag/model-evaluation"> <i class="fa-solid fa-hashtag fa-sm"></i> Model Evaluation</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hello fellow explorers of the data universe!</p> <p>If you’re anything like me, you’ve probably been captivated by the seemingly magical predictions of machine learning models. From recommending your next favorite song to powering self-driving cars, these algorithms are transforming our world. But behind every successful model lies a struggle, a fundamental challenge that data scientists wrestle with every single day: the “Goldilocks Problem.”</p> <p>No, we’re not talking about porridge. We’re talking about finding the model that’s “just right” – not too simple, not too complex, but perfectly balanced to make accurate predictions on data it has <em>never seen before</em>. This, my friends, is the heart of <strong>generalization</strong>, and it’s where the twin evils of <strong>overfitting</strong> and <strong>underfitting</strong> come into play.</p> <p>Let’s embark on a journey to demystify these concepts and understand how we train our models to be smart, not just parrots.</p> <h3 id="the-grand-goal-generalization">The Grand Goal: Generalization</h3> <p>Before we dive into the pitfalls, let’s nail down what we’re aiming for. In machine learning, our ultimate goal isn’t just for a model to perform well on the data it was trained on. That’s like a student acing a test they’ve seen before. The real challenge, and the real value, comes from a model’s ability to make accurate predictions on <em>new, unseen data</em>. This ability is what we call <strong>generalization</strong>. A model that generalizes well has truly learned the underlying patterns, not just memorized the training examples.</p> <h3 id="the-tale-of-two-errors-underfitting">The Tale of Two Errors: Underfitting</h3> <p>Imagine you’re trying to learn a new language. You decide to use a very basic phrasebook with only 10 common phrases. You try to converse with a native speaker, but you quickly realize your understanding is far too simplistic. You can’t construct proper sentences, express nuanced thoughts, or understand complex questions. You’re constantly making mistakes, not because you’re bad, but because your <em>tool</em> (the phrasebook/model) is inadequate.</p> <p>This, in a nutshell, is <strong>underfitting</strong>.</p> <p>An underfit model is too simple to capture the underlying patterns and relationships in the training data. It’s like trying to fit a straight line to a dataset that clearly shows a curved relationship. The model fails to learn even the basic structure of the data, resulting in poor performance not just on new data, but often on the training data itself!</p> <p><strong>Characteristics of Underfitting:</strong></p> <ul> <li> <strong>High Bias:</strong> This refers to the simplifying assumptions made by the model. A high bias model makes strong assumptions about the data’s shape or relationships, often leading it to miss out on important features. Think of it as having a strong, often incorrect, pre-conceived notion.</li> <li><strong>Poor performance on both training and test data.</strong></li> <li> <strong>Often too simple:</strong> Uses too few features or a model type that’s not complex enough for the problem.</li> </ul> <p><strong>Visualizing Underfitting:</strong> Imagine a scatter plot of data points forming a gentle curve. An underfit model might try to draw a straight line through these points. It’s clear that the line doesn’t capture the trend well; many points are far from the line.</p> <p><strong>Causes of Underfitting:</strong></p> <ul> <li> <strong>Insufficient Features:</strong> Not providing enough relevant information to the model.</li> <li> <strong>Over-simplified Model:</strong> Using a linear model for highly non-linear data, for example.</li> <li> <strong>Too Much Regularization:</strong> Regularization techniques (which we’ll discuss later) are designed to <em>prevent</em> overfitting, but too much can overly constrain the model, leading to underfitting.</li> </ul> <p><strong>How to combat Underfitting:</strong></p> <ul> <li> <strong>Increase Model Complexity:</strong> Use a more flexible model (e.g., switch from linear regression to polynomial regression, or use a neural network with more layers/neurons).</li> <li> <strong>Add More Features:</strong> Provide the model with more relevant input variables.</li> <li> <strong>Reduce Regularization:</strong> Ease up on the constraints you’ve placed on the model’s complexity.</li> </ul> <h3 id="the-other-extreme-overfitting">The Other Extreme: Overfitting</h3> <p>Now, let’s flip the script. Imagine you’re studying for an exam. Instead of understanding the concepts, you decide to <em>memorize every single word</em> from the textbook, including page numbers, typos, and the author’s personal anecdotes. You might ace a test designed exactly like the textbook examples. But if the questions are phrased even slightly differently, or ask you to apply concepts in a new way, you’d be completely lost. You’ve memorized, not learned.</p> <p>This is <strong>overfitting</strong>.</p> <p>An overfit model is excessively complex. It doesn’t just learn the underlying patterns; it also memorizes the noise, random fluctuations, and specific quirks of the training data. It’s like drawing an incredibly intricate line that perfectly connects <em>every single data point</em> on your training set, even the outliers. While it looks perfect on the training data, it performs terribly on new, unseen data because it hasn’t learned the general rules – it’s just memorized the specific answers.</p> <p><strong>Characteristics of Overfitting:</strong></p> <ul> <li> <strong>High Variance:</strong> This means the model is highly sensitive to the specific training data it sees. If you change the training data slightly, the model would change dramatically. It lacks stability.</li> <li><strong>Excellent performance on training data, but poor performance on test (unseen) data.</strong></li> <li> <strong>Often too complex:</strong> Uses too many features, a model type that’s overly flexible, or has too many parameters.</li> </ul> <p><strong>Visualizing Overfitting:</strong> Again, consider our scatter plot with data points forming a gentle curve. An overfit model might draw a squiggly, highly convoluted line that passes through or very close to <em>every single point</em>, even the outliers. This line looks perfect for the training data, but it would be terrible at predicting where new points on the underlying curve would fall.</p> <p><strong>Causes of Overfitting:</strong></p> <ul> <li> <strong>Excessive Model Complexity:</strong> Using a very powerful model (like a deep neural network) with insufficient data, or too many degrees of freedom.</li> <li> <strong>Too Many Features:</strong> Including irrelevant or redundant features that the model tries to incorporate.</li> <li> <strong>Insufficient Training Data:</strong> Not having enough examples for the model to learn the true patterns without resorting to memorization.</li> <li> <strong>Too Little Regularization:</strong> Not applying enough constraints to the model’s complexity.</li> </ul> <p><strong>How to combat Overfitting:</strong></p> <ul> <li> <strong>More Data:</strong> The most effective solution! With more diverse examples, the model is less likely to memorize noise.</li> <li> <strong>Simplify the Model:</strong> Use a less complex model (e.g., fewer layers in a neural network, simpler decision trees).</li> <li> <strong>Feature Selection/Engineering:</strong> Choose only the most relevant features or create better, more informative features.</li> <li> <strong>Regularization:</strong> Techniques like L1 (Lasso) or L2 (Ridge) regularization add a penalty to the model’s complexity, discouraging large coefficients and thus smoother models. <ul> <li> <strong>L1 Regularization (Lasso):</strong> Adds a penalty proportional to the absolute value of the coefficients. It can lead to sparse models, effectively performing feature selection by driving some coefficients to zero. <ul> <li> <table> <tbody> <tr> <td>Cost Function: $J(\theta) + \lambda \sum_{j=1}^{m}</td> <td>\theta_j</td> <td>$</td> </tr> </tbody> </table> </li> </ul> </li> <li> <strong>L2 Regularization (Ridge):</strong> Adds a penalty proportional to the square of the magnitude of the coefficients. It shrinks the coefficients towards zero but doesn’t usually make them exactly zero. <ul> <li>Cost Function: $J(\theta) + \lambda \sum_{j=1}^{m} \theta_j^2$</li> </ul> </li> <li>Here, $J(\theta)$ is the original cost function (e.g., Mean Squared Error), $\theta_j$ are the model parameters (coefficients), and $\lambda$ is the regularization strength (a hyperparameter you tune). A larger $\lambda$ means more regularization.</li> </ul> </li> <li> <strong>Early Stopping:</strong> For iterative models (like neural networks), stop training when the performance on a separate <em>validation set</em> starts to degrade, even if the training set performance is still improving.</li> <li> <strong>Cross-Validation:</strong> A robust technique to evaluate model performance and detect overfitting by splitting the data into multiple train/validation folds.</li> <li> <strong>Ensemble Methods:</strong> Combine predictions from multiple models (e.g., Random Forests, Gradient Boosting) to reduce variance.</li> </ul> <h3 id="the-bias-variance-trade-off-the-sweet-spot">The Bias-Variance Trade-off: The Sweet Spot</h3> <p>At this point, you might be thinking: underfitting is too simple (high bias), and overfitting is too complex (high variance). How do we find the middle ground? This, my friends, is the <strong>Bias-Variance Trade-off</strong>, one of the most fundamental concepts in machine learning.</p> <p>Every model’s error can be decomposed into three main components:</p> <p>$Total\ Error = Bias^2 + Variance + Irreducible\ Error$</p> <p>Let’s break that down:</p> <ul> <li> <strong>Bias:</strong> The error introduced by approximating a real-world problem (which might be complicated) with a simplified model. High bias leads to underfitting.</li> <li> <strong>Variance:</strong> The amount that the estimate of the target function will change if different training data was used. High variance leads to overfitting.</li> <li> <strong>Irreducible Error:</strong> This is the noise inherent in the data itself that cannot be reduced by any model. It’s the inherent randomness or measurement error that no algorithm can perfectly capture.</li> </ul> <p>Our goal is to find a model complexity level that minimizes the total error. As we increase model complexity:</p> <ul> <li>Bias tends to <strong>decrease</strong> (the model makes fewer simplifying assumptions).</li> <li>Variance tends to <strong>increase</strong> (the model becomes more sensitive to specific training data).</li> </ul> <p>We’re looking for that “just right” point where the sum of $Bias^2$ and $Variance$ is minimized, and we achieve the best possible generalization.</p> <h3 id="tools-for-finding-goldilocks">Tools for Finding Goldilocks</h3> <p>So, how do we practically navigate this trade-off and find our “just right” model?</p> <ol> <li> <p><strong>Train-Test Split:</strong> The absolute first step. We split our dataset into a training set (usually 70-80%) and a test set (20-30%). We <em>only</em> train the model on the training set. The test set is kept completely separate and is used only once, at the very end, to evaluate the final model’s performance on unseen data. This helps us catch overfitting.</p> </li> <li> <p><strong>Validation Sets &amp; Cross-Validation:</strong> Often, a simple train-test split isn’t enough, especially for hyperparameter tuning (like deciding the $\lambda$ for regularization or the number of layers in a neural network). We introduce a <em>validation set</em> (a portion of the training data set aside) to tune hyperparameters. Even better is <strong>K-Fold Cross-Validation</strong>, where the training data is split into K smaller “folds.” The model is trained K times, each time using a different fold as the validation set and the remaining K-1 folds as the training set. This gives a more robust estimate of the model’s performance and helps in selecting the best hyperparameters without touching the final test set.</p> </li> <li> <p><strong>Learning Curves:</strong> These plots show the model’s performance (e.g., accuracy or error) on both the training set and a validation set as a function of the training set size or training iterations.</p> <ul> <li>If both training and validation error are high and flat, it often indicates <strong>underfitting</strong>.</li> <li>If training error is low and validation error is high (with a significant gap), it’s a classic sign of <strong>overfitting</strong>.</li> <li>The “sweet spot” is where both errors are low and converge.</li> </ul> </li> </ol> <h3 id="conclusion-the-art-of-balance">Conclusion: The Art of Balance</h3> <p>Understanding overfitting and underfitting is not just academic; it’s fundamental to building effective machine learning models in the real world. It’s about ensuring our models are truly intelligent, capable of adapting to new situations, rather than just being brilliant memorizers.</p> <p>The journey of a data scientist often feels like that of a sculptor, constantly refining and adjusting, trying to find the perfect form for their model. We use powerful tools and techniques, but at its core, it’s about appreciating the delicate balance between simplicity and complexity, between bias and variance.</p> <p>So, the next time you encounter a machine learning model, take a moment to appreciate the “Goldilocks Problem” that its creators had to solve to make it “just right.” Keep exploring, keep questioning, and happy modeling!</p> <hr> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>