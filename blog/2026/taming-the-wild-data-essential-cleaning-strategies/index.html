<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Taming the Wild Data: Essential Cleaning Strategies for Aspiring Data Scientists | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2026/taming-the-wild-data-essential-cleaning-strategies/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Taming the Wild Data: Essential Cleaning Strategies for Aspiring Data Scientists</h1> <p class="post-meta"> Created on January 23, 2026 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2026"> <i class="fa-solid fa-calendar fa-sm"></i> 2026 </a>   ·   <a href="/blog/blog/tag/data-cleaning"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Cleaning</a>   <a href="/blog/blog/tag/data-preprocessing"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Preprocessing</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/python"> <i class="fa-solid fa-hashtag fa-sm"></i> Python</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>Welcome back to the portfolio deep-dive. Today, we’re tackling a topic that often feels like the unsung hero of data science: <strong>data cleaning</strong>. You might think the glitz and glamour are all in building complex machine learning models or crafting stunning visualizations. And sure, those parts are exciting! But trust me, as someone who’s spent countless hours wrestling with uncooperative datasets, the real magic, the real <em>foundation</em> of any successful data project, lies in how well you clean your data.</p> <p>Think of it this way: would you build a magnificent skyscraper on a swampy, unstable ground? Absolutely not! Similarly, feeding dirty, inconsistent, or incomplete data into even the most sophisticated algorithm is a recipe for disaster. We call it the “garbage in, garbage out” (GIGO) principle. If your input is flawed, your output — your insights, your predictions — will be equally flawed, if not worse.</p> <p>In fact, industry experts often claim that data scientists spend anywhere from 50% to 80% of their time just on cleaning and preparing data. That’s a huge chunk of our work! So, mastering this skill isn’t just a nicety; it’s a necessity.</p> <p>In this post, I want to walk you through a personal playbook of data cleaning strategies, sharing the methods and mindset that have helped me turn chaotic raw data into reliable fuel for robust machine learning models. Whether you’re just starting your data science journey or looking to refine your cleaning game, you’ll find practical tips and techniques here.</p> <h3 id="why-is-our-data-so-wild-anyway-the-roots-of-the-mess">Why is Our Data So Wild, Anyway? The Roots of the Mess</h3> <p>Before we jump into cleaning, let’s quickly understand <em>why</em> data gets messy in the first place. It’s usually not malicious, just a natural consequence of how data is collected:</p> <ol> <li> <strong>Human Error:</strong> Typos during manual data entry, incorrect selections in forms.</li> <li> <strong>Systemic Issues:</strong> Faulty sensors, software bugs, errors during data migration or integration from multiple sources.</li> <li> <strong>Incomplete Information:</strong> Users skipping fields, technical glitches causing data loss, or data simply not existing for certain entries.</li> <li> <strong>Inconsistent Standards:</strong> Different departments or systems recording the same information in varying formats (e.g., “NY,” “New York,” “nyc”).</li> <li> <strong>Data Generation Process:</strong> Sometimes, the nature of the data collection itself can lead to inherent noise or missing values (e.g., surveys where certain questions are only asked if a previous condition is met).</li> </ol> <p>Acknowledging these sources helps us approach the cleaning process with a problem-solving mindset rather than just frustration.</p> <h3 id="the-data-cleaning-playbook-strategies-to-tame-the-beast">The Data Cleaning Playbook: Strategies to Tame the Beast</h3> <p>Ready to roll up your sleeves? Let’s dive into the core strategies.</p> <h4 id="i-the-first-rule-understand-your-data-exploratory-data-analysis---eda">I. The First Rule: Understand Your Data (Exploratory Data Analysis - EDA)</h4> <p>Before you even <em>think</em> about cleaning, you <em>must</em> understand your data. This is where Exploratory Data Analysis (EDA) comes in, and it’s the most critical first step. EDA is like getting to know your new roommate before deciding where to put their furniture.</p> <p><strong>What to look for during EDA:</strong></p> <ul> <li> <strong>General Information:</strong> How many rows and columns? What are the data types? Are there memory issues? <ul> <li> <em>Tool:</em> <code class="language-plaintext highlighter-rouge">df.info()</code> in Pandas is your best friend here. It gives you a quick overview of non-null counts and data types for each column.</li> </ul> </li> <li> <strong>Descriptive Statistics:</strong> What are the central tendencies and spread of your numerical data? <ul> <li> <em>Tool:</em> <code class="language-plaintext highlighter-rouge">df.describe()</code> provides mean, median (not directly, but can be computed), standard deviation, min, max, and quartiles for numerical columns.</li> </ul> </li> <li> <strong>Missing Values:</strong> Where are they? How many? <ul> <li> <em>Tool:</em> <code class="language-plaintext highlighter-rouge">df.isnull().sum()</code> will give you a count of missing values per column. Combine it with <code class="language-plaintext highlighter-rouge">df.isnull().sum() / len(df) * 100</code> to get percentages, which are often more insightful.</li> </ul> </li> <li> <strong>Unique Values &amp; Frequencies:</strong> For categorical data, how many unique categories are there? Are there spelling mistakes or inconsistencies? <ul> <li> <em>Tool:</em> <code class="language-plaintext highlighter-rouge">df['column_name'].value_counts()</code> reveals the frequency of each unique value.</li> </ul> </li> <li> <strong>Visualizations:</strong> Histograms, box plots, scatter plots, and bar charts are invaluable. They help you spot outliers, strange distributions, and relationships that purely numerical summaries might miss. <ul> <li> <em>Tool:</em> Libraries like Matplotlib, Seaborn, and Plotly make this easy in Python.</li> </ul> </li> </ul> <p>This initial exploration helps you identify the <em>types</em> of cleaning needed. You can’t fix what you don’t know is broken!</p> <h4 id="ii-handling-the-voids-missing-values">II. Handling the Voids: Missing Values</h4> <p>Missing values, often represented as <code class="language-plaintext highlighter-rouge">NaN</code> (Not a Number) or <code class="language-plaintext highlighter-rouge">None</code>, are one of the most common headaches. How you handle them can significantly impact your model’s performance.</p> <p><strong>Strategies for Missing Values:</strong></p> <ol> <li> <strong>Deletion:</strong> <ul> <li> <strong>Row-wise Deletion (<code class="language-plaintext highlighter-rouge">df.dropna()</code>):</strong> If a row has <em>any</em> missing value, you drop the entire row. <ul> <li> <em>When to use:</em> When the percentage of missing values in a particular row is very small, and dropping it won’t lead to significant data loss. If you have 100,000 rows and only 100 have missing values, dropping them is often a safe and easy approach.</li> </ul> </li> <li> <strong>Column-wise Deletion (<code class="language-plaintext highlighter-rouge">df.dropna(axis=1)</code>):</strong> If a column has <em>too many</em> missing values (e.g., more than 50-70%), it might be better to drop the entire column. <ul> <li> <em>When to use:</em> When a column is largely empty, it likely provides little predictive power and might introduce noise.</li> </ul> </li> <li> <em>Caution:</em> Be careful with deletion. If you delete too much data, you might remove valuable information or introduce bias if missingness isn’t random.</li> </ul> </li> <li> <strong>Imputation (Filling Missing Values):</strong> Imputation means replacing missing values with substituted values. This is often preferred over deletion to preserve data size. <ul> <li> <strong>Simple Statistical Imputation:</strong> <ul> <li> <strong>Mean:</strong> Replace <code class="language-plaintext highlighter-rouge">NaN</code> with the column’s mean. ($x_{imputed} = \bar{x}$) <ul> <li> <em>When to use:</em> For numerical data with a relatively normal distribution and no significant outliers.</li> </ul> </li> <li> <strong>Median:</strong> Replace <code class="language-plaintext highlighter-rouge">NaN</code> with the column’s median. ($x_{imputed} = \text{median}(x)$) <ul> <li> <em>When to use:</em> For numerical data, especially when it’s skewed or has outliers, as the median is less sensitive to extreme values than the mean.</li> </ul> </li> <li> <strong>Mode:</strong> Replace <code class="language-plaintext highlighter-rouge">NaN</code> with the column’s mode (most frequent value). ($x_{imputed} = \text{mode}(x)$) <ul> <li> <em>When to use:</em> Primarily for categorical data, but can also be used for numerical data.</li> </ul> </li> </ul> </li> <li> <strong>Forward Fill (<code class="language-plaintext highlighter-rouge">ffill</code>) or Backward Fill (<code class="language-plaintext highlighter-rouge">bfill</code>):</strong> <ul> <li> <em>When to use:</em> Particularly useful for time-series data, where the value at time $t$ might be best approximated by the value at $t-1$ (forward fill) or $t+1$ (backward fill).</li> </ul> </li> <li> <strong>Constant Value Imputation:</strong> <ul> <li>Replace <code class="language-plaintext highlighter-rouge">NaN</code> with a constant like 0, ‘Unknown’, or a specific sentinel value.</li> <li> <em>When to use:</em> For categorical data, ‘Unknown’ can be a category itself. For numerical data, 0 might make sense if missingness truly implies absence (e.g., missing sales count = 0 sales).</li> </ul> </li> <li> <strong>Advanced Imputation Techniques:</strong> <ul> <li> <strong>K-Nearest Neighbors (KNN Imputer):</strong> This method finds the ‘k’ nearest neighbors to a data point with a missing value and imputes the missing value based on those neighbors. It’s more sophisticated but computationally intensive.</li> <li> <strong>Regression Imputation:</strong> You can build a predictive model (e.g., linear regression) to predict the missing values in one column based on other columns.</li> </ul> </li> </ul> </li> </ol> <p>The choice of imputation strategy is crucial and often depends on the nature of the data and the domain context.</p> <h4 id="iii-spotting-the-odd-ones-out-outliers">III. Spotting the Odd Ones Out: Outliers</h4> <p>Outliers are data points that significantly deviate from other observations. They can be genuine anomalies (e.g., a record-breaking stock price) or simply data entry errors (e.g., a person’s age entered as 200). Outliers can skew statistical analyses and negatively impact machine learning models.</p> <p><strong>Strategies for Outliers:</strong></p> <ol> <li> <strong>Identification:</strong> <ul> <li> <strong>Visual Inspection:</strong> <ul> <li> <strong>Box Plots:</strong> Show the distribution of data and clearly highlight points outside the “whiskers.”</li> <li> <strong>Scatter Plots:</strong> Useful for identifying outliers in two dimensions.</li> <li> <strong>Histograms:</strong> Can show unusually sparse bins at the extremes.</li> </ul> </li> <li> <strong>Statistical Methods:</strong> <ul> <li> <strong>Z-score:</strong> Measures how many standard deviations a data point is from the mean. <ul> <li>$Z = \frac{x - \mu}{\sigma}$</li> <li> <table> <tbody> <tr> <td>Where $x$ is the data point, $\mu$ is the mean, and $\sigma$ is the standard deviation. A common threshold for outliers is $</td> <td>Z</td> <td>&gt; 3$.</td> </tr> </tbody> </table> </li> </ul> </li> <li> <strong>Interquartile Range (IQR):</strong> A robust measure of spread. <ul> <li>$IQR = Q_3 - Q_1$</li> <li>Outliers are often defined as values falling below $Q_1 - 1.5 \times IQR$ or above $Q_3 + 1.5 \times IQR$. These are the “fences” in a box plot.</li> </ul> </li> </ul> </li> </ul> </li> <li> <strong>Handling Strategies:</strong> <ul> <li> <strong>Removal:</strong> <ul> <li> <em>When to use:</em> If you are certain the outlier is due to data entry error or a measurement mistake and does not represent a real phenomenon. Be very careful with this, as deleting genuine extreme values can lead to loss of valuable information.</li> </ul> </li> <li> <strong>Transformation:</strong> <ul> <li> <strong>Log Transformation:</strong> Applying <code class="language-plaintext highlighter-rouge">log(x)</code> (or <code class="language-plaintext highlighter-rouge">log1p(x)</code> for values including zero) can reduce the skewness of data and compress the range of values, thereby reducing the impact of extreme outliers.</li> <li> <strong>Square Root Transformation:</strong> Similar to log transformation, it reduces skewness and stabilizes variance.</li> <li> <em>When to use:</em> When the data is heavily skewed and outliers are stretching the distribution.</li> </ul> </li> <li> <strong>Capping (Winsorization):</strong> <ul> <li>Replace outliers with values at a certain percentile. For example, replace all values above the 99th percentile with the value at the 99th percentile, and all values below the 1st percentile with the value at the 1st percentile.</li> <li> <em>When to use:</em> When you want to reduce the influence of outliers without completely removing them or changing the distribution shape too drastically.</li> </ul> </li> <li> <strong>Keep Them:</strong> <ul> <li> <em>When to use:</em> If outliers represent important, rare events (e.g., fraud detection, disease outbreaks). In such cases, they are not errors but critical data points that your model should learn from.</li> </ul> </li> </ul> </li> </ol> <h4 id="iv-tidying-up-the-details-inconsistent-data--duplicates">IV. Tidying Up the Details: Inconsistent Data &amp; Duplicates</h4> <p>These issues might seem small, but they can wreak havoc on your analysis.</p> <ol> <li> <strong>Inconsistent Formatting:</strong> <ul> <li> <strong>Case Sensitivity:</strong> “USA”, “usa”, “Usa” should probably be treated as the same country. <ul> <li> <em>Fix:</em> Convert all text to a consistent case: <code class="language-plaintext highlighter-rouge">df['country'].str.lower()</code>.</li> </ul> </li> <li> <strong>Typos &amp; Variations:</strong> “New York”, “NY”, “NYC”; “M”, “Male”; “TRUE”, “True”, “true”. <ul> <li> <em>Fix:</em> Use <code class="language-plaintext highlighter-rouge">replace()</code> or <code class="language-plaintext highlighter-rouge">map()</code> functions to standardize values. For complex cases, regular expressions can be powerful (<code class="language-plaintext highlighter-rouge">re</code> module in Python). For example, mapping “NY”, “NYC” to “New York”.</li> </ul> </li> <li> <strong>Whitespace:</strong> Extra spaces (“ USA “) can make unique values appear different. <ul> <li> <em>Fix:</em> <code class="language-plaintext highlighter-rouge">df['column'].str.strip()</code> to remove leading/trailing whitespace.</li> </ul> </li> </ul> </li> <li> <strong>Duplicate Records:</strong> <ul> <li>Sometimes, entire rows or specific combinations of columns might be exact duplicates, indicating errors in data collection or merging.</li> <li> <strong>Identification:</strong> <code class="language-plaintext highlighter-rouge">df.duplicated()</code> returns a boolean Series indicating which rows are duplicates (after their first occurrence).</li> <li> <strong>Removal:</strong> <code class="language-plaintext highlighter-rouge">df.drop_duplicates()</code> removes duplicate rows. You can specify a <code class="language-plaintext highlighter-rouge">subset</code> of columns if you only want to consider certain columns for duplication checks (e.g., two people with the same name might be okay, but two people with the same name <em>and</em> same email are likely duplicates).</li> <li> <em>Caution:</em> Ensure you understand <em>why</em> duplicates exist. Sometimes, what looks like a duplicate might represent distinct events (e.g., multiple transactions by the same customer).</li> </ul> </li> </ol> <h4 id="v-ensuring-correct-types-data-type-conversion">V. Ensuring Correct Types: Data Type Conversion</h4> <p>Sometimes, numerical data might be imported as strings (<code class="language-plaintext highlighter-rouge">object</code> type in Pandas) due to non-numeric characters (like ‘$’ or commas) or errors. Dates might be treated as strings. This prevents mathematical operations or proper chronological sorting.</p> <ul> <li> <strong>Identification:</strong> <code class="language-plaintext highlighter-rouge">df.info()</code> will show you the data types.</li> <li> <strong>Fix:</strong> <ul> <li> <code class="language-plaintext highlighter-rouge">pd.to_numeric(df['column'], errors='coerce')</code>: This attempts to convert a column to a numeric type. <code class="language-plaintext highlighter-rouge">errors='coerce'</code> is vital; it turns values that cannot be converted into <code class="language-plaintext highlighter-rouge">NaN</code>, which you can then handle with imputation strategies.</li> <li> <code class="language-plaintext highlighter-rouge">pd.to_datetime(df['date_column'])</code>: Converts string representations to datetime objects, allowing for date-based operations.</li> <li> <code class="language-plaintext highlighter-rouge">df['category_column'].astype('category')</code>: Converts object columns with a limited number of unique values into the more memory-efficient ‘category’ type.</li> </ul> </li> </ul> <h3 id="the-iterative-nature-its-not-a-one-shot-deal">The Iterative Nature: It’s Not a One-Shot Deal</h3> <p>Here’s a crucial insight: data cleaning is rarely a linear process. You’ll perform some EDA, clean a bit, then do more EDA, discover new issues, and clean again. It’s an iterative cycle. Each cleaning step might reveal a new anomaly or make a previous assumption invalid. Embrace this back-and-forth dance!</p> <p>Moreover, your cleaning strategy should always be informed by the problem you’re trying to solve. For a fraud detection model, outliers (the fraudulent transactions) are gold. For a model predicting house prices, an extremely high price might be a data entry error. Context is king.</p> <h3 id="best-practices-and-my-personal-tips">Best Practices and My Personal Tips:</h3> <ul> <li> <strong>Work on a Copy:</strong> Always, <em>always</em> work on a copy of your original dataset (<code class="language-plaintext highlighter-rouge">df.copy()</code>). This saves you from irreversible mistakes and allows you to easily revert.</li> <li> <strong>Document Everything:</strong> Keep a detailed record of every cleaning step you take. Why did you drop those rows? How did you impute those missing values? This makes your work reproducible and understandable to others (and your future self!).</li> <li> <strong>Automate Where Possible:</strong> If you find yourself doing the same cleaning steps repeatedly for similar datasets, write functions or scripts to automate them.</li> <li> <strong>Visualize Before and After:</strong> Compare distributions, value counts, or key statistics before and after a cleaning step to ensure your changes had the desired effect without introducing new problems.</li> <li> <strong>Question Everything:</strong> Don’t just blindly apply techniques. Ask yourself: “Does this make sense for my data? What impact will this have on my analysis/model?”</li> </ul> <h3 id="conclusion-embrace-the-mess-become-the-master">Conclusion: Embrace the Mess, Become the Master</h3> <p>Data cleaning might not be the flashiest part of data science, but it’s arguably the most important. It’s where you truly get to know your data, understand its quirks, and build a solid foundation for everything that follows. Investing time and effort here pays dividends in the form of more accurate models, more reliable insights, and ultimately, more impactful data science projects.</p> <p>So, don’t shy away from the messy reality of raw data. Embrace the challenge, apply these strategies, and watch your skills grow. Your machine learning models (and your future self) will thank you for it!</p> <p>Happy cleaning!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>