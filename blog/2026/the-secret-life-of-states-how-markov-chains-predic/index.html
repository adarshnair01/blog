<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!) | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</h1> <p class="post-meta"> Created on February 09, 2026 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2026"> <i class="fa-solid fa-calendar fa-sm"></i> 2026 </a>   ·   <a href="/blog/blog/tag/markov-chains"> <i class="fa-solid fa-hashtag fa-sm"></i> Markov Chains</a>   <a href="/blog/blog/tag/probability"> <i class="fa-solid fa-hashtag fa-sm"></i> Probability</a>   <a href="/blog/blog/tag/stochastic-processes"> <i class="fa-solid fa-hashtag fa-sm"></i> Stochastic Processes</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Lying in bed, staring at the ceiling, my mind often wanders to the simple routines that make up our lives. Waking up, making coffee, checking emails, getting to work. Each step seems to lead to the next, almost predictably. But what if the only thing that <em>truly</em> mattered for my next action was my current action, and nothing else from my past? What if I could predict my future, even without remembering my history?</p> <p>Sounds a bit like a superpower, right? Well, in the world of data science and machine learning, this “superpower” has a name: <strong>Markov Chains</strong>. They’re a fundamental concept, incredibly simple at their core, yet capable of modeling complex real-world phenomena. If you’ve ever typed a sentence and had your phone suggest the next word, or seen a weather forecast predicting rain after a cloudy day, you’ve witnessed Markov Chains in action.</p> <p>Today, I want to take you on a journey through the elegant simplicity and surprising power of Markov Chains. We’ll strip away the jargon and understand how this memoryless marvel works.</p> <h3 id="the-heart-of-the-matter-the-markov-property-no-memory-no-problem">The Heart of the Matter: The Markov Property (No Memory, No Problem!)</h3> <p>Imagine you’re playing a board game. Your next move depends entirely on where you are <em>right now</em> on the board, not on all the previous squares you’ve landed on, nor the dice rolls from five turns ago. That, my friends, is the essence of the <strong>Markov Property</strong>:</p> <p><strong>The future is independent of the past given the present.</strong></p> <p>Let’s unpack that. It means that to predict the next state (or event), all you need to know is the <em>current</em> state. Any information about how you arrived at this current state is irrelevant. Your previous journey doesn’t change the probability of your next step.</p> <p>Think about the weather. If it’s cloudy today, the chance of rain tomorrow depends primarily on it being cloudy <em>today</em>, not on whether it was sunny last week, rainy two days ago, and then cloudy yesterday. The “cloudy today” state is sufficient to determine the probabilities for tomorrow’s weather.</p> <p>This “memoryless” property is what makes Markov Chains so elegant and, frankly, so powerful in modeling sequences.</p> <h3 id="building-blocks-of-a-markov-chain-states-transitions-and-probabilities">Building Blocks of a Markov Chain: States, Transitions, and Probabilities</h3> <p>To truly understand Markov Chains, let’s break them down into their core components:</p> <ol> <li> <p><strong>States:</strong> These are the possible situations, conditions, or locations your system can be in. In our weather example, the states could be <code class="language-plaintext highlighter-rouge">Sunny</code>, <code class="language-plaintext highlighter-rouge">Cloudy</code>, <code class="language-plaintext highlighter-rouge">Rainy</code>. If you’re modeling a student’s activity during the day, states might be <code class="language-plaintext highlighter-rouge">Studying</code>, <code class="language-plaintext highlighter-rouge">Procrastinating</code>, <code class="language-plaintext highlighter-rouge">Eating</code>, <code class="language-plaintext highlighter-rouge">Sleeping</code>. The collection of all possible states is called the <strong>state space</strong>.</p> </li> <li> <p><strong>Transitions:</strong> These are the movements or changes from one state to another. From <code class="language-plaintext highlighter-rouge">Sunny</code>, you might transition to <code class="language-plaintext highlighter-rouge">Cloudy</code>. From <code class="language-plaintext highlighter-rouge">Studying</code>, you might transition to <code class="language-plaintext highlighter-rouge">Eating</code>.</p> </li> <li> <p><strong>Transition Probabilities:</strong> This is where the “probability” in Markov Chains comes in. For every possible transition from one state to another, there’s a probability associated with it. For example:</p> <ul> <li>If it’s <code class="language-plaintext highlighter-rouge">Sunny</code> today, there’s a 70% chance it stays <code class="language-plaintext highlighter-rouge">Sunny</code> tomorrow, a 20% chance it becomes <code class="language-plaintext highlighter-rouge">Cloudy</code>, and a 10% chance it becomes <code class="language-plaintext highlighter-rouge">Rainy</code>.</li> <li>If it’s <code class="language-plaintext highlighter-rouge">Cloudy</code> today, there’s a 30% chance it becomes <code class="language-plaintext highlighter-rouge">Sunny</code>, a 40% chance it stays <code class="language-plaintext highlighter-rouge">Cloudy</code>, and a 30% chance it becomes <code class="language-plaintext highlighter-rouge">Rainy</code>.</li> </ul> </li> </ol> <p>Crucially, these probabilities must sum to 1 for all transitions <em>out of</em> a given state. You have to go <em>somewhere</em>!</p> <h4 id="a-simple-example-my-mood-swings-a-highly-simplified-model">A Simple Example: My Mood Swings (A highly simplified model!)</h4> <p>Let’s say my internal states are <code class="language-plaintext highlighter-rouge">Happy</code>, <code class="language-plaintext highlighter-rouge">Neutral</code>, <code class="language-plaintext highlighter-rouge">Grumpy</code>. And, being a creature of habit (and the Markov Property), my next mood depends <em>only</em> on my current mood.</p> <ul> <li>If I’m <code class="language-plaintext highlighter-rouge">Happy</code> today: <ul> <li>50% chance I’m <code class="language-plaintext highlighter-rouge">Happy</code> tomorrow</li> <li>40% chance I’m <code class="language-plaintext highlighter-rouge">Neutral</code> tomorrow</li> <li>10% chance I’m <code class="language-plaintext highlighter-rouge">Grumpy</code> tomorrow</li> </ul> </li> <li>If I’m <code class="language-plaintext highlighter-rouge">Neutral</code> today: <ul> <li>30% chance I’m <code class="language-plaintext highlighter-rouge">Happy</code> tomorrow</li> <li>30% chance I’m <code class="language-plaintext highlighter-rouge">Neutral</code> tomorrow</li> <li>40% chance I’m <code class="language-plaintext highlighter-rouge">Grumpy</code> tomorrow</li> </ul> </li> <li>If I’m <code class="language-plaintext highlighter-rouge">Grumpy</code> today: <ul> <li>10% chance I’m <code class="language-plaintext highlighter-rouge">Happy</code> tomorrow</li> <li>20% chance I’m <code class="language-plaintext highlighter-rouge">Neutral</code> tomorrow</li> <li>70% chance I’m <code class="language-plaintext highlighter-rouge">Grumpy</code> tomorrow</li> </ul> </li> </ul> <p>We can visualize this as a <strong>state diagram</strong> where circles are states and arrows are transitions with their probabilities.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>       0.5 (Happy)
      / |\
     /  | \
    v   |  v
 Happy--0.4--&gt;Neutral
 ^  ^   |   ^
 |  |   |   |
 |  |___0.1_|___0.3
 |  |   |   |
 |  |   v   |
 0.1|   0.3 |   0.2
 |  |   |   |
 |  |   |   |
 Grumpy&lt;-----0.7----
</code></pre></div></div> <p>(Apologies for the ASCII art, but it gets the point across!)</p> <h3 id="the-math-behind-the-magic-transition-matrices">The Math Behind the Magic: Transition Matrices</h3> <p>While state diagrams are great for intuition, mathematicians (and data scientists!) love matrices. We can represent our transition probabilities in a <strong>transition matrix</strong>, often denoted as $P$.</p> <p>Each row in the matrix represents the <em>current</em> state, and each column represents the <em>next</em> state. The entry $p_{ij}$ is the probability of moving from state $i$ to state $j$.</p> <p>For my mood example, let’s order our states as (Happy, Neutral, Grumpy):</p> <p>$P = \begin{pmatrix} 0.5 &amp; 0.4 &amp; 0.1 <br> 0.3 &amp; 0.3 &amp; 0.4 <br> 0.1 &amp; 0.2 &amp; 0.7 \end{pmatrix}$</p> <p>Notice that each row sums to 1. This is a crucial property of a transition matrix.</p> <p>Now, here’s where it gets interesting! If we know our initial state (or, more commonly, an initial <em>probability distribution</em> over our states), we can predict the probability distribution for future states.</p> <p>Let $\pi_0$ be a row vector representing our initial probability distribution. For example, if I wake up <code class="language-plaintext highlighter-rouge">Happy</code> with 100% certainty, then $\pi_0 = \begin{pmatrix} 1 &amp; 0 &amp; 0 \end{pmatrix}$.</p> <p>The probability distribution after one step (tomorrow’s mood probabilities) would be $\pi_1 = \pi_0 P$. After two steps (the day after tomorrow), it would be $\pi_2 = \pi_1 P = (\pi_0 P) P = \pi_0 P^2$. In general, after $n$ steps, the probability distribution is $\pi_n = \pi_0 P^n$.</p> <p>This matrix multiplication allows us to project probabilities far into the future, all based on that simple memoryless transition matrix!</p> <h3 id="diving-deeper-key-concepts">Diving Deeper: Key Concepts</h3> <p>As we peer further into the future of a Markov Chain, some fascinating properties emerge:</p> <ul> <li> <p><strong>Irreducibility:</strong> Can you get from <em>any</em> state to <em>any other</em> state (not necessarily in one step)? If yes, the chain is irreducible. This is important for many long-term behaviors. My mood chain is irreducible because eventually, I can go from <code class="language-plaintext highlighter-rouge">Grumpy</code> to <code class="language-plaintext highlighter-rouge">Happy</code> (even if it takes a few steps via <code class="language-plaintext highlighter-rouge">Neutral</code>).</p> </li> <li> <p><strong>Aperiodicity:</strong> Does the chain always return to a state in a fixed cycle, or can it return at irregular intervals? If it’s not trapped in a fixed cycle (e.g., Happy -&gt; Grumpy -&gt; Happy -&gt; Grumpy…), it’s aperiodic.</p> </li> <li> <p><strong>Stationary Distribution (Steady State):</strong> If a Markov Chain is both irreducible and aperiodic (and some other technical conditions), it will eventually reach a point where the probability distribution over its states no longer changes, even after more transitions. This is called the <strong>stationary distribution</strong>, denoted as $\pi$.</p> <p>Mathematically, this means $\pi P = \pi$. Intuitively, it means that if you run the process long enough, the proportion of time spent in each state will settle into a fixed pattern. For my mood, after many days, there will be a certain long-term probability of me being <code class="language-plaintext highlighter-rouge">Happy</code>, <code class="language-plaintext highlighter-rouge">Neutral</code>, or <code class="language-plaintext highlighter-rouge">Grumpy</code>, regardless of my initial mood. This is a incredibly powerful concept for understanding the long-term behavior of a system.</p> </li> <li> <p><strong>Absorbing States:</strong> Some chains have “absorbing states,” which are states you can enter but cannot leave. Think of a “Game Over” state in a game, or a “bankrupt” state in finance. Once you’re in an absorbing state, you’re stuck there.</p> </li> </ul> <h3 id="where-do-we-see-markov-chains-in-action">Where Do We See Markov Chains in Action?</h3> <p>Markov Chains, despite their apparent simplicity, are the backbone of numerous real-world applications:</p> <ol> <li> <p><strong>Natural Language Processing (NLP): Text Generation &amp; Prediction:</strong> This is perhaps the most relatable application. When your phone suggests the next word in a sentence, or when language models generate coherent text, they’re often (at a basic level) using Markov Chain principles. Each word is a state, and the transition probability is how likely one word is to follow another. For example, after “the”, “cat” is more likely than “antidisestablishmentarianism”. More advanced models like neural networks have taken over, but the foundational idea of predicting sequences based on preceding elements owes a lot to Markovian concepts.</p> </li> <li> <p><strong>Google PageRank (Simplified):</strong> One of the earliest and most impactful uses of Markov Chains was in Google’s original PageRank algorithm. Imagine every webpage on the internet is a state. When you click a link, you transition from one page to another. PageRank models the probability of a “random surfer” landing on any given page. The stationary distribution of this massive Markov Chain gives each page a “PageRank” – essentially, how likely a random surfer is to end up on that page. Pages with higher stationary probabilities are considered more important or authoritative.</p> </li> <li> <p><strong>Weather Forecasting:</strong> As we discussed, this is a classic example. Meteorologists can model weather patterns using states like <code class="language-plaintext highlighter-rouge">Sunny</code>, <code class="language-plaintext highlighter-rouge">Cloudy</code>, <code class="language-plaintext highlighter-rouge">Rainy</code>, and estimate transition probabilities based on historical data. While modern weather models are far more complex, the Markovian framework offers a good starting point.</p> </li> <li> <p><strong>Genetics:</strong> Markov Chains are used to model DNA sequences, predicting the likelihood of certain bases (A, T, C, G) appearing after others. They’re also vital in hidden Markov models for gene finding and sequence alignment.</p> </li> <li> <p><strong>Reinforcement Learning:</strong> The entire framework of many reinforcement learning problems is built on Markov Chains (specifically, Markov Decision Processes, which add actions and rewards). An agent interacts with an environment, transitioning between states, and the goal is to learn a policy that maximizes rewards over time.</p> </li> </ol> <h3 id="the-memoryless-limitation-and-why-its-often-okay">The Memoryless Limitation (And Why It’s Often Okay)</h3> <p>The biggest “catch” with Markov Chains is their memoryless property. In many real-world scenarios, the future <em>does</em> depend on more than just the immediate present. For instance, my mood might not just depend on my mood <em>today</em>, but also on whether I got enough sleep <em>last night</em> and if I had a big presentation <em>earlier in the week</em>.</p> <p>However, this limitation is often mitigated by:</p> <ul> <li> <strong>Defining richer states:</strong> Instead of just <code class="language-plaintext highlighter-rouge">Happy</code>, <code class="language-plaintext highlighter-rouge">Neutral</code>, <code class="language-plaintext highlighter-rouge">Grumpy</code>, I could define states like <code class="language-plaintext highlighter-rouge">Happy (after good sleep)</code>, <code class="language-plaintext highlighter-rouge">Happy (after bad sleep)</code>. This effectively bakes “memory” into the state definition.</li> <li> <strong>Higher-order Markov Chains:</strong> Instead of depending only on the <em>last</em> state, a second-order Markov Chain depends on the <em>last two</em> states, a third-order on the <em>last three</em>, and so on. This adds memory at the cost of significantly increasing the number of possible states.</li> <li> <strong>Hidden Markov Models (HMMs):</strong> These are an extension where the underlying states are <em>hidden</em> or unobservable, and we only observe some probabilistic output of those states. This allows for more complex modeling where noise and uncertainty are present.</li> </ul> <p>Despite these limitations, the simplicity and analytical tractability of basic Markov Chains make them an indispensable tool in a data scientist’s arsenal, especially as a foundational concept.</p> <h3 id="my-next-move-and-yours">My Next Move… and Yours!</h3> <p>From predicting my mood to powering Google’s search engine, Markov Chains offer an elegant way to model systems that evolve through states over time, all based on that wonderfully simple idea: the future only cares about the present.</p> <p>So, the next time your phone auto-completes your sentence or you hear a weather forecast, take a moment to appreciate the humble yet mighty Markov Chain working tirelessly behind the scenes, predicting the future, one memoryless step at a time.</p> <p>Now that you’ve journeyed through the world of Markov Chains, what’s your next state? Perhaps diving deeper into a specific application, or even trying to implement one in Python? The possibilities are as endless as the states in a well-connected chain!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/from-hello-world-to-hello-human-my-adventure-in-na/">From 'Hello World' to 'Hello Human': My Adventure in Natural Language Processing</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>