<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Unraveling the Brain of AI: A Journey into Neural Networks | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2026/unraveling-the-brain-of-ai-a-journey-into-neural-n/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Unraveling the Brain of AI: A Journey into Neural Networks</h1> <p class="post-meta"> Created on January 13, 2026 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2026"> <i class="fa-solid fa-calendar fa-sm"></i> 2026 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/neural-networks"> <i class="fa-solid fa-hashtag fa-sm"></i> Neural Networks</a>   <a href="/blog/blog/tag/artificial-intelligence"> <i class="fa-solid fa-hashtag fa-sm"></i> Artificial Intelligence</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>I remember the first time I truly wrapped my head around the concept of Artificial Intelligence. It wasn’t about robots or sci-fi movies; it was about the profound idea that a machine could <em>learn</em>. But how? How could a hunk of silicon and wires mimic something as complex as human thought? My quest to understand led me down a fascinating rabbit hole, right into the heart of what makes modern AI tick: <strong>Neural Networks</strong>.</p> <p>If you’ve ever felt intimidated by the jargon surrounding deep learning, you’re not alone. But trust me, at its core, a neural network is an elegant, surprisingly intuitive system. Think of it as a collaborative team of tiny, interconnected decision-makers, all working together to solve incredibly complex problems. Ready to peel back the layers? Let’s dive in!</p> <h3 id="the-inspiration-our-own-brains">The Inspiration: Our Own Brains</h3> <p>Before we get technical, let’s take a moment to appreciate the biological marvel that inspired it all: the human brain. Our brains are made of billions of cells called <strong>neurons</strong>. Each neuron is a tiny processor with a specific job:</p> <ul> <li> <strong>Dendrites:</strong> These are like antennas, receiving signals from other neurons.</li> <li> <strong>Cell Body (Soma):</strong> This is the neuron’s “brain,” where all incoming signals are processed.</li> <li> <strong>Axon:</strong> If the signals are strong enough, the neuron “fires” an electrical impulse down its axon to other neurons.</li> <li> <strong>Synapses:</strong> These are the tiny gaps where neurons connect and pass signals. The strength of these connections changes over time, which is how we learn!</li> </ul> <p>This incredible network allows us to recognize faces, understand language, learn new skills, and make decisions – often without us even consciously realizing the intricate processing happening beneath the surface.</p> <h3 id="building-blocks-the-artificial-neuron-or-perceptron">Building Blocks: The Artificial Neuron (or Perceptron)</h3> <p>So, how do we translate this biological marvel into something a computer can understand? We build an <em>artificial neuron</em>, often called a <strong>perceptron</strong>, which is a mathematical model of its biological counterpart.</p> <p>Imagine our artificial neuron as a tiny decision-making unit. It takes several inputs, processes them, and then spits out an output.</p> <ol> <li> <strong>Inputs ($x_i$):</strong> These are pieces of information, like pixels in an image, words in a sentence, or sensor readings.</li> <li> <strong>Weights ($w_i$):</strong> Each input is multiplied by a ‘weight’. Think of a weight as a measure of importance. A larger weight means that input has a greater influence on the neuron’s decision. Initially, these weights are random, but they are crucial for learning.</li> <li> <strong>Bias ($b$):</strong> This is an additional value added to the weighted sum. It helps the neuron activate even if all inputs are zero, or conversely, makes it harder to activate. Think of it as a neuron’s inherent “activation threshold.”</li> </ol> <p>These weighted inputs and the bias are summed together to get a value, let’s call it $z$:</p> <p>$ z = \sum_{i=1}^{n} w_i x_i + b $</p> <p>Or, more explicitly for a few inputs:</p> <p>$ z = w_1 x_1 + w_2 x_2 + \dots + w_n x_n + b $</p> <ol> <li> <p><strong>Activation Function ($f$):</strong> This is the final step where the neuron decides whether to “fire” (or activate) based on the value of $z$. The activation function introduces non-linearity, which is vital for neural networks to learn complex patterns. Without it, stacking multiple layers would just be like having one single layer.</p> <p>Common activation functions include:</p> <ul> <li> <strong>Sigmoid:</strong> Squashes values between 0 and 1, useful for binary classification. $f(z) = \frac{1}{1 + e^{-z}}$</li> <li> <strong>ReLU (Rectified Linear Unit):</strong> Outputs the input directly if it’s positive, otherwise it outputs zero. It’s simple and very popular: $f(z) = \max(0, z)$</li> </ul> <p>So, the final output of our single artificial neuron is:</p> <p>$ a = f(z) $</p> <p>This output $a$ then becomes an input to other neurons, or it might be the final prediction.</p> </li> </ol> <h3 id="from-one-neuron-to-a-network-layers-of-thought">From One Neuron to a Network: Layers of Thought</h3> <p>A single perceptron is quite limited. Its true power emerges when we connect many of them together into a <strong>network</strong>!</p> <p>Imagine linking hundreds, thousands, even millions of these simple decision-makers together. This forms the structure of a Neural Network:</p> <ol> <li> <strong>Input Layer:</strong> This isn’t a layer of neurons doing processing; it’s simply where our raw data ($x_1, x_2, \dots, x_n$) enters the network.</li> <li> <strong>Hidden Layers:</strong> These are the unsung heroes of a neural network. Between the input and output layers, there can be one or many “hidden” layers of artificial neurons. Each neuron in a hidden layer takes inputs from the previous layer, processes them, and passes its output to the next layer. This is where the network learns increasingly complex representations of the input data. The more hidden layers a network has, the “deeper” it is – hence the term “Deep Learning.”</li> <li> <strong>Output Layer:</strong> This is the final layer of neurons that produces the network’s prediction or decision. For example, if we’re classifying images of cats and dogs, the output layer might have two neurons, one for “cat” and one for “dog.”</li> </ol> <p>Information flows sequentially through the network, from the input layer, through the hidden layers, and finally to the output layer. This process is called <strong>forward propagation</strong>. It’s like data flowing through an assembly line, with each neuron performing a small, specific task before passing the partially processed information to the next.</p> <h3 id="the-learning-part-how-neural-networks-get-smart">The “Learning” Part: How Neural Networks Get Smart</h3> <p>A randomly initialized neural network is like a baby’s brain: it has the structure, but no knowledge. The magic happens during the <strong>training</strong> phase, where the network learns by adjusting its weights and biases.</p> <p>Let’s say we’re training a network to recognize handwritten digits. We show it an image of a ‘7’. Through forward propagation, it makes a guess – maybe it says ‘3’. Clearly, it’s wrong. How does it learn from this mistake?</p> <ol> <li> <p><strong>The Loss Function (Cost Function): Quantifying “Wrongness”</strong> First, we need a way to measure how “wrong” the network’s prediction is. This is where the <strong>loss function</strong> comes in. It calculates the difference between the network’s prediction ($\hat{y}$) and the actual correct answer ($y$). A common loss function for regression problems (predicting a number) is the Mean Squared Error (MSE):</p> <p>$ L = \frac{1}{2m} \sum_{j=1}^{m} (\hat{y}^{(j)} - y^{(j)})^2 $</p> <p>Here, $m$ is the number of examples, $\hat{y}^{(j)}$ is the network’s prediction for example $j$, and $y^{(j)}$ is the true value. Our goal is to make this $L$ as small as possible.</p> </li> <li> <p><strong>Optimization: Finding the Best Path Downhill (Gradient Descent)</strong> Minimizing the loss function means finding the optimal set of weights and biases that yield the most accurate predictions. This is an optimization problem, and a powerful technique for solving it is called <strong>Gradient Descent</strong>.</p> <p>Imagine yourself blindfolded on a mountain, trying to find the lowest point (the minimum loss). You can’t see the whole landscape, but you can feel the slope directly beneath your feet. To go down, you’d take a small step in the direction of the steepest descent. This is precisely what gradient descent does!</p> <ul> <li>The “slope” in our analogy is the <strong>gradient</strong> of the loss function with respect to each weight and bias. It tells us how much the loss changes if we slightly adjust a particular weight or bias.</li> <li>We repeatedly adjust the weights and biases by taking small steps in the opposite direction of the gradient (because we want to <em>decrease</em> the loss).</li> </ul> <p>For each weight $w$ and bias $b$ in the network, we update them using this rule:</p> <p>$ w = w - \alpha \frac{\partial L}{\partial w} $ $ b = b - \alpha \frac{\partial L}{\partial b} $</p> <p>Here, $\frac{\partial L}{\partial w}$ and $\frac{\partial L}{\partial b}$ are the partial derivatives of the loss function with respect to $w$ and $b$ respectively, representing the gradient.</p> </li> <li> <p><strong>The Magic Sauce: Backpropagation</strong> Okay, so we know we need to adjust weights and biases based on the gradient of the loss. But how do we calculate these gradients for <em>every single weight and bias</em> across potentially many layers? This is where <strong>Backpropagation</strong> comes in, a truly ingenious algorithm.</p> <p>Think back to our assembly line analogy. If the final product is faulty, how do you know which worker (neuron) contributed how much to the error? Backpropagation provides a systematic way to distribute the blame (or credit) for the error across all the neurons and their connections in the network.</p> <p>It works by calculating the error at the output layer and then “propagating” this error <em>backward</em> through the network, layer by layer. Using the chain rule from calculus, it efficiently determines how much each weight and bias contributed to the final error, allowing us to compute all the necessary gradients. These gradients are then used by gradient descent to update the parameters.</p> <p>This forward pass (prediction) and backward pass (error correction) cycle is repeated many times over thousands or millions of training examples. With each iteration, the network’s weights and biases are slightly refined, causing it to make better and better predictions.</p> </li> <li> <p><strong>Learning Rate ($\alpha$): How Big a Step?</strong> The $\alpha$ in our gradient descent update rule is the <strong>learning rate</strong>. It’s a crucial hyperparameter that determines the size of the steps we take down the loss mountain.</p> <ul> <li>A <strong>large learning rate</strong> might cause us to overshoot the minimum, bouncing around or even diverging.</li> <li>A <strong>small learning rate</strong> will make the learning process very slow, potentially getting stuck in local minima.</li> </ul> <p>Finding the right learning rate is often a delicate balancing act!</p> </li> </ol> <h3 id="why-are-neural-networks-so-powerful">Why Are Neural Networks So Powerful?</h3> <p>After understanding how they learn, it becomes clearer why Neural Networks have taken the world by storm:</p> <ul> <li> <strong>Automatic Feature Learning:</strong> Unlike traditional machine learning algorithms where you often have to manually design “features” (e.g., edges, textures for images), neural networks learn these features <em>automatically</em> from the raw data. The hidden layers essentially learn to extract increasingly abstract and meaningful representations of the input.</li> <li> <strong>Universal Approximators:</strong> Theoretically, a neural network with at least one hidden layer can approximate any continuous function. This means they can learn incredibly complex, non-linear relationships in data that other algorithms might struggle with.</li> <li> <strong>Scalability with Data:</strong> While traditional algorithms often plateau in performance after a certain amount of data, deep neural networks tend to perform better with more data, making them ideal for the big data era.</li> </ul> <h3 id="real-world-applications-a-glimpse">Real-World Applications (A Glimpse)</h3> <p>Neural networks are not just theoretical constructs; they are the engines behind much of the AI we interact with daily:</p> <ul> <li> <strong>Image Recognition:</strong> From identifying objects in photos to powering facial recognition in your smartphone (Convolutional Neural Networks or CNNs).</li> <li> <strong>Natural Language Processing (NLP):</strong> Understanding speech, translating languages, powering chatbots, and generating text (Recurrent Neural Networks or RNNs, and more recently, Transformers).</li> <li> <strong>Recommendation Systems:</strong> Suggesting movies on Netflix, products on Amazon, or music on Spotify.</li> <li> <strong>Autonomous Driving:</strong> Helping vehicles perceive their surroundings and make navigation decisions.</li> <li> <strong>Medical Diagnosis:</strong> Assisting doctors in detecting diseases from medical images.</li> </ul> <h3 id="challenges-and-the-road-ahead">Challenges and the Road Ahead</h3> <p>While incredibly powerful, neural networks aren’t a silver bullet. They come with their own set of challenges:</p> <ul> <li> <strong>Computational Cost:</strong> Training large neural networks requires significant computational resources and time.</li> <li> <strong>Data Hunger:</strong> They often need vast amounts of labeled data to perform well.</li> <li> <strong>Explainability (The Black Box):</strong> It can be difficult to understand <em>why</em> a neural network makes a particular decision. Their internal workings are often opaque, making them “black boxes.”</li> <li> <strong>Ethical Considerations:</strong> As AI becomes more integrated into our lives, questions of bias in data, fairness, and accountability become paramount.</li> </ul> <p>The field of neural networks is constantly evolving. Researchers are developing new architectures (like Transformers, GANs, etc.), more efficient training techniques, and methods to address explainability and ethical concerns.</p> <h3 id="conclusion-our-journey-continues">Conclusion: Our Journey Continues</h3> <p>From the humble inspiration of a biological neuron to the complex, layered architectures driving today’s AI, neural networks represent a profound leap in our ability to build intelligent machines. My own journey into understanding them was filled with moments of “aha!” and deep appreciation for the ingenuity involved.</p> <p>They are not magic, but rather elegant mathematical systems designed to learn from data, identify patterns, and make predictions. As we continue to explore the frontiers of AI, understanding these fundamental building blocks will be key. So, keep asking questions, keep experimenting, and maybe, just maybe, you’ll be the one to unlock the next big breakthrough!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>