<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Unpacking the High-Dimensional Universe: Why Less is More in Data Science | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/2026/unpacking-the-high-dimensional-universe-why-less-i/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Unpacking the High-Dimensional Universe: Why Less is More in Data Science</h1> <p class="post-meta"> Created on January 03, 2026 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2026"> <i class="fa-solid fa-calendar fa-sm"></i> 2026 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/dimensionality-reduction"> <i class="fa-solid fa-hashtag fa-sm"></i> Dimensionality Reduction</a>   <a href="/blog/blog/tag/pca"> <i class="fa-solid fa-hashtag fa-sm"></i> PCA</a>   <a href="/blog/blog/tag/t-sne"> <i class="fa-solid fa-hashtag fa-sm"></i> t-SNE</a>   <a href="/blog/blog/tag/umap"> <i class="fa-solid fa-hashtag fa-sm"></i> UMAP</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey there, fellow data explorer!</p> <p>Have you ever looked at a massive spreadsheet with hundreds, or even thousands, of columns and felt a shiver run down your spine? Or maybe tried to build a machine learning model on a dataset so vast it choked your computer? If so, you’ve stared into the abyss of high-dimensional data, a place where intuition often fails, and computational costs soar.</p> <p>It’s a feeling I know well. Early in my data science journey, I was tackling a dataset with over 1000 features describing customer behavior. My models were slow, prone to overfitting, and honestly, I couldn’t make heads or tails of what was truly driving the patterns. It felt like trying to understand a complex machine by looking at every single screw, cog, and wire simultaneously – impossible!</p> <p>That’s when I discovered <strong>Dimensionality Reduction</strong>. It’s not just a fancy term; it’s a superpower for data scientists, a way to distill the essence of complex data into a more manageable, yet still informative, form. Think of it as finding the perfect summary for a really long book, or packing just the essentials for a trip around the world.</p> <h3 id="the-problem-when-too-much-information-is-too-much">The Problem: When Too Much Information is… Too Much</h3> <p>Before we dive into the solutions, let’s chat about <em>why</em> this is a problem. It’s often called the “Curse of Dimensionality.” Imagine trying to evenly scatter 100 points along a line (1D). They’ll be quite dense. Now, try to scatter those same 100 points over a square (2D). They’ll be much sparser. In a cube (3D), even sparser.</p> <p>As the number of dimensions (features) increases, the volume of the space grows exponentially. Our data points become incredibly sparse, making everything harder:</p> <ul> <li> <strong>Visualization</strong>: Good luck plotting data beyond 3 dimensions! Our human brains are simply not wired for it.</li> <li> <strong>Computational Cost</strong>: More features mean more memory, slower training times for models, and often more complex algorithms needed to cope.</li> <li> <strong>Overfitting</strong>: With sparse data in high dimensions, models can find spurious patterns that don’t generalize well to new data. They “memorize” the noise.</li> <li> <strong>Interpretability</strong>: Trying to understand the interplay of hundreds of features is a Herculean task.</li> </ul> <p>So, the goal of dimensionality reduction is simple yet profound: <strong>transform our data from a high-dimensional space into a lower-dimensional space while retaining as much meaningful information as possible.</strong> It’s about revealing the hidden, simpler structure that often underlies complex data.</p> <p>There are two main flavors:</p> <ol> <li> <strong>Feature Selection</strong>: You pick a <em>subset</em> of your original features that are most relevant. (Think deleting redundant columns).</li> <li> <strong>Feature Extraction</strong>: You <em>create new, combined features</em> from the originals, often fewer than the original count. (Think merging related columns into a new, more informative one).</li> </ol> <p>Today, we’re focusing on the magic of feature extraction, which often yields more powerful results.</p> <h3 id="our-toolkit-the-superheroes-of-dimensionality-reduction">Our Toolkit: The Superheroes of Dimensionality Reduction</h3> <p>Let’s meet some of the most popular techniques in our arsenal.</p> <h4 id="1-principal-component-analysis-pca-the-data-rotator">1. Principal Component Analysis (PCA): The Data Rotator</h4> <p>PCA is arguably the most famous and widely used dimensionality reduction technique. It’s a linear method, meaning it looks for straight-line relationships in your data.</p> <p><strong>The Idea</strong>: Imagine you have a cloud of data points in a 3D space. If these points mostly lie along a flat plane within that 3D space, you could describe their positions using just two dimensions instead of three, without losing much information. PCA finds these “best fitting” planes or lines.</p> <p><strong>Analogy</strong>: Think of PCA like taking a complex 3D object (your high-dimensional data) and trying to cast a shadow (your lower-dimensional representation) that best captures its overall shape and spread. You want the shadow that shows the most “information” or variance.</p> <p><strong>How it works (The Gist)</strong>: PCA identifies new axes, called <strong>Principal Components (PCs)</strong>.</p> <ul> <li>The first principal component ($PC_1$) captures the direction in your data where there’s the most variance (the most spread out the data is).</li> <li>The second principal component ($PC_2$) is orthogonal (at a right angle) to $PC_1$ and captures the next most variance, and so on.</li> </ul> <p>Each principal component is a linear combination of the original features. This transformation effectively rotates your data so that the new axes align with the directions of greatest variance.</p> <p><strong>A Peek at the Math</strong>: The core of PCA involves finding the eigenvectors and eigenvalues of your data’s covariance matrix. The covariance matrix, $\Sigma$, describes how much each feature varies with every other feature. \(\Sigma = \frac{1}{n-1}(X - \bar{X})^T(X - \bar{X})\) where $X$ is your data matrix and $\bar{X}$ is the mean-centered data. We then find the eigenvectors $v_i$ and eigenvalues $\lambda_i$ such that: \(\Sigma v_i = \lambda_i v_i\) The eigenvectors correspond to the principal components (the directions of variance), and their corresponding eigenvalues tell us how much variance each principal component captures. We then select the top $k$ principal components (those with the largest eigenvalues) to form our new, lower-dimensional space.</p> <p><strong>Use Cases</strong>:</p> <ul> <li> <strong>Noise reduction</strong>: Dimensions with low variance are often noise.</li> <li> <strong>Data compression</strong>: Representing data with fewer features.</li> <li> <strong>Initial data exploration</strong>: To get a high-level view of patterns.</li> </ul> <p><strong>Limitations</strong>: PCA is a linear technique. If your data has a complex, non-linear structure (like points forming a spiral), PCA might not capture it well.</p> <h4 id="2-t-distributed-stochastic-neighbor-embedding-t-sne-the-cluster-whisperer">2. t-Distributed Stochastic Neighbor Embedding (t-SNE): The Cluster Whisperer</h4> <p>When I first encountered t-SNE, it felt like magic. My messy, high-dimensional data instantly transformed into beautiful, discernible clusters on a 2D plot.</p> <p><strong>The Idea</strong>: Unlike PCA, t-SNE (and its cousin UMAP) focuses on preserving <em>local</em> relationships. It tries to ensure that data points that were close together in the high-dimensional space remain close in the lower-dimensional space, and points that were far apart remain far apart. It’s fantastic for visualization.</p> <p><strong>Analogy</strong>: Imagine you have a city with many neighborhoods and landmarks. PCA is like shrinking a satellite photo of the entire city. t-SNE is like creating a subway map: it prioritizes the connections and relative distances between important stations (data points), making it easy to see which neighborhoods are close, even if the absolute geographical distances are distorted.</p> <p><strong>How it works (The Gist)</strong>: t-SNE works by converting the high-dimensional Euclidean distances between data points into conditional probabilities that represent similarities.</p> <ul> <li> <table> <tbody> <tr> <td>It models a Gaussian distribution around each data point in the high-dimensional space to define probabilities $P_{j</td> <td>i}$ (the probability that point $j$ is a neighbor of point $i$).</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>It then creates a similar set of probabilities $Q_{j</td> <td>i}$ in the low-dimensional space, but using a t-distribution (which allows for more accurate representation of distances in low dimensions).</td> </tr> </tbody> </table> </li> <li>Finally, it tries to minimize the difference between these two probability distributions using an optimization algorithm (gradient descent), specifically the Kullback-Leibler (KL) divergence.</li> </ul> <p><strong>A Peek at the Math</strong>: The probability that point $x_j$ is a neighbor of $x_i$ in high-dimensional space is given by: \(P_{j|i} = \frac{\exp(-\|x_i - x_j\|^2 / 2\sigma_i^2)}{\sum_{k \neq i} \exp(-\|x_i - x_k\|^2 / 2\sigma_i^2)}\) where $\sigma_i$ is the bandwidth of the Gaussian kernel, chosen based on a hyperparameter called “perplexity.”</p> <p>Similarly, in the low-dimensional space, we use a t-distribution: \(Q_{j|i} = \frac{(1 + \|y_i - y_j\|^2)^{-1}}{\sum_{k \neq i} (1 + \|y_i - y_k\|^2)^{-1}}\) The algorithm then minimizes the sum of KL divergences over all data points: \(\text{Cost} = \sum_i KL(P_i || Q_i) = \sum_i \sum_j P_{j|i} \log \frac{P_{j|i}}{Q_{j|i}}\) This effectively pushes points with high $P_{j|i}$ to have high $Q_{j|i}$ and vice-versa.</p> <p><strong>Use Cases</strong>:</p> <ul> <li> <strong>Visualization of high-dimensional clusters</strong>: Think of image features, document embeddings, genetic data.</li> <li> <strong>Exploratory data analysis</strong>: Discovering hidden structures in complex datasets.</li> </ul> <p><strong>Limitations</strong>: Computationally intensive for very large datasets, non-deterministic (different runs can yield slightly different plots), and sensitive to its ‘perplexity’ hyperparameter. It’s primarily for visualization, not typically for generating features to train other models directly.</p> <h4 id="3-umap-uniform-manifold-approximation-and-projection-the-fast--global-cartographer">3. UMAP (Uniform Manifold Approximation and Projection): The Fast &amp; Global Cartographer</h4> <p>UMAP is a newer kid on the block, often seen as an evolution of t-SNE. It tends to be significantly faster than t-SNE and often does a better job of preserving both local and global data structures.</p> <p><strong>The Idea</strong>: UMAP operates on the principle of manifold learning. It assumes that your high-dimensional data actually lies on a lower-dimensional “manifold” (like a crumpled piece of paper existing in 3D space, but fundamentally 2D). UMAP tries to “unroll” this manifold and project it into a lower dimension.</p> <p><strong>Analogy</strong>: Imagine you have a tangled ball of yarn (your high-dimensional data). UMAP tries to untangle it and lay it flat (lower-dimensional projection) in a way that preserves how the threads were originally connected.</p> <p><strong>How it works (The Gist)</strong>: UMAP constructs a “fuzzy simplicial complex” (don’t worry too much about the term!) in the high-dimensional space. This complex is essentially a graph where nodes are data points, and edges represent relationships between neighbors, with associated probabilities (fuzziness) of connection. It then tries to find a low-dimensional embedding that has the “most similar fuzzy topological structure” to the high-dimensional one. This is achieved by minimizing the cross-entropy between the high-dimensional and low-dimensional graph representations.</p> <p><strong>A Peek at the Math</strong>: UMAP aims to minimize the cross-entropy between the high-dimensional probability graph $P$ and the low-dimensional probability graph $Q$: \(C(P, Q) = \sum_i \sum_j [P_{ij} \log(\frac{P_{ij}}{Q_{ij}}) + (1 - P_{ij}) \log(\frac{1 - P_{ij}}{1 - Q_{ij}})]\) where $P_{ij}$ and $Q_{ij}$ are the edge weights (probabilities of connection) in the high and low-dimensional graphs, respectively.</p> <p><strong>Use Cases</strong>:</p> <ul> <li> <strong>General-purpose visualization</strong>: Often preferred over t-SNE for larger datasets due to speed and better global structure preservation.</li> <li> <strong>Exploratory data analysis</strong>: Quickly identifying clusters and structures.</li> <li> <strong>Feature extraction for some downstream tasks</strong>: While primarily for visualization, its ability to preserve global structure can sometimes make its embeddings useful as features.</li> </ul> <h3 id="choosing-your-weapon-which-technique-to-use">Choosing Your Weapon: Which Technique to Use?</h3> <p>This is where the “art” comes into data science!</p> <ul> <li> <strong>PCA</strong>: <ul> <li> <strong>When</strong>: You need a fast, linear transformation, want to reduce noise, or need input features for another model. You care about maximizing variance. Your data is roughly linearly separable.</li> <li> <strong>Consider</strong>: Interpretability can be tricky as PCs are combinations of original features.</li> </ul> </li> <li> <strong>t-SNE/UMAP</strong>: <ul> <li> <strong>When</strong>: Your primary goal is to <em>visualize</em> high-dimensional data, understand clusters, and reveal non-linear structures.</li> <li> <strong>Consider</strong>: Not ideal for feature extraction for predictive models. UMAP is generally faster and better at preserving global structure than t-SNE, especially for large datasets. Both require careful tuning of hyperparameters.</li> </ul> </li> </ul> <h3 id="practical-tips-before-you-reduce">Practical Tips Before You Reduce</h3> <ol> <li> <strong>Scale Your Features!</strong> Most dimensionality reduction techniques (especially PCA) are sensitive to the scale of your features. Always normalize or standardize your data before applying these methods. <ul> <li>Standardization: $x’ = (x - \mu) / \sigma$</li> <li>Normalization: $x’ = (x - x_{min}) / (x_{max} - x_{min})$</li> </ul> </li> <li> <p><strong>It’s a Trade-off</strong>: You will <em>always</em> lose some information when reducing dimensions. The goal is to lose the <em>least amount of important</em> information.</p> </li> <li> <p><strong>Interpret with Care</strong>: The reduced dimensions (e.g., principal components, t-SNE/UMAP embeddings) are often abstract and don’t directly correspond to original features. It’s hard to say “this component means X.”</p> </li> <li> <strong>Experiment</strong>: Try different techniques and hyperparameters. The best approach often depends on your specific dataset and goals.</li> </ol> <h3 id="wrapping-up-the-power-of-simplicity">Wrapping Up: The Power of Simplicity</h3> <p>Dimensionality reduction, for me, has been a game-changer. It transformed frustratingly complex datasets into manageable, insightful stories. It’s not just about making data smaller; it’s about making it smarter, more interpretable, and ultimately, more useful.</p> <p>So, next time you face a dataset that feels like a tangled ball of yarn, remember these techniques. Go forth, experiment, and unveil the elegant simplicity hidden within your high-dimensional universe! Happy reducing!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>