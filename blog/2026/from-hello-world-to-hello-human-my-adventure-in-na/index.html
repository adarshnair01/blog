<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> From 'Hello World' to 'Hello Human': My Adventure in Natural Language Processing | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2026/from-hello-world-to-hello-human-my-adventure-in-na/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">From 'Hello World' to 'Hello Human': My Adventure in Natural Language Processing</h1> <p class="post-meta"> Created on February 07, 2026 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2026"> <i class="fa-solid fa-calendar fa-sm"></i> 2026 </a>   ·   <a href="/blog/blog/tag/natural-language-processing"> <i class="fa-solid fa-hashtag fa-sm"></i> Natural Language Processing</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>My fascination with computers started with simple <code class="language-plaintext highlighter-rouge">print("Hello, World!")</code> statements. It was a clear, logical world of commands and outputs. But then, I stumbled upon a different kind of “hello”: the one where my phone actually <em>understood</em> what I said, or where a machine could translate an ancient text. That was a game-changer. How could these rigid, binary machines possibly grasp the nuances, the poetry, the sheer complexity of human language?</p> <p>This question led me down a rabbit hole, and I emerged with a profound appreciation for <strong>Natural Language Processing (NLP)</strong>. It’s not just a branch of Artificial Intelligence; it’s a bridge between the meticulously structured world of computers and the wonderfully chaotic, rich tapestry of human communication. For anyone dipping their toes into data science or machine learning, NLP isn’t just a powerful tool – it’s an entire universe waiting to be explored.</p> <h3 id="the-great-divide-why-language-is-hard-for-computers">The Great Divide: Why Language is Hard for Computers</h3> <p>Imagine explaining the concept of “sarcasm” to a robot. Or the difference between “I saw a bat flying” and “I grabbed a bat for baseball.” Humans pick up on context, tone, and shared knowledge almost instinctively. Computers, however, see text as just a sequence of characters. To bridge this gap, NLP engineers have developed ingenious methods to convert this messy human input into something a machine can <em>compute</em>.</p> <p>Let’s embark on a journey through how we teach computers to “understand” us, starting from the very basics.</p> <h3 id="phase-1-cleaning-up-the-mess--text-preprocessing">Phase 1: Cleaning Up the Mess – Text Preprocessing</h3> <p>Before we can ask a computer to understand something, we need to make sure the input is clean and standardized. Think of it like preparing ingredients before cooking a gourmet meal; you wouldn’t just throw raw vegetables into a pot!</p> <ol> <li> <strong>Tokenization:</strong> The first step is to break down continuous text into smaller, meaningful units called “tokens.” These are usually words, but can also be sentences, sub-word units, or even characters. <ul> <li>Example: “Hello, world!” $\to$ [“Hello”, “,”, “world”, “!”]</li> </ul> </li> <li> <strong>Lowercasing:</strong> To treat “Apple” and “apple” as the same word, we convert everything to lowercase. This reduces the vocabulary size and simplifies comparisons. <ul> <li>Example: “The Apple is red.” $\to$ “the apple is red.”</li> </ul> </li> <li> <strong>Removing Punctuation and Special Characters:</strong> Punctuation usually doesn’t carry significant semantic meaning in many NLP tasks and can be removed. <ul> <li>Example: “Hello, world!” $\to$ “hello world”</li> </ul> </li> <li> <strong>Stop Word Removal:</strong> Words like “the,” “a,” “is,” “and” appear frequently but often don’t add much unique information to the overall meaning of a sentence. Removing them helps focus on more significant terms. <ul> <li>Example: “The quick brown fox jumps over the lazy dog.” $\to$ “quick brown fox jumps lazy dog.”</li> </ul> </li> <li> <strong>Stemming and Lemmatization:</strong> These techniques aim to reduce words to their base or root form. <ul> <li> <strong>Stemming</strong> is a crude heuristic process that chops off suffixes from words, often resulting in “stems” that aren’t actual words. It’s faster but less accurate. <ul> <li>Example: “running”, “runs”, “ran” $\to$ “run”</li> <li>Example: “abilities” $\to$ “abil” (not a real word)</li> </ul> </li> <li> <strong>Lemmatization</strong> is a more sophisticated process that uses vocabulary and morphological analysis (knowledge of word structures) to return the base or dictionary form of a word, known as the “lemma.” It’s slower but more accurate. <ul> <li>Example: “running”, “runs”, “ran” $\to$ “run”</li> <li>Example: “better” $\to$ “good” (its lemma)</li> </ul> </li> </ul> </li> </ol> <p>These preprocessing steps are crucial; they lay the groundwork for transforming raw text into a format suitable for machine learning models.</p> <h3 id="phase-2-the-language-of-numbers--representing-words">Phase 2: The Language of Numbers – Representing Words</h3> <p>Computers are excellent with numbers, not words. So, how do we convert “hello” into something a computer can crunch? This is where word representation techniques come into play.</p> <ol> <li> <p><strong>One-Hot Encoding:</strong> The simplest way to represent words numerically is one-hot encoding. Imagine you have a vocabulary of $N$ unique words. Each word is represented by a vector of $N$ dimensions, where a ‘1’ is placed at the index corresponding to that word, and ‘0’s elsewhere.</p> <ul> <li>Vocabulary: {“cat”, “dog”, “mouse”}</li> <li>“cat” $\to$ $[1, 0, 0]$</li> <li>“dog” $\to$ $[0, 1, 0]$</li> <li>“mouse” $\to$ $[0, 0, 1]$</li> </ul> <p>While straightforward, one-hot encoding has major drawbacks:</p> <ul> <li> <strong>High Dimensionality:</strong> For large vocabularies (e.g., 50,000 words), each vector is 50,000 dimensions long, most of which are zeros (sparse).</li> <li> <strong>Lack of Semantic Relationship:</strong> Every word is equidistant from every other word. It tells us nothing about “cat” and “dog” being related animals, or “king” and “queen” being related roles.</li> </ul> </li> <li> <p><strong>TF-IDF (Term Frequency-Inverse Document Frequency):</strong> To capture some notion of importance within a document, TF-IDF comes in handy. It’s a numerical statistic that reflects how important a word is to a document in a collection or corpus.</p> <ul> <li> <strong>Term Frequency (TF):</strong> How often a term $t$ appears in a document $d$. $ \text{TF}(t,d) = \frac{\text{Number of times term } t \text{ appears in document } d}{\text{Total number of terms in document } d} $</li> <li> <strong>Inverse Document Frequency (IDF):</strong> This measures how common or rare a term is across all documents in the corpus $D$. Rare words are often more informative. $ \text{IDF}(t,D) = \log \left( \frac{\text{Total number of documents in corpus } D}{\text{Number of documents with term } t \text{ (plus 1 to avoid division by zero)}} \right) $</li> </ul> <p>The final TF-IDF score is the product: $ \text{TF-IDF}(t,d,D) = \text{TF}(t,d) \times \text{IDF}(t,D) $</p> <p>A high TF-IDF score means the word is frequent in <em>this specific document</em> but rare <em>across all documents</em>, making it a good indicator of the document’s content. It’s better than one-hot, but still doesn’t capture complex semantic relationships.</p> </li> <li> <p><strong>Word Embeddings (The Game Changer):</strong> This is where things get truly exciting! Word embeddings are dense, low-dimensional vector representations of words that capture semantic meaning and relationships. Instead of sparse 50,000-dimensional vectors, we might have dense 100-dimensional vectors.</p> <p>The core idea is that words that appear in similar contexts tend to have similar meanings. Algorithms like Word2Vec (and later GloVe, FastText) learn these embeddings by trying to predict a word from its neighbors, or vice versa.</p> <p>The magic here is that these vectors capture meaning! We can perform arithmetic with them: $ \text{vector(“king”)} - \text{vector(“man”)} + \text{vector(“woman”)} \approx \text{vector(“queen”)} $</p> <p>This isn’t just a party trick; it means words with similar meanings are located close to each other in this multi-dimensional “embedding space.” It allows computers to grasp analogies, synonyms, and even antonyms to a degree never before possible.</p> </li> </ol> <h3 id="phase-3-the-brains--understanding-context-and-sequence-with-deep-learning">Phase 3: The Brains – Understanding Context and Sequence with Deep Learning</h3> <p>While word embeddings give us rich representations of individual words, language is more than just a bag of words. The order matters. “Man bites dog” is very different from “Dog bites man.” To capture these sequential dependencies and long-range context, we turn to deep neural networks.</p> <ol> <li> <p><strong>Recurrent Neural Networks (RNNs):</strong> RNNs were among the first neural networks designed specifically for sequential data. They have a “memory” in the form of a hidden state that is updated at each step, taking into account the current input and the previous hidden state.</p> <p>This allows them to process sequences like sentences, where the understanding of the current word depends on the words that came before it. However, standard RNNs struggled with <strong>long-term dependencies</strong> – they tended to forget information from the far past (the <strong>vanishing gradient problem</strong>).</p> </li> <li> <p><strong>LSTMs and GRUs:</strong> To combat the vanishing gradient problem, more sophisticated RNN architectures like <strong>Long Short-Term Memory (LSTM)</strong> networks and <strong>Gated Recurrent Units (GRUs)</strong> were introduced. These models use “gates” (input, forget, output gates in LSTMs) that regulate the flow of information, allowing the network to selectively remember or forget past information. This was a massive leap forward for tasks like machine translation and speech recognition.</p> </li> <li> <p><strong>Transformers (The Current Kings):</strong> While LSTMs and GRUs were powerful, they still processed sequences word-by-word, which was slow and made it hard to capture very long-range dependencies efficiently. Enter the <strong>Transformer architecture</strong>, introduced in the 2017 paper “Attention Is All You Need.”</p> <p>The key innovation of Transformers is the <strong>attention mechanism</strong>. Instead of processing words sequentially, Transformers can process all words in a sentence <em>in parallel</em>. The attention mechanism allows each word to “pay attention” to other relevant words in the sentence, regardless of their position. For example, when processing the pronoun “it,” the model can directly attend to the noun it refers to, even if they are far apart.</p> <p>The Transformer architecture, particularly its <strong>self-attention</strong> component, has revolutionized NLP. It forms the backbone of modern large language models (LLMs) like <strong>BERT</strong> (Bidirectional Encoder Representations from Transformers) and <strong>GPT</strong> (Generative Pre-trained Transformer), which have pushed the boundaries of what machines can do with language. These models, pre-trained on vast amounts of text data, can then be fine-tuned for a multitude of specific tasks.</p> </li> </ol> <h3 id="nlp-in-action-a-world-of-possibilities">NLP in Action: A World of Possibilities</h3> <p>The techniques we’ve discussed power countless applications we interact with daily:</p> <ul> <li> <strong>Machine Translation:</strong> Google Translate, DeepL.</li> <li> <strong>Sentiment Analysis:</strong> Understanding the emotional tone of reviews or social media posts.</li> <li> <strong>Chatbots and Virtual Assistants:</strong> Siri, Alexa, customer service bots.</li> <li> <strong>Spam Detection:</strong> Filtering unwanted emails.</li> <li> <strong>Text Summarization:</strong> Condensing long documents into key points.</li> <li> <strong>Speech Recognition:</strong> Converting spoken language into text.</li> <li> <strong>Named Entity Recognition (NER):</strong> Identifying names of people, organizations, locations.</li> </ul> <h3 id="the-road-ahead-challenges-and-my-thoughts">The Road Ahead: Challenges and My Thoughts</h3> <p>Despite the incredible progress, NLP is far from “solved.” Human language is complex, full of ambiguity, sarcasm, irony, and cultural nuances that are still incredibly challenging for machines.</p> <ul> <li> <strong>Ambiguity:</strong> “Time flies like an arrow; fruit flies like a banana.” How do you teach a machine the difference without explicit rules?</li> <li> <strong>Bias:</strong> If our training data reflects societal biases (e.g., gender stereotypes in job descriptions), the NLP models will learn and perpetuate those biases. Addressing this is a major ethical challenge.</li> <li> <strong>True Understanding:</strong> Do these models truly “understand” language, or are they just incredibly good at pattern matching? This philosophical debate continues.</li> </ul> <p>For me, NLP isn’t just a technical field; it’s a window into the human mind, a quest to deconstruct and reconstruct one of our most defining characteristics. The journey from treating words as isolated tokens to creating models that can generate coherent, contextually relevant text has been astounding.</p> <p>If you’re reading this, whether you’re a fellow data science enthusiast or a curious high school student, I hope you feel the pull of this field. It’s a frontier where linguistics, computer science, and mathematics converge, and the possibilities are still unfolding. Dive in, experiment, and perhaps you’ll be the one to teach the next generation of machines how to truly say “hello human” in a way we’ve only dreamed of. The future of communication, intertwined with intelligence, is waiting for you to help write its next chapter.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>