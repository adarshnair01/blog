<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Beyond the Numbers: My Journey Confronting Bias in Machine Learning | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2026/beyond-the-numbers-my-journey-confronting-bias-in/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Beyond the Numbers: My Journey Confronting Bias in Machine Learning</h1> <p class="post-meta"> Created on February 06, 2026 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2026"> <i class="fa-solid fa-calendar fa-sm"></i> 2026 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/ai-ethics"> <i class="fa-solid fa-hashtag fa-sm"></i> AI Ethics</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/bias"> <i class="fa-solid fa-hashtag fa-sm"></i> Bias</a>   <a href="/blog/blog/tag/fairness"> <i class="fa-solid fa-hashtag fa-sm"></i> Fairness</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>As a budding data scientist and machine learning engineer, I’ve always been captivated by the sheer power of algorithms to find patterns, make predictions, and automate complex tasks. From recommending your next favorite song to predicting stock market trends, AI seems to touch every corner of our lives. But as I delved deeper, a crucial question began to gnaw at me: <em>Are these powerful systems always fair?</em></p> <p>My journey into the world of machine learning bias was less of a theoretical exercise and more of a sobering realization. It’s easy to get lost in the elegance of mathematical models and the efficiency of optimized code. Yet, beneath that pristine surface lies a profound challenge: <strong>bias</strong>. Not the kind of bias we might recognize in human decision-making, though it’s deeply connected, but a subtle, pervasive form that can sneak into our AI systems, often with devastating real-world consequences.</p> <h3 id="what-exactly-is-bias-in-machine-learning">What Exactly is Bias in Machine Learning?</h3> <p>When we talk about “bias” in everyday language, we often mean a predisposition or prejudice for or against one thing, person, or group compared with another, usually in a way considered to be unfair. In machine learning, the definition is similar but manifests uniquely.</p> <p>At its core, <strong>machine learning bias</strong> refers to systematic and repeatable errors in a computer system that create unfair outcomes, such as favoring one arbitrary group over another. It’s not about the model “intending” to be biased – machines don’t have intentions. Instead, it’s about the patterns and relationships it learns from the data, which often mirror the historical and societal biases present in our world.</p> <p>Think of it this way: an AI is like a highly diligent student who learns <em>everything</em> from the textbooks we give it. If those textbooks contain skewed, incomplete, or prejudiced information, the student, no matter how intelligent, will absorb and reproduce those biases in their understanding and actions. This is the essence of algorithmic bias: it’s a reflection, not an invention.</p> <p>My initial thought was, “Can’t we just feed it more data?” I quickly learned that the problem isn’t always about quantity; it’s profoundly about <em>quality</em> and <em>representativeness</em>.</p> <h3 id="the-roots-of-the-problem-where-does-bias-come-from">The Roots of the Problem: Where Does Bias Come From?</h3> <p>Understanding the origins of bias is the first step toward combating it. I’ve found that bias typically creeps in at two major stages: the data itself, and the way we design and evaluate our algorithms.</p> <h4 id="1-data-bias-the-mirror-to-our-world">1. Data Bias: The Mirror to Our World</h4> <p>This is arguably the most common and insidious source of bias. Our data, far from being a pristine, objective record, is a reflection of human history, decisions, and societal structures – which are themselves rife with biases.</p> <ul> <li> <strong>Selection Bias:</strong> This occurs when the data used to train the model is not representative of the real-world population or scenario the model will be deployed in. <ul> <li> <strong>Historical Bias:</strong> Perhaps the most pervasive. If you train a hiring algorithm on historical hiring data, where certain demographics (e.g., women or minorities) were historically underrepresented in senior roles, the algorithm will learn to associate those demographics with lower suitability for leadership. It’s simply learning the <em>status quo</em>, not necessarily the <em>fair</em> or <em>optimal</em> outcome.</li> <li> <strong>Sampling Bias:</strong> If you only collect data from a specific group (e.g., only English speakers, or only urban populations), your model will perform poorly, or unfairly, on groups not represented in the sample. Imagine a speech recognition system trained predominantly on male voices struggling with higher-pitched female voices.</li> </ul> </li> <li> <strong>Measurement Bias:</strong> This happens when there are systematic errors in how data is collected or measured. For example, if facial recognition systems are primarily developed and tested on individuals with lighter skin tones, their accuracy for individuals with darker skin tones will likely be lower due to insufficient or poorly lit training images for that demographic. The “measurement” (facial feature extraction) itself becomes biased.</li> <li> <strong>Reporting Bias:</strong> Certain outcomes or behaviors might be over- or under-reported in the data. Think of social media data, where some groups might be more vocal or present than others, skewing the perception of public opinion.</li> <li> <strong>Pre-existing Bias / Label Bias:</strong> This refers to human biases that are directly embedded into the labels or annotations of the dataset. If human annotators, for example, label certain non-English names as “spam” more frequently due to implicit bias, the model will learn to discriminate against those names.</li> </ul> <h4 id="2-algorithmic-bias-our-design-choices">2. Algorithmic Bias: Our Design Choices</h4> <p>Even with seemingly clean data, bias can emerge from the way we design and evaluate our machine learning models.</p> <ul> <li> <strong>Learning Bias:</strong> This arises from the specific algorithms and techniques chosen. Some models might optimize for overall accuracy, inadvertently sacrificing performance or fairness for minority groups. For instance, if a dataset has a severe class imbalance (e.g., 99% benign, 1% malignant cancer cases), a model optimizing for accuracy might simply predict “benign” for everyone, achieving 99% accuracy but failing catastrophically for the 1% who need accurate detection.</li> <li> <strong>Evaluation Bias:</strong> The metrics and datasets we use to evaluate our models are crucial. If we evaluate a model only on a subset of the population, or use metrics that don’t account for fairness across different groups, we might mistakenly believe our model is fair when it isn’t. For example, if a model’s accuracy is 90% overall, but 95% for one group and 70% for another, relying solely on overall accuracy can mask significant disparities.</li> </ul> <h3 id="real-world-consequences-when-ai-gets-it-wrong">Real-World Consequences: When AI Gets It Wrong</h3> <p>The impact of bias in ML is not just theoretical; it’s deeply tangible and often harmful, affecting people’s access to opportunities, services, and even their freedom.</p> <ul> <li> <strong>Facial Recognition Systems:</strong> Studies have repeatedly shown that many commercial facial recognition systems have significantly higher error rates for women and people of color, particularly those with darker skin tones. This can lead to wrongful arrests, security breaches, and general distrust.</li> <li> <strong>Hiring Algorithms:</strong> Amazon famously scrapped an AI recruiting tool after discovering it was biased against women. The system, trained on historical resumes, penalized applications containing the word “women’s” (as in “women’s chess club”) and downgraded graduates of all-women colleges. This wasn’t because the algorithm “hated” women, but because it learned that men were historically more successful in the company’s tech roles.</li> <li> <strong>Credit Scoring and Loan Applications:</strong> Algorithms used by banks and financial institutions, if trained on historical lending data, can inadvertently perpetuate existing socioeconomic biases, making it harder for certain demographics to access loans, even if they are creditworthy.</li> <li> <strong>Healthcare Diagnostics:</strong> An AI trained to diagnose skin conditions might perform poorly on patients with darker skin, simply because the training dataset predominantly featured lighter skin tones. This could lead to misdiagnoses or delayed treatment for specific ethnic groups.</li> <li> <strong>Criminal Justice:</strong> Predictive policing tools or recidivism risk assessment tools, if trained on historical arrest and conviction data (which itself reflects societal biases in policing), can disproportionately flag individuals from certain communities as higher risk, perpetuating a cycle of surveillance and incarceration.</li> </ul> <p>These examples vividly illustrate that AI’s promises of efficiency and objectivity can quickly turn into tools of discrimination if bias is left unchecked.</p> <h3 id="fighting-back-strategies-for-detecting-and-mitigating-bias">Fighting Back: Strategies for Detecting and Mitigating Bias</h3> <p>My exploration into bias revealed that combating it is a multi-faceted challenge, requiring diligence at every stage of the machine learning pipeline. It’s not a one-time fix but an ongoing commitment.</p> <h4 id="1-proactive-data-centric-approaches">1. Proactive Data-Centric Approaches</h4> <p>Since data is often the primary source of bias, focusing on it is paramount.</p> <ul> <li> <strong>Fairness-Aware Data Collection:</strong> The ideal scenario is to collect diverse, representative, and high-quality data from the outset. This means actively seeking out underrepresented groups and ensuring comprehensive coverage.</li> <li> <strong>Data Auditing and Analysis:</strong> Before training, rigorously examine your data for imbalances, missing values, and potential proxies for sensitive attributes (like ZIP code acting as a proxy for race or income).</li> <li> <strong>Resampling and Reweighting:</strong> If a dataset is imbalanced (e.g., fewer samples for a minority group), techniques like oversampling the minority class, undersampling the majority class, or reweighting samples during training can help.</li> <li> <strong>Feature Engineering:</strong> Remove features that are directly sensitive attributes (e.g., race, gender) and critically evaluate other features that might act as <em>proxies</em> for these attributes. For example, income or residential area might correlate strongly with race or ethnicity.</li> </ul> <h4 id="2-algorithmic-solutions-building-fairness-into-the-model">2. Algorithmic Solutions: Building Fairness into the Model</h4> <p>Beyond the data, we can design our algorithms to be more fair. This often involves incorporating fairness constraints directly into the training process.</p> <ul> <li> <strong>Fairness-Aware Regularization:</strong> We can modify the model’s loss function to not only minimize prediction errors but also penalize unfairness. This might involve adding terms that encourage similar outcomes for different demographic groups.</li> <li> <strong>Adversarial Debiasing:</strong> In this technique, a “debiasing” network tries to predict the sensitive attribute (e.g., gender) from the model’s learned data representation. The main model is then trained to make predictions while simultaneously trying to “fool” the debiasing network, effectively learning representations that are independent of the sensitive attribute.</li> <li> <strong>Group Fairness Metrics:</strong> This is where some mathematics helps us define and measure fairness. Different definitions of fairness exist, and choosing the right one depends heavily on the application and ethical considerations. <ul> <li> <strong>Demographic Parity (or Statistical Parity):</strong> This metric demands that the proportion of positive outcomes ($\hat{Y}=1$) should be roughly equal across different groups defined by a protected attribute ($S$). \(P(\hat{Y}=1 | S=s_0) \approx P(\hat{Y}=1 | S=s_1)\) Where $s_0$ and $s_1$ represent two different values of the sensitive attribute (e.g., male and female). In simple terms, it means the model should predict a positive outcome for a similar percentage of people in both groups.</li> <li> <strong>Equal Opportunity:</strong> This metric focuses on ensuring that among individuals who <em>truly</em> deserve a positive outcome ($Y=1$), the model’s ability to identify them is similar across groups. \(P(\hat{Y}=1 | S=s_0, Y=1) \approx P(\hat{Y}=1 | S=s_1, Y=1)\) This is essentially equating the true positive rate (recall) across groups. For example, if a loan approval model is equally good at approving creditworthy men as it is at approving creditworthy women.</li> <li> <strong>Equal Accuracy:</strong> This metric requires that the overall accuracy of the model ($P(\hat{Y}=Y | S=s_0)$) is similar across different groups. \(P(\hat{Y}=Y | S=s_0) \approx P(\hat{Y}=Y | S=s_1)\) The choice of metric is critical because optimizing for one type of fairness might not guarantee another, and sometimes, they can even be contradictory! This highlights the complexity of fairness.</li> </ul> </li> </ul> <h4 id="3-post-processing-techniques">3. Post-Processing Techniques</h4> <p>Even after training, we can adjust model outputs to achieve greater fairness.</p> <ul> <li> <strong>Threshold Adjustment:</strong> We can calibrate the prediction thresholds for different groups. For example, if a model outputs a probability score for loan approval, we might use a lower probability threshold for a historically disadvantaged group to achieve equal opportunity.</li> </ul> <h4 id="4-ongoing-monitoring-and-transparency">4. Ongoing Monitoring and Transparency</h4> <p>Bias isn’t a static problem; it can evolve as data distributions change or as models are updated.</p> <ul> <li> <strong>Continuous Auditing:</strong> Regularly monitor model performance and fairness metrics in real-world deployment. Set up alerts for any significant disparities emerging between groups.</li> <li> <strong>Explainable AI (XAI):</strong> Tools that help us understand <em>why</em> a model made a particular decision (e.g., LIME, SHAP) are invaluable. By understanding the features driving decisions, we can uncover hidden biases and ensure decisions are made for legitimate reasons.</li> <li> <strong>Human Oversight:</strong> For critical applications, maintaining a “human-in-the-loop” allows for expert judgment and intervention when the AI’s decision is questionable or potentially biased.</li> </ul> <h3 id="the-road-ahead-challenges-and-our-role">The Road Ahead: Challenges and Our Role</h3> <p>Confronting bias in machine learning is arguably one of the most significant ethical challenges facing AI development today. There’s no single definition of “fairness” that applies universally, and often, there are trade-offs between different fairness goals and even between fairness and overall accuracy. This means that designing fair AI systems isn’t just a technical problem; it’s a socio-technical one, requiring interdisciplinary collaboration and ethical deliberation.</p> <p>As data scientists and ML engineers, we carry immense responsibility. We are the architects of these powerful systems, and our choices at every stage – from data collection to model deployment – shape their impact on society. We must move beyond simply aiming for higher accuracy and embrace a broader definition of success that includes fairness, transparency, and accountability.</p> <h3 id="conclusion">Conclusion</h3> <p>My journey into understanding bias in machine learning has been eye-opening. It has transformed my perspective from merely optimizing algorithms to considering their profound societal implications. Bias isn’t a bug; it’s often a feature of the imperfect human world that our AI systems learn from.</p> <p>The good news is that we are not helpless. By meticulously examining our data, thoughtfully designing our models, critically evaluating their performance across diverse groups, and committing to ongoing monitoring, we can build more equitable and responsible AI systems. This isn’t just about building better algorithms; it’s about building a better, fairer future for everyone touched by the power of machine learning. Let’s commit to being the agents of change, ensuring our AI serves humanity with integrity and fairness.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>