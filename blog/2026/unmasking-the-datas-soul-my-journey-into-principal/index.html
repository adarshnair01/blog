<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Unmasking the Data's Soul: My Journey into Principal Component Analysis (PCA) | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/2026/unmasking-the-datas-soul-my-journey-into-principal/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Unmasking the Data's Soul: My Journey into Principal Component Analysis (PCA)</h1> <p class="post-meta"> Created on January 24, 2026 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2026"> <i class="fa-solid fa-calendar fa-sm"></i> 2026 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/dimensionality-reduction"> <i class="fa-solid fa-hashtag fa-sm"></i> Dimensionality Reduction</a>   <a href="/blog/blog/tag/pca"> <i class="fa-solid fa-hashtag fa-sm"></i> PCA</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/linear-algebra"> <i class="fa-solid fa-hashtag fa-sm"></i> Linear Algebra</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>My data science journey has often felt like navigating a dense, enchanted forest. Each tree represents a feature, each path a potential relationship. Sometimes, there are so many trees, so many paths, that I get lost. The sheer volume of information, the “curse of dimensionality,” can paralyze even the most seasoned explorer. That’s where Principal Component Analysis, or PCA, steps in – a beacon of clarity in the statistical wilderness.</p> <p>PCA isn’t just another algorithm; it’s a fundamental shift in perspective. It teaches us to look beyond the surface, to find the underlying structure and most impactful patterns within our data. It’s a testament to the elegance of linear algebra, making the complex beautifully simple.</p> <h3 id="why-we-need-a-data-whisperer-the-curse-of-dimensionality">Why We Need a Data Whisperer: The Curse of Dimensionality</h3> <p>Imagine you’re trying to describe a person to someone who has never met them. If you give them 10 features (height, hair color, eye color, etc.), they get a good picture. But what if you give them 10,000 features? (The exact shade of their 1,000th hair, the precise angle of their left eyebrow when they’re surprised, the chemical composition of their favorite shirt…) You’d overwhelm them! Not only would it be hard to process, but many of those features would be redundant or irrelevant. This is, in essence, the “curse of dimensionality” in data science.</p> <p>When our datasets have a vast number of features (dimensions), several problems arise:</p> <ol> <li> <strong>Computational Burden:</strong> Training models takes significantly longer, and requires more memory.</li> <li> <strong>Increased Noise:</strong> Many features might just be noise or irrelevant details that confuse our models and lead to overfitting.</li> <li> <strong>Data Sparsity:</strong> In high dimensions, data points become incredibly sparse. It’s like scattering a handful of grains of sand across an entire beach – they look far apart, even if they’re “close” in a lower-dimensional sense. This makes it hard for algorithms to find meaningful patterns.</li> <li> <strong>Visualization Challenges:</strong> We humans struggle to visualize anything beyond three dimensions. How do you plot 100 features at once? You can’t.</li> </ol> <p>This is where dimensionality reduction techniques, and specifically PCA, become invaluable. They allow us to distill the essence of our data into fewer, more manageable dimensions, making it easier to analyze, visualize, and model.</p> <h3 id="the-intuition-casting-the-most-informative-shadow">The Intuition: Casting the Most Informative Shadow</h3> <p>For me, the core intuition behind PCA clicked with a simple analogy: imagine you have a complex 3D object – say, a peculiar, elongated sculpture – and you want to photograph it from a single angle to capture its most defining characteristics. If you photograph it from directly above, you might only see a small circular base. If you photograph it from the side, you might see its full length and some of its intricate carvings.</p> <p>PCA is like finding <em>the best angle</em> to cast a shadow of that 3D object onto a 2D wall, such that the shadow reveals the <em>most information</em> or <em>spread</em> of the original object. It looks for the directions in our high-dimensional space where our data varies the most. Why variance? Because high variance means the data points are spread out along that direction, indicating that this direction captures a lot of the differences or information present in the data.</p> <p>These “best angles” are our <strong>Principal Components (PCs)</strong>. They are new axes, perpendicular to each other, that capture the maximum possible variance from the original data. The first principal component captures the most variance, the second captures the most remaining variance orthogonal to the first, and so on.</p> <h3 id="the-math-y-bit-unpacking-pcas-elegant-steps">The Math-y Bit: Unpacking PCA’s Elegant Steps</h3> <p>Don’t let the math scare you! The beauty of PCA lies in its elegant use of linear algebra to achieve this “best shadow” effect. Here’s a simplified breakdown of the steps:</p> <h4 id="step-1-standardize-the-data">Step 1: Standardize the Data</h4> <p>Before we do anything else, we need to ensure all our features are on the same playing field. Imagine one feature, “income,” ranges from $10,000 to $1,000,000, while another, “years of experience,” ranges from 0 to 50. If we don’t scale them, “income” will dominate the variance calculation just because of its larger range.</p> <p>So, we <strong>standardize</strong> each feature to have a mean of 0 and a standard deviation of 1. This is often done using Z-score standardization:</p> <p>$x_{new} = \frac{x - \mu}{\sigma}$</p> <p>Where $x$ is the original value, $\mu$ is the mean of the feature, and $\sigma$ is its standard deviation.</p> <h4 id="step-2-compute-the-covariance-matrix">Step 2: Compute the Covariance Matrix</h4> <p>Now that our data is standardized, we need to understand how the features relate to each other. This is where the <strong>covariance matrix</strong> comes in.</p> <ul> <li> <strong>Variance</strong> tells us how much a single feature varies from its mean.</li> <li> <strong>Covariance</strong> tells us how two features vary <em>together</em>. <ul> <li>A positive covariance means that as one feature increases, the other tends to increase.</li> <li>A negative covariance means that as one feature increases, the other tends to decrease.</li> <li>A covariance close to zero means there’s no strong linear relationship between them.</li> </ul> </li> </ul> <p>The covariance matrix $\Sigma$ (often denoted as $C$) is a square matrix where the element $\Sigma_{ij}$ is the covariance between the $i$-th feature and the $j$-th feature. The diagonal elements $\Sigma_{ii}$ are simply the variances of each individual feature.</p> <p>For a dataset with $p$ features, the covariance matrix will be $p \times p$. Its formula for two features $j$ and $k$ is:</p> <p>$C_{jk} = \frac{1}{n-1} \sum_{i=1}^n (x_{ij} - \bar{x}<em>j)(x</em>{ik} - \bar{x}_k)$</p> <p>Where $n$ is the number of data points, $x_{ij}$ is the $i$-th observation of the $j$-th feature, and $\bar{x}_j$ is the mean of the $j$-th feature.</p> <p>This matrix is crucial because it summarizes the relationships and variance within our entire dataset. PCA will use this information to find the directions of maximum variance.</p> <h4 id="step-3-calculate-eigenvectors-and-eigenvalues">Step 3: Calculate Eigenvectors and Eigenvalues</h4> <p>This is the heart of PCA. Once we have the covariance matrix, we calculate its <strong>eigenvectors</strong> and <strong>eigenvalues</strong>.</p> <ul> <li> <p><strong>Eigenvectors</strong>: Imagine a linear transformation (like stretching or rotating data). An eigenvector is a special kind of vector that, when transformed, only changes its <em>magnitude</em> (it gets scaled) but not its <em>direction</em>. In PCA, the eigenvectors of the covariance matrix are our <strong>principal components</strong>. They represent the new axes along which our data is most spread out. Each eigenvector is a direction in the original feature space.</p> </li> <li> <p><strong>Eigenvalues</strong>: Each eigenvector has a corresponding eigenvalue. The eigenvalue tells us the <em>magnitude</em> of variance along its corresponding eigenvector. A larger eigenvalue means that its eigenvector captures more variance, and thus, more information, from the data.</p> </li> </ul> <p>Mathematically, for a matrix $A$ (our covariance matrix), a vector $v$ (an eigenvector), and a scalar $\lambda$ (an eigenvalue), the relationship is:</p> <p>$Av = \lambda v$</p> <p>We find all $p$ eigenvectors and their corresponding eigenvalues from our $p \times p$ covariance matrix.</p> <h4 id="step-4-select-principal-components">Step 4: Select Principal Components</h4> <p>We now have $p$ eigenvectors (our potential principal components) and $p$ eigenvalues. We sort the eigenvalues in descending order. The eigenvector with the largest eigenvalue is the first principal component (PC1), capturing the most variance. The eigenvector with the second largest eigenvalue is PC2, and so on.</p> <p>The beauty is that we don’t have to keep all of them! We choose the top ‘k’ eigenvectors that correspond to the largest eigenvalues. How do we choose ‘k’? We often look at the <strong>explained variance ratio</strong>, which tells us what proportion of the total variance each principal component explains. We might decide to keep enough components to explain, say, 95% of the total variance. A <strong>scree plot</strong> (plotting eigenvalues in descending order) can also visually help us find an “elbow” where the explained variance drops off sharply, suggesting an optimal ‘k’.</p> <p>These selected ‘k’ eigenvectors form a projection matrix $P$.</p> <h4 id="step-5-project-data-onto-new-dimensions">Step 5: Project Data onto New Dimensions</h4> <p>Finally, we take our original standardized data and transform it using the selected principal components. This projects our high-dimensional data onto the new, lower-dimensional space defined by our ‘k’ principal components.</p> <p>If $X$ is our original standardized data matrix (n samples $\times$ p features), and $P$ is our $p \times k$ matrix of selected principal components (eigenvectors), then our new, reduced-dimension data matrix $Y$ (n samples $\times$ k features) is:</p> <p>$Y = X P$</p> <p>Each column in $Y$ represents a new principal component, and each row is a data point expressed in this new, simpler coordinate system. This is our “informative shadow”!</p> <h3 id="real-world-magic-pca-in-action">Real-World Magic: PCA in Action</h3> <p>PCA is a workhorse in data science, used across countless domains:</p> <ul> <li> <strong>Image Processing</strong>: Think of “Eigenfaces” in facial recognition, where PCA is used to reduce the high dimensionality of pixel data from faces, creating a compact representation that still distinguishes individuals.</li> <li> <strong>Bioinformatics</strong>: Analyzing gene expression data, where thousands of genes might be measured, PCA can identify core patterns and reduce noise.</li> <li> <strong>Finance</strong>: Reducing the number of variables in risk models or portfolio optimization, where countless market factors are at play.</li> <li> <strong>Preprocessing for Machine Learning</strong>: Before feeding data into a classification or clustering algorithm, PCA can simplify the input, often leading to faster training times and sometimes even better model performance by removing noise.</li> <li> <strong>Data Visualization</strong>: Reducing data from, say, 50 dimensions down to 2 or 3 allows us to plot it and visually identify clusters or outliers that were previously hidden.</li> </ul> <h3 id="considerations-and-limitations">Considerations and Limitations</h3> <p>While powerful, PCA isn’t a silver bullet:</p> <ul> <li> <strong>Linearity Assumption</strong>: PCA assumes that the principal components are linear combinations of the original features. If your data has complex, non-linear structures (like a “Swiss roll” shape), PCA might not capture these effectively. For such cases, techniques like Kernel PCA or t-SNE might be more appropriate.</li> <li> <strong>Interpretability</strong>: The new principal components are linear combinations of the original features. For example, PC1 might be “0.3 * income - 0.7 * age + 0.2 * education”. This can make it difficult to interpret what a specific principal component <em>means</em> in real-world terms, especially compared to original features.</li> <li> <strong>Information Loss</strong>: By reducing dimensions, we are inherently losing <em>some</em> information. The goal is to lose the <em>least important</em> information, but it’s a trade-off.</li> <li> <strong>Sensitivity to Scaling</strong>: As discussed, PCA is highly sensitive to the scaling of the features. Always standardize your data before applying PCA!</li> </ul> <h3 id="my-takeaway-an-essential-tool">My Takeaway: An Essential Tool</h3> <p>For me, PCA represents one of those fundamental, elegant tools that every data scientist should have in their arsenal. It’s not just about crunching numbers; it’s about gaining deeper insights, simplifying complexity, and making sense of the overwhelming amount of information we encounter daily. It’s a testament to how abstract mathematical concepts like eigenvectors and eigenvalues can manifest as incredibly practical solutions to real-world problems.</p> <p>Next time you face a high-dimensional dataset, don’t despair. Remember PCA – your personal data whisperer, ready to unmask the soul of your data and reveal its most important stories. Experiment with it, play with the number of components, and watch your understanding of complex datasets transform.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>