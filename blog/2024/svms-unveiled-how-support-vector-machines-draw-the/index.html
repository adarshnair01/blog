<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> SVMs Unveiled: How Support Vector Machines Draw the Perfect Line (Even When There Isn't One) | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/svms-unveiled-how-support-vector-machines-draw-the/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">SVMs Unveiled: How Support Vector Machines Draw the Perfect Line (Even When There Isn't One)</h1> <p class="post-meta"> Created on July 09, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/support-vector-machines"> <i class="fa-solid fa-hashtag fa-sm"></i> Support Vector Machines</a>   <a href="/blog/blog/tag/classification"> <i class="fa-solid fa-hashtag fa-sm"></i> Classification</a>   <a href="/blog/blog/tag/kernels"> <i class="fa-solid fa-hashtag fa-sm"></i> Kernels</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hello fellow data adventurers! Today, I want to share a story about one of the most elegant and powerful algorithms in the machine learning toolkit: the Support Vector Machine (SVM). When I first encountered SVMs, they struck me as a beautifully intuitive solution to a common problem, yet they held a surprising depth that made them incredibly versatile.</p> <p>Imagine you’re trying to separate two different kinds of candy, say, gummy bears and jelly beans, scattered on a table. Your goal is to draw a line that best separates them. You could draw many lines, right? But which one is <em>the best</em>? This simple analogy is our starting point for understanding SVMs.</p> <h3 id="the-quest-for-the-best-separator-hyperplanes-and-margins">The Quest for the Best Separator: Hyperplanes and Margins</h3> <p>At its heart, an SVM is a <em>discriminative classifier</em>. This means it tries to find a boundary that separates data points belonging to different classes.</p> <h4 id="whats-a-hyperplane">What’s a Hyperplane?</h4> <p>In our candy example, where we have two dimensions (length and width of the table), the “line” we draw is called a <strong>hyperplane</strong>.</p> <ul> <li>In 2 dimensions, a hyperplane is a line.</li> <li>In 3 dimensions, a hyperplane is a plane.</li> <li>In more than 3 dimensions, well, it’s still a hyperplane, but it’s harder for us humans to visualize! Just know it’s a $(D-1)$-dimensional subspace that divides a $D$-dimensional space.</li> </ul> <p>The simplest case for an SVM is when our data is <strong>linearly separable</strong>. This means you can draw a straight line (or a flat plane/hyperplane) to perfectly separate the two classes without any overlap.</p> <h4 id="maximizing-the-margin-the-svms-secret-sauce">Maximizing the Margin: The SVM’s Secret Sauce</h4> <p>Now, back to our candy. Many lines could separate the gummy bears from the jelly beans. But which one is <em>the best</em>? An SVM argues that the best line is the one that has the largest “cushion” or “street” between it and the closest points of each class. This “cushion” is called the <strong>margin</strong>.</p> <p align="center"> <img src="https://upload.wikimedia.org/wikipedia/commons/2/22/Svm_max_sep_hyperplane_with_margin.png" alt="SVM Margin Example" width="600"> <br> <em>Image Source: Wikimedia Commons - A hyperplane with the maximum margin separating two classes.</em> </p> <p>Why maximize the margin? Think about it: a wider margin means the classifier is more robust. If your candies shift slightly, a wide margin is less likely to misclassify them than a narrow one. In machine learning terms, a wider margin often translates to better <strong>generalization</strong> – meaning the model performs better on new, unseen data, not just the data it was trained on.</p> <p>The data points that lie on the edges of this “street” (the closest points to the separating hyperplane) are incredibly important. We call them <strong>Support Vectors</strong>. They are literally “supporting” the margin. If you move or remove any other data point, the hyperplane and the margin wouldn’t change. But if you move a support vector, the hyperplane <em>has</em> to move. This makes SVMs very memory efficient, as they only need to remember these support vectors, not the entire dataset.</p> <h3 id="diving-deeper-the-mathematics-of-the-margin">Diving Deeper: The Mathematics of the Margin</h3> <p>Let’s get a little technical, but I promise we’ll keep it intuitive.</p> <p>The equation of a hyperplane in a D-dimensional space can be written as: \(w \cdot x + b = 0\) where:</p> <ul> <li>$w$ is a vector perpendicular to the hyperplane (its “normal vector”).</li> <li>$x$ is a data point.</li> <li>$b$ is a bias term (it shifts the hyperplane away from the origin).</li> </ul> <p>For a given data point $x_i$ with its class label $y_i$ (where $y_i = +1$ for one class and $y_i = -1$ for the other), we want our classifier to output:</p> <ul> <li>$w \cdot x_i + b \ge +1$ if $y_i = +1$</li> <li>$w \cdot x_i + b \le -1$ if $y_i = -1$</li> </ul> <p>We can combine these two conditions into one elegant inequality: \(y_i(w \cdot x_i + b) \ge 1\) This single constraint ensures that every data point is not just on the correct side of the hyperplane, but also <em>outside</em> the margin.</p> <p>The two “margin hyperplanes” (the edges of our “street”) are defined by:</p> <ul> <li>$w \cdot x + b = +1$</li> <li>$w \cdot x + b = -1$</li> </ul> <table> <tbody> <tr> <td>The distance between these two parallel hyperplanes turns out to be $ \frac{2}{</td> <td> </td> <td>w</td> <td> </td> <td>} $.</td> <td> </td> <td> </td> <td> </td> <td> </td> </tr> <tr> <td>Our goal is to maximize this distance, which is equivalent to **minimizing $</td> <td> </td> <td>w</td> <td> </td> <td>$** (or, for mathematical convenience, minimizing $ \frac{1}{2}</td> <td> </td> <td>w</td> <td> </td> <td>^2 $).</td> </tr> </tbody> </table> <p>So, the core optimization problem for a linear SVM becomes: <strong>Minimize</strong> $ \frac{1}{2} ||w||^2 $ <strong>Subject to</strong> $ y_i(w \cdot x_i + b) \ge 1 $ for all $i = 1, \dots, n$ (where $n$ is the number of data points).</p> <p>This is a convex optimization problem, which is great news! It means there’s a unique global minimum, and we don’t have to worry about getting stuck in local optima.</p> <h3 id="the-kernel-trick-when-data-isnt-linearly-separable">The Kernel Trick: When Data Isn’t Linearly Separable</h3> <p>What happens when our gummy bears and jelly beans aren’t neatly separated by a straight line? What if they’re mixed up, or one type forms a circle around the other? Trying to draw a straight line would lead to many misclassifications.</p> <p align="center"> <img src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/c5/Kernel_trick_idea.svg/640px-Kernel_trick_idea.svg.png" alt="Kernel Trick Idea" width="500"> <br> <em>Image Source: Wikimedia Commons - Data points not linearly separable in 2D, but separable when mapped to 3D.</em> </p> <p>This is where the magic of the <strong>Kernel Trick</strong> comes in. The idea is brilliant: if we can’t separate the data in its current dimension, let’s map it to a <em>higher-dimensional space</em> where it <em>can</em> be linearly separated.</p> <p>Imagine you have two classes of points forming concentric circles in 2D. You can’t draw a line to separate them. But if you could “lift” the inner circle data points upwards into a 3rd dimension, you could then easily slice through the 3D space with a flat plane to separate them!</p> <p>The challenge is that mapping data to a very high (potentially infinite) dimensional space can be computationally expensive or even impossible. This is where the “trick” part of the kernel trick shines.</p> <p>SVMs, when finding the optimal hyperplane, only ever need to compute the <em>dot product</em> between data points, typically in the form of $x_i \cdot x_j$. If we map our data using a feature function $\phi(x)$, then the dot product becomes $\phi(x_i) \cdot \phi(x_j)$.</p> <p>A <strong>kernel function</strong>, $K(x_i, x_j)$, allows us to directly compute this dot product in the higher-dimensional space <em>without ever explicitly performing the mapping $\phi(x)$ or even knowing what $\phi(x)$ is!</em> It’s like finding the result of a complex calculation without doing all the intermediate steps.</p> <p>Some popular kernel functions include:</p> <ol> <li> <strong>Polynomial Kernel:</strong> $ K(x_i, x_j) = (x_i \cdot x_j + c)^d $ This kernel maps data into a polynomial feature space, creating polynomial decision boundaries.</li> <li> <strong>Radial Basis Function (RBF) / Gaussian Kernel:</strong> $ K(x_i, x_j) = \exp(-\gamma ||x_i - x_j||^2) $ The RBF kernel is one of the most widely used. It essentially measures the similarity between two points. Intuitively, it projects data into an infinite-dimensional space, often allowing for complex, non-linear decision boundaries that wrap around your data. Think of it like placing a “hill” over each data point in a higher dimension; you can then slice through these hills to separate the classes.</li> </ol> <p>The kernel trick transforms SVMs from being purely linear classifiers to incredibly powerful non-linear ones, capable of handling highly complex data patterns.</p> <h3 id="soft-margins-embracing-imperfection">Soft Margins: Embracing Imperfection</h3> <p>In the real world, data is rarely perfectly separable. There’s noise, outliers, and sometimes the classes just genuinely overlap. If we insisted on a perfectly clean margin, our SVM might fail or become overly sensitive to individual data points, leading to <strong>overfitting</strong>.</p> <p>To handle this, SVMs introduce the concept of <strong>soft margins</strong>. Instead of strictly enforcing that all points must be outside the margin, we allow for some “violations” – points that fall inside the margin or even on the wrong side of the hyperplane.</p> <p>We introduce <strong>slack variables</strong> $\xi_i$ (Greek letter “xi”) for each data point:</p> <ul> <li>If $\xi_i = 0$, the point is correctly classified and outside the margin.</li> <li>If $0 &lt; \xi_i &lt; 1$, the point is correctly classified but <em>inside</em> the margin.</li> <li>If $\xi_i \ge 1$, the point is misclassified (on the wrong side of the hyperplane).</li> </ul> <p>Our optimization problem is then modified to include a penalty for these violations: <strong>Minimize</strong> $ \frac{1}{2} ||w||^2 + C \sum_{i=1}^n \xi_i $ <strong>Subject to</strong> $ y_i(w \cdot x_i + b) \ge 1 - \xi_i $ and $ \xi_i \ge 0 $ for all $i$.</p> <p>Here, $C$ is a crucial hyperparameter called the <strong>regularization parameter</strong> or <strong>cost parameter</strong>.</p> <ul> <li>A <strong>small $C$</strong> value allows for more margin violations (larger $\xi_i$ values). This leads to a wider margin, making the model more tolerant to misclassifications. It’s like saying, “I prefer a general, robust separation, even if it means some points are on the wrong side.” This can help prevent overfitting.</li> <li>A <strong>large $C$</strong> value heavily penalizes margin violations. This forces the model to find a narrower margin and try to classify every training point correctly, potentially leading to overfitting if the data is noisy.</li> </ul> <p>The $C$ parameter allows us to strike a balance between maximizing the margin and minimizing classification errors on the training data. This flexibility makes SVMs much more applicable to real-world, messy datasets.</p> <h3 id="advantages-and-disadvantages-of-svms">Advantages and Disadvantages of SVMs</h3> <p>Like any algorithm, SVMs have their strengths and weaknesses:</p> <h4 id="advantages">Advantages:</h4> <ul> <li> <strong>Effective in High-Dimensional Spaces:</strong> SVMs work remarkably well even when you have more features than data samples, a common scenario in fields like text classification or genomics.</li> <li> <strong>Memory Efficient:</strong> Because they only rely on the support vectors to define the decision boundary, they can be efficient in terms of memory usage during prediction.</li> <li> <strong>Versatile with Kernels:</strong> The kernel trick allows SVMs to adapt to various data types and complex non-linear relationships, making them incredibly flexible.</li> <li> <strong>Strong Theoretical Foundation:</strong> The principle of maximizing the margin provides a robust theoretical basis, which often leads to good generalization performance.</li> </ul> <h4 id="disadvantages">Disadvantages:</h4> <ul> <li> <strong>Scalability:</strong> Training SVMs can be computationally intensive and slow, especially on very large datasets (millions of samples). The training time generally scales between $O(n^2)$ and $O(n^3)$ in the number of samples, though modern implementations and techniques (like SGD-based SVMs) help mitigate this.</li> <li> <strong>Hyperparameter Tuning:</strong> The choice of the kernel function (e.g., RBF, polynomial) and its parameters (like $\gamma$ for RBF or $d$ for polynomial), along with the regularization parameter $C$, can significantly impact performance. Finding the optimal combination often requires extensive hyperparameter tuning.</li> <li> <strong>Lack of Direct Probability Estimates:</strong> Unlike models like Logistic Regression, SVMs inherently output a class prediction (e.g., +1 or -1), not a probability score. While extensions exist to provide probability estimates, they are not a native feature of the algorithm.</li> <li> <strong>Interpretability:</strong> While the concept of support vectors is intuitive, interpreting the meaning of complex non-linear decision boundaries in high-dimensional spaces can be challenging.</li> </ul> <h3 id="conclusion-the-enduring-elegance-of-svms">Conclusion: The Enduring Elegance of SVMs</h3> <p>From a simple idea of drawing the “best” line, we’ve journeyed through the clever mechanics of margin maximization, the magical transformation of the kernel trick for non-linear data, and the pragmatic flexibility of soft margins to handle real-world noise.</p> <p>Support Vector Machines are truly an elegant testament to mathematical ingenuity meeting practical data challenges. They have found widespread application in diverse fields such as:</p> <ul> <li> <strong>Text Classification:</strong> Spam detection, sentiment analysis.</li> <li> <strong>Image Recognition:</strong> Object detection, facial recognition.</li> <li> <strong>Bioinformatics:</strong> Classification of proteins, gene expression analysis.</li> <li> <strong>Handwriting Recognition:</strong> Identifying characters.</li> </ul> <p>While newer deep learning techniques have taken the spotlight for many tasks, SVMs remain a fundamental and powerful algorithm, especially for datasets that aren’t enormous, or when interpretability and robust generalization are key.</p> <p>So, the next time you hear about classification, remember the humble yet mighty SVM, diligently finding that optimal separating boundary, whether it’s a straight line or a complex curve in a high-dimensional space. It’s a journey from simple candies on a table to sophisticated machine intelligence, all thanks to the genius of maximizing margins.</p> <p>Keep learning, keep exploring, and who knows what elegant solutions you’ll uncover next!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>