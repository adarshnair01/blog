<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> From Mess to Model: The Unsung Art of Data Cleaning Strategies | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/from-mess-to-model-the-unsung-art-of-data-cleaning/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">From Mess to Model: The Unsung Art of Data Cleaning Strategies</h1> <p class="post-meta"> Created on November 17, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/data-cleaning"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Cleaning</a>   <a href="/blog/blog/tag/data-preprocessing"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Preprocessing</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/python"> <i class="fa-solid fa-hashtag fa-sm"></i> Python</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>As data scientists and machine learning engineers, we often dream of the exciting parts: building complex models, designing neural networks, or uncovering groundbreaking insights with fancy algorithms. We pore over research papers, debate activation functions, and fine-tune hyperparameters with the precision of a Swiss watchmaker. But here’s a secret, one that every seasoned pro will tell you: the vast majority of our time (some say 70-80%!) isn’t spent on those glamorous tasks. It’s spent on something far more fundamental, often tedious, but absolutely critical: <strong>data cleaning</strong>.</p> <p>I’ve been there. I remember excitedly grabbing a new dataset, eager to jump straight into model building, only to be met with a cascade of errors, strange outputs, and models that performed worse than a coin flip. The culprit? Dirty data. It’s like trying to build a magnificent skyscraper on a foundation of quicksand. No matter how brilliant your architecture, it will crumble. This experience taught me a profound lesson: <strong>“Garbage In, Garbage Out” (GIGO)</strong> isn’t just a catchy phrase; it’s the iron law of data science.</p> <p>This isn’t just a technical skill; it’s an art, a detective mission, and sometimes, a true test of patience. But mastering it is non-negotiable for anyone serious about building reliable, accurate, and impactful data products. So, let’s roll up our sleeves and explore the essential strategies I use to turn chaotic raw data into a pristine, model-ready dataset.</p> <h2 id="why-bother-with-the-dirty-work">Why Bother with the “Dirty Work”?</h2> <p>Before we dive into the ‘how,’ let’s solidify the ‘why.’ Why is data cleaning so crucial?</p> <ol> <li> <strong>Model Performance</strong>: Machine learning models learn patterns from the data they’re fed. If those patterns are obscured by errors, inconsistencies, or missing values, the model will learn incorrect relationships, leading to poor predictions and flawed insights.</li> <li> <strong>Accuracy and Reliability</strong>: Clean data ensures that your analyses and conclusions are based on factual, consistent information, making your findings trustworthy.</li> <li> <strong>Better Business Decisions</strong>: Businesses rely on data to make strategic choices. If that data is faulty, decisions based on it can lead to costly mistakes, missed opportunities, or even reputational damage.</li> <li> <strong>Reduced Debugging Time</strong>: Trust me, spending a little extra time cleaning data upfront saves <em>exponentially</em> more time debugging mysterious model behavior later.</li> <li> <strong>Ethical Considerations</strong>: Biases can be inadvertently introduced or amplified by dirty data, leading to unfair or discriminatory outcomes. Cleaning data can help mitigate some of these issues.</li> </ol> <h2 id="the-data-cleaning-workflow-an-iterative-journey">The Data Cleaning Workflow: An Iterative Journey</h2> <p>Data cleaning isn’t a linear checklist; it’s an iterative process, often involving revisiting steps as you uncover new issues. My typical workflow looks something like this:</p> <h3 id="step-1-understanding-your-data---the-exploratory-data-analysis-eda-deep-dive">Step 1: Understanding Your Data - The Exploratory Data Analysis (EDA) Deep Dive</h3> <p>You can’t clean what you don’t understand. This is where <strong>Exploratory Data Analysis (EDA)</strong> comes in. It’s the detective phase where you get to know your dataset intimately.</p> <ul> <li> <strong>Initial Inspection</strong>: Start with basic commands. <ul> <li> <code class="language-plaintext highlighter-rouge">df.head()</code>: See the first few rows.</li> <li> <code class="language-plaintext highlighter-rouge">df.info()</code>: Get a summary including data types, non-null counts, and memory usage. This is a goldmine for spotting missing values and incorrect data types.</li> <li> <code class="language-plaintext highlighter-rouge">df.describe()</code>: Statistical summary of numerical columns (count, mean, std, min, max, quartiles). Essential for understanding distribution and potential outliers.</li> <li> <code class="language-plaintext highlighter-rouge">df.shape</code>: Know the number of rows and columns.</li> </ul> </li> <li> <strong>Value Counts</strong>: For categorical features, <code class="language-plaintext highlighter-rouge">df['column'].value_counts()</code> is invaluable for spotting inconsistent entries (e.g., ‘USA’, ‘U.S.A.’, ‘usa’).</li> <li> <strong>Visualizations</strong>: This is where data truly speaks! <ul> <li> <strong>Histograms/KDE plots</strong>: Show the distribution of numerical features. Look for skewness, multiple peaks, or strange ranges.</li> <li> <strong>Box plots</strong>: Excellent for visualizing the distribution, quartiles, and especially for identifying potential outliers.</li> <li> <strong>Scatter plots</strong>: Useful for examining relationships between two numerical variables and spotting outliers in a multi-dimensional context.</li> <li> <strong>Bar plots</strong>: For categorical data, to see the frequency of each category.</li> </ul> </li> </ul> <p><em>My personal takeaway from EDA is that it sets the stage. It’s like checking the pulse of your data before performing surgery.</em></p> <h3 id="step-2-handling-missing-values---the-missing-pieces-puzzle">Step 2: Handling Missing Values - The Missing Pieces Puzzle</h3> <p>Missing data, often represented as <code class="language-plaintext highlighter-rouge">NaN</code>, <code class="language-plaintext highlighter-rouge">null</code>, or empty strings, is perhaps the most common data quality issue. How you deal with it can significantly impact your model.</p> <ul> <li> <strong>Identification</strong>: <ul> <li> <code class="language-plaintext highlighter-rouge">df.isnull().sum()</code>: Shows the count of missing values per column.</li> <li> <code class="language-plaintext highlighter-rouge">df.isnull().sum() / len(df) * 100</code>: Gives the percentage of missing values, which helps prioritize.</li> </ul> </li> <li> <strong>Strategies for Treatment</strong>: <ol> <li> <strong>Deletion</strong>: <ul> <li> <strong>Row-wise (<code class="language-plaintext highlighter-rouge">df.dropna(axis=0)</code>):</strong> If only a few rows have missing values across many columns, or if a row has missing values in critical features, you might drop the entire row. <em>Caution: This can lead to significant data loss if not used judiciously.</em> </li> <li> <strong>Column-wise (<code class="language-plaintext highlighter-rouge">df.dropna(axis=1)</code>):</strong> If a column has a very high percentage of missing values (e.g., &gt;70-80%) and is not critical for your analysis, you might drop the entire column.</li> </ul> </li> <li> <strong>Imputation (Filling Missing Values)</strong>: <ul> <li> <strong>Mean/Median/Mode</strong>: <ul> <li> <strong>Mean</strong>: Best for numerical data that is normally distributed (not skewed) and without significant outliers.</li> <li> <strong>Median</strong>: Robust for numerical data, especially when it’s skewed or contains outliers, as it’s less affected by extreme values.</li> <li> <strong>Mode</strong>: Ideal for categorical data or numerical data with a limited set of discrete values.</li> <li> <em>Example (Pandas):</em> <code class="language-plaintext highlighter-rouge">df['column'].fillna(df['column'].mean(), inplace=True)</code> </li> </ul> </li> <li> <strong>Constant Value</strong>: Fill with a specific value (e.g., 0, ‘Unknown’, ‘N/A’). Useful when the missingness itself conveys information.</li> <li> <strong>Forward Fill (ffill) / Backward Fill (bfill)</strong>: Especially useful for time-series data, where you might want to carry forward the last valid observation or carry backward the next valid observation.</li> <li> <strong>More Advanced Methods</strong>: <ul> <li> <strong>Regression Imputation</strong>: Predict missing values using other features in your dataset.</li> <li> <strong>K-Nearest Neighbors (KNN) Imputation</strong>: Find ‘k’ nearest neighbors to a data point with missing values and impute based on their values.</li> </ul> </li> </ul> </li> </ol> </li> </ul> <p><em>Choosing the right imputation strategy is crucial. There’s no one-size-fits-all answer; it depends on the nature of your data and the domain.</em></p> <h3 id="step-3-dealing-with-outliers---the-anomaly-hunt">Step 3: Dealing with Outliers - The Anomaly Hunt</h3> <p>Outliers are data points that significantly deviate from other observations. They can be genuine extreme values or errors, and they can severely skew statistical analyses and model training.</p> <ul> <li> <strong>Identification</strong>: <ol> <li> <strong>Visual Inspection</strong>: Box plots are fantastic for this, showing points outside the “whiskers.” Histograms can also reveal unusual spikes or tails.</li> <li> <strong>Statistical Methods</strong>: <ul> <li> <strong>Z-score</strong>: For data that is approximately normally distributed. A Z-score measures how many standard deviations an element is from the mean. $Z = \frac{x - \mu}{\sigma}$ Where $x$ is the data point, $\mu$ is the mean, and $\sigma$ is the standard deviation. Values typically exceeding $\pm 2$ or $\pm 3$ are considered outliers.</li> <li> <strong>Interquartile Range (IQR)</strong>: More robust for skewed data. IQR is the range between the first quartile ($Q1$) and the third quartile ($Q3$). $IQR = Q3 - Q1$ Outliers are often defined as values below $Q1 - 1.5 \times IQR$ or above $Q3 + 1.5 \times IQR$.</li> </ul> </li> </ol> </li> <li> <strong>Strategies for Treatment</strong>: <ol> <li> <strong>Deletion</strong>: If outliers are clearly data entry errors or highly extreme, you might remove them. <em>Again, exercise caution; deleting too much can lead to loss of information.</em> </li> <li> <strong>Capping/Winsorization</strong>: Instead of removing, you “cap” the outliers, replacing them with a threshold value (e.g., the 99th percentile or the IQR upper bound). This limits their extreme influence.</li> <li> <strong>Transformation</strong>: Applying mathematical transformations like logarithmic or square root transformations can reduce the impact of extreme values and make the data more normally distributed.</li> <li> <strong>Treat as Missing</strong>: Sometimes, outliers are so extreme or unexplainable that treating them as missing values and then imputing them (perhaps with the median) is a valid strategy.</li> <li> <strong>Robust Models</strong>: Some machine learning models (e.g., tree-based models like Random Forest or Gradient Boosting) are less sensitive to outliers compared to others (e.g., Linear Regression, k-Means).</li> </ol> </li> </ul> <p><em>It’s crucial to investigate outliers. Are they errors, or do they represent rare but valid occurrences that hold significant information? Sometimes, an outlier is the most interesting part of your data!</em></p> <h3 id="step-4-fixing-inconsistent-data-and-formatting-errors---the-standardization-task">Step 4: Fixing Inconsistent Data and Formatting Errors - The Standardization Task</h3> <p>Real-world data is messy because humans enter it. Inconsistencies are rampant.</p> <ul> <li> <strong>Categorical Data Inconsistencies</strong>: <ul> <li> <strong>Varying Casing</strong>: ‘USA’, ‘usa’, ‘U.S.A.’ for the same country. Standardize them: <code class="language-plaintext highlighter-rouge">df['country'].str.lower().replace({'u.s.a.': 'usa'}, inplace=True)</code>.</li> <li> <strong>Typos/Misspellings</strong>: ‘Californa’ instead of ‘California’. Manual correction or fuzzy matching for large datasets.</li> <li> <strong>Synonyms</strong>: ‘Dr.’ vs ‘Doctor’.</li> <li> <strong>Combining Rare Categories</strong>: If you have many categories with very few observations, group them into an ‘Other’ category to simplify analysis and prevent overfitting.</li> </ul> </li> <li> <strong>Numerical Data Inconsistencies</strong>: <ul> <li> <strong>Incorrect Data Types</strong>: Numbers stored as strings with currency symbols (e.g., ‘$1,200’). You’d need to remove symbols and convert to numeric: <code class="language-plaintext highlighter-rouge">df['price'].str.replace('$', '').str.replace(',', '').astype(float)</code>.</li> <li> <strong>Units</strong>: ‘cm’ vs ‘m’. Standardize to a single unit.</li> <li> <strong>Ranges</strong>: Values outside logical bounds (e.g., age = 200).</li> </ul> </li> <li> <strong>Date/Time Data</strong>: <ul> <li> <strong>Inconsistent Formats</strong>: ‘2023-01-15’, ‘15/01/2023’, ‘Jan 15, 2023’. Convert to a standard format using <code class="language-plaintext highlighter-rouge">pd.to_datetime()</code> which is incredibly powerful.</li> <li> <strong>Invalid Dates</strong>: ‘31/02/2023’ (February only has 28 or 29 days).</li> </ul> </li> </ul> <h3 id="step-5-addressing-duplicates---the-redundancy-removal">Step 5: Addressing Duplicates - The Redundancy Removal</h3> <p>Duplicate rows can skew analyses, inflate counts, and lead to biased model training.</p> <ul> <li> <strong>Identification</strong>: <code class="language-plaintext highlighter-rouge">df.duplicated().sum()</code> will tell you how many duplicate rows exist. You can also specify a subset of columns to check for duplicates (<code class="language-plaintext highlighter-rouge">df.duplicated(subset=['col1', 'col2'])</code>).</li> <li> <strong>Deletion</strong>: <code class="language-plaintext highlighter-rouge">df.drop_duplicates(inplace=True)</code> will remove exact duplicate rows. If you’re checking a subset, specify it.</li> </ul> <p><em>Always consider if a “duplicate” is truly redundant or if multiple entries with the same values across certain columns are valid (e.g., multiple purchases by the same customer).</em></p> <h3 id="step-6-data-type-conversion---the-final-touch">Step 6: Data Type Conversion - The Final Touch</h3> <p>Ensuring each column has the correct data type is fundamental. <code class="language-plaintext highlighter-rouge">df.info()</code> will reveal common issues (e.g., numbers as <code class="language-plaintext highlighter-rouge">object</code>/<code class="language-plaintext highlighter-rouge">string</code>, dates as <code class="language-plaintext highlighter-rouge">object</code>).</p> <ul> <li> <strong>Numerical</strong>: <code class="language-plaintext highlighter-rouge">int</code>, <code class="language-plaintext highlighter-rouge">float</code>.</li> <li> <strong>Categorical</strong>: Convert <code class="language-plaintext highlighter-rouge">object</code> types that are truly categorical to <code class="language-plaintext highlighter-rouge">category</code> for memory efficiency and better performance with some ML libraries.</li> <li> <strong>Date/Time</strong>: <code class="language-plaintext highlighter-rouge">datetime</code>.</li> </ul> <p>Example: <code class="language-plaintext highlighter-rouge">df['some_column'] = df['some_column'].astype('category')</code></p> <h2 id="the-iterative-nature-a-cycle-of-refinement">The Iterative Nature: A Cycle of Refinement</h2> <p>I can’t stress this enough: data cleaning is not a one-and-done task. You’ll often go through these steps, clean some data, then perform more EDA, only to discover new issues or realize that your initial cleaning strategy created other problems. It’s a continuous cycle of:</p> <p><strong>EDA -&gt; Identify Problems -&gt; Apply Cleaning Strategy -&gt; Re-EDA -&gt; Identify New Problems (or validate fixes) -&gt; Repeat.</strong></p> <p>This iterative approach is where the “art” truly comes into play. It requires critical thinking, domain knowledge, and a willingness to get your hands dirty.</p> <h2 id="tools-of-the-trade">Tools of the Trade</h2> <p>While we’ve discussed the strategies, the primary tool in almost every data scientist’s arsenal for these tasks is <strong>Pandas</strong> in Python. Its DataFrame structure and rich set of functions make it incredibly powerful for manipulating, cleaning, and transforming data. Other libraries like <strong>NumPy</strong> for numerical operations and <strong>Scikit-learn</strong> for imputation techniques (e.g., <code class="language-plaintext highlighter-rouge">SimpleImputer</code>, <code class="language-plaintext highlighter-rouge">KNNImputer</code>) are also invaluable.</p> <h2 id="conclusion-the-unsung-hero">Conclusion: The Unsung Hero</h2> <p>Data cleaning might not be the most glamorous part of a data scientist’s job, but it is unequivocally the most important. It’s the foundation upon which all reliable analyses, robust models, and valuable insights are built. Neglecting it is like trying to bake a gourmet cake with rotten ingredients – no matter how skilled the chef, the result will be disappointing.</p> <p>So, the next time you dive into a dataset, take a moment. Put on your detective hat. Embrace the mess. Because truly understanding and cleaning your data isn’t just a chore; it’s a critical skill that elevates you from a data tinkerer to a true data professional, ensuring your work has a real, meaningful impact.</p> <p>Happy cleaning!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>