<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Backpropagation Demystified: How Neural Networks Learn from Their Mistakes | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/backpropagation-demystified-how-neural-networks-le/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Backpropagation Demystified: How Neural Networks Learn from Their Mistakes</h1> <p class="post-meta"> Created on November 02, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/neural-networks"> <i class="fa-solid fa-hashtag fa-sm"></i> Neural Networks</a>   <a href="/blog/blog/tag/algorithms"> <i class="fa-solid fa-hashtag fa-sm"></i> Algorithms</a>   <a href="/blog/blog/tag/backpropagation"> <i class="fa-solid fa-hashtag fa-sm"></i> Backpropagation</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hello fellow explorers of the AI frontier!</p> <p>I remember vividly the first time I encountered the term “Backpropagation.” It sounded like something out of a sci-fi movie, a secret incantation that breathed intelligence into inert silicon. As a student diving into the world of Data Science and Machine Learning, neural networks initially felt like a black box. Data goes in, magic happens, and a prediction comes out. But how does that <em>magic</em> actually learn to be so good? How do these complex networks, with millions of connections, adjust themselves to perform incredible feats like image recognition or natural language understanding?</p> <p>The answer, my friends, is Backpropagation. It’s not magic; it’s elegant, differential calculus applied ingeniously. And today, we’re going to pull back the curtain and understand this cornerstone algorithm that powers almost all modern deep learning.</p> <h3 id="the-big-picture-why-do-we-need-backpropagation">The Big Picture: Why Do We Need Backpropagation?</h3> <p>Imagine you’re teaching a child to identify different animals. You show them a picture of a dog and say, “That’s a dog.” If they point to a cat and say “dog,” you correct them. “No, that’s a cat. See the pointy ears and whiskers?” Gradually, through feedback and correction, the child learns to differentiate between animals.</p> <p>Neural networks learn in a remarkably similar fashion. They make a prediction, compare it to the correct answer, realize they made a mistake, and then <em>adjust</em> their internal “understanding” so they’ll make a better prediction next time. This adjustment process, across potentially hundreds of layers and millions of parameters, is what Backpropagation orchestrates.</p> <p>Without Backpropagation, neural networks would be glorified calculators, fixed in their initial, random state. With it, they become adaptive, powerful learning machines.</p> <h3 id="the-forward-pass-making-a-guess">The Forward Pass: Making a Guess</h3> <p>Before we can correct a mistake, we first have to make one (or at least, a guess!). This is called the <strong>forward pass</strong>.</p> <p>In a neural network, data (like an image or text) enters the first layer (the input layer). Each neuron in this layer passes its information, weighted by connections (called <strong>weights</strong>, $w$), to the next layer. This process continues, layer by layer, until we reach the final <strong>output layer</strong>, which gives us the network’s prediction ($\hat{y}$).</p> <p>Think of it like a chain reaction:</p> <ol> <li>Input features ($x$) arrive.</li> <li>Each input $x_i$ is multiplied by its corresponding weight $w_i$.</li> <li>These weighted inputs are summed up, and a <strong>bias</strong> ($b$) is added. This is the “net input” to a neuron: $z = \sum_i (w_i x_i) + b$.</li> <li>This $z$ then passes through an <strong>activation function</strong> ($\sigma$, e.g., ReLU, Sigmoid), which introduces non-linearity and produces the neuron’s output, $a = \sigma(z)$.</li> <li>This output $a$ becomes the input for the next layer, and the process repeats until we get the final prediction $\hat{y}$.</li> </ol> <p>After the forward pass, we have our network’s prediction, $\hat{y}$. Now we compare it to the actual correct answer, $y$.</p> <h3 id="quantifying-the-mistake-the-loss-function">Quantifying the Mistake: The Loss Function</h3> <p>How “wrong” was our prediction? We quantify this mistake using a <strong>loss function</strong> (or cost function). A common one for regression tasks is the Mean Squared Error (MSE):</p> <p>$L = (y - \hat{y})^2$</p> <p>Here, $y$ is the true value, and $\hat{y}$ is our network’s prediction. The larger the difference, the larger the loss, meaning our network made a bigger mistake. Our ultimate goal is to minimize this loss.</p> <h3 id="the-aha-moment-how-to-improve">The “Aha!” Moment: How to Improve?</h3> <p>Now we know how wrong we were. The crucial question is: <em>how do we change the weights and biases in our network to make the loss smaller?</em></p> <p>This is where calculus, specifically the concept of <strong>gradients</strong>, comes into play. A gradient tells us the direction and magnitude of the steepest increase of a function. If we want to <em>minimize</em> the loss, we need to move in the <em>opposite</em> direction of the gradient. This is the core idea behind <strong>Gradient Descent</strong>, the optimization algorithm used to update weights.</p> <p>For each weight $w$ and bias $b$ in the network, we want to calculate $\frac{\partial L}{\partial w}$ and $\frac{\partial L}{\partial b}$. These are the partial derivatives of the loss function with respect to each weight and bias, telling us how much a tiny change in that weight/bias would affect the total loss.</p> <p>But here’s the kicker: the loss $L$ directly depends on $\hat{y}$, which depends on the output of the previous layer, which depends on its weights and biases, and so on, all the way back to the input layer. This is a chain of dependencies.</p> <h3 id="the-chain-rule-the-heart-of-backpropagation">The Chain Rule: The Heart of Backpropagation</h3> <p>This chain of dependencies is precisely where the <strong>Chain Rule</strong> from calculus becomes our best friend. The Chain Rule allows us to calculate the derivative of a composite function. If we have a function $f(g(x))$, its derivative with respect to $x$ is $f’(g(x)) \cdot g’(x)$.</p> <p>Let’s illustrate with a single neuron, where we want to find how much a weight $w_j$ in that neuron contributes to the overall loss $L$.</p> <p>Our single neuron’s prediction is $\hat{y} = \sigma(z)$, where $z = \sum_i w_i x_i + b$. The loss is $L = (y - \hat{y})^2$.</p> <p>To find $\frac{\partial L}{\partial w_j}$, we apply the chain rule:</p> <p>$\frac{\partial L}{\partial w_j} = \frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z} \cdot \frac{\partial z}{\partial w_j}$</p> <p>Let’s break down each term:</p> <ol> <li> <strong>$\frac{\partial L}{\partial \hat{y}}$</strong>: This tells us how much the loss changes with respect to the neuron’s output. <ul> <li>For $L = (y - \hat{y})^2$, this derivative is $-2(y - \hat{y})$. This is our “error signal” from the output.</li> </ul> </li> <li> <strong>$\frac{\partial \hat{y}}{\partial z}$</strong>: This tells us how much the neuron’s output changes with respect to its net input ($z$). This depends on the activation function. <ul> <li>If $\hat{y} = \sigma(z)$ (a sigmoid function), then $\frac{\partial \hat{y}}{\partial z} = \sigma(z)(1 - \sigma(z))$, or simply $\hat{y}(1-\hat{y})$. This is the <em>local gradient</em> of the activation.</li> </ul> </li> <li> <strong>$\frac{\partial z}{\partial w_j}$</strong>: This tells us how much the net input changes with respect to a specific weight $w_j$. <ul> <li>Since $z = w_1 x_1 + w_2 x_2 + \dots + w_j x_j + \dots + b$, the derivative with respect to $w_j$ is simply $x_j$. This is the input that weight $w_j$ received.</li> </ul> </li> </ol> <p>Putting it all together for one weight $w_j$:</p> <p>$\frac{\partial L}{\partial w_j} = -2(y - \hat{y}) \cdot \hat{y}(1-\hat{y}) \cdot x_j$</p> <p>And similarly for the bias $b$:</p> <p>$\frac{\partial L}{\partial b} = \frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z} \cdot \frac{\partial z}{\partial b}$ $\frac{\partial L}{\partial b} = -2(y - \hat{y}) \cdot \hat{y}(1-\hat{y}) \cdot 1$</p> <p>These are the gradients for a <em>single</em> neuron’s weights and bias. This is fantastic! We know exactly how much to tweak $w_j$ and $b$ to reduce the loss.</p> <h3 id="propagating-the-error-backwards">Propagating the Error Backwards</h3> <p>Here’s where “Backpropagation” earns its name. What if we have a network with multiple layers? The error signal calculated at the output layer (our $\frac{\partial L}{\partial \hat{y}}$ term) needs to be “propagated backward” through the network.</p> <p>Imagine our network has output $\hat{y}$ from layer $L$, which depends on the activations $a^{(L-1)}$ from layer $L-1$, which in turn depend on weights $W^{(L-1)}$ and activations $a^{(L-2)}$, and so on.</p> <p>When we calculate $\frac{\partial L}{\partial w^{(L)}}$ (for weights in the last layer), we use the error signal from the output layer.</p> <p>But for weights $W^{(L-1)}$ in the <em>second-to-last</em> layer, we need to know how much <em>its</em> outputs (which are the inputs to the last layer) contributed to the overall loss. This is the beauty of the chain rule. The error signal for layer $L-1$ is derived from the error signal that was already calculated for layer $L$. We essentially multiply the “local gradient” of layer $L-1$ by the “error signal received from the layer ahead.”</p> <p>The general idea for a hidden layer $l$: The error sensitivity for a neuron’s activation $a^{(l)}$ at layer $l$ is $\delta^{(l)} = \frac{\partial L}{\partial a^{(l)}}$. This $\delta^{(l)}$ then becomes the “error signal” used to calculate the gradients for the weights and biases <em>feeding into</em> layer $l$.</p> <p>The computation proceeds as follows:</p> <ol> <li> <strong>Calculate output layer error:</strong> Compute $\delta^{(L)} = \frac{\partial L}{\partial \hat{y}} \cdot \sigma’(z^{(L)})$. This is the error signal specific to the output layer.</li> <li> <strong>Backpropagate error:</strong> For each hidden layer, from $L-1$ down to 1: <ul> <li>Calculate $\delta^{(l)} = ( (W^{(l+1)})^T \delta^{(l+1)} ) \cdot \sigma’(z^{(l)})$.</li> <li>This step is crucial: the error signal for layer $l$ is computed by taking the weighted sum of the error signals from the <em>next</em> layer ($l+1$) and multiplying it by the local gradient of the activation function at layer $l$. This is the “backward flow” of error.</li> </ul> </li> <li> <strong>Compute gradients for weights and biases:</strong> Once we have all the $\delta^{(l)}$ values, we can compute the gradients for weights and biases in each layer: <ul> <li>$\frac{\partial L}{\partial W^{(l)}} = \delta^{(l)} (a^{(l-1)})^T$</li> <li>$\frac{\partial L}{\partial b^{(l)}} = \delta^{(l)}$</li> </ul> </li> <li> <strong>Update weights and biases:</strong> Finally, using an optimizer like Gradient Descent, we update the parameters: <ul> <li>$W^{(l)} \leftarrow W^{(l)} - \eta \frac{\partial L}{\partial W^{(l)}}$</li> <li>$b^{(l)} \leftarrow b^{(l)} - \eta \frac{\partial L}{\partial b^{(l)}}$</li> <li>Here, $\eta$ (eta) is the <strong>learning rate</strong>, a small positive number that controls how big a step we take in the direction opposite to the gradient. A well-chosen learning rate is critical!</li> </ul> </li> </ol> <p>This entire process—forward pass, loss calculation, backward pass to compute gradients, and parameter update—is called an <strong>epoch</strong> or an <strong>iteration</strong>. We repeat this thousands or millions of times, gradually minimizing the loss and improving the network’s performance.</p> <h3 id="why-is-it-so-powerful">Why is it so powerful?</h3> <p>Backpropagation is a genius algorithm because:</p> <ol> <li> <strong>Efficiency:</strong> It efficiently computes all gradients needed for all parameters in the network. A naive approach of calculating each derivative individually would be computationally prohibitive for deep networks.</li> <li> <strong>Credit Assignment:</strong> It elegantly solves the “credit assignment problem.” Each weight in the network gets an accurate signal telling it exactly how much it contributed to the overall error, even if it’s deep within a multi-layered structure.</li> <li> <strong>Generalizability:</strong> It’s applicable to virtually any differentiable neural network architecture, making it the bedrock of deep learning.</li> </ol> <h3 id="the-unsung-heros-challenges">The Unsung Hero’s Challenges</h3> <p>While revolutionary, Backpropagation isn’t without its quirks. Concepts like <strong>vanishing gradients</strong> (where gradients become extremely small in earlier layers, making learning slow or impossible) and <strong>exploding gradients</strong> (where gradients become too large, leading to unstable training) are real challenges that researchers have addressed with innovations like ReLU activation functions, batch normalization, and more sophisticated optimizers (Adam, RMSprop, etc.). But understanding these challenges makes us appreciate the original algorithm even more.</p> <h3 id="wrapping-up">Wrapping Up</h3> <p>Backpropagation, at its core, is a beautifully orchestrated dance of derivatives and the chain rule, allowing neural networks to learn from their mistakes and refine their understanding of the world. It transformed neural networks from theoretical curiosities into the powerful, intelligent systems we see today, driving advancements in everything from medical diagnosis to self-driving cars.</p> <p>So, the next time you see an AI performing an impressive feat, remember the silent, tireless work of Backpropagation, meticulously adjusting connections, one gradient at a time, to make that magic happen. It’s not just an algorithm; it’s the engine of modern AI. Keep exploring, keep questioning, and you’ll uncover even more wonders!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>