<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Making NumPy Sing: Optimizing Your Code for Peak Performance | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/making-numpy-sing-optimizing-your-code-for-peak-pe/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Making NumPy Sing: Optimizing Your Code for Peak Performance</h1> <p class="post-meta"> Created on September 16, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/numpy"> <i class="fa-solid fa-hashtag fa-sm"></i> NumPy</a>   <a href="/blog/blog/tag/optimization"> <i class="fa-solid fa-hashtag fa-sm"></i> Optimization</a>   <a href="/blog/blog/tag/python"> <i class="fa-solid fa-hashtag fa-sm"></i> Python</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/performance"> <i class="fa-solid fa-hashtag fa-sm"></i> Performance</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hello fellow data adventurers!</p> <p>Have you ever found yourself staring at a progress bar, waiting for your Python script to finish crunching numbers, perhaps muttering under your breath about how long it’s taking? I certainly have. There was this one time, working on a project involving millions of sensor readings, where my initial approach of iterating through data with plain old Python loops felt like trying to empty a swimming pool with a teacup. My code was correct, but it was excruciatingly slow.</p> <p>Then, I rediscovered the magic of NumPy, and more importantly, how to use it <em>optimally</em>. It wasn’t just about replacing lists with <code class="language-plaintext highlighter-rouge">np.array</code>; it was about understanding <em>how</em> NumPy works under the hood and leveraging its strengths. This journey transformed my slow, sluggish scripts into blazing-fast computational powerhouses. And today, I want to share some of those secrets with you.</p> <p>This post isn’t just about making your code faster (though it definitely will!). It’s about empowering you to tackle bigger datasets, build more complex models, and ultimately, become a more efficient and confident data scientist or machine learning engineer. So, grab your virtual seat, because we’re about to dive deep into the world of NumPy optimization!</p> <h3 id="why-numpy-is-already-fast-and-why-we-need-to-make-it-faster">Why NumPy is Already Fast (and Why We Need to Make it Faster)</h3> <p>Before we talk about optimizing NumPy, let’s briefly appreciate why it’s already a cornerstone of scientific computing in Python.</p> <ol> <li> <strong>Vectorization:</strong> Instead of writing explicit loops in Python (which are notoriously slow due to Python’s interpreted nature), NumPy allows you to perform operations on entire arrays at once. This concept is called vectorization.</li> <li> <strong>C/Fortran Backends:</strong> The heavy lifting in NumPy isn’t done in Python. The core of NumPy is implemented in highly optimized C and Fortran code. When you perform an operation like <code class="language-plaintext highlighter-rouge">arr_a + arr_b</code>, NumPy dispatches this to compiled C routines, which execute much faster than Python loops.</li> <li> <strong>Contiguous Memory Layout:</strong> NumPy arrays store elements of the same data type in contiguous blocks of memory. This allows for efficient access and processing by the CPU, making the most of CPU caches.</li> </ol> <p>Even with these built-in advantages, it’s easy to inadvertently write NumPy code that doesn’t fully utilize its potential. That’s where our optimization journey begins!</p> <h3 id="1-the-vectorization-superpower-ditching-python-loops">1. The Vectorization Superpower: Ditching Python Loops</h3> <p>This is arguably the most critical optimization technique. If you’re using explicit <code class="language-plaintext highlighter-rouge">for</code> loops to iterate over NumPy arrays for element-wise operations, you’re missing out on massive performance gains.</p> <p>Let’s illustrate with a simple example: adding two arrays.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">timeit</span>

<span class="n">size</span> <span class="o">=</span> <span class="mi">10</span><span class="o">**</span><span class="mi">6</span>
<span class="n">a_list</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">size</span><span class="p">))</span>
<span class="n">b_list</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">size</span><span class="p">))</span>

<span class="n">arr_a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
<span class="n">arr_b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>

<span class="c1"># Python List Loop
</span><span class="k">def</span> <span class="nf">python_add</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="n">l2</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">x</span> <span class="o">+</span> <span class="n">y</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">l1</span><span class="p">,</span> <span class="n">l2</span><span class="p">)]</span>

<span class="c1"># NumPy Vectorized
</span><span class="k">def</span> <span class="nf">numpy_add</span><span class="p">(</span><span class="n">a1</span><span class="p">,</span> <span class="n">a2</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a1</span> <span class="o">+</span> <span class="n">a2</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Python List Loop time:</span><span class="sh">"</span><span class="p">)</span>
<span class="o">%</span><span class="n">timeit</span> <span class="nf">python_add</span><span class="p">(</span><span class="n">a_list</span><span class="p">,</span> <span class="n">b_list</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">NumPy Vectorized time:</span><span class="sh">"</span><span class="p">)</span>
<span class="o">%</span><span class="n">timeit</span> <span class="nf">numpy_add</span><span class="p">(</span><span class="n">arr_a</span><span class="p">,</span> <span class="n">arr_b</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Expected Output (yours might vary slightly):</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Python List Loop time:
10 loops, best of 5: 98.4 ms per loop

NumPy Vectorized time:
1000 loops, best of 5: 861 µs per loop
</code></pre></div></div> <p>Notice the huge difference! We’re talking about milliseconds versus microseconds – a speedup of over 100x! The vectorized operation $ \mathbf{C} = \mathbf{A} + \mathbf{B} $ where $C_i = A_i + B_i$ is handled entirely by optimized C code, bypassing Python’s slow loop interpreter.</p> <p>This principle extends to almost any element-wise operation: multiplication ($ \mathbf{A} * \mathbf{B} $), division ($ \mathbf{A} / \mathbf{B} $), exponentiation ($ \mathbf{A} ** \mathbf{B} $), and universal functions (ufuncs) like <code class="language-plaintext highlighter-rouge">np.sin()</code>, <code class="language-plaintext highlighter-rouge">np.log()</code>, etc. Always, always, always favor vectorized operations over Python loops when working with NumPy arrays.</p> <p>Even for more complex operations like matrix multiplication, NumPy provides highly optimized functions. For example, <code class="language-plaintext highlighter-rouge">np.dot(matrix_a, matrix_b)</code> or <code class="language-plaintext highlighter-rouge">matrix_a @ matrix_b</code> are vastly superior to implementing matrix multiplication with nested Python loops. The underlying BLAS (Basic Linear Algebra Subprograms) libraries are incredibly efficient.</p> <h3 id="2-broadcasting-the-silent-performer">2. Broadcasting: The Silent Performer</h3> <p>Broadcasting is a powerful mechanism in NumPy that allows it to perform operations on arrays of different shapes. The magic here is that NumPy does this <em>without making copies</em> of the smaller array to match the larger one, saving both memory and computation time.</p> <p>Think of it like this: if you have a big team (a large array) and you want to give everyone the same instructions (a scalar or smaller array), instead of writing the instructions out for each person, you just give one set of instructions and everyone understands.</p> <p><strong>Example: Adding a scalar to an array</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">arr</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">scalar</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">arr</span> <span class="o">+</span> <span class="n">scalar</span>
<span class="nf">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span> <span class="c1"># Output: [6 7 8]
</span></code></pre></div></div> <p>Here, the scalar <code class="language-plaintext highlighter-rouge">5</code> is “broadcast” across the entire <code class="language-plaintext highlighter-rouge">arr</code>. Conceptually, it’s like $ \mathbf{A} + s $ where $A_i = A_i + s$.</p> <p><strong>Example: Adding a 1D array to a 2D array</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">matrix</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
                   <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span>
                   <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]])</span>
<span class="n">row_vector</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">])</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">matrix</span> <span class="o">+</span> <span class="n">row_vector</span>
<span class="nf">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Output:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[11 22 33]
 [14 25 36]
 [17 28 39]]
</code></pre></div></div> <p>NumPy effectively stretched <code class="language-plaintext highlighter-rouge">row_vector</code> to match the rows of <code class="language-plaintext highlighter-rouge">matrix</code>, allowing the operation $ \mathbf{M} + \mathbf{v} $ to proceed element-wise. Understanding broadcasting rules is key to writing elegant and performant NumPy code without explicit reshaping or looping. When used correctly, it’s a huge time-saver.</p> <h3 id="3-choosing-the-right-tools-built-in-numpy-functions">3. Choosing the Right Tools: Built-in NumPy Functions</h3> <p>NumPy offers a vast collection of functions designed for specific mathematical and statistical operations. These functions are almost always optimized for performance, as they leverage the same C/Fortran backends we discussed.</p> <p><strong>Avoid Python’s built-in functions on NumPy arrays where a NumPy equivalent exists.</strong></p> <p>Consider summing elements:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">big_array</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">10</span><span class="o">**</span><span class="mi">7</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Python</span><span class="sh">'</span><span class="s">s sum() on NumPy array:</span><span class="sh">"</span><span class="p">)</span>
<span class="o">%</span><span class="n">timeit</span> <span class="nf">sum</span><span class="p">(</span><span class="n">big_array</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">NumPy</span><span class="sh">'</span><span class="s">s np.sum():</span><span class="sh">"</span><span class="p">)</span>
<span class="o">%</span><span class="n">timeit</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">big_array</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Expected Output:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Python's sum() on NumPy array:
10 loops, best of 5: 352 ms per loop

NumPy's np.sum():
100 loops, best of 5: 7.23 ms per loop
</code></pre></div></div> <p>Again, a massive difference! <code class="language-plaintext highlighter-rouge">sum(big_array)</code> has to convert each NumPy element back into a Python integer for addition, incurring significant overhead. <code class="language-plaintext highlighter-rouge">np.sum()</code> works directly on the underlying C array.</p> <p>The same principle applies to <code class="language-plaintext highlighter-rouge">min()</code>, <code class="language-plaintext highlighter-rouge">max()</code>, <code class="language-plaintext highlighter-rouge">len()</code>, <code class="language-plaintext highlighter-rouge">any()</code>, <code class="language-plaintext highlighter-rouge">all()</code>, etc. Always prefer <code class="language-plaintext highlighter-rouge">np.min()</code>, <code class="language-plaintext highlighter-rouge">np.max()</code>, <code class="language-plaintext highlighter-rouge">arr.size</code>, <code class="language-plaintext highlighter-rouge">np.any()</code>, <code class="language-plaintext highlighter-rouge">np.all()</code> when working with NumPy arrays.</p> <p>Furthermore, leverage specialized functions like:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">np.linalg.solve()</code> for solving linear equations (highly optimized).</li> <li> <code class="language-plaintext highlighter-rouge">np.fft.fft()</code> for Fast Fourier Transforms.</li> <li> <code class="language-plaintext highlighter-rouge">np.convolve()</code> for convolutions.</li> <li> <code class="language-plaintext highlighter-rouge">np.unique()</code>, <code class="language-plaintext highlighter-rouge">np.sort()</code>, <code class="language-plaintext highlighter-rouge">np.where()</code>, etc.</li> </ul> <p>These are written to be as efficient as possible.</p> <h3 id="4-mind-your-dtypes-memory-and-speed">4. Mind Your Dtypes: Memory and Speed</h3> <p>NumPy allows you to specify the data type (<code class="language-plaintext highlighter-rouge">dtype</code>) of elements in an array. This might seem like a small detail, but it can have a significant impact on both memory consumption and performance, especially with very large arrays.</p> <p>By default, NumPy often chooses <code class="language-plaintext highlighter-rouge">int64</code> for integers and <code class="language-plaintext highlighter-rouge">float64</code> for floating-point numbers. While safe, these might be overkill if your data doesn’t require such precision or range.</p> <p><strong>Example: Memory usage and potential speed differences</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">arr_int64</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">10</span><span class="o">**</span><span class="mi">7</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">int64</span><span class="p">)</span>
<span class="n">arr_int32</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">10</span><span class="o">**</span><span class="mi">7</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>
<span class="n">arr_int8</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">10</span><span class="o">**</span><span class="mi">7</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">int8</span><span class="p">)</span> <span class="c1"># Will overflow if range &gt; 127, for demonstration
</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Size of int64 array: </span><span class="si">{</span><span class="n">arr_int64</span><span class="p">.</span><span class="n">nbytes</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> MB</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Size of int32 array: </span><span class="si">{</span><span class="n">arr_int32</span><span class="p">.</span><span class="n">nbytes</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> MB</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Size of int8 array: </span><span class="si">{</span><span class="n">arr_int8</span><span class="p">.</span><span class="n">nbytes</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> MB</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Speed comparison (simple sum)
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Summing int64 array:</span><span class="sh">"</span><span class="p">)</span>
<span class="o">%</span><span class="n">timeit</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">arr_int64</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Summing int32 array:</span><span class="sh">"</span><span class="p">)</span>
<span class="o">%</span><span class="n">timeit</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">arr_int32</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Expected Output:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Size of int64 array: 76.29 MB
Size of int32 array: 38.15 MB
Size of int8 array: 9.54 MB

Summing int64 array:
100 loops, best of 5: 7.23 ms per loop

Summing int32 array:
100 loops, best of 5: 6.89 ms per loop
</code></pre></div></div> <p><em>(Note: Speed differences for simple operations like sum might be less pronounced due to CPU optimizations and cache effects, but for memory-bound operations or larger computations, smaller dtypes often win.)</em></p> <p>Halving the memory footprint (e.g., from <code class="language-plaintext highlighter-rouge">int64</code> to <code class="language-plaintext highlighter-rouge">int32</code>) means more data fits into your CPU’s cache, leading to fewer memory accesses and potentially faster operations. Always consider the smallest <code class="language-plaintext highlighter-rouge">dtype</code> that can safely represent your data.</p> <h3 id="5-memory-layout-and-avoiding-unnecessary-copies">5. Memory Layout and Avoiding Unnecessary Copies</h3> <p>NumPy arrays can be stored in memory in two primary ways: C-contiguous (row-major) or Fortran-contiguous (column-major). Python and C generally prefer C-contiguous arrays, while Fortran prefers Fortran-contiguous.</p> <p>Operations that respect the memory layout of an array tend to be faster because they access elements sequentially, maximizing cache efficiency. For example, iterating over rows of a C-contiguous array is fast, while iterating over columns can be slower as it “jumps” in memory.</p> <p>When you perform operations like <code class="language-plaintext highlighter-rouge">arr.T</code> (transpose), <code class="language-plaintext highlighter-rouge">arr.reshape()</code>, or slicing, NumPy often creates a <em>view</em> of the original array without copying data. This is super efficient! However, if an operation requires a contiguous block of memory but the view isn’t contiguous in the required way, NumPy might make a <em>copy</em>.</p> <p>Functions like <code class="language-plaintext highlighter-rouge">arr.flatten()</code> always return a new C-contiguous array, while <code class="language-plaintext highlighter-rouge">arr.ravel()</code> returns a view if possible, otherwise a copy. Be mindful of when copies are made, especially with very large arrays, as they consume memory and CPU cycles. Use <code class="language-plaintext highlighter-rouge">arr.flags['C_CONTIGUOUS']</code> or <code class="language-plaintext highlighter-rouge">arr.flags['F_CONTIGUOUS']</code> to check. If you <em>need</em> a copy, explicitly call <code class="language-plaintext highlighter-rouge">arr.copy()</code>.</p> <h3 id="6-measuring-what-matters-using-timeit">6. Measuring What Matters: Using <code class="language-plaintext highlighter-rouge">%timeit</code> </h3> <p>You can’t optimize what you don’t measure. The <code class="language-plaintext highlighter-rouge">%timeit</code> magic command (available in IPython/Jupyter notebooks) is your best friend for profiling small snippets of code.</p> <p>It runs your code multiple times and provides the mean and standard deviation of the execution time, giving you a robust measure of performance.</p> <p><strong>How to use:</strong></p> <ul> <li> <code class="language-plaintext highlighter-rouge">%timeit &lt;statement&gt;</code>: For single-line statements.</li> <li> <code class="language-plaintext highlighter-rouge">%%timeit</code>: For multi-line code blocks (place at the beginning of the cell).</li> </ul> <p>Always use <code class="language-plaintext highlighter-rouge">%timeit</code> to compare different approaches and verify your optimizations. Sometimes, what you <em>think</em> is faster might not be in reality.</p> <h3 id="a-word-of-caution-when-not-to-optimize">A Word of Caution: When Not to Optimize</h3> <p>While optimization is powerful, remember the adage: “Premature optimization is the root of all evil.”</p> <ol> <li> <strong>Readability First:</strong> Write clear, understandable code first.</li> <li> <strong>Profile:</strong> Only optimize bottlenecks – the parts of your code that are actually slowing things down. Don’t spend hours optimizing a function that contributes only 1% to your total runtime.</li> <li> <strong>Correctness:</strong> Ensure your optimized code still produces the correct results! Speed without correctness is useless.</li> </ol> <p>Focus on getting your code working, identify the slow parts using profiling tools, and <em>then</em> apply these NumPy optimization techniques.</p> <h3 id="conclusion-your-journey-to-faster-code">Conclusion: Your Journey to Faster Code</h3> <p>You’ve now got a solid toolkit for making your NumPy code sing! We’ve covered:</p> <ul> <li> <strong>Vectorization:</strong> The golden rule – ditch Python loops for NumPy’s optimized operations.</li> <li> <strong>Broadcasting:</strong> Performing operations on differently shaped arrays efficiently without copying.</li> <li> <strong>Built-in Functions:</strong> Leveraging NumPy’s optimized functions over Python’s general-purpose ones.</li> <li> <strong>Dtypes:</strong> Choosing the right data types to save memory and potentially boost speed.</li> <li> <strong>Memory Layout:</strong> Understanding how data is stored and avoiding unnecessary copies.</li> <li> <strong><code class="language-plaintext highlighter-rouge">%timeit</code>:</strong> The essential tool for measuring and validating your optimizations.</li> </ul> <p>Mastering these techniques will not only make your data science projects run faster but also deepen your understanding of how powerful tools like NumPy work. So go forth, experiment, profile, and transform your slow-motion computations into lightning-fast operations!</p> <p>Happy coding, and may your arrays always be optimized!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>