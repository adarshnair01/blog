<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Neural Network with a Memory: Unraveling Recurrent Neural Networks | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/2024/the-neural-network-with-a-memory-unraveling-recurr/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Neural Network with a Memory: Unraveling Recurrent Neural Networks</h1> <p class="post-meta"> Created on July 06, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/recurrent-neural-networks"> <i class="fa-solid fa-hashtag fa-sm"></i> Recurrent Neural Networks</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> NLP</a>   <a href="/blog/blog/tag/sequence-models"> <i class="fa-solid fa-hashtag fa-sm"></i> Sequence Models</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey there, fellow data explorers!</p> <p>Have you ever wondered how your phone’s keyboard predicts your next word, or how Google Translate instantly converts an entire sentence from one language to another, maintaining context? It’s not magic, it’s a specific kind of neural network that truly understands sequences, not just individual points of data. Today, I want to pull back the curtain on one of the most elegant and powerful architectures in the deep learning world: Recurrent Neural Networks, or RNNs.</p> <p>Think of it this way: traditional neural networks, like the ones you might have seen that classify images, are a bit like someone with short-term amnesia. They look at an image, make a prediction, and then completely forget about it before looking at the next image. Each input is a fresh start, an independent event. And for many tasks, this works beautifully! But what if the order of things matters? What if understanding the past is crucial for interpreting the present?</p> <h3 id="the-problem-with-short-term-memory">The Problem with Short-Term Memory</h3> <p>Let’s say we’re trying to predict the next word in a sentence: “The cat sat on the…”</p> <p>A standard feedforward neural network would take “The cat sat on the” as input, process it, and try to guess the next word. But here’s the catch: it treats each word <em>independently</em> when it comes to processing it for context. It struggles to truly understand the <em>flow</em> and <em>relationship</em> between words over time. If the sentence was “As the sun set, the sky turned a brilliant orange. The view was absolutely…”, a traditional network might struggle to connect “view” back to “sun set” and “sky turned orange” for a truly meaningful prediction. It lacks a persistent memory.</p> <p>This “memory problem” isn’t just about text. Imagine trying to predict stock prices without considering historical trends, or trying to understand spoken language one isolated phoneme at a time. The world, more often than not, unfolds in sequences – time series data, video frames, audio clips, sentences. And to process these sequences effectively, our neural networks need a way to remember.</p> <h3 id="enter-recurrent-neural-networks-the-memory-keepers">Enter Recurrent Neural Networks: The Memory Keepers</h3> <p>This is where RNNs stride onto the stage, ready to tackle the challenge. The core idea behind an RNN is wonderfully simple, yet incredibly profound: <strong>it feeds its own output back into itself as an input for the next step.</strong></p> <p>Imagine our “amnesiac” neural network. An RNN gives it a notebook. After processing the first input, it writes down some key takeaways in its notebook. Then, when it processes the second input, it not only looks at the new input but also consults its notebook from the previous step. This “notebook” is what we call the <strong>hidden state</strong>.</p> <p>Let’s visualize this. Instead of a single, straight-through path, an RNN has a loop:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input (x_t) -&gt; RNN Cell -&gt; Output (y_t)
                  ^
                  | (fed back)
                  |
             Hidden State (h_{t-1})
</code></pre></div></div> <p>This diagram shows the RNN cell at a single time step $t$. The input $x_t$ comes in, and the <em>previous</em> hidden state $h_{t-1}$ also comes in. These two pieces of information are combined to produce the <em>current</em> hidden state $h_t$ and potentially an output $y_t$. The $h_t$ then becomes $h_{t-1}$ for the <em>next</em> time step.</p> <h3 id="unrolling-the-loop-seeing-the-sequence">Unrolling the Loop: Seeing the Sequence</h3> <p>To better understand how an RNN processes a sequence, we “unroll” the loop over time. If we have a sequence of inputs $x_1, x_2, \dots, x_T$, the RNN processes them step-by-step:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>x_1 ----&gt; RNN Cell ----&gt; h_1 ----&gt; y_1
             ^            |
             |            |
             |           h_0 (initial state)
             |
x_2 ----&gt; RNN Cell ----&gt; h_2 ----&gt; y_2
             ^            |
             |            |
             |           h_1 (from previous step)
             |
...          ...          ...          ...
             |
x_t ----&gt; RNN Cell ----&gt; h_t ----&gt; y_t
             ^            |
             |            |
             |           h_{t-1} (from previous step)
</code></pre></div></div> <p>Notice a critical point here: the “RNN Cell” (the weights and biases within it) are the <em>same</em> at every time step. This is what allows RNNs to learn patterns that occur across different positions in a sequence. It’s like applying the same rule or understanding across the entire story, rather than learning a new rule for each sentence. This parameter sharing is incredibly powerful and efficient.</p> <h3 id="the-math-behind-the-memory">The Math Behind the Memory</h3> <p>Let’s peek under the hood of that RNN cell. At each time step $t$, the hidden state $h_t$ is computed based on the current input $x_t$ and the previous hidden state $h_{t-1}$. The equations for a simple RNN unit look something like this:</p> <ol> <li> <p><strong>Updating the Hidden State:</strong> $h_t = \text{tanh}(W_{hh}h_{t-1} + W_{xh}x_t + b_h)$</p> <ul> <li>$x_t$: The input at the current time step (e.g., a word embedding).</li> <li>$h_{t-1}$: The hidden state from the previous time step – this is the “memory.”</li> <li>$W_{xh}$: Weight matrix for the input $x_t$. This learns how to transform the current input into a contribution to the hidden state.</li> <li>$W_{hh}$: Weight matrix for the previous hidden state $h_{t-1}$. This learns how to transform the “memory” from the past into the current hidden state.</li> <li>$b_h$: Bias vector for the hidden state.</li> <li>$\text{tanh}$: An activation function (like sigmoid or ReLU) that introduces non-linearity, allowing the network to learn complex patterns.</li> </ul> </li> <li> <p><strong>Generating an Output (Optional, at each step):</strong> $y_t = W_{hy}h_t + b_y$</p> <ul> <li>$y_t$: The output at the current time step (e.g., the predicted next word, or a sentiment score).</li> <li>$W_{hy}$: Weight matrix for the hidden state $h_t$. This learns how to transform the current hidden state into the desired output.</li> <li>$b_y$: Bias vector for the output.</li> </ul> </li> </ol> <p>The beauty of this is that the weights ($W_{xh}, W_{hh}, W_{hy}$) and biases ($b_h, b_y$) are <em>shared across all time steps</em>. This is a crucial concept for sequence processing, as it means the RNN learns a single set of parameters that can apply to any part of a sequence, regardless of its position.</p> <h3 id="the-vanishing-gradient-problem-when-memory-fails">The Vanishing Gradient Problem: When Memory Fails</h3> <p>While simple RNNs are elegant, they have a significant Achilles’ heel: the <strong>vanishing/exploding gradient problem</strong>.</p> <p>Imagine trying to remember a detail from the very beginning of a long movie to understand an event at the end. As the movie progresses, and new scenes are processed, that initial detail might get fainter and fainter until it’s effectively forgotten. In RNNs, this is due to how gradients (the signals that tell the network how to adjust its weights during training) are propagated backward through time.</p> <ul> <li> <strong>Vanishing Gradients:</strong> If the gradients become very small, the network struggles to learn long-range dependencies. The updates to the weights corresponding to earlier time steps become negligible, making it hard to “remember” information from the distant past. It’s like playing a game of “telephone” over a very long line – the original message gets lost.</li> <li> <strong>Exploding Gradients:</strong> Conversely, if gradients become too large, the network’s weights can become unstable, leading to erratic training.</li> </ul> <p>This limitation meant that simple RNNs struggled with tasks requiring a very long-term memory, like understanding complex narratives or long pieces of code.</p> <h3 id="the-gates-of-memory-lstms-and-grus">The Gates of Memory: LSTMs and GRUs</h3> <p>To solve the vanishing gradient problem and allow RNNs to learn much longer dependencies, researchers developed more sophisticated variants, the most famous being <strong>Long Short-Term Memory (LSTM) networks</strong> and <strong>Gated Recurrent Units (GRUs)</strong>.</p> <p>The core idea behind LSTMs and GRUs is the introduction of “gates.” Think of these gates as intelligent filters that control the flow of information into and out of the hidden state (or, in the case of LSTMs, an additional “cell state”).</p> <ul> <li> <strong>Forget Gate:</strong> Decides what information from the previous cell state should be thrown away or kept. Is that old detail still relevant, or can we discard it?</li> <li> <strong>Input Gate:</strong> Decides what new information from the current input is important and should be stored in the cell state.</li> <li> <strong>Output Gate:</strong> Decides what parts of the cell state should be outputted at the current time step.</li> </ul> <p>By selectively remembering and forgetting, these gates allow LSTMs and GRUs to maintain a more stable memory over much longer sequences, effectively mitigating the vanishing gradient problem. While the internal math is more complex (involving multiple sigmoid and tanh activations), the <em>concept</em> is about carefully managing information flow. GRUs are a slightly simplified version of LSTMs, often offering a good balance of performance and computational efficiency.</p> <h3 id="where-do-rnns-shine-real-world-applications">Where Do RNNs Shine? Real-World Applications</h3> <p>RNNs, especially their gated variants (LSTMs and GRUs), have revolutionized many fields:</p> <ul> <li> <strong>Natural Language Processing (NLP):</strong> <ul> <li> <strong>Machine Translation:</strong> Translating sentences from one language to another (e.g., Google Translate).</li> <li> <strong>Text Generation:</strong> Writing stories, poems, or even code.</li> <li> <strong>Sentiment Analysis:</strong> Determining the emotional tone of text.</li> <li> <strong>Speech Recognition:</strong> Converting spoken words into text.</li> </ul> </li> <li> <strong>Time Series Prediction:</strong> Forecasting stock prices, weather patterns, or energy consumption.</li> <li> <strong>Video Analysis:</strong> Describing actions in videos, detecting events.</li> <li> <strong>Music Generation:</strong> Creating new melodies or harmonies.</li> </ul> <p>They are the backbone of many “smart” features we use daily.</p> <h3 id="the-evolving-landscape-beyond-rnns">The Evolving Landscape: Beyond RNNs</h3> <p>It would be remiss not to mention that while RNNs (especially LSTMs and GRUs) have been incredibly impactful, the field of sequence modeling is always evolving. More recently, architectures like <strong>Transformers</strong> have gained immense popularity, particularly in NLP, often outperforming RNNs on very long sequences and benefiting from parallel processing capabilities that RNNs inherently struggle with due to their sequential nature. However, RNNs remain a fundamental and powerful building block in deep learning, especially for real-time applications or when computational resources are constrained.</p> <h3 id="wrapping-up">Wrapping Up</h3> <p>Recurrent Neural Networks represent a pivotal step in enabling AI to understand the world as an unfolding narrative rather than a series of disconnected snapshots. By introducing the concept of memory into neural networks, they unlocked the potential to process sequential data with unprecedented accuracy and nuance.</p> <p>From predicting your next word to translating languages, RNNs have undeniably shaped the landscape of modern AI. Understanding their core mechanism – the elegant loop of feeding information back into itself – is a fundamental insight into how we build intelligent systems that can truly learn from the past to make sense of the present and predict the future.</p> <p>The journey of deep learning is one of continuous innovation, and RNNs are a beautiful testament to the power of a simple, yet profound, idea. Keep exploring, keep building, and remember that even the most complex AI often starts with a clever tweak to a foundational concept!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>