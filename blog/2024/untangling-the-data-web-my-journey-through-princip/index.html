<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Untangling the Data Web: My Journey Through Principal Component Analysis (PCA) | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/untangling-the-data-web-my-journey-through-princip/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Untangling the Data Web: My Journey Through Principal Component Analysis (PCA)</h1> <p class="post-meta"> Created on July 01, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/dimensionality-reduction"> <i class="fa-solid fa-hashtag fa-sm"></i> Dimensionality Reduction</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/linear-algebra"> <i class="fa-solid fa-hashtag fa-sm"></i> Linear Algebra</a>   <a href="/blog/blog/tag/pca"> <i class="fa-solid fa-hashtag fa-sm"></i> PCA</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="untangling-the-data-web-my-journey-through-principal-component-analysis-pca">Untangling the Data Web: My Journey Through Principal Component Analysis (PCA)</h2> <p>Imagine your room. It’s filled with everything you own – clothes, books, gadgets, souvenirs. Now imagine you need to quickly describe its essence to someone who’s never seen it, without listing every single item. You’d probably talk about the <em>main things</em>: “It’s a cozy study room with a lot of books,” or “It’s a minimalist bedroom with a large window.” You’re performing a kind of mental <em>dimensionality reduction</em> – focusing on the most important, descriptive aspects and letting go of the less critical details.</p> <p>This, in a nutshell, is the intuitive spirit behind Principal Component Analysis (PCA), one of the most fundamental and elegant techniques in a data scientist’s toolkit. When I first encountered PCA, the sheer volume of new terms – eigenvalues, eigenvectors, covariance matrices – felt like staring at a complex knot. But as I peeled back the layers, I realized it’s not just a collection of intimidating formulas; it’s a brilliant, intuitive approach to understanding and simplifying complex data. And today, I want to share that journey with you.</p> <h3 id="the-elephant-in-the-room-the-curse-of-dimensionality">The Elephant in the Room: The Curse of Dimensionality</h3> <p>In the world of data, we often face a problem far more daunting than a messy room: the “curse of dimensionality.” Imagine a dataset with hundreds, or even thousands, of features (columns). Each feature represents a dimension.</p> <ul> <li>A dataset with just two features is easy to visualize on a 2D plot.</li> <li>Three features? We can use a 3D plot.</li> <li>But what about 10, 50, or 100 features? Our human brains, and even our powerful algorithms, start to struggle.</li> </ul> <p>Why is high dimensionality a curse?</p> <ol> <li> <strong>Computational Cost:</strong> More features mean more calculations, slowing down models.</li> <li> <strong>Increased Noise:</strong> Many features might be redundant, correlated, or simply noise, obscuring true patterns.</li> <li> <strong>Data Sparsity:</strong> In high dimensions, data points become incredibly sparse, making it harder for algorithms to find meaningful relationships.</li> <li> <strong>Visualization Nightmare:</strong> You can’t visualize data in more than three dimensions, making exploratory data analysis a challenge.</li> </ol> <p>This is where PCA steps in, offering a graceful solution by reducing the number of features while retaining as much valuable information as possible.</p> <h3 id="pca-at-its-core-finding-the-best-shadow-of-your-data">PCA at its Core: Finding the Best “Shadow” of Your Data</h3> <p>Think of PCA as finding a new perspective on your data. Instead of describing your data using its original features (e.g., “height,” “weight,” “age”), PCA finds new, synthetic features called <strong>Principal Components</strong> (PCs). These PCs are ordered by how much variance they capture from the original data.</p> <p>The first principal component (PC1) is the direction in your data along which there is the <em>most variance</em> (the most spread). Imagine shining a flashlight on a 3D object to cast a 2D shadow. PCA tries to find the angle of the flashlight that creates the “most informative” shadow – the one that best preserves the object’s original shape and spread.</p> <p>The second principal component (PC2) is the direction of next highest variance, but with a crucial constraint: it must be <em>orthogonal</em> (perpendicular) to the first principal component. This orthogonality ensures that the principal components capture independent information about the data. If you have many dimensions, you can find PC3, PC4, and so on, each orthogonal to the previous ones.</p> <p>By selecting only the top <code class="language-plaintext highlighter-rouge">k</code> principal components (where <code class="language-plaintext highlighter-rouge">k</code> is much smaller than the original number of features), we effectively project our high-dimensional data onto a lower-dimensional subspace, preserving the most crucial information.</p> <h3 id="the-intuition-behind-variance-why-it-matters">The Intuition Behind Variance: Why It Matters</h3> <p>When we say “variance,” what do we mean? In statistics, variance measures how spread out a set of data points are around their mean. A high variance means the data points are widely dispersed, indicating a lot of “information” or “distinction” among the samples. Low variance means data points are clustered closely, suggesting less unique information along that direction.</p> <p>PCA’s goal is to find directions (our principal components) where the data exhibits the most spread. Why? Because these directions are where our data points are most distinguishable from one another. If we project our data onto a direction with low variance, all points would appear squashed together, losing most of their individual identity. By maximizing variance, we ensure that the separation and structure within our data are best preserved in the reduced dimension.</p> <h3 id="the-math-under-the-hood-eigen-magic">The Math Under the Hood: Eigen-Magic!</h3> <p>Alright, let’s peek under the hood. Don’t worry, we’ll keep it conceptual and focus on understanding <em>what</em> the math achieves, rather than getting bogged down in every algebraic step. The heroes of PCA are <strong>eigenvalues</strong> and <strong>eigenvectors</strong>, concepts from linear algebra.</p> <h4 id="1-the-covariance-matrix-unveiling-relationships">1. The Covariance Matrix: Unveiling Relationships</h4> <p>Before we get to eigenvectors, we need to understand the <strong>covariance matrix</strong>. If you have a dataset with <code class="language-plaintext highlighter-rouge">n</code> features, the covariance matrix is an <code class="language-plaintext highlighter-rouge">n x n</code> square matrix that summarizes the relationships between all pairs of features.</p> <ul> <li>The diagonal elements show the variance of each individual feature (how spread out that feature’s values are).</li> <li>The off-diagonal elements show the covariance between pairs of features. <ul> <li>Positive covariance means they tend to increase or decrease together.</li> <li>Negative covariance means one tends to increase while the other decreases.</li> <li>Near-zero covariance means they have little linear relationship.</li> </ul> </li> </ul> <p>The covariance matrix essentially paints a picture of how our features relate to each other and how they vary. It’s critical because PCA searches for directions that capture the maximum variance, and variance <em>between features</em> is exactly what the covariance matrix quantifies.</p> <p>For a dataset $X$ with $m$ samples and $n$ features, after centering the data (subtracting the mean from each feature), the covariance matrix $C$ can be calculated as: $C = \frac{1}{m-1} X^T X$</p> <h4 id="2-eigenvalues-and-eigenvectors-the-directions-of-maximum-variance">2. Eigenvalues and Eigenvectors: The Directions of Maximum Variance</h4> <p>Now for the main event! Eigenvectors are special vectors that, when a linear transformation (like multiplying by our covariance matrix) is applied to them, only get scaled, not changed in direction. The scaling factor is called the <strong>eigenvalue</strong>.</p> <p>Mathematically, for a square matrix $A$, a vector $v$ is an eigenvector if: $Av = \lambda v$ where $v$ is the eigenvector and $\lambda$ is its corresponding eigenvalue.</p> <p>In the context of PCA:</p> <ul> <li>The <strong>eigenvectors</strong> of the covariance matrix are our <strong>principal components</strong>. They are the directions in the feature space along which the data varies the most.</li> <li>The <strong>eigenvalues</strong> tell us the <em>magnitude</em> of that variance along each eigenvector. A larger eigenvalue means its corresponding eigenvector (principal component) captures more variance from the original data.</li> </ul> <p>This is the “aha!” moment. By finding the eigenvectors of the covariance matrix, we inherently find the directions that maximize variance. And by ordering them by their eigenvalues, we get our principal components ranked by their importance!</p> <h3 id="the-pca-algorithm-a-step-by-step-blueprint">The PCA Algorithm: A Step-by-Step Blueprint</h3> <p>Let’s break down the practical steps to perform PCA:</p> <ol> <li> <p><strong>Standardize the Data:</strong> PCA is sensitive to the scale of your features. If one feature ranges from 0 to 1000 and another from 0 to 1, the feature with the larger range will dominate the variance calculation, potentially skewing the principal components. Therefore, it’s crucial to standardize your data by scaling each feature to have a mean of 0 and a standard deviation of 1 (Z-score normalization). $z = \frac{x - \mu}{\sigma}$ where $\mu$ is the mean and $\sigma$ is the standard deviation of the feature.</p> </li> <li> <p><strong>Compute the Covariance Matrix:</strong> As we discussed, calculate the covariance matrix for your standardized data. This matrix will be <code class="language-plaintext highlighter-rouge">n x n</code>, where <code class="language-plaintext highlighter-rouge">n</code> is the number of features.</p> </li> <li> <p><strong>Calculate Eigenvalues and Eigenvectors:</strong> Find the eigenvalues and eigenvectors of the covariance matrix. This is typically done using numerical methods provided by libraries like NumPy in Python (e.g., <code class="language-plaintext highlighter-rouge">np.linalg.eig</code>).</p> </li> <li> <strong>Sort and Select Principal Components:</strong> You’ll get <code class="language-plaintext highlighter-rouge">n</code> eigenvectors and <code class="language-plaintext highlighter-rouge">n</code> corresponding eigenvalues. Sort them in descending order based on their eigenvalues. The eigenvector with the largest eigenvalue is PC1, the next largest is PC2, and so on. Decide how many principal components (<code class="language-plaintext highlighter-rouge">k</code>) you want to keep. You can do this by: <ul> <li>Choosing a fixed number (e.g., 2 for visualization).</li> <li>Keeping components that explain a certain cumulative percentage of variance (e.g., 95% of total variance). You can calculate the “explained variance ratio” for each component: $ \text{Explained Variance Ratio}<em>i = \frac{\lambda_i}{\sum</em>{j=1}^n \lambda_j} $</li> </ul> </li> <li> <strong>Project Data onto New Subspace:</strong> Form a “projection matrix” (also called a “feature vector” or “weights matrix”) <code class="language-plaintext highlighter-rouge">W</code> using the top <code class="language-plaintext highlighter-rouge">k</code> eigenvectors. This matrix will have dimensions <code class="language-plaintext highlighter-rouge">n x k</code>. Finally, transform your original standardized data <code class="language-plaintext highlighter-rouge">X_scaled</code> into the new <code class="language-plaintext highlighter-rouge">k</code>-dimensional space: $ Y = X_{scaled} W $ Where <code class="language-plaintext highlighter-rouge">Y</code> is your new dataset with <code class="language-plaintext highlighter-rouge">k</code> principal components, and it has dimensions <code class="language-plaintext highlighter-rouge">m x k</code> (m samples, k features).</li> </ol> <p>Voila! You now have a lower-dimensional representation of your data, where the new features (principal components) are uncorrelated and capture the most significant variance.</p> <h3 id="why-is-pca-so-powerful-use-cases">Why is PCA So Powerful? Use Cases</h3> <p>PCA isn’t just a theoretical exercise; it’s a workhorse in data science and machine learning:</p> <ol> <li> <strong>Dimensionality Reduction:</strong> The most obvious benefit. Reduces data storage, speeds up training times for models, and can often improve model performance by reducing noise.</li> <li> <strong>Noise Reduction:</strong> Components with very small eigenvalues often capture random noise in the data. By discarding these components, PCA can act as a de-noising technique.</li> <li> <strong>Visualization:</strong> Perhaps one of the most beloved applications. Reducing data to 2 or 3 principal components allows us to plot high-dimensional data and visually identify clusters, outliers, or patterns that were previously hidden.</li> <li> <strong>Feature Engineering/Extraction:</strong> PCA creates entirely new features that are linear combinations of the original ones. These new features (principal components) are uncorrelated, which can be beneficial for certain machine learning algorithms that assume independence (like Naive Bayes or linear regression).</li> <li> <strong>Preprocessing for Machine Learning:</strong> Often used as a preprocessing step before feeding data into a classifier, regressor, or clustering algorithm.</li> </ol> <h3 id="limitations-and-considerations">Limitations and Considerations</h3> <p>While powerful, PCA isn’t a silver bullet:</p> <ul> <li> <strong>Linearity Assumption:</strong> PCA assumes that the principal components are linear combinations of the original features. It won’t work well if the underlying structure of the data is non-linear (e.g., data points arranged on a curved manifold). For such cases, non-linear dimensionality reduction techniques (like t-SNE or UMAP) might be more appropriate.</li> <li> <strong>Interpretability:</strong> Principal components are abstract linear combinations of the original features. For example, PC1 might be <code class="language-plaintext highlighter-rouge">0.3 * feature_A + 0.6 * feature_B - 0.1 * feature_C</code>. This can make interpreting the meaning of the reduced dimensions challenging, especially for stakeholders who prefer direct explanations of original features.</li> <li> <strong>Information Loss:</strong> By reducing dimensions, you <em>are</em> discarding some information. The key is to discard the <em>least important</em> information (the variance not explained by the top components). The choice of <code class="language-plaintext highlighter-rouge">k</code> (number of components to keep) is crucial.</li> <li> <strong>Scaling is Key:</strong> As mentioned, if you forget to standardize your data, features with larger scales will disproportionately influence the principal components.</li> </ul> <h3 id="conclusion-embracing-simplicity-in-complexity">Conclusion: Embracing Simplicity in Complexity</h3> <p>My journey into PCA taught me the profound beauty of simplifying complexity. It’s a reminder that sometimes, to truly understand something intricate, you need to step back, find the most impactful angles, and disregard the extraneous details. PCA empowers us to transform a daunting maze of high-dimensional data into a clear, understandable landscape, revealing patterns and insights that would otherwise remain hidden.</p> <p>So, the next time you’re faced with a dataset that feels overwhelmingly large or complex, remember PCA. It’s not just a mathematical trick; it’s a testament to how elegant linear algebra can be in solving real-world problems, helping us untangle the data web and make sense of the information that surrounds us. Go forth, experiment, and let PCA illuminate the hidden structures in your data!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>