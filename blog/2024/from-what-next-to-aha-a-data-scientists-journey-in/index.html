<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> From "What Next?" to "Aha!": A Data Scientist's Journey into Recommender Systems | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/2024/from-what-next-to-aha-a-data-scientists-journey-in/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">From "What Next?" to "Aha!": A Data Scientist's Journey into Recommender Systems</h1> <p class="post-meta"> Created on August 13, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/recommender-systems"> <i class="fa-solid fa-hashtag fa-sm"></i> Recommender Systems</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/collaborative-filtering"> <i class="fa-solid fa-hashtag fa-sm"></i> Collaborative Filtering</a>   <a href="/blog/blog/tag/content-based-filtering"> <i class="fa-solid fa-hashtag fa-sm"></i> Content-Based Filtering</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>As a budding data scientist, there’s a particular kind of magic that always captivates me: the ability for machines to <em>understand</em> us, to anticipate our desires, and to guide us through an overwhelming sea of choices. I remember countless evenings spent scrolling through endless movie titles, the decision fatigue setting in before I’d even pressed play. Or the frustration of trying to find new music that truly resonated with my niche tastes.</p> <p>Then, something shifted. Netflix started recommending documentaries I never knew existed but instantly loved. Spotify began suggesting artists that felt like they’d been plucked straight from my personal thoughts. Suddenly, the digital world felt less like a vast, intimidating ocean and more like a curated gallery just for me. This wasn’t magic, of course. It was the sophisticated, invisible hand of <strong>Recommender Systems</strong> at work.</p> <p>In this post, I want to take you on a journey through the fundamental ideas behind these systems. We’ll peel back the layers of these algorithms, from the intuitive to the complex, and see how they transform raw data into personalized insights. So, grab a drink, settle in, and let’s explore how data helps answer that eternal question: “What should I watch/read/listen to/buy next?”</p> <hr> <h3 id="what-are-recommender-systems-anyway">What ARE Recommender Systems, Anyway?</h3> <p>At their core, recommender systems are algorithms designed to predict the “preference” or “rating” a user would give to an item. Their goal is simple: help users discover items they might like, or even love, based on their past behavior and the behavior of others.</p> <p>Think about it:</p> <ul> <li> <strong>Netflix, Hulu, YouTube:</strong> Recommend movies, TV shows, and videos.</li> <li> <strong>Amazon, eBay:</strong> Suggest products you might want to buy.</li> <li> <strong>Spotify, Apple Music:</strong> Curate playlists and discover new artists.</li> <li> <strong>TikTok, Instagram:</strong> Show you content you’ll likely engage with.</li> <li> <strong>Google News, Apple News:</strong> Personalize your news feed.</li> </ul> <p>The “why” behind their ubiquity is clear: they enhance user experience, drive engagement, and significantly boost sales or consumption. They turn information overload into personalized discovery.</p> <hr> <h3 id="the-core-philosophies-how-do-they-work">The Core Philosophies: How Do They Work?</h3> <p>While the implementations can get incredibly complex, most recommender systems build upon a few core principles. Let’s start with the two big ones: Content-Based Filtering and Collaborative Filtering.</p> <h4 id="1-content-based-filtering-if-you-liked-this-youll-like-that">1. Content-Based Filtering: “If you liked this, you’ll like that!”</h4> <p>Imagine you love sci-fi movies, especially those with time travel and witty dialogue. A content-based recommender system would analyze the <em>features</em> of movies you’ve enjoyed (genre: sci-fi, subgenre: time travel, keywords: witty, humor, paradox) and then look for other movies sharing those same characteristics.</p> <p><strong>How it works:</strong></p> <ol> <li> <strong>Item Representation:</strong> Each item (movie, song, article) is described by its attributes or “features.” For a movie, these could be genre, actors, director, keywords, plot summary. We often turn these features into a numerical vector.</li> <li> <strong>User Profile Creation:</strong> The system builds a profile for each user based on the features of items they have previously liked, rated highly, or interacted positively with. If you loved “Back to the Future” and “Looper,” your profile would emphasize “sci-fi,” “time travel,” and “adventure.”</li> <li> <strong>Similarity Matching:</strong> When recommending, the system compares the user’s profile vector to the item feature vectors of all available items. The items most “similar” to the user’s preferences are then recommended.</li> </ol> <p>A common way to measure similarity between these feature vectors is <strong>Cosine Similarity</strong>. If you imagine each item and user profile as points in a multi-dimensional space, cosine similarity measures the cosine of the angle between these vectors. A smaller angle (cosine closer to 1) means higher similarity.</p> <p>Mathematically, for two vectors $\mathbf{A}$ and $\mathbf{B}$ (representing an item’s features or a user’s profile):</p> \[\cos(\theta) = \frac{\mathbf{A} \cdot \mathbf{B}}{||\mathbf{A}|| \cdot ||\mathbf{B}||}\] <p>Where:</p> <ul> <li>$\mathbf{A} \cdot \mathbf{B}$ is the dot product of the vectors.</li> <li> <table> <tbody> <tr> <td>$</td> <td> </td> <td>\mathbf{A}</td> <td> </td> <td>$ and $</td> <td> </td> <td>\mathbf{B}</td> <td> </td> <td>$ are the magnitudes (lengths) of the vectors.</td> </tr> </tbody> </table> </li> </ul> <p><strong>Pros:</strong></p> <ul> <li> <strong>Explainable:</strong> Recommendations are easy to justify (“You liked X because it has similar actors/genres to Y”).</li> <li> <strong>Handles new items:</strong> Can recommend new items as long as their features are known, even if no one has interacted with them yet.</li> <li> <strong>User-specific:</strong> Recommendations are tailored to an individual’s unique taste.</li> </ul> <p><strong>Cons:</strong></p> <ul> <li> <strong>Limited Serendipity:</strong> Tends to recommend items very similar to what a user already likes, making it hard to discover truly new categories. This is the “echo chamber” or “filter bubble” problem.</li> <li> <strong>Feature Engineering:</strong> Requires rich, well-structured metadata for items, which can be labor-intensive to create and maintain.</li> <li> <strong>Cold Start (for new users):</strong> If a new user hasn’t interacted with enough items, the system can’t build an accurate profile.</li> </ul> <hr> <h4 id="2-collaborative-filtering-people-like-you-liked-this">2. Collaborative Filtering: “People like you liked this!”</h4> <p>This approach is different. Instead of looking at item features, it leverages the collective wisdom of the crowd. The core idea: if two users share similar tastes in the past, they will likely have similar tastes in the future. Or, if two items are often liked by the same people, they are similar.</p> <p>There are two main flavors of collaborative filtering:</p> <h5 id="a-user-based-collaborative-filtering">a) User-Based Collaborative Filtering</h5> <p>This is the most intuitive. Imagine you and your friend, Sarah, have very similar movie tastes. If Sarah recently watched and loved a movie you haven’t seen, a user-based system would recommend it to you.</p> <p><strong>How it works:</strong></p> <ol> <li> <strong>Find Similar Users:</strong> The system identifies users who have rated or interacted with items similarly to you. This is often done by looking at a “user-item interaction matrix,” where rows are users and columns are items, with entries being ratings or implicit feedback (e.g., watched, clicked).</li> <li> <strong>Aggregate Preferences:</strong> Once similar users are found, the system looks at items <em>they</em> liked but <em>you</em> haven’t yet interacted with.</li> <li> <strong>Recommend:</strong> The items most highly rated by your “neighboring” users are recommended to you.</li> </ol> <p>Similarity between users can be calculated using metrics like Cosine Similarity (on their rating vectors) or <strong>Pearson Correlation Coefficient</strong>, which measures the linear relationship between two users’ ratings, taking into account their average rating.</p> <p>For two users, $u$ and $v$, who have both rated a set of items $I_{uv}$:</p> \[r_{uv} = \frac{\sum_{i \in I_{uv}} (R_{ui} - \bar{R}_u)(R_{vi} - \bar{R}_v)}{\sqrt{\sum_{i \in I_{uv}} (R_{ui} - \bar{R}_u)^2}\sqrt{\sum_{i \in I_{uv}} (R_{vi} - \bar{R}_v)^2}}\] <p>Where:</p> <ul> <li>$R_{ui}$ is user $u$’s rating for item $i$.</li> <li>$\bar{R}_u$ is user $u$’s average rating.</li> </ul> <p><strong>Pros:</strong></p> <ul> <li> <strong>Serendipity:</strong> Can recommend items you’d never discover through content-based methods because it relies on the diverse tastes of others.</li> <li> <strong>No item features needed:</strong> Doesn’t require manual feature engineering for items.</li> </ul> <p><strong>Cons:</strong></p> <ul> <li> <strong>Scalability:</strong> Finding similar users among millions can be computationally expensive.</li> <li> <strong>Sparsity:</strong> Most user-item matrices are very sparse (users only interact with a tiny fraction of items), making it hard to find enough overlapping items to calculate similarity accurately.</li> <li> <strong>Cold Start (new users/items):</strong> New users have no past interactions, and new items have no ratings, making it impossible to find similar users or include them in recommendations.</li> </ul> <h5 id="b-item-based-collaborative-filtering">b) Item-Based Collaborative Filtering</h5> <p>This approach flips the script: instead of finding similar users, it finds similar <em>items</em>. If you liked “Harry Potter and the Sorcerer’s Stone,” an item-based system would recommend “Harry Potter and the Chamber of Secrets” not because of its content features, but because people who liked the first book also tended to like the second.</p> <p><strong>How it works:</strong></p> <ol> <li> <strong>Find Similar Items:</strong> The system pre-computes similarity between all pairs of items based on how users have rated them.</li> <li> <strong>Generate Recommendation:</strong> When a user needs a recommendation, the system looks at items they have liked, identifies items similar to those, and recommends the highest-scoring similar items the user hasn’t seen.</li> </ol> <p>Similarity between items is often calculated using Cosine Similarity on the “item rating vectors” (i.e., treating columns of the user-item matrix as item vectors, where entries are user ratings for that item).</p> <p><strong>Pros:</strong></p> <ul> <li> <strong>Scalability:</strong> Item similarity tends to be more stable over time than user similarity (user preferences change, but item relationships less so). Pre-computing item similarities can make recommendations faster.</li> <li> <strong>Better for sparse data:</strong> Often performs better than user-based filtering when the user-item matrix is very sparse.</li> </ul> <p><strong>Cons:</strong></p> <ul> <li> <strong>Less serendipitous</strong> than user-based collaborative filtering, as it often recommends items that are very “close” to what a user already liked.</li> <li> <strong>Cold Start (new items):</strong> Still struggles with new items that haven’t been rated by anyone.</li> </ul> <hr> <h3 id="matrix-factorization-uncovering-hidden-connections">Matrix Factorization: Uncovering Hidden Connections</h3> <p>While traditional collaborative filtering is powerful, it struggles with sparsity and scalability. This is where <strong>Matrix Factorization</strong> comes in, bringing a more sophisticated mathematical approach to uncover the “hidden” or “latent” factors that explain user-item interactions.</p> <p>Imagine you have our user-item interaction matrix $R$. Matrix factorization aims to decompose this large, sparse matrix into two smaller, dense matrices: a user-latent factor matrix ($P$) and an item-latent factor matrix ($Q$).</p> \[R \approx P Q^T\] <p>Where:</p> <ul> <li>$R$ is an $M \times N$ matrix (M users, N items).</li> <li>$P$ is an $M \times K$ matrix, representing each user’s relationship to $K$ latent factors.</li> <li>$Q$ is an $N \times K$ matrix, representing each item’s relationship to the same $K$ latent factors. ($Q^T$ is its transpose, $K \times N$).</li> <li>$K$ is the number of latent factors, which is much smaller than $M$ or $N$. These factors are not directly interpretable (like “sci-fi” or “comedy”) but represent underlying dimensions of taste.</li> </ul> <p>The idea is that a user’s rating for an item ($r_{ui}$) can be approximated by the dot product of their user-factor vector ($p_u$) and the item’s factor vector ($q_i$).</p> \[r_{ui} \approx p_u^T q_i = \sum_{k=1}^{K} p_{uk} q_{ik}\] <p>We learn these matrices $P$ and $Q$ by minimizing the error between the predicted rating $p_u^T q_i$ and the actual rating $r_{ui}$ for all known ratings, often adding regularization terms to prevent overfitting:</p> \[\min_{P, Q} \sum_{(u,i) \in K} (r_{ui} - p_u^T q_i)^2 + \lambda (||p_u||^2 + ||q_i||^2)\] <p>Where:</p> <ul> <li>$K$ is the set of known user-item interactions.</li> <li>$\lambda$ is a regularization parameter.</li> </ul> <p>Common techniques for solving this optimization problem include Singular Value Decomposition (SVD) (though direct SVD on sparse matrices is complex, approximations like Funk SVD are used) and Alternating Least Squares (ALS).</p> <p><strong>Pros:</strong></p> <ul> <li> <strong>Handles sparsity well:</strong> By projecting into a lower-dimensional space, it can infer relationships even with limited data.</li> <li> <strong>Scalability:</strong> Once the matrices are learned, predictions are fast.</li> <li> <strong>Captures complex patterns:</strong> Can uncover nuanced relationships that aren’t obvious from explicit features.</li> </ul> <p><strong>Cons:</strong></p> <ul> <li> <strong>Interpretability:</strong> The latent factors are often abstract and hard to explain.</li> <li> <strong>Cold Start (new users/items):</strong> Still a challenge, as new users/items don’t have corresponding rows/columns in the interaction matrix to decompose.</li> </ul> <hr> <h3 id="hybrid-recommender-systems-the-best-of-all-worlds">Hybrid Recommender Systems: The Best of All Worlds</h3> <p>Each approach has its strengths and weaknesses. So, why not combine them? <strong>Hybrid recommender systems</strong> aim to leverage the advantages of multiple techniques while mitigating their drawbacks.</p> <p>Examples include:</p> <ul> <li> <strong>Weighting/Switching:</strong> Combining content-based and collaborative filtering scores with a weighted average, or switching between methods based on context (e.g., using content-based for new users, then collaborative filtering as more data accumulates).</li> <li> <strong>Feature Enrichment:</strong> Using item features (content-based) as input alongside user ratings for a matrix factorization model.</li> <li> <strong>Ensemble Methods:</strong> Training multiple recommender models and combining their predictions.</li> </ul> <p>Hybrid approaches are common in production systems, delivering more robust, accurate, and diverse recommendations.</p> <hr> <h3 id="the-road-ahead-challenges-and-innovations">The Road Ahead: Challenges and Innovations</h3> <p>While recommender systems are incredibly powerful, they’re far from perfect and are constantly evolving.</p> <ul> <li> <strong>Cold Start Problem:</strong> Still a major hurdle. Strategies involve recommending popular items, using demographic data, or starting with content-based recommendations until enough interaction data is gathered.</li> <li> <strong>Sparsity:</strong> Despite matrix factorization, very sparse datasets remain challenging.</li> <li> <strong>Scalability:</strong> As user bases and item catalogs grow, the computational demands increase exponentially. Efficient distributed algorithms are crucial.</li> <li> <strong>Explainability:</strong> Users often want to know <em>why</em> an item was recommended. This is a growing area of research, especially with more complex models.</li> <li> <strong>Bias and Fairness:</strong> Recommender systems can reflect and even amplify biases present in historical data, leading to unfair or discriminatory recommendations. Ensuring fairness and diversity in recommendations is an ethical imperative.</li> <li> <strong>Deep Learning for Recommenders:</strong> Modern systems increasingly integrate deep learning. Techniques like embedding items and users into a shared vector space, using neural networks to predict interactions, or sequence models (like RNNs or Transformers) to understand evolving user preferences are at the forefront of research. These can capture highly non-linear relationships and temporal dynamics.</li> </ul> <hr> <h3 id="conclusion-your-personal-digital-guide">Conclusion: Your Personal Digital Guide</h3> <p>Recommender systems are more than just fancy algorithms; they are the unseen architects of our personalized digital experiences. From helping us discover our next favorite band to guiding our shopping decisions, they’ve become indispensable tools in navigating the vastness of the internet.</p> <p>My journey into understanding them has been incredibly rewarding. It’s a field where statistics, linear algebra, and machine learning beautifully converge to solve a very human problem: choice. As a data scientist, getting to build, evaluate, and improve these systems is like crafting a personalized digital genie for millions of users.</p> <p>I hope this dive into the world of content-based, collaborative filtering, and matrix factorization has demystified some of the magic for you. The next time Netflix perfectly queues up your Saturday night movie, take a moment to appreciate the elegant algorithms working tirelessly behind the scenes – turning “What next?” into a delightful “Aha!”</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>