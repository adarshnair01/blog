<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> When Many Minds Are Better Than One: A Deep Dive into Ensemble Learning | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/when-many-minds-are-better-than-one-a-deep-dive-in/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">When Many Minds Are Better Than One: A Deep Dive into Ensemble Learning</h1> <p class="post-meta"> Created on May 05, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a> ¬† ¬∑ ¬† <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a> ¬† <a href="/blog/blog/tag/ensemble-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Ensemble Learning</a> ¬† <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a> ¬† <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a> ¬† <a href="/blog/blog/tag/predictive-modeling"> <i class="fa-solid fa-hashtag fa-sm"></i> Predictive Modeling</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone! üëã</p> <p>Have you ever been faced with a tough decision, perhaps trying to predict the outcome of a game, or estimate how long a task will take? If you‚Äôre like me, your first instinct might be to consult a few friends, or maybe even a diverse group of experts, rather than relying on just one person‚Äôs opinion. Why? Because we intuitively understand that a collective decision, especially from a diverse group, often yields a more accurate and robust result than any single individual‚Äôs guess.</p> <p>This human intuition is precisely the core idea behind one of the most powerful and widely used techniques in machine learning: <strong>Ensemble Learning</strong>.</p> <h3 id="the-power-of-collaboration-what-is-ensemble-learning">The Power of Collaboration: What is Ensemble Learning?</h3> <p>Imagine you‚Äôre trying to build a machine learning model to predict whether a customer will churn (cancel their subscription). You could train a single Decision Tree, or a Logistic Regression, or a Neural Network. Each of these models, on its own, might perform reasonably well. But what if we could combine their strengths, allowing them to ‚Äúvote‚Äù or ‚Äúcollaborate‚Äù on the final prediction?</p> <p>That‚Äôs Ensemble Learning in a nutshell! Instead of relying on a single ‚Äúexpert‚Äù (a machine learning model), we train multiple ‚Äúexperts‚Äù (often called <strong>base learners</strong> or <strong>weak learners</strong>) and then intelligently combine their predictions to form a much stronger, more robust <strong>ensemble model</strong>.</p> <p>The beauty of ensemble methods is their ability to significantly improve accuracy, reduce overfitting, and make models more stable, often pushing performance beyond what any single model could achieve. It‚Äôs like assembling the Avengers of algorithms to tackle the toughest data science challenges!</p> <h3 id="why-does-it-work-so-well-the-wisdom-of-crowds">Why Does It Work So Well? The Wisdom of Crowds</h3> <p>The effectiveness of ensemble learning largely stems from a principle known as the ‚ÄúWisdom of Crowds.‚Äù This phenomenon suggests that a diverse group of individuals, acting independently, will collectively produce more accurate predictions or decisions than any single individual within the group.</p> <p>Think about a classic experiment: participants are asked to guess the number of jelly beans in a large jar. While individual guesses might vary wildly, the average of all guesses often comes remarkably close to the actual number.</p> <p>In machine learning, this translates to:</p> <ol> <li> <strong>Reducing Bias:</strong> If individual models tend to make certain types of systematic errors (bias), combining them can help cancel out these biases.</li> <li> <strong>Reducing Variance:</strong> If individual models are too sensitive to the training data (high variance, leading to overfitting), averaging or combining their predictions can smooth out these fluctuations, leading to a more stable and generalizable model.</li> <li> <strong>Handling Noise:</strong> Outliers or noisy data points might confuse a single model, but an ensemble is less likely to be swayed by a few anomalies.</li> </ol> <h3 id="the-bias-variance-trade-off-ensembles-secret-weapon">The Bias-Variance Trade-off: Ensemble‚Äôs Secret Weapon</h3> <p>To truly appreciate why ensembles shine, let‚Äôs briefly touch upon the fundamental <strong>bias-variance trade-off</strong> in machine learning.</p> <ul> <li> <strong>Bias:</strong> Represents the error introduced by approximating a real-world problem, which may be complex, by a simpler model. High bias often leads to <strong>underfitting</strong>, where the model is too simple to capture the underlying patterns in the data (e.g., trying to fit a straight line to a curved relationship).</li> <li> <strong>Variance:</strong> Refers to the amount that the model‚Äôs prediction changes when trained on different subsets of the training data. High variance often leads to <strong>overfitting</strong>, where the model learns the training data too well, including its noise and idiosyncrasies, and performs poorly on unseen data.</li> </ul> <p>The goal is to find a balance. Ensembles are incredibly powerful because they can effectively tackle both high bias and high variance:</p> <ul> <li> <strong>Methods like Bagging</strong> primarily aim to <strong>reduce variance</strong>.</li> <li> <strong>Methods like Boosting</strong> primarily aim to <strong>reduce bias</strong>.</li> </ul> <p>Let‚Äôs dive into the fascinating world of different ensemble techniques!</p> <h3 id="types-of-ensemble-learning-a-toolkit-for-super-models">Types of Ensemble Learning: A Toolkit for Super-Models</h3> <p>Ensemble methods generally fall into a few main categories based on how the base learners are trained and how their predictions are combined.</p> <h4 id="1-bagging-bootstrap-aggregating-reducing-variance">1. Bagging (Bootstrap Aggregating): Reducing Variance</h4> <p>‚ÄúBagging‚Äù stands for <strong>Bootstrap Aggregating</strong>. It‚Äôs a parallel ensemble method, meaning base learners are trained independently of each other. Its main goal is to <strong>reduce variance</strong> and prevent overfitting.</p> <p>Here‚Äôs how it works:</p> <ol> <li> <strong>Bootstrap Samples:</strong> We create multiple subsets of our original training data by <strong>sampling with replacement</strong>. This means some data points might appear multiple times in a sample, while others might not appear at all. Each subset is roughly the same size as the original dataset.</li> <li> <strong>Train Base Learners:</strong> We train a separate base learner (often the same type of model, like a decision tree) on each of these bootstrap samples.</li> <li> <strong>Aggregate Predictions:</strong> For classification tasks, we typically use a <strong>majority vote</strong> among the base learners. For regression tasks, we <strong>average</strong> their predictions.</li> </ol> <p>The most famous example of bagging is <strong>Random Forest</strong>.</p> <h5 id="random-forest-the-king-of-trees">Random Forest: The King of Trees</h5> <p>Random Forest is an ensemble of decision trees. It introduces an additional layer of randomness to make the trees even more diverse:</p> <ul> <li> <strong>Bootstrapping:</strong> Each tree is trained on a different bootstrap sample of the data.</li> <li> <strong>Feature Randomness:</strong> When building each tree, at each split point, it considers only a random subset of the available features, rather than all features. This de-correlates the trees, making them more independent and robust to noisy features.</li> </ul> <p>By combining many diverse, slightly imperfect trees, Random Forest achieves remarkable accuracy and generalization.</p> <p>Mathematically, for a classification problem with $K$ trees, the final prediction $H(x)$ for an input $x$ is: $ H(x) = \text{mode}{h<em>1(x), h_2(x), ‚Ä¶, h_K(x)} $ where $h_k(x)$ is the prediction of the $k$-th tree. For regression, it‚Äôs simply the average: $ H(x) = \frac{1}{K}\sum</em>{k=1}^K h_k(x) $</p> <p>Random Forests are incredibly popular due to their high performance, ease of use, and robustness against overfitting.</p> <h4 id="2-boosting-reducing-bias">2. Boosting: Reducing Bias</h4> <p>Boosting is a sequential ensemble method. Unlike bagging, where models are built in parallel, boosting builds models one after another. Each new model tries to <strong>correct the errors</strong> made by the previous models, effectively focusing on the ‚Äúhard-to-learn‚Äù examples. This primarily aims to <strong>reduce bias</strong>.</p> <h5 id="adaboost-adaptive-boosting-the-error-corrector">AdaBoost (Adaptive Boosting): The Error Corrector</h5> <p>AdaBoost was one of the first successful boosting algorithms. It works by:</p> <ol> <li> <strong>Initial Weights:</strong> All training samples are initially given equal weights.</li> <li> <strong>Sequential Training:</strong> A weak learner (e.g., a shallow decision tree, often called a ‚Äústump‚Äù) is trained on the data.</li> <li> <strong>Weight Adjustment:</strong> Samples that were <strong>misclassified</strong> by the current weak learner have their weights <strong>increased</strong>, making them more important for the next learner. Correctly classified samples have their weights decreased.</li> <li> <strong>Learner Weighting:</strong> Each weak learner itself is assigned a weight based on its accuracy. More accurate learners get higher weights in the final ensemble.</li> <li> <strong>Repeat:</strong> Steps 2-4 are repeated for a specified number of iterations, with each new learner focusing on the examples that previous learners struggled with.</li> <li> <strong>Final Prediction:</strong> The final prediction is a weighted sum (or majority vote) of all the weak learners.</li> </ol> <p>For a binary classification problem, if we have $K$ weak classifiers $h_k(x)$ and their respective weights $\alpha_k$, the final prediction is: $ H(x) = \text{sign}\left(\sum_{k=1}^K \alpha_k h_k(x)\right) $ The $sign()$ function converts the weighted sum into a +1 or -1 class prediction.</p> <h5 id="gradient-boosting-the-next-level">Gradient Boosting: The Next Level</h5> <p>Gradient Boosting is a more generalized form of boosting. Instead of adjusting sample weights, it trains subsequent models to predict the ‚Äúresiduals‚Äù or ‚Äúerrors‚Äù of the previous models‚Äô predictions. It essentially tries to push the overall error down the ‚Äúgradient‚Äù (direction of steepest descent) of the loss function.</p> <p>This approach is incredibly powerful and forms the basis for some of the most winning algorithms in machine learning competitions:</p> <ul> <li> <strong>XGBoost (Extreme Gradient Boosting):</strong> Highly optimized, fast, and very efficient.</li> <li> <strong>LightGBM (Light Gradient Boosting Machine):</strong> Faster training speed and lower memory usage, especially for large datasets.</li> <li> <strong>CatBoost (Categorical Boosting):</strong> Excellent handling of categorical features and robust against overfitting.</li> </ul> <p>These algorithms are often the go-to choice when you need top-tier performance on tabular data.</p> <h4 id="3-stacking-stacked-generalization-the-meta-learner">3. Stacking (Stacked Generalization): The Meta-Learner</h4> <p>Stacking is a slightly more complex but very powerful ensemble method that can combine diverse models. It involves training a ‚Äúmeta-learner‚Äù to learn how to best combine the predictions of several ‚Äúbase learners.‚Äù</p> <p>Here‚Äôs the two-layer idea:</p> <ol> <li> <strong>Base Learners (Layer 0):</strong> You train several diverse models (e.g., a Logistic Regression, a Random Forest, a Support Vector Machine) on your training data. Each of these models makes predictions.</li> <li> <strong>Meta-Learner (Layer 1):</strong> The predictions from these base learners are then used as <em>input features</em> for a second-level model, the ‚Äúmeta-learner.‚Äù This meta-learner learns to combine the strengths of the base learners and make the final prediction.</li> </ol> <p>The trick with stacking is to prevent <strong>data leakage</strong>. You can‚Äôt train the base learners and the meta-learner on the same data immediately, because the base learners would have ‚Äúseen‚Äù the labels, and the meta-learner would just learn to perfectly replicate their (overfit) predictions. A common solution involves <strong>k-fold cross-validation</strong> to generate out-of-sample predictions from the base learners, which are then used to train the meta-learner.</p> <p>Stacking often achieves very high accuracy because it allows the meta-learner to intelligently weigh and combine the outputs of models that might excel at different aspects of the problem.</p> <h3 id="practical-considerations-and-when-to-use-ensembles">Practical Considerations and When to Use Ensembles</h3> <p>While ensemble methods are incredibly powerful, they come with their own set of considerations:</p> <ul> <li> <strong>Computational Cost:</strong> Training multiple models can be more computationally expensive and time-consuming than training a single model, especially for large datasets.</li> <li> <strong>Interpretability:</strong> A single decision tree is easy to interpret. An ensemble of hundreds of trees, or a stacked model, becomes a ‚Äúblack box,‚Äù making it harder to understand <em>why</em> it made a particular prediction.</li> <li> <strong>Hyperparameter Tuning:</strong> More models mean more hyperparameters to tune, which can increase complexity.</li> <li> <strong>When to Use:</strong> <ul> <li>When you need the absolute highest accuracy.</li> <li>When your individual models are good but seem to hit a performance ceiling.</li> <li>When you suspect individual models might be overfitting or underfitting to different degrees.</li> <li>In competitive scenarios (like Kaggle), ensembles are almost always a crucial part of winning solutions.</li> </ul> </li> </ul> <h3 id="my-aha-moment-with-ensembles">My ‚ÄúAha!‚Äù Moment with Ensembles</h3> <p>I remember working on a project predicting housing prices. My initial attempts with a single Ridge Regression model were okay, but I was constantly hitting a wall around a certain error margin. It felt like I was squeezing everything I could out of that one model.</p> <p>Then, I decided to try a Gradient Boosting Regressor (XGBoost). The jump in performance was astonishing! Not just a small improvement, but a significant leap that instantly placed my model in a much more competitive range. It was like going from a diligent individual student to a well-coordinated, super-smart study group that caught every little detail. That‚Äôs when ensemble learning truly clicked for me ‚Äì it‚Äôs not just a tweak; it‚Äôs a paradigm shift in how you approach building robust predictive systems.</p> <h3 id="conclusion-embracing-the-collective-intelligence">Conclusion: Embracing the Collective Intelligence</h3> <p>Ensemble learning is a testament to the idea that in machine learning, just like in life, collaboration often leads to superior outcomes. By strategically combining the predictions of multiple diverse models, we can build systems that are more accurate, more robust, and more reliable than any single model could ever be.</p> <p>Whether you‚Äôre battling overfitting with Bagging, conquering bias with Boosting, or creating intelligent hierarchies with Stacking, mastering ensemble techniques is an essential skill in any data scientist‚Äôs toolkit. So, go forth and experiment! Build your own Avengers team of algorithms and watch your models achieve superhero-level performance.</p> <p>Happy ensembling! ‚ú®</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>