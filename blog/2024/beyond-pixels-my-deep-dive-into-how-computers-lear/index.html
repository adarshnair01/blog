<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Beyond Pixels: My Deep Dive into How Computers Learn to See | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/beyond-pixels-my-deep-dive-into-how-computers-lear/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Beyond Pixels: My Deep Dive into How Computers Learn to See</h1> <p class="post-meta"> Created on September 04, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/computer-vision"> <i class="fa-solid fa-hashtag fa-sm"></i> Computer Vision</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   <a href="/blog/blog/tag/image-processing"> <i class="fa-solid fa-hashtag fa-sm"></i> Image Processing</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>As humans, we often take our sense of sight for granted. We effortlessly recognize faces, navigate complex environments, and interpret the subtle cues in a friend’s expression. But have you ever paused to think about what it truly means to “see”? And then, what would it take to teach a machine to do the same? This question has always captivated me, leading me down a rabbit hole into the incredible field of Computer Vision.</p> <p>It’s not just about cameras capturing light; it’s about turning that light into understanding. Imagine a self-driving car identifying a stop sign, a doctor diagnosing a disease from an X-ray, or your phone unlocking with just a glance. These aren’t sci-fi fantasies anymore; they’re everyday realities powered by computer vision.</p> <p>Join me as we explore this fascinating domain, breaking down complex ideas into understandable concepts, and uncovering how computers are learning to interpret the visual world, one pixel at a time.</p> <h3 id="the-world-through-a-computers-eyes-pixels-and-numbers">The World Through a Computer’s Eyes: Pixels and Numbers</h3> <p>For us, an image is a scene, a memory, a moment. For a computer, an image is just a grid of numbers. Seriously, that’s it!</p> <p>Think of a digital image as a giant spreadsheet. Each cell in this spreadsheet is called a <strong>pixel</strong> (picture element). In a grayscale image, each pixel holds a single number representing its intensity, typically from 0 (black) to 255 (white).</p> <p>For a color image, it’s a bit more complex. Most color images use the <strong>RGB (Red, Green, Blue)</strong> model. So, for each pixel, there are <em>three</em> numbers, one for the intensity of red, one for green, and one for blue. Combining these three values in different proportions creates millions of colors.</p> <p>For example, a small $3 \times 3$ grayscale image might look like this to a computer:</p> \[\begin{pmatrix} 200 &amp; 150 &amp; 100 \\ 120 &amp; 80 &amp; 40 \\ 25 &amp; 10 &amp; 5 \end{pmatrix}\] <p>And for a color image, imagine three such $3 \times 3$ matrices stacked on top of each other – one for Red, one for Green, and one for Blue.</p> <p>So, when a computer “sees” an image, it’s not seeing a cat, a tree, or a human face. It’s seeing a vast array of numbers. The core challenge of computer vision is to take these raw numbers and extract meaningful information from them – to identify shapes, objects, textures, and ultimately, to understand the content of the image.</p> <h3 id="the-dawn-of-vision-handcrafted-features">The Dawn of Vision: Handcrafted Features</h3> <p>Early pioneers in computer vision faced a daunting task: how do you go from a matrix of numbers to recognizing something as complex as an edge, let alone an entire object? Their approach involved painstakingly designing algorithms to detect specific visual patterns, which we call <strong>features</strong>.</p> <h4 id="1-filters-and-kernels-the-image-magnifying-glass">1. Filters and Kernels: The Image Magnifying Glass</h4> <p>One of the foundational techniques involves using <strong>filters</strong> (also known as <strong>kernels</strong> or <strong>convolution matrices</strong>). A filter is a small matrix of numbers that “slides” over the image. At each position, it performs a mathematical operation (element-wise multiplication and summation) with the underlying pixels to produce a single output pixel in a new image.</p> <p>Let’s consider an <strong>edge detection</strong> filter. Edges are crucial for identifying object boundaries. An edge is essentially a sudden change in pixel intensity.</p> <p>Consider this simple $3 \times 3$ kernel for detecting vertical edges:</p> \[K = \begin{pmatrix} -1 &amp; 0 &amp; 1 \\ -1 &amp; 0 &amp; 1 \\ -1 &amp; 0 &amp; 1 \end{pmatrix}\] <p>When this kernel slides over an image, if it encounters a transition from dark to light (or vice-versa) in the vertical direction, the result will be a high (or low) value, indicating an edge. If the pixels underneath are all similar, the result will be close to zero.</p> <p>Other kernels can be designed for blurring (e.g., a Gaussian kernel for smoothing noise), sharpening, or embossing. These handcrafted kernels were powerful tools for manipulating images and extracting low-level features.</p> <h4 id="2-beyond-edges-describing-shapes-and-textures">2. Beyond Edges: Describing Shapes and Textures</h4> <p>As the field progressed, researchers developed more sophisticated algorithms to extract richer features:</p> <ul> <li> <strong>HOG (Histogram of Oriented Gradients):</strong> This technique describes the local appearance and shape of objects by creating a histogram of gradient orientations in localized regions of an image. It was very effective for human detection.</li> <li> <strong>SIFT (Scale-Invariant Feature Transform):</strong> SIFT could detect and describe local features that were robust to changes in image scale, rotation, and illumination. This allowed for reliable object recognition even if the object appeared smaller, rotated, or in different lighting conditions.</li> </ul> <p>These methods relied on expert knowledge to design algorithms that could identify specific patterns. They were ingenious for their time, but they had limitations. They struggled with variations in lighting, viewpoint, and object deformation. A tiny change in conditions could break the carefully crafted feature detectors. It was like teaching a child to recognize a cat by giving them a detailed checklist of “must have pointy ears,” “must have whiskers,” “must have a tail,” and then having them fail if the cat was sleeping in a ball or partially hidden.</p> <p>We needed a more adaptive, more <em>intelligent</em> way for computers to learn to see.</p> <h3 id="the-deep-learning-revolution-learning-to-see-for-themselves">The Deep Learning Revolution: Learning to See for Themselves</h3> <p>The real breakthrough in computer vision came with the advent of <strong>Deep Learning</strong>, specifically <strong>Convolutional Neural Networks (CNNs)</strong>. Instead of us painstakingly designing features, CNNs learn to extract features directly from the data. This paradigm shift was monumental.</p> <h4 id="how-cnns-work-a-hierarchical-approach">How CNNs Work: A Hierarchical Approach</h4> <p>Imagine the human visual cortex: different parts of your brain are responsible for recognizing different aspects of an image, from simple lines and edges to complex objects and faces. CNNs mimic this hierarchical structure.</p> <p>A typical CNN architecture is a sequence of layers, each performing a specific transformation on the input image.</p> <ol> <li> <p><strong>Convolutional Layers:</strong> This is the heart of a CNN. Remember those handcrafted filters? In a convolutional layer, the filters (or kernels) are not predefined; they are <strong>learnable parameters</strong>. The network <em>learns</em> the optimal filters during training.</p> <p>A convolutional layer applies multiple filters to the input image. Each filter slides across the image, just like our edge detection kernel, performing the element-wise multiplication and sum. The output of one filter over the entire image is called a <strong>feature map</strong>. Each feature map highlights a particular characteristic learned by that filter (e.g., vertical edges, horizontal lines, specific textures).</p> <p>Let’s say we have an input image $I$ and a filter $K$. The output feature map $F$ at position $(i, j)$ is given by the convolution operation:</p> \[F_{i,j} = \sum_{m} \sum_{n} I_{i-m, j-n} K_{m,n}\] <p>This operation effectively “extracts” a specific feature from the input. A CNN will have many such filters, learning to detect hundreds, even thousands, of different features.</p> </li> <li> <p><strong>Activation Functions (ReLU):</strong> After each convolutional operation, an <strong>activation function</strong> is applied to the feature map. The most common one is the <strong>Rectified Linear Unit (ReLU)</strong>.</p> \[\text{ReLU}(x) = \max(0, x)\] <p>Why ReLU? It introduces non-linearity into the network. Without non-linearity, no matter how many layers you stack, the network would only be able to learn linear relationships. Non-linearity allows CNNs to learn complex, non-linear patterns present in images, like curved lines or intricate textures.</p> </li> <li> <strong>Pooling Layers (Max Pooling):</strong> Pooling layers are used to reduce the spatial dimensions (width and height) of the feature maps, which helps in two ways: <ul> <li> <strong>Reduces computation:</strong> Fewer parameters mean faster processing.</li> <li> <strong>Introduces translation invariance:</strong> It makes the network less sensitive to the exact position of a feature. If an edge shifts slightly, the pooling layer will still likely detect it.</li> </ul> <p><strong>Max Pooling</strong> is a common type. It takes a small window (e.g., $2 \times 2$) from the feature map and outputs the maximum value within that window.</p> <p>For example, if we have a $2 \times 2$ window: \(\begin{pmatrix} 10 &amp; 20 \\ 5 &amp; 15 \end{pmatrix}\) Max pooling would output $20$.</p> <p>This essentially summarizes the presence of a feature in a region, discarding less important details and keeping the most prominent ones.</p> </li> <li> <p><strong>Fully Connected Layers:</strong> After several alternating convolutional and pooling layers, the high-level features learned by the network are “flattened” into a single vector. This vector is then fed into one or more <strong>fully connected layers</strong>. These are traditional neural network layers where every neuron is connected to every neuron in the previous layer.</p> <p>These layers take the high-level features extracted by the convolutional part of the network and use them to make predictions.</p> </li> <li> <strong>Output Layer (Softmax):</strong> The final fully connected layer typically uses an <strong>activation function like Softmax</strong> for classification tasks. Softmax outputs a probability distribution over the possible classes. For instance, if you’re classifying images of cats, dogs, and birds, the output might be $[0.9, 0.05, 0.05]$, indicating a 90% probability of being a cat.</li> </ol> <p>The magic of CNNs lies in their ability to learn these feature hierarchies. The first convolutional layers might learn to detect simple edges and blobs. Subsequent layers combine these simple features to detect more complex patterns like eyes, ears, or wheels. Even deeper layers combine these to recognize entire objects like faces, cars, or animals.</p> <h4 id="training-a-cnn-learning-from-experience">Training a CNN: Learning from Experience</h4> <p>How do these filters learn? Through a process called <strong>training</strong>.</p> <ol> <li>We feed the CNN a vast dataset of images, each labeled with its correct category (e.g., an image of a cat labeled “cat”).</li> <li>The network makes a prediction.</li> <li>We calculate a <strong>loss function</strong> (e.g., cross-entropy loss) that measures how far off the prediction was from the true label.</li> <li>Using an algorithm called <strong>backpropagation</strong> and an <strong>optimizer</strong> (like Adam or SGD), the network adjusts its internal weights (the numbers in those filters and fully connected layers) slightly to reduce the loss.</li> <li>This process is repeated millions of times, across many images, gradually refining the network’s ability to accurately classify images.</li> </ol> <p>It’s like a child learning by trial and error. “Is this a cat?” “No, that’s a dog.” “Okay, what features made it a dog?” The child adjusts their internal model until they can reliably tell the difference.</p> <h3 id="the-impact-where-computer-vision-shines">The Impact: Where Computer Vision Shines</h3> <p>The transformation brought about by deep learning has made computer vision applicable to an astonishing array of real-world problems:</p> <ul> <li> <strong>Autonomous Vehicles:</strong> Object detection (cars, pedestrians, traffic signs), lane keeping, depth perception, and navigation.</li> <li> <strong>Medical Imaging:</strong> Detecting tumors in MRI scans, identifying diseases from X-rays, analyzing microscopic images for pathology.</li> <li> <strong>Facial Recognition:</strong> Security systems, unlocking smartphones, identity verification.</li> <li> <strong>Augmented Reality (AR):</strong> Overlaying digital information onto the real world (e.g., Pokémon GO, AR filters on social media).</li> <li> <strong>Industrial Automation:</strong> Quality control on assembly lines, robotic picking and placing, defect detection.</li> <li> <strong>Security and Surveillance:</strong> Anomaly detection, crowd analysis.</li> <li> <strong>Retail:</strong> Inventory management, checkout-free stores, customer behavior analysis.</li> </ul> <p>The list goes on, constantly expanding as researchers push the boundaries of what’s possible.</p> <h3 id="challenges-and-the-road-ahead">Challenges and the Road Ahead</h3> <p>While computer vision has achieved incredible feats, it’s far from a solved problem. Significant challenges remain:</p> <ul> <li> <strong>Robustness:</strong> Real-world conditions are messy – varying lighting, occlusions (objects partially hidden), unusual viewpoints, and adverse weather conditions can still confuse even the best models.</li> <li> <strong>Data Scarcity and Bias:</strong> High-quality, labeled image datasets are expensive and time-consuming to create. Furthermore, biases in training data can lead to models that perform poorly or unfairly for certain demographics or conditions.</li> <li> <strong>Explainability (XAI):</strong> Deep learning models are often “black boxes.” Understanding <em>why</em> a model made a particular decision (e.g., why did it misclassify this patient’s X-ray?) is crucial, especially in high-stakes applications like medicine or autonomous driving.</li> <li> <strong>Real-time Performance:</strong> Many applications require instantaneous processing, which demands efficient models and powerful hardware.</li> <li> <strong>Ethical Considerations:</strong> The power of computer vision raises important ethical questions around privacy (facial recognition), fairness (bias in algorithms), and potential misuse.</li> </ul> <p>The future of computer vision is bright, focusing on areas like:</p> <ul> <li> <strong>Few-shot and Zero-shot Learning:</strong> Teaching models to learn from very little data, or even generalize to unseen categories.</li> <li> <strong>Generative Models:</strong> Creating realistic images and videos (e.g., deepfakes, but also for synthetic data generation to augment datasets).</li> <li> <strong>Multimodal Learning:</strong> Combining vision with other senses like language (e.g., image captioning, visual question answering) or audio.</li> <li> <strong>Edge AI:</strong> Running powerful vision models directly on devices (e.g., smartphones, drones) without relying on cloud infrastructure.</li> </ul> <h3 id="my-journey-continues">My Journey Continues</h3> <p>Exploring computer vision has been an incredible adventure. From the simple elegance of a pixel matrix to the complex, hierarchical learning of a CNN, it’s a field that continues to amaze and inspire me. It’s a testament to human ingenuity that we can teach machines to emulate one of our most fundamental senses.</p> <p>As a Data Scientist and MLE, I find immense satisfaction in contributing to this field. The ability to give machines the gift of sight opens up endless possibilities, allowing us to solve problems that were once intractable and create technologies that seemed like pure fantasy.</p> <p>If you’re fascinated by how computers see, I encourage you to dive deeper! Start with some Python libraries like OpenCV or TensorFlow/PyTorch, experiment with basic image processing, and perhaps even train your own small CNN. The journey from raw numbers to profound understanding is truly one of the most exciting frontiers in artificial intelligence.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>