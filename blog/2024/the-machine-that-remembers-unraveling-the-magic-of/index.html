<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Machine That Remembers: Unraveling the Magic of Recurrent Neural Networks | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/the-machine-that-remembers-unraveling-the-magic-of/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Machine That Remembers: Unraveling the Magic of Recurrent Neural Networks</h1> <p class="post-meta"> Created on August 20, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/recurrent-neural-networks"> <i class="fa-solid fa-hashtag fa-sm"></i> Recurrent Neural Networks</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> NLP</a>   <a href="/blog/blog/tag/sequence-data"> <i class="fa-solid fa-hashtag fa-sm"></i> Sequence Data</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>As a data science enthusiast, there’s nothing quite like that “aha!” moment when a complex concept clicks. For me, one of those moments came when I started grappling with sequence data – things like text, speech, or time series. My trusty feedforward neural networks, the workhorses of many projects, suddenly felt… limited. They were brilliant at processing independent inputs, like classifying images of cats and dogs, but they had a glaring flaw: they couldn’t remember the past. Each input was treated as if it had no relationship to the one before it.</p> <p>Imagine trying to understand a story by reading each sentence in isolation, completely forgetting what happened in the previous ones. You’d be lost! Our human brains excel at processing sequences because we have memory. We carry context forward. This fundamental difference is what led to one of the most elegant and powerful innovations in deep learning: <strong>Recurrent Neural Networks (RNNs)</strong>.</p> <h3 id="the-aha-moment-giving-machines-a-memory">The “Aha!” Moment: Giving Machines a Memory</h3> <p>So, how do you give a machine memory? The core idea behind RNNs is deceptively simple but profoundly powerful: <strong>a loop</strong>. Instead of just passing information forward, an RNN has an internal state (often called a “hidden state” or “context vector”) that gets updated at each step of a sequence, much like our short-term memory. This state, $h_t$, captures information from all the previous inputs in the sequence.</p> <p>Think of it like this: You’re trying to predict the next word in a sentence, “The cat sat on the…” To do this accurately, you need to remember “The cat sat on the” – not just “the.” An RNN achieves this by taking not only the current input ($x_t$) but also its own internal memory ($h_{t-1}$) from the previous step to produce the current output ($y_t$) and update its memory ($h_t$) for the next step.</p> <p>When we talk about an RNN, we often visualize it in two ways:</p> <ol> <li> <strong>With a Loop:</strong> This is the most compact representation, showing the hidden state feeding back into the network. <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>x_t ----&gt; [RNN] ----&gt; y_t
           ^ |
           | |
           ---
           h_t
</code></pre></div> </div> </li> <li> <p><strong>Unrolled:</strong> To understand how it processes a sequence over time, we “unroll” the loop. This makes it look like a deep feedforward network, but with a crucial difference: <strong>the same weights are used at each time step</strong>. This weight sharing is what makes RNNs so powerful for sequences, as they learn patterns that apply across different positions in the sequence.</p> <div class="language-plaintext highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>x_0 ----&gt; [RNN_0] ----&gt; y_0
          ^ |
          | | h_0
          ---
              |
              v
x_1 ----&gt; [RNN_1] ----&gt; y_1
          ^ |
          | | h_1
          ---
              |
              v
x_2 ----&gt; [RNN_2] ----&gt; y_2
          ^ |
          | | h_2
          ---
... and so on
</code></pre></div> </div> <p>Each <code class="language-plaintext highlighter-rouge">[RNN_t]</code> box in the unrolled version uses the <em>identical</em> set of weights. This is key!</p> </li> </ol> <h3 id="under-the-hood-the-mechanics-of-a-simple-rnn">Under the Hood: The Mechanics of a Simple RNN</h3> <p>Let’s peek inside one of those <code class="language-plaintext highlighter-rouge">[RNN]</code> boxes. At each time step $t$, a vanilla (basic) RNN cell performs two main calculations:</p> <ol> <li> <strong>Updating the Hidden State:</strong> The new hidden state $h_t$ is computed using the current input $x_t$ and the previous hidden state $h_{t-1}$. \(h_t = \tanh(W_{hh}h_{t-1} + W_{xh}x_t + b_h)\) <ul> <li>$x_t$: The input vector at time $t$ (e.g., a word embedding).</li> <li>$h_{t-1}$: The hidden state vector from the previous time step. This is the “memory.”</li> <li>$W_{xh}$: Weight matrix connecting the input to the hidden state.</li> <li>$W_{hh}$: Weight matrix connecting the previous hidden state to the current hidden state (this is where the “recurrence” comes from).</li> <li>$b_h$: Bias vector for the hidden state.</li> <li>$\tanh$: An activation function (often $\tanh$ or ReLU) that squashes the values between -1 and 1, introducing non-linearity.</li> </ul> </li> <li> <strong>Computing the Output:</strong> The output $y_t$ at time $t$ is typically a function of the current hidden state $h_t$. \(y_t = W_{hy}h_t + b_y\) <ul> <li>$W_{hy}$: Weight matrix connecting the hidden state to the output.</li> <li>$b_y$: Bias vector for the output.</li> <li>(Sometimes, an activation function like softmax is applied here if it’s a classification task, e.g., predicting the next word).</li> </ul> </li> </ol> <p>Notice how the weights ($W_{xh}, W_{hh}, W_{hy}$) and biases ($b_h, b_y$) are <em>shared across all time steps</em>. This is a crucial concept. It means the RNN is learning a set of transformations that apply consistently throughout the entire sequence, making it incredibly efficient for learning sequential patterns.</p> <h3 id="training-an-rnn-backpropagation-through-time-bptt">Training an RNN: Backpropagation Through Time (BPTT)</h3> <p>Training an RNN is similar to training a regular neural network, but with a twist. We use a technique called <strong>Backpropagation Through Time (BPTT)</strong>. Essentially, once the network is unrolled, BPTT applies the standard backpropagation algorithm to calculate gradients and update weights. The gradients are computed for each time step and then summed up to update the shared weights.</p> <p>However, this repeated multiplication of gradients across many time steps leads to significant challenges.</p> <h3 id="the-achilles-heel-vanishing-and-exploding-gradients">The Achilles’ Heel: Vanishing and Exploding Gradients</h3> <p>This is where the story gets a bit more complex. While RNNs are great at remembering short-term dependencies, they notoriously struggle with <strong>long-term dependencies</strong>. Imagine trying to remember a detail from the first sentence of a novel while you’re 200 pages deep. This problem manifests as:</p> <ol> <li> <strong>Vanishing Gradients</strong>: As gradients are backpropagated through many time steps, they can shrink exponentially, becoming extremely small. This means the updates to the weights corresponding to earlier inputs in the sequence become negligible. The network effectively “forgets” information from earlier time steps because those parts of the network aren’t learning effectively. It’s like playing a game of “telephone” over a very long line – the original message gets lost or distorted by the time it reaches the end.</li> <li> <strong>Exploding Gradients</strong>: On the flip side, gradients can also grow exponentially large. This leads to very large weight updates, causing the model to become unstable, outputting <code class="language-plaintext highlighter-rouge">NaN</code> (Not a Number) values, or oscillating wildly. This is less common than vanishing gradients and can often be mitigated with gradient clipping (simply capping the maximum value of gradients).</li> </ol> <p>The vanishing gradient problem, in particular, was a major roadblock for vanilla RNNs in tasks requiring memory over long sequences (e.g., understanding long paragraphs, or long audio clips). The “memory” of these basic RNNs was, frustratingly, quite short-lived.</p> <h3 id="the-evolution-lstms-and-grus-to-the-rescue">The Evolution: LSTMs and GRUs to the Rescue!</h3> <p>The limitations of vanilla RNNs sparked a revolution, leading to more sophisticated architectures designed to combat vanishing gradients and better capture long-term dependencies. The two most prominent are:</p> <ol> <li> <strong>Long Short-Term Memory (LSTM) Networks</strong>: Invented by Hochreiter and Schmidhuber in 1997, LSTMs introduced a “cell state” ($C_t$) that runs parallel to the hidden state ($h_t$). This cell state acts like a conveyor belt, carrying information across many time steps with minimal modification. LSTMs regulate information flow into and out of this cell state using special <strong>gates</strong>: <ul> <li> <strong>Forget Gate</strong>: Decides what information to throw away from the cell state.</li> <li> <strong>Input Gate</strong>: Decides what new information to store in the cell state.</li> <li> <strong>Output Gate</strong>: Decides what part of the cell state to output as the hidden state.</li> </ul> <p>These gates are controlled by sigmoid functions and point-wise multiplications, allowing the LSTM to selectively remember or forget information. This gating mechanism is the key to their success in remembering relevant information over long sequences.</p> </li> <li> <strong>Gated Recurrent Units (GRUs)</strong>: A slightly simpler variant of LSTMs, GRUs combine the forget and input gates into a single “update gate” and merge the cell state and hidden state. They also have a “reset gate.” While simpler, GRUs often perform just as well as LSTMs on many tasks and are computationally less intensive.</li> </ol> <p>Both LSTMs and GRUs have become the de-facto standard for sequence modeling, largely overcoming the vanishing gradient problem and enabling impressive advancements in many fields.</p> <h3 id="where-do-rnns-and-their-children-shine-applications">Where Do RNNs (and their children) Shine? Applications!</h3> <p>The ability of RNNs, LSTMs, and GRUs to process sequential data has opened doors to incredible applications across various domains:</p> <ul> <li> <strong>Natural Language Processing (NLP)</strong>: <ul> <li> <strong>Machine Translation</strong>: Translating sentences from one language to another (e.g., Google Translate).</li> <li> <strong>Text Generation</strong>: Generating human-like text, poems, or code.</li> <li> <strong>Sentiment Analysis</strong>: Determining the emotional tone of a piece of text.</li> <li> <strong>Speech Recognition</strong>: Converting spoken language into text.</li> <li> <strong>Named Entity Recognition</strong>: Identifying names of people, organizations, locations in text.</li> </ul> </li> <li> <strong>Time Series Analysis</strong>: <ul> <li> <strong>Stock Price Prediction</strong>: Forecasting future stock prices based on historical data.</li> <li> <strong>Weather Forecasting</strong>: Predicting weather patterns.</li> </ul> </li> <li> <strong>Image Captioning</strong>: Combining CNNs (for image feature extraction) with RNNs (for generating descriptive captions).</li> <li> <strong>Music Generation</strong>: Composing new melodies or extending existing ones.</li> </ul> <h3 id="my-journey-continues-your-turn">My Journey Continues: Your Turn!</h3> <p>Diving into RNNs was a pivotal moment in my understanding of deep learning. It showed me how clever architectural designs can solve seemingly intractable problems, moving us closer to truly intelligent machines. The journey from a simple loop to the intricate gating mechanisms of LSTMs and GRUs is a testament to the continuous innovation in this field.</p> <p>If you’re fascinated by how machines learn from sequences, I encourage you to explore these concepts further. Try implementing a simple RNN, then an LSTM, using libraries like TensorFlow or PyTorch. The best way to solidify your understanding is to get your hands dirty with code and see these powerful networks in action.</p> <p>What sequence problem will you tackle first? The possibilities are endless!</p> <p>Happy learning!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>