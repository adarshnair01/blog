<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Decoding Dimensions: A Personal Journey into Principal Component Analysis | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/decoding-dimensions-a-personal-journey-into-princi/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Decoding Dimensions: A Personal Journey into Principal Component Analysis</h1> <p class="post-meta"> Created on November 19, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/dimensionality-reduction"> <i class="fa-solid fa-hashtag fa-sm"></i> Dimensionality Reduction</a>   <a href="/blog/blog/tag/pca"> <i class="fa-solid fa-hashtag fa-sm"></i> PCA</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/linear-algebra"> <i class="fa-solid fa-hashtag fa-sm"></i> Linear Algebra</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey there, fellow data explorer!</p> <p>Have you ever looked at a massive dataset with hundreds, maybe even thousands, of columns (what we call ‘features’ in data science), and just felt… overwhelmed? I certainly have. It’s like standing in front of a giant puzzle with far too many pieces, and you know many of them are just sky or grass – important for the full picture, but not for understanding the <em>main subject</em>. This feeling, my friends, is often a symptom of the “Curse of Dimensionality,” and it’s precisely where a technique called <strong>Principal Component Analysis (PCA)</strong> rides in like a superhero.</p> <p>Today, I want to take you on a journey – a personal dive into what PCA is, why it’s so incredibly useful, and how it actually works. Don’t worry, we’ll sprinkle in some math, but we’ll always keep our feet on the ground with intuition and relatable examples. Think of this as me sharing my own “Aha!” moments with you.</p> <h3 id="the-problem-too-much-of-a-good-thing">The Problem: Too Much of a Good Thing</h3> <p>Imagine you’re trying to predict house prices. You might have features like square footage, number of bedrooms, location, year built, and so on. But what if you also had features like “color of front door,” “average temperature last Tuesday,” or “number of blades of grass in the front yard”? Some features are crucial, some are somewhat relevant but redundant, and some are just plain noise.</p> <p>Having too many features, especially correlated or irrelevant ones, creates several headaches:</p> <ol> <li> <strong>Computational Cost:</strong> More features mean more memory and longer processing times for your machine learning models.</li> <li> <strong>Overfitting:</strong> Your model might start learning the noise in the data instead of the underlying patterns, performing poorly on new, unseen data.</li> <li> <strong>Visualization:</strong> It’s impossible to plot data with more than 3 dimensions directly. How do you find patterns in 100 dimensions?</li> <li> <strong>Data Sparsity:</strong> In high dimensions, data points become incredibly sparse, meaning the “empty space” between data points grows exponentially. This makes it harder for models to find meaningful relationships.</li> </ol> <p>This, in a nutshell, is the <strong>Curse of Dimensionality</strong>. We need a way to simplify without losing the essence of our data.</p> <h3 id="pca-to-the-rescue-finding-the-essence">PCA to the Rescue: Finding the Essence</h3> <p>This is where PCA steps onto the stage. At its heart, PCA is a <strong>dimensionality reduction technique</strong>. Its goal isn’t just to throw away features randomly, but to transform your existing features into a <em>new set of features</em>, called <strong>Principal Components</strong>, that capture as much of the original data’s variance (information) as possible, but in fewer dimensions.</p> <p>Think of it like this: You have a 3D object, say, a crumpled piece of paper, and you want to take a 2D photograph of it. If you just take a picture straight on, you might miss a lot of its interesting wrinkles and folds. But if you carefully rotate it and choose the <em>best angle</em> to project it onto a 2D plane, you can capture most of its defining characteristics. PCA does something similar, but mathematically, for any number of dimensions.</p> <p>The core idea is to find directions (vectors) in your high-dimensional space along which your data varies the most. These directions are your principal components.</p> <h4 id="key-intuition-maximizing-variance">Key Intuition: Maximizing Variance</h4> <p>Why maximize variance? Because variance represents the spread or dispersion of the data. If data points are spread out along a direction, it means that direction contains a lot of unique information. If they are clustered together (low variance), that direction is pretty boring and doesn’t tell you much. PCA aims to find new axes such that:</p> <ol> <li>The first principal component (PC1) captures the maximum possible variance in the data.</li> <li>The second principal component (PC2) captures the maximum remaining variance, and is <em>orthogonal</em> (perpendicular) to PC1.</li> <li>And so on, for subsequent components.</li> </ol> <p>By prioritizing directions with high variance, PCA effectively compresses the data while trying to retain as much original information as possible. The new features (principal components) are also <strong>linearly uncorrelated</strong>, which is a huge bonus for many machine learning algorithms.</p> <h3 id="how-does-pca-work-a-step-by-step-breakdown">How Does PCA Work? A Step-by-Step Breakdown</h3> <p>Alright, let’s peek behind the curtain and see the magic happen. PCA uses fundamental concepts from linear algebra, particularly <strong>eigenvalues</strong> and <strong>eigenvectors</strong>. Don’t let those words scare you; we’ll break them down.</p> <p>Imagine our dataset $X$ has $n$ observations and $D$ features.</p> <h4 id="step-1-standardize-the-data">Step 1: Standardize the Data</h4> <p>Before we do anything, we need to ensure all our features are on the same scale. If one feature (e.g., house size in square feet) ranges from 1000 to 5000, and another (e.g., number of bathrooms) ranges from 1 to 5, the “house size” feature will dominate the variance calculation just because of its larger numerical values.</p> <p>To prevent this, we standardize the data. For each feature, we subtract its mean and divide by its standard deviation. This transforms the data so that each feature has a mean of 0 and a standard deviation of 1.</p> <p>The formula for standardization (also called Z-score normalization) for a data point $x$ for a feature with mean $\mu$ and standard deviation $\sigma$ is:</p> <p>$ z = \frac{x - \mu}{\sigma} $</p> <p>After this step, our data matrix $X$ (now standardized) is ready.</p> <h4 id="step-2-compute-the-covariance-matrix">Step 2: Compute the Covariance Matrix</h4> <p>Now, we need to understand how our features relate to each other. Are they correlated? If one feature increases, does another tend to increase or decrease? This is where the <strong>covariance matrix</strong> comes in.</p> <p>The covariance matrix, often denoted as $C$ or $\Sigma$, is a square matrix where:</p> <ul> <li>The elements on the diagonal ($C_{ii}$) represent the variance of each individual feature.</li> <li>The off-diagonal elements ($C_{ij}$ for $i \ne j$) represent the covariance between feature $i$ and feature $j$. A positive covariance means they tend to increase together; a negative covariance means one tends to increase while the other decreases; zero covariance means they don’t have a linear relationship.</li> </ul> <p>For our standardized data matrix $X$ (where each column is a feature and each row is an observation), the covariance matrix can be calculated as:</p> <p>$ C = \frac{1}{n-1} X^T X $</p> <p>Here, $X^T$ is the transpose of $X$, and $n$ is the number of observations. This matrix will tell us the spread and inter-relationships among all our features.</p> <h4 id="step-3-compute-eigenvalues-and-eigenvectors">Step 3: Compute Eigenvalues and Eigenvectors</h4> <p>This is the mathematical core of PCA, and it’s where the principal components are born.</p> <p>An <strong>eigenvector</strong> is a special vector that, when a linear transformation (like multiplying by our covariance matrix $C$) is applied to it, only changes in magnitude, not direction. It simply gets scaled by a scalar factor. That scalar factor is its corresponding <strong>eigenvalue</strong>.</p> <p>Mathematically, for a square matrix $C$, an eigenvector $v$ and its eigenvalue $\lambda$ satisfy the equation:</p> <p>$ Cv = \lambda v $</p> <p>In our context:</p> <ul> <li>The <strong>eigenvectors</strong> of the covariance matrix are the <strong>principal components</strong> themselves. They represent the directions (axes) of maximum variance in the data.</li> <li>The <strong>eigenvalues</strong> tell us the amount of variance captured along each of those principal components. A larger eigenvalue means more variance (and thus more information) is captured by its corresponding eigenvector.</li> </ul> <p>When we calculate the eigenvectors and eigenvalues of our covariance matrix, we’ll get $D$ eigenvectors (since our original data has $D$ features/dimensions), and $D$ corresponding eigenvalues. Each eigenvector will have a specific eigenvalue, representing the importance (variance) of that direction.</p> <h4 id="step-4-select-principal-components">Step 4: Select Principal Components</h4> <p>Now that we have all our principal components (eigenvectors) and their corresponding variances (eigenvalues), we need to decide how many to keep. We sort the eigenvectors in descending order based on their eigenvalues. The eigenvector with the largest eigenvalue is PC1, the one with the second largest is PC2, and so on.</p> <p>To reduce dimensionality, we select the top $k$ eigenvectors (principal components) that correspond to the largest eigenvalues. But how do we choose $k$?</p> <ul> <li> <strong>Explained Variance Ratio:</strong> We can look at the cumulative sum of the eigenvalues. Each eigenvalue’s proportion of the total sum tells us how much variance that principal component explains. We usually aim to retain a certain percentage of the total variance (e.g., 95% or 99%). The explained variance ratio for the $i$-th component is $ \frac{\lambda<em>i}{\sum</em>{j=1}^D \lambda<em>j} $. The cumulative explained variance for the top $k$ components is $ \frac{\sum</em>{i=1}^k \lambda<em>i}{\sum</em>{j=1}^D \lambda_j} $.</li> <li> <strong>Scree Plot:</strong> This is a plot of eigenvalues vs. the number of components. You look for an “elbow” in the plot, where the eigenvalues start to drop off significantly. This indicates that subsequent components explain much less variance.</li> </ul> <p>Once we’ve chosen our top $k$ principal components, we form a projection matrix $W$ by concatenating these $k$ eigenvectors side-by-side. $W$ will be a $D \times k$ matrix.</p> <h4 id="step-5-project-data-onto-new-subspace">Step 5: Project Data onto New Subspace</h4> <p>Finally, we transform our original standardized data $X$ into the new, lower-dimensional space using our projection matrix $W$.</p> <p>The new dataset, $Y$, with $k$ dimensions, is calculated as:</p> <p>$ Y = X W $</p> <p>Where $X$ is the standardized $n \times D$ data matrix, and $W$ is the $D \times k$ matrix of selected principal components. The resulting $Y$ is an $n \times k$ matrix, where $n$ is the number of observations and $k$ is the reduced number of dimensions.</p> <p>Voilà! You now have a dataset with fewer features ($k &lt; D$), where the new features (principal components) capture the most important information (variance) from your original data.</p> <h3 id="applications-of-pca-in-the-real-world">Applications of PCA in the Real World</h3> <p>PCA isn’t just a theoretical exercise; it’s a workhorse in data science:</p> <ul> <li> <strong>Image Compression:</strong> Ever heard of “eigenfaces”? PCA can be used to represent faces with fewer dimensions, which is crucial for facial recognition systems and image storage.</li> <li> <strong>Noise Reduction:</strong> By focusing on directions with high variance, PCA often filters out dimensions that primarily contain noise, leading to cleaner data.</li> <li> <strong>Data Visualization:</strong> Reducing 50-dimensional data to 2 or 3 principal components allows us to plot and visually inspect relationships that were previously hidden.</li> <li> <strong>Preprocessing for ML Models:</strong> Many algorithms perform better and faster when given a reduced, uncorrelated set of features. Think about training times for complex deep learning models!</li> <li> <strong>Bioinformatics:</strong> Analyzing gene expression data, which can have thousands of dimensions.</li> </ul> <h3 id="limitations-and-considerations">Limitations and Considerations</h3> <p>While powerful, PCA isn’t a magic bullet:</p> <ul> <li> <strong>Linearity Assumption:</strong> PCA only works well for <strong>linear</strong> relationships in your data. If your data has complex non-linear structures, standard PCA might not capture them effectively (though Kernel PCA can help with this).</li> <li> <strong>Interpretability:</strong> The principal components are linear combinations of the original features. This means PC1 might be <code class="language-plaintext highlighter-rouge">0.3 * feature_A + 0.6 * feature_B - 0.1 * feature_C</code>, which can be harder to interpret directly than a single original feature.</li> <li> <strong>Scaling:</strong> As we saw, proper standardization is crucial. If features aren’t scaled, features with larger scales will disproportionately influence the principal components.</li> <li> <strong>Information Loss:</strong> PCA is a lossy compression technique. You <em>will</em> lose some information when reducing dimensionality. The trick is to lose the <em>least important</em> information.</li> </ul> <h3 id="conclusion-embracing-simplicity-in-complexity">Conclusion: Embracing Simplicity in Complexity</h3> <p>My journey through understanding PCA was a significant one. It showed me the elegance of linear algebra in solving real-world data challenges. It’s a testament to the idea that sometimes, the best way to understand something complex is to find its simplest, most informative representation.</p> <p>PCA empowers us to:</p> <ul> <li>Tame the “Curse of Dimensionality.”</li> <li>Improve model performance and efficiency.</li> <li>Unlock hidden insights through visualization.</li> </ul> <p>So, the next time you find yourself drowning in a sea of features, remember PCA. It might just be the lifesaver your data needs to surface its true story. Dive in, experiment, and enjoy the clarity it brings!</p> <p>Keep exploring, and happy modeling!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>