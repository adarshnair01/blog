<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Quest for Quality: Unveiling the Magic of Q-Learning | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/the-quest-for-quality-unveiling-the-magic-of-q-lea/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Quest for Quality: Unveiling the Magic of Q-Learning</h1> <p class="post-meta"> Created on April 14, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/reinforcement-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Reinforcement Learning</a>   <a href="/blog/blog/tag/q-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Q-Learning</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>As someone who’s constantly fascinated by how intelligence works, both biological and artificial, diving into the world of Reinforcement Learning (RL) felt like a natural next step in my data science journey. It’s a field brimming with algorithms that teach agents to learn optimal behaviors through interaction with an environment, much like how we learn from our own experiences. And among these algorithms, one of the most foundational, intuitive, and, frankly, coolest, is <strong>Q-Learning</strong>.</p> <p>Today, I want to take you on a journey to understand Q-Learning. My goal is to break it down in a way that’s accessible whether you’re just starting your exploration into AI or looking to solidify your understanding of this powerful technique. Think of this as me sharing my “Aha!” moments as I grasped this concept, hoping to spark yours too!</p> <h3 id="the-grand-idea-learning-from-experience">The Grand Idea: Learning from Experience</h3> <p>Before we zoom into Q-Learning, let’s zoom out a bit to the bigger picture: Reinforcement Learning. Imagine you’re teaching a dog a new trick. You don’t give it a manual; instead, you give it treats (rewards) when it does something right and maybe a gentle “no” (penalty) when it doesn’t. Over time, the dog learns which actions lead to treats and which don’t.</p> <p>Reinforcement Learning works similarly. We have an <strong>agent</strong> (our AI) that lives in an <strong>environment</strong>. The agent performs <strong>actions</strong> in a given <strong>state</strong> of the environment. For each action, the environment responds with a <strong>reward</strong> (positive or negative) and transitions to a <strong>new state</strong>. The agent’s ultimate goal? To learn a <strong>policy</strong> – a strategy of what action to take in each state – that maximizes its total accumulated reward over time.</p> <p>This trial-and-error process, driven by rewards, is the essence of RL. No explicit supervisor tells the agent what to do; it learns through interaction.</p> <h3 id="enter-q-learning-the-quality-of-choices">Enter Q-Learning: The “Quality” of Choices</h3> <p>Now, let’s talk about Q-Learning. The “Q” in Q-Learning stands for <strong>“Quality”</strong> (or sometimes, “Q-value”). At its heart, Q-Learning is about learning an <strong>action-value function</strong>, denoted as $Q(s, a)$. This function tells us the “quality” or expected future reward of taking a specific <em>action</em> ($a$) in a specific <em>state</em> ($s$).</p> <p>Think of it like this: Imagine you’re trying to navigate a complex maze to find a treasure. For every junction (state) you encounter, and for every path you could take (action), $Q(s, a)$ would tell you how “good” that path choice is in terms of eventually leading you to the treasure and maximizing your overall score. Initially, you have no idea which paths are good, so all your $Q$-values might be zero or random. But as you explore and find rewards (or run into dead ends), you’d update your understanding of these paths.</p> <p>Q-Learning is a <strong>model-free</strong> algorithm, which is super cool! It means the agent doesn’t need to know anything about the environment’s dynamics (like what the next state will be for a given action). It learns purely from observed experiences. It’s also an <strong>off-policy</strong> algorithm, meaning it can learn the optimal policy (the best sequence of actions) while following a different exploration policy (like trying out random things sometimes).</p> <h3 id="the-q-table-our-agents-scorecard">The Q-Table: Our Agent’s Scorecard</h3> <p>For environments with a finite, manageable number of states and actions (which we call discrete state and action spaces), Q-Learning often uses a simple data structure called a <strong>Q-table</strong>. This table is essentially a lookup table where rows represent states and columns represent actions. Each cell $(s, a)$ in the table stores the $Q(s, a)$ value.</p> <table> <thead> <tr> <th style="text-align: left">State / Action</th> <th style="text-align: left">Action 1</th> <th style="text-align: left">Action 2</th> <th style="text-align: left">Action 3</th> <th style="text-align: left">…</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">State A</td> <td style="text-align: left">$Q(A,1)$</td> <td style="text-align: left">$Q(A,2)$</td> <td style="text-align: left">$Q(A,3)$</td> <td style="text-align: left">…</td> </tr> <tr> <td style="text-align: left">State B</td> <td style="text-align: left">$Q(B,1)$</td> <td style="text-align: left">$Q(B,2)$</td> <td style="text-align: left">$Q(B,3)$</td> <td style="text-align: left">…</td> </tr> <tr> <td style="text-align: left">State C</td> <td style="text-align: left">$Q(C,1)$</td> <td style="text-align: left">$Q(C,2)$</td> <td style="text-align: left">$Q(C,3)$</td> <td style="text-align: left">…</td> </tr> <tr> <td style="text-align: left">…</td> <td style="text-align: left">…</td> <td style="text-align: left">…</td> <td style="text-align: left">…</td> <td style="text-align: left">…</td> </tr> </tbody> </table> <p>At the beginning, all the $Q$-values in this table are usually initialized to zero (or small random numbers). The agent then starts exploring the environment, taking actions, receiving rewards, and most importantly, <em>updating</em> these $Q$-values.</p> <h3 id="the-heart-of-q-learning-the-update-rule">The Heart of Q-Learning: The Update Rule</h3> <p>This is where the magic happens, and it’s captured in a single, powerful equation derived from the Bellman Equation. Don’t worry if it looks intimidating at first; we’ll break it down piece by piece.</p> <p>When our agent is in state $s$, takes action $a$, receives an immediate reward $R$, and transitions to a new state $s’$, it updates its knowledge about $Q(s, a)$ using the following formula:</p> \[Q(s, a) \leftarrow Q(s, a) + \alpha [R + \gamma \max_{a'} Q(s', a') - Q(s, a)]\] <p>Let’s dissect each component:</p> <ul> <li> <strong>$Q(s, a)$ (on the left side):</strong> This is the <strong>new Q-value</strong> we are calculating for the state-action pair $(s, a)$. We’re updating our estimate.</li> <li> <strong>$Q(s, a)$ (on the right side):</strong> This is the <strong>old Q-value</strong> – our current estimate before the update.</li> <li> <strong>$\alpha$ (alpha): The Learning Rate.</strong> This value, between 0 and 1, determines how much our new information (the “TD Error” part) affects our current Q-value. A high $\alpha$ means the agent learns quickly from new experiences but might be volatile. A low $\alpha$ means slower, more stable learning.</li> <li> <strong>$R$: The Immediate Reward.</strong> This is the reward the agent received right after taking action $a$ in state $s$. It’s a direct signal of how good or bad that action was <em>in that moment</em>.</li> <li> <strong>$\gamma$ (gamma): The Discount Factor.</strong> Also between 0 and 1. This factor determines the importance of future rewards. <ul> <li>If $\gamma$ is close to 0, the agent focuses only on immediate rewards. It’s short-sighted.</li> <li>If $\gamma$ is close to 1, the agent considers future rewards almost as important as immediate ones. It’s far-sighted.</li> <li>This is crucial for preventing infinite loops in some environments and for prioritizing rewards that are “closer” in time.</li> </ul> </li> <li> <strong>$\max_{a’} Q(s’, a’)$: The Maximum Future Q-value.</strong> This is the most crucial part for “optimality.” From the <em>new state</em> $s’$, the agent imagines taking the <em>best possible action</em> $a’$ that would lead to the maximum future reward. This is where Q-Learning’s “greedy” future prediction comes in – it assumes the <em>optimal</em> path will be followed from the next state onwards.</li> <li> <strong>$[R + \gamma \max_{a’} Q(s’, a’) - Q(s, a)]$: The Temporal Difference (TD) Error.</strong> This entire bracketed term is the “surprise” or the difference between what the agent <em>predicted</em> its Q-value for $(s, a)$ would be (which is $Q(s,a)$) and what it <em>actually experienced/updated</em> it to be (the $R + \gamma \max_{a’} Q(s’, a’)$ part). <ul> <li>If the TD Error is positive, the action $a$ in state $s$ was better than expected.</li> <li>If it’s negative, it was worse.</li> <li>We use this error to nudge our $Q(s, a)$ estimate in the right direction.</li> </ul> </li> </ul> <p>This update rule is applied repeatedly as the agent interacts with the environment, gradually refining its Q-table until the Q-values converge to represent the optimal action-value function.</p> <h3 id="the-eternal-dilemma-explore-vs-exploit">The Eternal Dilemma: Explore vs. Exploit</h3> <p>Imagine you’ve found a restaurant you really like (it gives you high “rewards”). Do you keep going back to that restaurant (exploit your knowledge)? Or do you try a new one, risking a bad meal but potentially discovering an even <em>better</em> restaurant (explore)?</p> <p>This is the <strong>exploration-exploitation dilemma</strong>, and it’s central to RL.</p> <ul> <li> <strong>Exploitation</strong> means choosing the action that currently has the highest Q-value in a given state. This makes the agent perform well based on its current knowledge.</li> <li> <strong>Exploration</strong> means trying out random or less-known actions, even if they don’t seem optimal right now. This is crucial for discovering better paths or avoiding local optima (where the agent thinks it’s found the best solution, but a truly better one exists elsewhere).</li> </ul> <p>A common strategy to balance this is <strong>$\epsilon$-greedy exploration</strong>:</p> <ul> <li>With a small probability $\epsilon$ (epsilon), the agent chooses a random action (explores).</li> <li>With probability $1 - \epsilon$, the agent chooses the action with the highest Q-value (exploits).</li> </ul> <p>Typically, $\epsilon$ starts high (e.g., 1.0, meaning always explore initially) and gradually decays over time. This way, the agent explores a lot at the beginning to learn its environment and then slowly shifts to exploiting its learned knowledge to perform optimally.</p> <h3 id="a-simple-walkthrough-the-frozen-lake">A Simple Walkthrough: The Frozen Lake</h3> <p>Let’s imagine a classic RL problem: the <strong>Frozen Lake environment</strong>. Our agent starts on a frozen lake, needing to reach a goal. Some tiles are safe (F for Frozen), others are holes (H for Hole), and the goal is G. If it falls into a hole, it gets a large negative reward. If it reaches the goal, it gets a large positive reward. Every other step gives a small negative reward to encourage efficiency.</p> <ol> <li> <strong>Initialization:</strong> A Q-table is created with rows for each tile and columns for each action (Up, Down, Left, Right), all values set to 0.</li> <li> <strong>Episode 1 (High $\epsilon$):</strong> <ul> <li>Agent starts at (0,0). $\epsilon$ is high, so it probably takes a random action, say “Right”.</li> <li>It moves to (0,1), receives a small negative reward (for taking a step), and updates $Q((0,0), \text{Right})$ using the update rule. Since $Q((0,1), a’)$ are all zero, the update is mostly based on the immediate reward.</li> <li>It continues randomly, perhaps falling into a hole. It gets a big negative reward, which propagates back through the $Q$-values for the actions that led to the hole. This “badness” starts spreading.</li> </ul> </li> <li> <strong>Subsequent Episodes (Decreasing $\epsilon$):</strong> <ul> <li>As $\epsilon$ decreases, the agent starts choosing actions with higher Q-values more often.</li> <li>If it previously discovered a path that led to a reward (even small), those Q-values will be slightly positive.</li> <li>The “max” term in the update rule ($ \max_{a’} Q(s’, a’)$) is key. If the agent reaches a state $s’$ from which it <em>knows</em> a good path to the goal, that strong future Q-value gets “backed up” to the previous state-action pair $Q(s, a)$.</li> <li>Over thousands of episodes, the positive rewards from reaching the goal will gradually propagate backward, making the $Q$-values for actions on the optimal path much higher than those leading to holes or dead ends.</li> </ul> </li> <li> <strong>Convergence:</strong> Eventually, the Q-table will stabilize, reflecting the optimal policy. The agent will “know” which action to take in every state to reach the goal safely and efficiently.</li> </ol> <h3 id="advantages-of-q-learning">Advantages of Q-Learning</h3> <ul> <li> <strong>Model-Free:</strong> It doesn’t need to know the environment’s rules or transition probabilities. It learns purely from interaction, which is incredibly powerful for complex real-world scenarios where explicitly modeling the environment is impossible.</li> <li> <strong>Off-Policy:</strong> It can learn the optimal policy even while following an exploratory policy (like $\epsilon$-greedy). This means it can gather information about optimal paths while simultaneously exploring the environment.</li> <li> <strong>Simplicity:</strong> For discrete state and action spaces, the Q-table is straightforward to implement and understand.</li> </ul> <h3 id="limitations-of-q-learning">Limitations of Q-Learning</h3> <ul> <li> <strong>Curse of Dimensionality:</strong> This is the big one! If the number of states or actions becomes very large (e.g., a high-resolution image as a state, or continuous control like steering a car), storing a Q-table becomes impossible. The table would be astronomically huge.</li> <li> <strong>Discrete Spaces Only (in its basic form):</strong> Standard Q-Learning struggles with continuous state or action spaces. You’d have to discretize them, which can lead to a loss of information or an explosion in the number of states/actions.</li> <li> <strong>Convergence Speed:</strong> For very complex problems, even with discrete spaces, the number of episodes required for the Q-table to converge can be immense.</li> </ul> <h3 id="beyond-the-table-the-future-of-q-learning">Beyond the Table: The Future of Q-Learning</h3> <p>The limitations of the Q-table paved the way for more advanced RL techniques. This is where <strong>Deep Q-Networks (DQNs)</strong> come into play, replacing the explicit Q-table with a neural network that approximates the $Q(s, a)$ function. This allows DQNs to handle high-dimensional, continuous state spaces (like pixels from a game screen) and generalize well, leading to agents that can play Atari games better than humans!</p> <p>But even with DQNs, the core idea of learning action-values, the Bellman equation, and the exploration-exploitation dilemma remain fundamental. Q-Learning is truly the bedrock upon which many modern RL advancements are built.</p> <h3 id="final-thoughts">Final Thoughts</h3> <p>Q-Learning, with its elegant update rule and intuitive approach to learning action values, is a cornerstone of Reinforcement Learning. It demystifies how an agent can learn optimal behavior from nothing but trial, error, and a reward signal. It’s a testament to the power of iterative learning and how simple rules can lead to complex, intelligent behavior.</p> <p>My hope is that this deep dive has demystified Q-Learning for you, making its equations and concepts feel less like abstract math and more like the ingenious engine behind autonomous learning. The journey into RL is incredibly rewarding, and understanding Q-Learning is a fantastic first step into this exciting frontier of AI!</p> <p>What’s your favorite RL analogy? Share your thoughts!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>