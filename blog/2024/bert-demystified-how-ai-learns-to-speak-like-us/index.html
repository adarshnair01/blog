<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> BERT Demystified: How AI Learns to Speak Like Us | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/bert-demystified-how-ai-learns-to-speak-like-us/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">BERT Demystified: How AI Learns to Speak Like Us</h1> <p class="post-meta"> Created on August 19, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/natural-language-processing"> <i class="fa-solid fa-hashtag fa-sm"></i> Natural Language Processing</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/transformers"> <i class="fa-solid fa-hashtag fa-sm"></i> Transformers</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hello, fellow language explorers!</p> <p>Have you ever stopped to think about how incredibly complex human language is? The way we effortlessly switch between meanings, understand context, and even detect sarcasm is truly remarkable. For years, teaching a computer to grasp these subtleties was one of the holy grails of Artificial Intelligence. I remember feeling a mix of frustration and awe when confronting the vast challenges of Natural Language Processing (NLP).</p> <p>Then came BERT. And for many of us in the data science world, it felt like a monumental shift. BERT isn’t just another algorithm; it’s a paradigm changer, a powerful linguistic detective that truly understands the “bidirectional” context of words. If you’ve ever wondered how Google Search got so good, or how chatbots seem to <em>get</em> you, chances are BERT (or one of its descendants) is working tirelessly behind the scenes.</p> <p>Join me on a journey to demystify BERT, understand its inner workings, and appreciate why it became such a game-changer in the world of AI.</p> <h3 id="the-bank-problem-why-language-is-tricky-for-computers">The “Bank” Problem: Why Language Is Tricky for Computers</h3> <p>Before we dive into BERT, let’s understand the problem it solves. Consider the word “bank.”</p> <ul> <li>“I went to the <strong>bank</strong> to deposit my paycheck.”</li> <li>“The boat was tied to the river <strong>bank</strong>.”</li> </ul> <p>As humans, we instantly know that “bank” refers to a financial institution in the first sentence and the edge of a river in the second. Our brains process the surrounding words, the “context,” to disambiguate.</p> <p>Earlier NLP models struggled with this. Simple word embedding techniques like Word2Vec and GloVe would assign a single, static vector (a numerical representation) to the word “bank,” regardless of its context. While a step up from one-hot encoding, this meant they couldn’t capture the fluidity of language. Recurrent Neural Networks (RNNs) and their more advanced cousins, Long Short-Term Memory (LSTM) networks, tried to process language sequentially, remembering past words. They were better, but still faced limitations:</p> <ol> <li> <strong>Sequential Bottleneck</strong>: They processed one word at a time, making them slow and unable to truly parallelize learning.</li> <li> <strong>Long-Term Dependencies</strong>: Struggled to connect words that were far apart in a sentence, losing context over longer sequences.</li> </ol> <p>We needed a model that could look at <em>all</em> the words in a sentence at once, understand their relationships, and dynamically adjust the meaning of each word based on its neighbours.</p> <h3 id="enter-the-transformer-attention-is-all-you-need">Enter the Transformer: Attention Is All You Need</h3> <p>The real breakthrough that paved the way for BERT came in 2017 with a paper from Google titled “Attention Is All You Need.” This paper introduced the <strong>Transformer</strong> architecture, which completely ditched sequential processing in favor of a mechanism called <strong>self-attention</strong>.</p> <p>Imagine you’re trying to understand a complex sentence. Instead of reading word by word, you might quickly scan the whole thing, then focus your attention on the words most relevant to each specific word you’re trying to interpret. That’s precisely what self-attention does!</p> <p>For each word in a sentence, the Transformer asks: “How much should I pay attention to <em>every other word</em> in this sentence (including myself) to understand <em>this specific word</em>?”</p> <p>It does this by creating three different vector representations for each word:</p> <ul> <li> <strong>Query (Q)</strong>: What I’m looking for.</li> <li> <strong>Key (K)</strong>: What I can offer.</li> <li> <strong>Value (V)</strong>: The actual information I carry.</li> </ul> <p>The attention score between a query word and every key word is calculated. The more similar a query vector is to a key vector, the higher their attention score. These scores are then normalized (often using a softmax function) and used to weigh the value vectors. The output is a new, context-rich representation for each word.</p> <p>Mathematically, for a given set of queries $Q$, keys $K$, and values $V$, the scaled dot-product attention is calculated as:</p> \[\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\] <p>Here, $d_k$ is the dimension of the key vectors, used for scaling to prevent very large dot products from pushing the softmax into regions with extremely small gradients. This formula effectively tells the model: “For each word (query), sum up the values of all other words (keys), weighted by how relevant they are.”</p> <p>But if we’re processing all words in parallel, how does the model know the <em>order</em> of words? That’s where <strong>Positional Encoding</strong> comes in. Small numerical vectors are added to the word embeddings based on their position in the sentence, reintroducing the crucial sense of order that a purely parallel self-attention mechanism would otherwise lose.</p> <h3 id="bert-the-bidirectional-revolution">BERT: The Bidirectional Revolution</h3> <p>In 2018, Google researchers unveiled <strong>BERT</strong>, which stands for <strong>Bidirectional Encoder Representations from Transformers</strong>. This was the moment everything changed for NLP. BERT took the powerful Transformer encoder architecture and applied a novel pre-training strategy that unlocked unprecedented understanding of language context.</p> <p>The “Bidirectional” part is key. Previous models, like the original GPT (Generative Pre-trained Transformer), processed text only from left-to-right. This is fine for generating text, but for deep comprehension, you need to see both “the past” and “the future.”</p> <p>Consider the sentence: “The animal didn’t cross the street because it was too wide.” Here, “wide” refers to the street. Now consider: “The animal didn’t cross the street because it was too tired.” Here, “tired” refers to the animal.</p> <p>To truly understand “it” in these sentences, an AI needs to look both backwards <em>and</em> forwards. BERT achieves this through its ingenious <strong>pre-training tasks</strong>:</p> <h4 id="1-masked-language-model-mlm">1. Masked Language Model (MLM)</h4> <p>BERT’s primary pre-training task is like a high-stakes “fill-in-the-blanks” game. The model randomly masks (hides) about 15% of the words in a sentence and then tries to predict those masked words based on <em>all</em> the other words in the sentence – both to the left and to the right.</p> <p>For example, if the sentence is “The quick brown fox jumps over the lazy dog,” BERT might see “The quick brown [MASK] jumps over the lazy dog” and has to predict “fox.”</p> <p>This forces the model to develop a deep, bidirectional understanding of context. It can’t just guess based on the word before; it has to infer the word’s identity from its entire surroundings. This is a significant improvement over traditional language models that only predict the next word in a sequence.</p> <h4 id="2-next-sentence-prediction-nsp">2. Next Sentence Prediction (NSP)</h4> <p>Understanding individual words is great, but real language involves understanding relationships <em>between</em> sentences. For tasks like Question Answering or Text Summarization, knowing if one sentence logically follows another is crucial.</p> <p>BERT tackles this with Next Sentence Prediction. It’s fed pairs of sentences (Sentence A and Sentence B) and asked to predict if Sentence B is the actual next sentence that follows Sentence A (labeled <code class="language-plaintext highlighter-rouge">IsNext</code>) or if it’s a random sentence from the corpus (labeled <code class="language-plaintext highlighter-rouge">NotNext</code>).</p> <p>To do this, BERT concatenates the two sentences, separating them with a special <code class="language-plaintext highlighter-rouge">[SEP]</code> token, and puts a <code class="language-plaintext highlighter-rouge">[CLS]</code> token at the beginning. It then uses the hidden state corresponding to the <code class="language-plaintext highlighter-rouge">[CLS]</code> token for the binary classification task. This trains BERT to understand sentence-level relationships, coherence, and discourse.</p> <h3 id="the-architecture-transformer-encoder-stack">The Architecture: Transformer Encoder Stack</h3> <p>At its core, BERT is simply a stack of Transformer <strong>encoders</strong>. Remember that each encoder block has two main sub-layers: a multi-head self-attention mechanism and a position-wise feed-forward network. BERT stacks many of these identical blocks (e.g., BERT-base has 12 layers, BERT-large has 24 layers), creating a deep network capable of capturing intricate language patterns.</p> <p>The input to BERT starts with <strong>token embeddings</strong> (numerical representations of words or sub-word units), <strong>segment embeddings</strong> (to distinguish between Sentence A and Sentence B), and <strong>position embeddings</strong> (to encode word order). These are summed to form the final input embeddings that feed into the first Transformer encoder layer.</p> <h3 id="the-power-of-transfer-learning-fine-tuning-bert">The Power of Transfer Learning: Fine-tuning BERT</h3> <p>One of BERT’s most revolutionary aspects is its ability to perform <strong>transfer learning</strong>. This means you pre-train a large, general-purpose model once on a massive amount of text data (like Wikipedia and BooksCorpus), and then you can reuse that pre-trained model for various specific NLP tasks with relatively small, task-specific datasets.</p> <p>Think of it like this: BERT has already gone through a rigorous language school, learning grammar, vocabulary, and context on billions of words. Now, if you want it to become a “sentiment analysis expert,” you don’t need to teach it language from scratch. You just need to add a small, specialized “sentiment analysis layer” on top of the pre-trained BERT and fine-tune it with your labeled sentiment data. The pre-trained BERT weights get slightly adjusted during this fine-tuning, adapting its general language understanding to your specific task.</p> <p>This process has democratized powerful NLP. Researchers and developers no longer need petabytes of text and massive computing clusters to achieve state-of-the-art results. They can leverage pre-trained BERT models and fine-tune them on their own modest datasets. BERT has achieved top performance on a wide range of tasks, including:</p> <ul> <li> <strong>Sentiment Analysis</strong>: Classifying text as positive, negative, or neutral.</li> <li> <strong>Question Answering (e.g., SQuAD)</strong>: Finding the answer to a question within a given passage of text.</li> <li> <strong>Named Entity Recognition (NER)</strong>: Identifying and classifying named entities (like people, organizations, locations) in text.</li> <li> <strong>Text Classification</strong>: Categorizing documents into predefined classes.</li> </ul> <h3 id="why-bert-changed-everything">Why BERT Changed Everything</h3> <p>BERT’s impact on NLP cannot be overstated. It didn’t just push the boundaries of accuracy; it fundamentally changed the methodology for developing NLP applications.</p> <ul> <li> <strong>State-of-the-Art Performance</strong>: BERT achieved new benchmarks across numerous NLP tasks, showcasing the power of deep bidirectional Transformers.</li> <li> <strong>True Contextual Understanding</strong>: It moved beyond static word embeddings to dynamic, context-aware representations, solving the “bank” problem once and for all.</li> <li> <strong>Enabled Transfer Learning</strong>: Made it possible for individuals and smaller teams to leverage powerful models without immense computational resources, greatly accelerating NLP research and application development.</li> <li> <strong>Foundation for Future Research</strong>: BERT became the foundational model upon which countless subsequent advancements were built, inspiring an entire generation of large language models.</li> </ul> <h3 id="beyond-bert-whats-next">Beyond BERT: What’s Next?</h3> <p>While BERT was revolutionary, the field didn’t stop there. Researchers quickly identified areas for improvement and built upon its success:</p> <ul> <li> <strong>RoBERTa (Robustly Optimized BERT Pretraining Approach)</strong>: Showed that BERT was undertrained and could perform even better with more data, longer training, and dynamic masking.</li> <li> <strong>ALBERT (A Lite BERT)</strong>: Reduced BERT’s memory footprint and increased its training speed through parameter sharing across layers and factorized embedding parameterization.</li> <li> <strong>ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately)</strong>: Introduced a more efficient pre-training task where the model predicts if each token was replaced by a small generator network, rather than just masking.</li> <li>And of course, models like <strong>GPT-3/4, PaLM, LLaMA</strong>, and many others have taken the Transformer architecture to even greater scales, leading to the sophisticated generative AI we see today.</li> </ul> <h3 id="conclusion">Conclusion</h3> <p>BERT truly was a game-changer. For me, it was a moment of profound understanding in the world of AI, showing how we could finally enable machines to grasp the rich, messy, and beautiful complexity of human language. It demonstrated that by rethinking how models perceive context and by leveraging massive amounts of data through clever pre-training tasks, we could unlock capabilities that once seemed like science fiction.</p> <p>If you’re embarking on your own data science journey, understanding BERT is an essential milestone. It’s not just a historical artifact; its core concepts of self-attention, bidirectional context, and transfer learning remain central to virtually all modern NLP. So, next time you interact with a smart AI application, take a moment to appreciate the silent linguistic hero, BERT, working behind the scenes, diligently helping computers learn to speak like us. I encourage you to explore its open-source implementations and experiment with fine-tuning it for your own language tasks – you might just discover your own linguistic superpowers!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>