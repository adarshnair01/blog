<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Peeking Inside the AI Brain: Your First Dive into Neural Networks | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/peeking-inside-the-ai-brain-your-first-dive-into-n/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Peeking Inside the AI Brain: Your First Dive into Neural Networks</h1> <p class="post-meta"> Created on April 01, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/neural-networks"> <i class="fa-solid fa-hashtag fa-sm"></i> Neural Networks</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>As a kid, I was always fascinated by how our brains worked. How do we learn to recognize a cat, distinguish a smile from a frown, or understand the nuances of a new language? These complex tasks, seemingly effortless for us, are actually the result of billions of tiny, interconnected cells called neurons firing away.</p> <p>Fast forward to today, and we’re seeing machines accomplish similarly astonishing feats: identifying objects in photos, composing music, driving cars, and even beating human champions at complex games. What’s the secret sauce behind this explosion of artificial intelligence? Often, it’s something inspired directly by our own biology: the <strong>Neural Network</strong>.</p> <p>Today, I want to take you on a journey – a peek inside the ‘brain’ of AI. We’ll demystify these powerful structures, understand their basic building blocks, and grasp the core idea of how they learn. Don’t worry, we’ll keep it accessible, but we won’t shy away from a little math to truly understand the mechanics!</p> <h3 id="the-spark-of-inspiration-our-own-brain">The Spark of Inspiration: Our Own Brain</h3> <p>Let’s start with a quick look at biological neurons. Imagine them as tiny processors in your brain. Each neuron has:</p> <ul> <li> <strong>Dendrites:</strong> Tree-like branches that receive signals from other neurons.</li> <li> <strong>Soma (Cell Body):</strong> The neuron’s “headquarters” that processes these signals.</li> <li> <strong>Axon:</strong> A long cable that transmits the processed signal to other neurons.</li> <li> <strong>Synapses:</strong> The tiny gaps where axons connect to dendrites, allowing signals to pass.</li> </ul> <p>When a neuron receives enough signals (excitatory input) that cross a certain threshold, it “fires,” sending its own signal down the axon to connected neurons. It’s a binary decision: fire or don’t fire. This simple, elegant mechanism is what forms the basis of all our thoughts, memories, and actions.</p> <p>Now, let’s translate this biological marvel into the digital realm.</p> <h3 id="the-artificial-neuron-the-perceptron">The Artificial Neuron: The Perceptron</h3> <p>The fundamental unit of an artificial neural network is, unsurprisingly, the <strong>artificial neuron</strong>, often called a <strong>Perceptron</strong>. It’s a simplified mathematical model of its biological counterpart.</p> <p>Imagine you’re trying to decide if a day is “good for a picnic.” You’d consider a few factors:</p> <ol> <li>Is it sunny?</li> <li>Is it warm?</li> <li>Is it windy?</li> </ol> <p>Each of these factors is an <strong>input</strong> to your decision-making process. But some factors might be more important than others. A little wind might be okay, but rain (which we can represent as a negative input, let’s say) is a definite no-go. This “importance” is captured by <strong>weights</strong>.</p> <p>Here’s how an artificial neuron works:</p> <ul> <li> <strong>Inputs ($x_1, x_2, …, x_n$):</strong> These are the pieces of information (features) we feed into the neuron. For our picnic example, $x_1$ could be a numerical value for “sunniness,” $x_2$ for “temperature,” etc.</li> <li> <strong>Weights ($w_1, w_2, …, w_n$):</strong> Each input $x_i$ is multiplied by a corresponding weight $w_i$. Weights represent the strength or importance of each input. A large positive weight means that input strongly contributes to the neuron “firing,” while a large negative weight inhibits it.</li> <li> <strong>Bias ($b$):</strong> Think of bias as an extra “knob” or an inherent inclination. It allows the neuron to activate even if all inputs are zero, or to remain inactive even if some inputs are positive. It essentially shifts the activation threshold.</li> <li> <strong>Summation:</strong> The neuron calculates the <em>weighted sum</em> of its inputs and adds the bias. \(z = \sum_{i=1}^{n} x_i w_i + b\) Or, using vector notation, which you’ll often see in more advanced contexts: \(z = \mathbf{x} \cdot \mathbf{w} + b\) where $\mathbf{x}$ is the vector of inputs and $\mathbf{w}$ is the vector of weights.</li> <li> <strong>Activation Function ($f$):</strong> This is the crucial non-linear “decision-maker.” After computing the weighted sum $z$, the activation function decides whether the neuron “fires” and what output it produces. It introduces non-linearity, which is vital for neural networks to learn complex patterns.</li> </ul> <p>Let’s combine it all. The output $y$ of a single artificial neuron is: \(y = f(\sum_{i=1}^{n} x_i w_i + b)\)</p> <p>Historically, the first activation function was a simple “step function”: if $z$ is above a threshold, output 1; otherwise, output 0. For more sophisticated learning, we use functions like the <strong>Sigmoid</strong> function, which squashes the output between 0 and 1, making it useful for probabilities: \(f(z) = \frac{1}{1 + e^{-z}}\)</p> <p>Or the <strong>Rectified Linear Unit (ReLU)</strong>, which is widely popular today: \(f(z) = \max(0, z)\)</p> <h3 id="from-one-neuron-to-many-the-network">From One Neuron to Many: The Network</h3> <p>A single neuron is quite limited; it can only solve very simple problems (like separating linearly separable data). The real power emerges when we connect many artificial neurons together in layers, forming a <strong>Neural Network</strong>.</p> <p>Imagine a series of layers:</p> <ol> <li> <strong>Input Layer:</strong> This layer simply takes your raw data. Each node here corresponds to an input feature (e.g., pixel values of an image, words in a sentence). No complex calculations happen here.</li> <li> <strong>Hidden Layers:</strong> These are the “thinking” layers. Each neuron in a hidden layer receives inputs from the previous layer, performs its weighted sum and activation, and then passes its output to the next layer. Networks can have one, two, or even hundreds of hidden layers. The more hidden layers, the “deeper” the network (hence, “Deep Learning”).</li> <li> <strong>Output Layer:</strong> This layer gives you the final result. For a “yes/no” classification (like our picnic example), it might have one neuron. For classifying an image into one of 10 categories (e.g., cat, dog, bird), it would have 10 neurons, each representing a category.</li> </ol> <p>The process of information flowing from the input layer, through the hidden layers, to the output layer is called <strong>Forward Propagation</strong>. It’s how the network makes a prediction based on its current set of weights and biases.</p> <h3 id="the-magic-of-learning-how-networks-get-smarter">The Magic of Learning: How Networks Get Smarter</h3> <p>Okay, so we have a network that can make predictions. But initially, its weights and biases are random, meaning its predictions are likely terrible. How does it learn to make accurate predictions? This is where the real “magic” happens, and it involves two core ideas: a <strong>loss function</strong> and <strong>backpropagation</strong>.</p> <h4 id="1-the-loss-function-measuring-wrongness">1. The Loss Function: Measuring “Wrongness”</h4> <p>First, we need a way to tell how “wrong” our network’s predictions are. This is the job of the <strong>Loss Function</strong> (or Cost Function). It quantifies the difference between the network’s predicted output ($\hat{y}$) and the actual correct output ($y$).</p> <p>A common loss function for regression tasks (predicting a numerical value) is the <strong>Mean Squared Error (MSE)</strong>: \(L = \frac{1}{N} \sum_{i=1}^{N} (\hat{y}_i - y_i)^2\) Here, $N$ is the number of data points, $\hat{y}_i$ is the network’s prediction for data point $i$, and $y_i$ is the true value. Our goal is to minimize this loss. A smaller loss means better predictions.</p> <h4 id="2-gradient-descent-finding-the-path-to-improvement">2. Gradient Descent: Finding the Path to Improvement</h4> <p>Imagine the loss function as a landscape of hills and valleys. Our network starts at a random point (random weights/biases) on this landscape, likely high up on a hill. We want to find the lowest point – the “valley” where the loss is minimal.</p> <p><strong>Gradient Descent</strong> is an optimization algorithm that helps us do this. Think of it like being blindfolded on this hilly landscape and trying to find the lowest point. What would you do? You’d feel the slope around you and take a small step downhill. You’d repeat this until you couldn’t go any further down.</p> <p>In mathematical terms, the “slope” is represented by the <strong>gradient</strong> – the derivative of the loss function with respect to each weight and bias. The gradient tells us the direction of the steepest ascent. To minimize the loss, we want to move in the opposite direction of the gradient.</p> <p>The update rule for a weight $w$ (and similarly for a bias $b$) looks like this: \(w_{new} = w_{old} - \alpha \frac{\partial L}{\partial w_{old}}\) Here:</p> <ul> <li>$w_{new}$ is the updated weight.</li> <li>$w_{old}$ is the current weight.</li> <li>$\frac{\partial L}{\partial w_{old}}$ is the partial derivative of the loss function $L$ with respect to $w_{old}$. This tells us how much the loss changes if we slightly change $w_{old}$.</li> <li>$\alpha$ (alpha) is the <strong>learning rate</strong>. This is a crucial hyperparameter that controls the size of our “steps” down the hill. A large learning rate might overshoot the minimum, while a small one might take too long to converge.</li> </ul> <h4 id="3-backpropagation-distributing-the-blame">3. Backpropagation: Distributing the Blame</h4> <p>Now, here’s the clever part: how do we calculate these derivatives $\frac{\partial L}{\partial w}$ for <em>all</em> the weights and biases in a complex, multi-layered network? This is where <strong>Backpropagation</strong> comes in.</p> <p>Backpropagation is an algorithm that efficiently calculates the gradient of the loss function with respect to every weight and bias in the network. It works by:</p> <ol> <li> <strong>Forward Propagation:</strong> First, the input data travels through the network, layer by layer, generating a prediction ($\hat{y}$).</li> <li> <strong>Calculate Loss:</strong> The loss function compares $\hat{y}$ with the true $y$ to get an overall error.</li> <li> <strong>Backward Propagation of Error:</strong> The core idea is to distribute this error backward through the network, layer by layer, using the chain rule of calculus.</li> </ol> <p>Imagine you’re playing a team sport, and you lose. The coach doesn’t just blame the last person who touched the ball; they analyze everyone’s contribution to the loss. Backpropagation does something similar: it figures out how much each weight and bias contributed to the final error. It assigns “blame” to the parameters, starting from the output layer and moving backward to the input layer.</p> <p>Once we have these gradients for every weight and bias, we use the gradient descent update rule to adjust them, making the network slightly better at its task. This entire process (forward prop, calculate loss, backprop, update weights) is repeated thousands, millions, or even billions of times over many data samples, gradually refining the network until it can make highly accurate predictions. This iterative process of learning is what we call <strong>training</strong> the neural network.</p> <h3 id="beyond-the-basics-the-deep-dive-awaits">Beyond the Basics: The Deep Dive Awaits</h3> <p>What we’ve covered today is the fundamental structure and learning mechanism of a basic <strong>Feedforward Neural Network</strong>. This is the bedrock upon which much more complex and specialized architectures are built:</p> <ul> <li> <strong>Convolutional Neural Networks (CNNs):</strong> Excellent for image recognition tasks, mimicking how our visual cortex processes information.</li> <li> <strong>Recurrent Neural Networks (RNNs):</strong> Designed for sequential data like text and time series, with memory of past inputs.</li> <li> <strong>Transformers:</strong> The state-of-the-art for Natural Language Processing (NLP), powering models like GPT-3.</li> </ul> <p>These advanced networks leverage the same core principles of neurons, layers, activation functions, and backpropagation, but add specialized structures and techniques to tackle specific types of data and problems with incredible efficiency.</p> <p>The sheer volume of data available today, coupled with increasingly powerful computing hardware (especially GPUs), has fueled the deep learning revolution, allowing these intricate networks to be trained to unprecedented levels of accuracy.</p> <h3 id="your-journey-into-ai-has-just-begun">Your Journey into AI Has Just Begun</h3> <p>Understanding neural networks is a foundational step into the fascinating world of artificial intelligence and machine learning. You’ve now grasped the essence of how these digital brains are constructed and, more importantly, how they learn from data.</p> <p>It’s a field constantly evolving, brimming with new discoveries and applications. So, whether you’re building a recommendation engine, detecting diseases from medical images, or creating a virtual assistant, the principles we discussed today are at the heart of it all.</p> <p>Don’t stop here! Play with a simple neural network library like Keras or PyTorch, try building your own, and watch it learn. The journey from human biology to artificial intelligence is truly remarkable, and you’re now equipped with the basic map to navigate it. Happy exploring!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>