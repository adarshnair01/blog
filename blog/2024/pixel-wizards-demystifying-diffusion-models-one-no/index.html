<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Pixel Wizards: Demystifying Diffusion Models, One Noise Particle at a Time | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/2024/pixel-wizards-demystifying-diffusion-models-one-no/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Pixel Wizards: Demystifying Diffusion Models, One Noise Particle at a Time</h1> <p class="post-meta"> Created on October 01, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/diffusion-models"> <i class="fa-solid fa-hashtag fa-sm"></i> Diffusion Models</a>   <a href="/blog/blog/tag/generative-ai"> <i class="fa-solid fa-hashtag fa-sm"></i> Generative AI</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/image-generation"> <i class="fa-solid fa-hashtag fa-sm"></i> Image Generation</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>As a budding data scientist and machine learning enthusiast, few things have captivated my imagination quite like the recent explosion of generative AI. You’ve seen the stunning images from DALL-E, Midjourney, and Stable Diffusion — AI conjuring photorealistic scenes or fantastical creatures from simple text prompts. For me, it felt like magic. Pure, unadulterated wizardry! But as any good scientist knows, magic is just science we don’t understand yet. And the “science” behind much of this visual sorcery? Diffusion Models.</p> <p>When I first delved into the papers, the math seemed daunting. Probabilistic models, Markov chains, Gaussian noise — it was a lot. But as I peeled back the layers, I realized the core idea is elegantly simple, almost poetic. It’s about learning to <em>un-do</em> chaos. Come along with me as we demystify these pixel wizards, step by step.</p> <h3 id="the-art-of-un-mixing-an-intuitive-leap">The Art of Un-Mixing: An Intuitive Leap</h3> <p>To truly grasp diffusion models, let’s start with an analogy far removed from computers. Imagine you have a pristine glass of clear water. Now, drop a tiny speck of ink into it. What happens? The ink slowly spreads, diffusing through the water until the entire glass is a uniformly light shade of blue. This is <em>diffusion</em> — a natural process where particles spread out from an area of high concentration to an area of low concentration, increasing entropy (randomness).</p> <p>Now, here’s the kicker: Can you <em>reverse</em> that process? Can you somehow “un-mix” the ink from the water and gather it back into a single, tiny speck? In the real world, no, not easily. Entropy is a one-way street. But in the digital realm, with enough computational power and clever algorithms, we can <em>model</em> that reverse process. We can teach a computer to reverse the “spreading out” of information, to turn chaos back into order, or in our case, pure noise back into a coherent image.</p> <p>This is the fundamental idea behind Diffusion Models. They learn to systematically <em>denoise</em> random pixels (like our uniformly blue water) until a recognizable image emerges (our concentrated speck of ink, but as an image!).</p> <h3 id="the-forward-process-embracing-chaos">The Forward Process: Embracing Chaos</h3> <p>Let’s get a little more technical. A Diffusion Model operates in two main phases: a <strong>forward diffusion process</strong> (the “noising” phase) and a <strong>reverse diffusion process</strong> (the “denoising” or “generation” phase).</p> <p>Think of the forward process as systematically destroying an image. We take an original image, let’s call it $x_0$, and over a series of $T$ steps, we gradually add Gaussian noise to it. Each step adds a tiny bit more noise, making the image progressively blurrier and more staticky, until eventually, after $T$ steps, we’re left with $x_T$, which is pure, unstructured noise — indistinguishable from random static.</p> <p>The beauty of this forward process is that it’s a fixed, predefined Markov chain. This means that the state of the image at step $t$ ($x_t$) only depends on the image at the previous step $t-1$ ($x_{t-1}$). We don’t need to consider its entire history.</p> <p>Mathematically, we can describe this process as:</p> <table> <tbody> <tr> <td>$q(x_t</td> <td>x_{t-1}) = \mathcal{N}(x_t; \sqrt{\alpha_t} x_{t-1}, (1-\alpha_t)I)$</td> </tr> </tbody> </table> <p>Where:</p> <ul> <li>$x_t$ is the image at time step $t$.</li> <li>$x_{t-1}$ is the image at the previous time step.</li> <li>$\mathcal{N}$ denotes a Gaussian (Normal) distribution.</li> <li>$\alpha_t$ is a hyperparameter that controls the variance schedule, essentially how much noise is added at each step. It’s usually a value close to 1, meaning we retain most of the previous image and add a small amount of noise.</li> <li>$(1-\alpha_t)I$ represents the variance of the added noise.</li> <li>$I$ is the identity matrix.</li> </ul> <p>A cool trick is that we can directly sample $x_t$ from $x_0$ for any step $t$, without needing to iterate through all intermediate steps. This shortcut is incredibly useful during training:</p> <table> <tbody> <tr> <td>$q(x_t</td> <td>x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1-\bar{\alpha}_t)I)$</td> </tr> </tbody> </table> <p>Here, $\bar{\alpha}<em>t = \prod</em>{s=1}^t \alpha_s$. This means we can get $x_t$ by scaling $x_0$ and adding a single, appropriately scaled noise vector. This property simplifies training significantly because we don’t have to sequentially calculate each step of noise during every training iteration.</p> <h3 id="the-reverse-process-learning-to-see">The Reverse Process: Learning to See</h3> <p>Now, for the “magic.” The real challenge, and where the intelligence of the Diffusion Model lies, is in learning the <strong>reverse diffusion process</strong>. We want to train a neural network to predict how to go <em>backwards</em> from a noisy image $x_t$ to a slightly less noisy image $x_{t-1}$. If we can do this repeatedly, starting from pure noise $x_T$, we can eventually reconstruct a pristine image $x_0$.</p> <p>This reverse step is also modeled as a Gaussian distribution, but its mean and variance are <em>learned</em> by our neural network:</p> <table> <tbody> <tr> <td>$p_\theta(x_{t-1}</td> <td>x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))$</td> </tr> </tbody> </table> <p>Here:</p> <ul> <li>$\mu_\theta(x_t, t)$ and $\Sigma_\theta(x_t, t)$ are the mean and variance, respectively, which are predicted by our neural network (parameterized by $\theta$) based on the noisy image $x_t$ and the current time step $t$.</li> </ul> <p>The groundbreaking insight from the Denoising Diffusion Probabilistic Models (DDPMs) paper was simplifying this. They realized that the variance $\Sigma_\theta$ could be fixed (approximated by the forward process variance), and the neural network only needed to learn to predict the <em>mean</em> of the reverse distribution. Even further, the mean can be reparameterized to predict the <em>noise itself</em>.</p> <p>So, our neural network (often a U-Net architecture, known for its ability to handle image data) is trained to predict the noise $\epsilon$ that was added to $x_{t-1}$ to get $x_t$. Let’s call this predicted noise $\epsilon_\theta(x_t, t)$.</p> <p>The core of the denoising step then becomes:</p> <p>$x_{t-1} = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}<em>t}} \epsilon</em>\theta(x_t, t) \right) + \sigma_t z$</p> <p>Where:</p> <ul> <li>$\epsilon_\theta(x_t, t)$ is the noise predicted by our U-Net.</li> <li>$z$ is a standard normal random variable.</li> <li>$\sigma_t$ controls the magnitude of the noise added back during sampling (often related to $1-\alpha_t$).</li> </ul> <p>Essentially, at each step, the U-Net receives a noisy image $x_t$ and the current time step $t$. It then tries to figure out “what noise was added here?” Once it predicts that noise, it subtracts it, moving the image one step closer to clarity. This process is repeated $T$ times, transforming pure noise into a stunning image.</p> <h3 id="the-neural-network-our-noise-predictor-u-net">The Neural Network: Our Noise Predictor (U-Net)</h3> <p>Why a U-Net? This architecture is a powerhouse in image-to-image tasks. It’s an encoder-decoder network with “skip connections” that directly pass information from the encoding (downsampling) path to the decoding (upsampling) path. This allows the network to learn both high-level semantic features and fine-grained details, which is crucial for accurately predicting noise at various scales within an image.</p> <p>Crucially, the U-Net also needs to know <em>at what stage of noise</em> it’s currently operating. This is where <strong>time embeddings</strong> come in. We encode the time step $t$ (or the noise level) into a high-dimensional vector and inject this information into the U-Net. This way, the network knows whether it’s dealing with a very noisy image (early steps, $t$ is large) or a slightly noisy image (later steps, $t$ is small) and can adjust its noise prediction accordingly.</p> <p>The training objective is beautifully simple: minimize the mean squared error (MSE) between the noise actually added in the forward pass and the noise predicted by our U-Net.</p> <table> <tbody> <tr> <td>$\mathcal{L} =</td> <td> </td> <td>\epsilon - \epsilon_\theta(x_t, t)</td> <td> </td> <td>^2$</td> </tr> </tbody> </table> <p>Where $\epsilon$ is the ground-truth noise that was added to $x_0$ to get $x_t$.</p> <h3 id="conditional-generation-telling-the-ai-what-to-draw">Conditional Generation: Telling the AI What to Draw</h3> <p>This is where the magic of “text-to-image” comes in. How do we tell the Diffusion Model to generate a “cat riding a skateboard” instead of just a random image? We introduce <strong>conditioning</strong>.</p> <p>During training, alongside the image $x_0$ and the time step $t$, we also feed the model a representation of <em>what</em> we want to generate. This could be a text embedding (from a language model like CLIP), a class label, or another image. The U-Net learns to predict noise <em>conditioned</em> on this additional information.</p> <p>For inference (generating new images), we start with pure noise and provide our text prompt (e.g., “a majestic robot horse in space”). The U-Net then uses this prompt to guide its denoising process, generating an image that aligns with our description.</p> <p>A powerful technique often used is <strong>Classifier-Free Guidance (CFG)</strong>. It’s a clever trick to make the generated images stick more closely to the provided prompt without needing a separate classifier. Essentially, during inference, we run the U-Net twice: once with the prompt, and once <em>without</em> (unconditional). Then we combine the two noise predictions in a way that pushes the generation strongly towards the prompted direction.</p> <h3 id="why-diffusion-models-outshine-the-competition-for-now">Why Diffusion Models Outshine the Competition (for now)</h3> <p>Before Diffusion Models, Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) were the go-to for image generation.</p> <ul> <li> <strong>GANs</strong> could produce incredibly realistic images, but they were notoriously difficult to train (the “adversarial” part meant two networks fighting, often leading to instability and “mode collapse” where they only generated a limited variety of images).</li> <li> <strong>VAEs</strong> were easier to train but often produced blurrier, less detailed images.</li> </ul> <p>Diffusion Models offer a superior alternative:</p> <ol> <li> <strong>High-Quality Samples:</strong> They produce incredibly photorealistic and detailed images.</li> <li> <strong>Diversity:</strong> Unlike GANs, they don’t suffer from mode collapse and can generate a wide variety of images.</li> <li> <strong>Training Stability:</strong> The training process is much more stable and predictable compared to GANs.</li> <li> <strong>Flexible Sampling:</strong> Different sampling schedules and techniques allow for trade-offs between generation speed and quality.</li> </ol> <h3 id="beyond-images-the-multimodal-future">Beyond Images: The Multimodal Future</h3> <p>The applications of Diffusion Models extend far beyond static image generation:</p> <ul> <li> <strong>Video Generation:</strong> Generating short video clips or interpolating between frames.</li> <li> <strong>Audio Synthesis:</strong> Creating realistic speech, music, or sound effects.</li> <li> <strong>3D Object Generation:</strong> Turning 2D inputs into 3D models.</li> <li> <strong>Drug Discovery:</strong> Designing new molecules with desired properties.</li> <li> <strong>Medical Imaging:</strong> Enhancing or reconstructing medical scans.</li> </ul> <p>The potential is immense, transforming how we interact with and create digital content.</p> <h3 id="challenges-and-the-road-ahead">Challenges and the Road Ahead</h3> <p>While powerful, Diffusion Models aren’t without their challenges. They are computationally intensive, especially during inference, as they require many sequential denoising steps (though techniques like DDIM have sped this up considerably). Research is continuously pushing the boundaries, focusing on faster sampling, more efficient architectures, and finer control over the generated output.</p> <h3 id="conclusion-embracing-the-future-of-creation">Conclusion: Embracing the Future of Creation</h3> <p>My journey into understanding Diffusion Models has been a revelation. It transforms what once seemed like abstract “AI magic” into a beautifully engineered process of controlled chaos and order. From a data science and machine learning perspective, it’s a testament to the power of breaking down complex problems into manageable, iterative steps, and then leveraging the incredible pattern-matching abilities of deep neural networks.</p> <p>These models are not just tools; they are collaborators, expanding the horizons of human creativity and pushing the boundaries of what machines can “imagine.” As we continue to refine and explore their capabilities, I’m excited to see how they will reshape industries, empower artists, and perhaps, even help us understand the very nature of information and entropy in our own universe. The future of creative AI is here, and it’s built on a foundation of meticulously un-mixing noise.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>