<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The AI Whisperer: How Backpropagation Teaches Neural Networks to Learn | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/the-ai-whisperer-how-backpropagation-teaches-neura/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The AI Whisperer: How Backpropagation Teaches Neural Networks to Learn</h1> <p class="post-meta"> Created on August 04, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/neural-networks"> <i class="fa-solid fa-hashtag fa-sm"></i> Neural Networks</a>   <a href="/blog/blog/tag/backpropagation"> <i class="fa-solid fa-hashtag fa-sm"></i> Backpropagation</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/gradient-descent"> <i class="fa-solid fa-hashtag fa-sm"></i> Gradient Descent</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hello fellow data adventurers!</p> <p>Remember that initial spark of fascination when you first heard about Artificial Neural Networks? For me, it was like peeking behind the curtain of magic. These incredible structures, loosely inspired by the human brain, can perform astonishing feats – image recognition, natural language processing, complex prediction tasks. But there was always this nagging question: <em>how do they actually learn?</em> How does a network, initially filled with random numbers, evolve into something so intelligent?</p> <p>Today, I want to take you on a journey, much like the one I took, to demystify the core mechanism behind this learning process: <strong>Backpropagation</strong>. It’s a term you’ll hear often in the world of Deep Learning, and while it might sound intimidating, I promise by the end of this post, you’ll have a solid, intuitive, and even a slightly mathematical understanding of this elegant algorithm. Think of it as peeling back the layers to reveal the engine that drives AI’s ability to “think.”</p> <h3 id="the-stage-our-neural-network">The Stage: Our Neural Network</h3> <p>Before we dive into how networks learn, let’s quickly recap what they <em>are</em>. Imagine a series of interconnected layers:</p> <ul> <li> <strong>Input Layer:</strong> Where our data (e.g., pixel values of an image, words in a sentence) enters.</li> <li> <strong>Hidden Layers:</strong> One or more layers in between, where the real “processing” and feature extraction happen. Each neuron in these layers takes inputs from the previous layer, performs a weighted sum, and then applies an activation function (like ReLU or Sigmoid) to introduce non-linearity.</li> <li> <strong>Output Layer:</strong> Where the network gives its final prediction (e.g., “cat” or “dog,” a translated word, a stock price).</li> </ul> <p>Each connection between neurons has a <strong>weight</strong> ($w$), and each neuron has a <strong>bias</strong> ($b$). These weights and biases are the parameters our network needs to learn. They are like the “knobs” and “dials” that we adjust to get the desired output.</p> <p>The process of data flowing from the input layer through the hidden layers to the output layer is called the <strong>Forward Pass</strong>. It’s how the network makes a prediction.</p> <p>Mathematically, for a single neuron, the output $a$ is often calculated as: $z = \sum_{i} w_i x_i + b$ $a = \sigma(z)$</p> <p>Where $x_i$ are inputs, $w_i$ are weights, $b$ is the bias, and $\sigma$ is the activation function. In a multi-layered network, this process repeats for each layer.</p> <h3 id="the-problem-when-predictions-go-wrong-the-loss-function">The Problem: When Predictions Go Wrong (The Loss Function)</h3> <p>When we first initialize a neural network, its weights and biases are usually random. So, naturally, its first predictions will be terrible! It’s like a newborn trying to walk – lots of stumbling.</p> <p>To guide our network, we need a way to quantify “how terrible” its predictions are. This is where the <strong>Loss Function</strong> (or Cost Function) comes in. It takes the network’s prediction ($\hat{y}$) and the actual correct answer ($y$) and outputs a single number representing the error. A higher number means a worse prediction.</p> <p>A common loss function for regression tasks is the Mean Squared Error (MSE): $L = \frac{1}{2} \sum (\hat{y} - y)^2$</p> <p>The $\frac{1}{2}$ is just for mathematical convenience (it makes the derivative cleaner). Our ultimate goal is to <strong>minimize this loss function</strong>. We want to adjust our weights and biases so that the network’s predictions are as close to the actual values as possible.</p> <h3 id="the-compass-gradient-descent">The Compass: Gradient Descent</h3> <p>Imagine you’re blindfolded on a mountain, trying to find the lowest point in a valley. How would you do it? You’d probably feel the slope around you and take a small step downhill, repeating the process until you couldn’t go any lower.</p> <p>This is exactly what <strong>Gradient Descent</strong> does for our neural network. The “slope” we’re interested in is called the <strong>gradient</strong>, which tells us the direction of the steepest ascent (and its negative tells us the steepest descent). In our case, it’s the rate of change of the loss function with respect to each weight and bias in the network.</p> <p>If we know the gradient for a weight $w$ (i.e., $\frac{\partial L}{\partial w}$), we know how changing $w$ will affect the loss $L$. To reduce the loss, we simply move $w$ in the <em>opposite</em> direction of its gradient:</p> <p>$w_{new} = w_{old} - \eta \frac{\partial L}{\partial w}$ $b_{new} = b_{old} - \eta \frac{\partial L}{\partial b}$</p> <p>Here, $\eta$ (pronounced “eta”) is the <strong>learning rate</strong>, a small positive number that controls the size of our steps. If $\eta$ is too large, we might overshoot the minimum; if it’s too small, learning will be very slow.</p> <p>This is the core update rule. But herein lies the challenge: how do we calculate $\frac{\partial L}{\partial w}$ for <em>every single weight and bias</em> in a network that could have millions or even billions of parameters, especially those in the early layers, far from the output?</p> <h3 id="the-genius-unveiled-the-chain-rule-and-backpropagation">The Genius Unveiled: The Chain Rule and Backpropagation</h3> <p>Calculating gradients for weights and biases in the <em>output layer</em> is relatively straightforward because they directly influence the output and thus the loss. But what about a weight in an early hidden layer? It affects the neurons in its layer, which affect the next layer, and so on, until it finally influences the output, which then affects the loss. This is where direct calculation becomes incredibly complex and inefficient.</p> <p>Enter the <strong>Chain Rule</strong> from calculus, the secret sauce of Backpropagation!</p> <p>The chain rule allows us to calculate the derivative of composite functions. If $A$ depends on $B$, and $B$ depends on $C$, then the rate of change of $A$ with respect to $C$ is the rate of change of $A$ with respect to $B$, multiplied by the rate of change of $B$ with respect to $C$:</p> <p>$\frac{dA}{dC} = \frac{dA}{dB} \cdot \frac{dB}{dC}$</p> <p>Backpropagation applies this chain rule in reverse. Instead of calculating gradients from input to output, it calculates them from output <em>backward</em> to input, cleverly reusing intermediate calculations. This is why it’s called “backpropagation” – the error signal is propagated backward through the network.</p> <p>Let’s break down the backward pass, layer by layer, starting from the output.</p> <h4 id="step-1-gradients-at-the-output-layer">Step 1: Gradients at the Output Layer</h4> <p>First, we need to figure out how much the loss changes with respect to the output of the final layer. Let’s denote the output layer as $L$.</p> <p>The ‘error signal’ for the output layer’s pre-activation value ($z^{(L)}$) is often denoted as $\delta^{(L)}$. $\delta^{(L)} = \frac{\partial L}{\partial a^{(L)}} \odot \sigma’(z^{(L)})$</p> <p>Here:</p> <ul> <li>$\frac{\partial L}{\partial a^{(L)}}$ is how much the loss changes with respect to the actual activations of the output layer (e.g., for MSE, it’s often $(\hat{y} - y)$).</li> <li>$\sigma’(z^{(L)})$ is the derivative of the activation function for the output layer.</li> <li>$\odot$ denotes the element-wise product (Hadamard product).</li> </ul> <p>Once we have $\delta^{(L)}$, we can easily calculate the gradients for the weights ($\mathbf{W}^{(L)}$) and biases ($\mathbf{b}^{(L)}$) of the output layer:</p> <p>$\frac{\partial L}{\partial \mathbf{W}^{(L)}} = \delta^{(L)} (\mathbf{a}^{(L-1)})^T$ $\frac{\partial L}{\partial \mathbf{b}^{(L)}} = \delta^{(L)}$</p> <p>Here, $\mathbf{a}^{(L-1)}$ are the activations (outputs) from the <em>previous</em> hidden layer, which act as inputs to the output layer. Essentially, how much a weight contributed to the error depends on the error signal itself and the input it received.</p> <h4 id="step-2-propagating-gradients-backward-to-hidden-layers">Step 2: Propagating Gradients Backward to Hidden Layers</h4> <p>Now for the magic! We have the error signal $\delta^{(L)}$ from the output layer. We want to use this to calculate the error signal for the <em>previous</em> layer, layer $L-1$.</p> <p>The error signal for a hidden layer $l$ (i.e., $\delta^{(l)} = \frac{\partial L}{\partial z^{(l)}}$) can be calculated based on the error signal of the <em>next</em> layer ($l+1$):</p> <p>$\delta^{(l)} = ((\mathbf{W}^{(l+1)})^T \delta^{(l+1)}) \odot \sigma’(\mathbf{z}^{(l)})$</p> <p>Let’s unpack this:</p> <ul> <li>$(\mathbf{W}^{(l+1)})^T \delta^{(l+1)}$: This is the critical step. It takes the error signal from the next layer ($\delta^{(l+1)}$) and propagates it backward through the <em>transpose</em> of the weights connecting layer $l$ to layer $l+1$. This effectively distributes the “blame” for the error back to the neurons in the current layer. Each neuron in layer $l$ gets a share of the error proportional to the strength of its connections (weights) to the next layer.</li> <li>$\odot \sigma’(\mathbf{z}^{(l)})$ : We then multiply this by the derivative of the activation function for the current layer $l$. This accounts for how sensitive the neuron’s output was to its input. If a neuron was “saturated” (e.g., a Sigmoid output close to 0 or 1 where its derivative is near zero), it means changes to its input won’t change its output much, so it takes less “blame” for the error.</li> </ul> <p>Once we have $\delta^{(l)}$ for the current hidden layer, we can calculate its weights and biases gradients just like we did for the output layer:</p> <p>$\frac{\partial L}{\partial \mathbf{W}^{(l)}} = \delta^{(l)} (\mathbf{a}^{(l-1)})^T$ $\frac{\partial L}{\partial \mathbf{b}^{(l)}} = \delta^{(l)}$</p> <p>This process repeats, layer by layer, until we reach the input layer, calculating all necessary gradients along the way.</p> <h3 id="the-full-learning-cycle-putting-it-all-together">The Full Learning Cycle: Putting It All Together</h3> <p>So, a single training step (or iteration) in a neural network looks like this:</p> <ol> <li> <strong>Forward Pass:</strong> Input data is fed into the network, and predictions are generated. All intermediate activation values and pre-activation values are stored.</li> <li> <strong>Calculate Loss:</strong> The predicted output is compared to the true output using the loss function.</li> <li> <strong>Backward Pass (Backpropagation):</strong> The gradients of the loss function with respect to all weights and biases are calculated, starting from the output layer and propagating backward through the network using the chain rule.</li> <li> <strong>Update Parameters (Gradient Descent):</strong> All weights and biases are adjusted slightly in the direction that minimizes the loss, using the calculated gradients and the learning rate.</li> <li> <strong>Repeat:</strong> Steps 1-4 are repeated thousands or millions of times over many batches of data (epochs) until the network’s performance converges or stops improving.</li> </ol> <h3 id="why-backpropagation-matters-the-unsung-hero">Why Backpropagation Matters: The Unsung Hero</h3> <p>Backpropagation is not just an algorithm; it’s the fundamental engine that made deep learning feasible. Before its widespread adoption and efficient implementations, training deep neural networks was computationally intractable. Without it, neural networks would be interesting theoretical constructs, but not the powerful AI systems we see today.</p> <ul> <li> <strong>Efficiency:</strong> It efficiently calculates all gradients needed for gradient descent, avoiding redundant computations by reusing intermediate terms.</li> <li> <strong>Scalability:</strong> It scales beautifully to networks with many layers and millions of parameters.</li> <li> <strong>Foundation:</strong> It’s the bedrock upon which more advanced optimization algorithms (like Adam, RMSprop) are built, all of which still rely on the principles of gradient computation via backpropagation.</li> </ul> <h3 id="my-reflection">My Reflection</h3> <p>Learning about backpropagation felt like solving a grand puzzle. It transformed my understanding of neural networks from a “black box” into a system whose learning mechanism, though complex, is remarkably elegant and logical. It showed me that even the most cutting-edge AI relies on foundational mathematical principles like calculus, applied ingeniously.</p> <p>Next time you see an AI performing some incredible feat, take a moment to appreciate backpropagation – the quiet, efficient force tirelessly whispering to the network, guiding it to learn, one gradient at a time. It truly is the unsung hero, enabling AI to transcend its initial random state and evolve into something intelligent.</p> <p>Keep learning, keep exploring, and who knows what other “black boxes” you’ll illuminate!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>