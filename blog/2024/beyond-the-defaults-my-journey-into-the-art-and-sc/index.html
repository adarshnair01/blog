<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Beyond the Defaults: My Journey into the Art and Science of Hyperparameter Tuning | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/beyond-the-defaults-my-journey-into-the-art-and-sc/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Beyond the Defaults: My Journey into the Art and Science of Hyperparameter Tuning</h1> <p class="post-meta"> Created on September 13, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/hyperparameter-tuning"> <i class="fa-solid fa-hashtag fa-sm"></i> Hyperparameter Tuning</a>   <a href="/blog/blog/tag/optimization"> <i class="fa-solid fa-hashtag fa-sm"></i> Optimization</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Welcome, fellow data explorers!</p> <p>Today, I want to pull back the curtain on one of the most critical, yet often overlooked, aspects of building truly robust and high-performing machine learning models: <strong>Hyperparameter Tuning</strong>. It’s a topic that, for a long time, felt a bit like black magic to me. I’d train a model, get decent results, but then I’d see others achieving seemingly impossible feats with similar data. What was their secret? More often than not, it was intelligent hyperparameter tuning.</p> <p>Think of it this way: when you’re baking a cake, you have a recipe (your chosen machine learning algorithm like a Random Forest or a Neural Network) and ingredients (your dataset). But there are also critical <em>oven settings</em> – the temperature, the baking time, perhaps even the brand of flour. These aren’t ingredients that go <em>into</em> the cake, but they profoundly influence the final outcome: a perfectly golden, moist cake, or a burnt, dry disaster. In the world of machine learning, these “oven settings” are our <strong>hyperparameters</strong>.</p> <h3 id="what-are-hyperparameters-anyway">What <em>Are</em> Hyperparameters, Anyway?</h3> <p>This is where we dive a little deeper. In machine learning, we talk about two main types of parameters:</p> <ol> <li> <strong>Model Parameters:</strong> These are the internal variables that the model <em>learns</em> from the data during training. They represent the “knowledge” the model acquires. For example: <ul> <li>The weights and biases in a neural network.</li> <li>The coefficients in a linear or logistic regression model.</li> <li>The split points and leaf values in a decision tree. These are optimized during the training process itself, typically through algorithms like gradient descent.</li> </ul> </li> <li> <strong>Hyperparameters:</strong> These are external configuration variables whose values are set <em>before</em> the training process begins. They dictate the <em>architecture</em> of the model or the <em>learning process</em> itself. They are <em>not</em> learned from the data. Examples include: <ul> <li> <strong>Learning Rate</strong> ($\alpha$): How big a step the optimization algorithm takes in the direction of the negative gradient (e.g., <code class="language-plaintext highlighter-rouge">0.01</code>, <code class="language-plaintext highlighter-rouge">0.001</code>). Too high, and you might overshoot the minimum; too low, and training takes forever.</li> <li> <strong>Number of Estimators/Trees</strong> (e.g., <code class="language-plaintext highlighter-rouge">n_estimators</code> in a Random Forest or Gradient Boosting): How many individual decision trees are built. More trees generally lead to better performance but also higher computational cost.</li> <li> <strong>Maximum Depth</strong> (e.g., <code class="language-plaintext highlighter-rouge">max_depth</code> in decision trees): How deep each tree can grow. Deeper trees can capture more complex relationships but risk overfitting.</li> <li> <strong>Regularization Strength</strong> (e.g., <code class="language-plaintext highlighter-rouge">C</code> in SVMs, <code class="language-plaintext highlighter-rouge">lambda</code> in Ridge/Lasso): A penalty applied to prevent overfitting by discouraging overly complex models.</li> <li> <strong>Batch Size</strong> (in neural networks): The number of training examples used in one iteration.</li> <li> <strong>Kernel Type</strong> (e.g., ‘linear’, ‘rbf’ in SVMs): The function used to transform data into a higher-dimensional space.</li> </ul> </li> </ol> <p>The crucial distinction? <strong>You, the data scientist, decide the hyperparameters; the model learns its parameters.</strong></p> <h3 id="why-cant-the-model-just-learn-them-itself">Why Can’t the Model Just Learn Them Itself?</h3> <p>This is a brilliant question that gets to the core of the challenge. The model learns its parameters by minimizing a specific loss function on the training data. But hyperparameters often define the very space in which that optimization occurs, or they control aspects of the optimization process itself that aren’t easily differentiable with respect to the loss function.</p> <p>For example, you can’t “gradient descent” on the number of trees in a Random Forest. How would you calculate a derivative for an integer? Similarly, the learning rate dictates how the gradient descent algorithm behaves; it’s a setting <em>for</em> the learning process, not something <em>learned by</em> the process.</p> <p>So, we’re left with a search problem: finding the combination of hyperparameters that yields the best performance on <em>unseen data</em>.</p> <h3 id="the-quest-for-the-best-hyperparameters-our-tuning-strategies">The Quest for the “Best” Hyperparameters: Our Tuning Strategies</h3> <p>Since our model can’t learn its hyperparameters, we have to find them ourselves. This involves training multiple models, each with a different set of hyperparameters, and evaluating their performance. But which combinations should we try?</p> <h4 id="1-manual-search-trial-and-error">1. Manual Search (Trial and Error)</h4> <p>This is where many of us start. You pick a few values based on intuition, previous projects, or default settings, run the model, see the performance, adjust, and repeat.</p> <ul> <li> <strong>Pros:</strong> Can be quick for a very small number of hyperparameters, builds intuition.</li> <li> <strong>Cons:</strong> Extremely inefficient, subjective, impossible in high-dimensional hyperparameter spaces, and highly unlikely to find the truly optimal settings. It’s like finding a needle in a haystack by randomly poking around.</li> </ul> <h4 id="2-grid-search">2. Grid Search</h4> <p>This was my first systematic approach, and it’s a solid stepping stone. Grid Search works by defining a grid of hyperparameter values, then exhaustively trying every single combination.</p> <p>Let’s say you want to tune two hyperparameters:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">learning_rate</code>: [0.1, 0.01, 0.001]</li> <li> <code class="language-plaintext highlighter-rouge">n_estimators</code>: [100, 200, 300]</li> </ul> <p>Grid Search would train a model for each of these combinations: (0.1, 100), (0.1, 200), (0.1, 300), (0.01, 100), (0.01, 200), (0.01, 300), (0.001, 100), (0.001, 200), (0.001, 300)</p> <p>That’s $3 \times 3 = 9$ models.</p> <p>If you have $P$ hyperparameters, and each hyperparameter $i$ has $K_i$ possible values, the total number of configurations is: $N_{configs} = \prod_{i=1}^{P} K_i$</p> <ul> <li> <strong>Pros:</strong> Simple to understand and implement (e.g., <code class="language-plaintext highlighter-rouge">GridSearchCV</code> in scikit-learn), guarantees finding the best combination <em>within the defined grid</em>.</li> <li> <strong>Cons:</strong> Becomes computationally expensive very quickly as the number of hyperparameters or values per hyperparameter increases (the dreaded “curse of dimensionality”). If you have 5 hyperparameters, each with 10 possible values, that’s $10^5 = 100,000$ models to train! It also spends equal time on potentially unimportant hyperparameters.</li> </ul> <h4 id="3-random-search">3. Random Search</h4> <p>After bumping into the computational wall with Grid Search, Random Search felt like a breath of fresh air. Instead of trying every combination, Random Search samples a fixed number of random combinations from the specified distributions or ranges for each hyperparameter.</p> <p>The magic of Random Search lies in an observation by Bergstra and Bengio (2012): for many problems, only a few hyperparameters truly matter. If you have, say, 10 hyperparameters, but only 2 of them have a significant impact on performance, Grid Search will spend a lot of time exploring variations of the 8 unimportant ones. Random Search, by picking values randomly, is more likely to hit optimal or near-optimal values for the <em>important</em> hyperparameters, even if it ignores some less impactful ones.</p> <ul> <li> <strong>Pros:</strong> Often significantly more efficient than Grid Search, especially in high-dimensional spaces where only a few hyperparameters are truly influential. Provides good coverage of the hyperparameter space.</li> <li> <strong>Cons:</strong> No guarantee of finding the global optimum, relies on good definition of the sampling distributions.</li> </ul> <p><em>My Experience:</em> I’ve found Random Search to be an excellent first systematic approach for complex models like Gradient Boosted Trees or deep neural networks. It balances exploration with efficiency.</p> <h4 id="4-advanced-techniques-the-cutting-edge">4. Advanced Techniques (The Cutting Edge)</h4> <p>Once you’ve mastered Grid and Random Search, you might find yourself craving even more intelligent optimization. This is where advanced methods come in:</p> <ul> <li> <strong>Bayesian Optimization:</strong> This is where things get really smart. Instead of blindly searching, Bayesian Optimization builds a probabilistic model (often a Gaussian Process) of the objective function (e.g., accuracy) based on past evaluations. It then uses this model to intelligently choose the next set of hyperparameters to evaluate, balancing <em>exploration</em> (trying new, potentially good regions) and <em>exploitation</em> (focusing on regions that have already shown promise). <ul> <li> <strong>Pros:</strong> Highly efficient, often finds better solutions with fewer evaluations than Grid or Random Search. It’s like having a smart guide in a dark landscape, using what it’s seen to guess where the highest peak might be.</li> <li> <strong>Cons:</strong> More complex to implement, can be slower for very high-dimensional hyperparameter spaces or if evaluations are very fast. Tools like Optuna, Hyperopt, and Spearmint implement this.</li> </ul> </li> <li> <p><strong>Gradient-based Optimization:</strong> For some rare cases where hyperparameters are continuous and differentiable with respect to the validation loss, one could potentially use gradient descent directly on the hyperparameters. This is often more theoretical or applicable in specific deep learning architectures.</p> </li> <li> <strong>Evolutionary Algorithms (e.g., Genetic Algorithms):</strong> Inspired by natural selection, these algorithms evolve a population of hyperparameter sets over generations, selecting the “fittest” ones to “reproduce” and “mutate” into new combinations. <ul> <li> <strong>Pros:</strong> Can explore complex, non-convex spaces.</li> <li> <strong>Cons:</strong> Can be computationally intensive, harder to guarantee optimality.</li> </ul> </li> </ul> <h3 id="practical-tips-from-my-tuning-toolbox">Practical Tips from My Tuning Toolbox</h3> <ol> <li> <p><strong>Start Broad, Then Narrow Down:</strong> Don’t try to find the perfect value on your first pass. With Random Search (or even Grid Search), define wide ranges for your hyperparameters. Once you identify promising regions, narrow down those ranges and run another, more focused search.</p> </li> <li> <p><strong>Understand Your Hyperparameters:</strong> Don’t just throw values at the wall. Take the time to understand what <code class="language-plaintext highlighter-rouge">max_depth</code>, <code class="language-plaintext highlighter-rouge">C</code>, <code class="language-plaintext highlighter-rouge">gamma</code>, or <code class="language-plaintext highlighter-rouge">learning_rate</code> actually <em>do</em>. This understanding helps you define sensible ranges and interpret results.</p> </li> <li> <p><strong>Always Use a Validation Set (or Cross-Validation):</strong> Never tune hyperparameters on your test set! The test set is for <em>final, unbiased evaluation</em>. Use a separate validation set or K-Fold Cross-Validation on your training data to evaluate different hyperparameter combinations. <code class="language-plaintext highlighter-rouge">GridSearchCV</code> and <code class="language-plaintext highlighter-rouge">RandomizedSearchCV</code> in scikit-learn handle this beautifully.</p> </li> <li> <p><strong>Resource Management is Key:</strong> Hyperparameter tuning can be a beast. Be mindful of computational resources. Cloud platforms (AWS, GCP, Azure) offer powerful machines, and tools like Dask or Spark can parallelize workloads.</p> </li> <li> <p><strong>Document Your Experiments:</strong> This is something I learned the hard way. Keep track of which hyperparameters you tried, the resulting performance, and any observations. Tools like MLflow, Weights &amp; Biases, or even a simple spreadsheet can be invaluable.</p> </li> <li> <p><strong>Don’t Overfit the Validation Set:</strong> It’s possible to tune your hyperparameters so perfectly to your validation set that they don’t generalize well to unseen data. This is rare but can happen if you iterate excessively or use a very small validation set.</p> </li> </ol> <h3 id="my-personal-workflow">My Personal Workflow</h3> <p>When I embark on a new modeling task, my hyperparameter tuning journey typically looks something like this:</p> <ol> <li> <strong>Initial Model with Defaults:</strong> Train a basic model with sensible default hyperparameters to get a baseline performance.</li> <li> <strong>Wide Random Search:</strong> Define broad ranges/distributions for the most impactful hyperparameters. Run a <code class="language-plaintext highlighter-rouge">RandomizedSearchCV</code> (or Optuna/Hyperopt if the problem is more complex) with a reasonable number of iterations and cross-validation. This quickly helps me identify promising general areas.</li> <li> <strong>Focused Grid/Random Search:</strong> Based on the results of step 2, I narrow down the ranges for the top-performing hyperparameters and might use a more granular Grid Search or another Random Search with more iterations.</li> <li> <strong>Bayesian Optimization (for stubborn problems):</strong> If I’m still struggling to eke out performance, or if the model training is very expensive, I’ll turn to Bayesian Optimization libraries.</li> <li> <strong>Final Evaluation:</strong> Once I’m happy with the hyperparameter set found on my validation data, I train the final model on the <em>entire</em> training set (training data + validation data) using these optimal hyperparameters, and then evaluate its performance <strong>once</strong> on the untouched test set.</li> </ol> <h3 id="conclusion-the-art-of-precision">Conclusion: The Art of Precision</h3> <p>Hyperparameter tuning isn’t just a technical step; it’s an art form, demanding patience, experimentation, and a deep understanding of your models. It’s the difference between a good model and a great one. While manual trial-and-error might get you started, embracing systematic approaches like Grid Search, Random Search, and eventually Bayesian Optimization will unlock the true potential of your machine learning models.</p> <p>So, the next time your model isn’t performing as expected, remember those hidden knobs and dials. A little tuning might be all it takes to turn a struggling model into a star performer. Happy tuning!</p> <hr> <p><em>P.S. If you’re interested in diving deeper, I highly recommend exploring libraries like <code class="language-plaintext highlighter-rouge">scikit-learn</code>’s <code class="language-plaintext highlighter-rouge">GridSearchCV</code> and <code class="language-plaintext highlighter-rouge">RandomizedSearchCV</code>, or more advanced tools like <code class="language-plaintext highlighter-rouge">Optuna</code> and <code class="language-plaintext highlighter-rouge">Hyperopt</code> for Bayesian Optimization.</em></p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>