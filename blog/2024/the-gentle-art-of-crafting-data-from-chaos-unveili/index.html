<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Gentle Art of Crafting Data from Chaos: Unveiling Diffusion Models | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/the-gentle-art-of-crafting-data-from-chaos-unveili/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Gentle Art of Crafting Data from Chaos: Unveiling Diffusion Models</h1> <p class="post-meta"> Created on August 11, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/generative-ai"> <i class="fa-solid fa-hashtag fa-sm"></i> Generative AI</a>   <a href="/blog/blog/tag/diffusion-models"> <i class="fa-solid fa-hashtag fa-sm"></i> Diffusion Models</a>   <a href="/blog/blog/tag/ai-art"> <i class="fa-solid fa-hashtag fa-sm"></i> AI Art</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone! Today, I want to talk about something that has completely captivated my imagination in the world of Artificial Intelligence: <strong>Diffusion Models</strong>. If you’ve ever been mesmerized by the stunning images created by tools like DALL-E, Midjourney, or Stable Diffusion, then you’ve witnessed these incredible models in action. They’re not just a technological marvel; they’re a testament to how elegantly we can teach machines to be creative.</p> <p>For a long time, the holy grail in AI wasn’t just about making machines smart enough to <em>understand</em> data, but to make them smart enough to <em>create</em> it. Think about it: our brains don’t just recognize a cat; they can conjure up an image of a cat that has never existed, perhaps a cat wearing a tiny hat while juggling. Generative AI aims to give machines this same imaginative power.</p> <p>Before Diffusion Models burst onto the scene, Generative Adversarial Networks (GANs) were the reigning champions. GANs were ingenious: they pitted two neural networks against each other – a “generator” trying to create fake data (e.g., images) and a “discriminator” trying to tell real from fake. It was a fascinating game of cat and mouse, pushing both to get better. However, training GANs could be notoriously tricky, often unstable, and sometimes suffered from “mode collapse” (where the generator would only produce a limited variety of outputs).</p> <p>This is where Diffusion Models step in, offering a refreshing and surprisingly intuitive alternative. Their core idea is beautifully simple, almost like a philosophical approach to creation: <strong>what if we learned to reverse the process of destruction?</strong></p> <h3 id="the-core-idea-forward-and-reverse">The Core Idea: Forward and Reverse</h3> <p>At the heart of Diffusion Models are two processes: a <strong>forward diffusion process</strong> and a <strong>reverse denoising process</strong>. Let’s break them down.</p> <h4 id="1-the-forward-diffusion-noising-process">1. The Forward Diffusion (Noising) Process</h4> <p>Imagine you have a beautiful, pristine photograph. Now, I start adding tiny, random speckles of noise to it, very gently at first. Then I add more, and more, until eventually, your photograph is completely obscured by static, indistinguishable from pure random noise.</p> <p>This is exactly what the forward diffusion process does. It takes an input image ($x_0$) and <em>gradually</em> adds Gaussian noise over many time steps ($T$). Each step introduces a little more noise, slowly transforming the clear image into pure random noise.</p> <p>Mathematically, this process can be described as follows: Given an image $x_0$, we generate a sequence of noisy images $x_1, x_2, …, x_T$. At each step $t$, we generate $x_t$ from $x_{t-1}$ by adding Gaussian noise: $q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t} x_{t-1}, \beta_t I)$</p> <p>Here:</p> <ul> <li>$x_t$ is the image at time step $t$.</li> <li>$\beta_t$ is a small, predefined variance schedule. It determines how much noise is added at each step. These $\beta_t$ values typically increase over time, meaning more noise is added in later steps.</li> <li>$\mathcal{N}$ denotes a normal (Gaussian) distribution.</li> <li>$I$ is the identity matrix.</li> </ul> <p>This might look a bit intimidating, but the intuition is straightforward: we’re slightly blurring/noising the image at each step. A super cool property of this setup is that we can directly sample $x_t$ for <em>any</em> $t$ without needing to sequentially apply noise $t$ times. We can just add the correct amount of noise directly to $x_0$:</p> <table> <tbody> <tr> <td>$q(x_t</td> <td>x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t) I)$</td> </tr> </tbody> </table> <p>where $\alpha_t = 1 - \beta_t$ and $\bar{\alpha}<em>t = \prod</em>{s=1}^{t} \alpha_s$. This formula tells us that we can get $x_t$ by taking a weighted average of the original image $x_0$ and some pure Gaussian noise $\epsilon \sim \mathcal{N}(0, I)$:</p> <p>$x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon$</p> <p>This means the forward process is <em>fixed</em> and <em>known</em>. We don’t need to learn anything here. We’re essentially just taking a controlled path from an image to pure noise.</p> <h4 id="2-the-reverse-denoising-process-the-learning-part">2. The Reverse Denoising Process (The Learning Part!)</h4> <p>Now, here’s where the magic truly happens. If we know how to go from a clean image to noise, can we learn how to go from noise back to a clean image? This is the core challenge.</p> <p>The reverse process starts with pure random noise ($x_T$) and iteratively denoises it, step by step, until it recovers a clean image ($x_0$). This is like having that completely noisy photo and, through some sophisticated process, gradually removing the static until the original image emerges.</p> <table> <tbody> <tr> <td>Crucially, we want to learn the probability distribution $p_\theta(x_{t-1}</td> <td>x_t)$, which describes how to get to a slightly less noisy image ($x_{t-1}$) given the current noisy image ($x_t$). Since the forward process adds Gaussian noise, it turns out that if $\beta_t$ is small enough, the reverse process <em>also</em> approximates a Gaussian distribution!</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$p_\theta(x_{t-1}</td> <td>x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))$</td> </tr> </tbody> </table> <p>Here, $\mu_\theta$ and $\Sigma_\theta$ are the mean and covariance that our neural network (parameterized by $\theta$) needs to learn. However, it’s often simpler and more stable for the model to learn to predict the <em>noise</em> $\epsilon$ that was added at step $t$ in the forward process.</p> <p>So, our neural network, typically a <strong>U-Net</strong> (more on this later!), takes the noisy image $x_t$ and the current time step $t$ as input, and tries to predict the noise $\hat{\epsilon}_\theta(x_t, t)$ that was originally added to get to $x_t$.</p> <p>Once we have this predicted noise, we can then subtract it (or rather, use it to estimate the original image and then reverse the noise addition) to get a slightly cleaner image $x_{t-1}$.</p> <p>The training objective is surprisingly simple: we want our neural network’s predicted noise $\hat{\epsilon}_\theta(x_t, t)$ to be as close as possible to the actual noise $\epsilon$ that was used to create $x_t$ from $x_0$. We use a simple Mean Squared Error (MSE) loss:</p> <table> <tbody> <tr> <td>$L_t =</td> <td> </td> <td>\epsilon - \hat{\epsilon}_\theta(x_t, t)</td> <td> </td> <td>^2$</td> </tr> </tbody> </table> <p>This elegantly avoids the adversarial training complexities of GANs.</p> <h3 id="training-and-sampling-in-practice">Training and Sampling in Practice</h3> <h4 id="training">Training:</h4> <ol> <li> <strong>Pick an image</strong> $x_0$ from our dataset.</li> <li> <strong>Pick a random time step</strong> $t$ (between 1 and $T$).</li> <li> <strong>Generate noise</strong> $\epsilon \sim \mathcal{N}(0, I)$.</li> <li> <strong>Create a noisy version</strong> $x_t$ using the forward process formula: $x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon$.</li> <li> <strong>Feed</strong> $x_t$ and $t$ into our U-Net model.</li> <li> <strong>The model outputs</strong> its predicted noise $\hat{\epsilon}_\theta(x_t, t)$.</li> <li> <strong>Calculate the loss</strong> between $\epsilon$ (the true noise) and $\hat{\epsilon}_\theta(x_t, t)$ (the predicted noise).</li> <li> <strong>Update the model’s weights</strong> using backpropagation. We repeat these steps millions of times until our model is really good at predicting the noise for any given noisy image at any given time step.</li> </ol> <h4 id="sampling-generating-an-image">Sampling (Generating an Image):</h4> <ol> <li> <strong>Start with pure random noise:</strong> $x_T \sim \mathcal{N}(0, I)$. This is our completely “cloudy” canvas.</li> <li> <strong>Iterate backwards from $T$ down to 1:</strong> a. Feed the current noisy image $x_t$ and the time step $t$ into our <em>trained</em> U-Net model. b. The model outputs its prediction of the noise, $\hat{\epsilon}<em>\theta(x_t, t)$. c. Use this predicted noise to estimate $x</em>{t-1}$ (a slightly less noisy image). This usually involves a formula that uses $x_t$, $t$, $\hat{\epsilon}<em>\theta$, and the $\beta_t$ values. A common approximation is: $x</em>{t-1} = \frac{1}{\sqrt{\alpha_t}} \left(x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}<em>t}} \hat{\epsilon}</em>\theta(x_t, t)\right) + \sigma_t z$ where $z \sim \mathcal{N}(0, I)$ and $\sigma_t$ is a variance term. The intuition here is that we’re subtracting the <em>predicted</em> noise to get closer to the original image.</li> <li>After many steps (e.g., 1000), we end up with $x_0$, which is our generated, clean image!</li> </ol> <h3 id="why-are-diffusion-models-so-good">Why Are Diffusion Models So Good?</h3> <ol> <li> <strong>Stable Training:</strong> Unlike GANs, which involve an adversarial dance, Diffusion Models have a very clear and stable training objective (predicting noise with MSE). This makes them much easier to train effectively.</li> <li> <strong>High-Quality and Diverse Samples:</strong> They excel at generating incredibly realistic and diverse images, often outperforming GANs in terms of visual quality and covering the entire “data manifold” (not collapsing modes).</li> <li> <strong>Scalability:</strong> The architecture (often U-Nets) and training method scale well to very large models and datasets.</li> <li> <strong>Flexibility:</strong> They are naturally well-suited for conditional generation. Want an image <em>of a dog</em>? Just feed the text “dog” alongside $x_t$ and $t$ to your model. This is the magic behind text-to-image models. They can also be conditioned on images for tasks like inpainting (filling in missing parts) or outpainting (extending images).</li> </ol> <h3 id="key-architectural-components">Key Architectural Components</h3> <ul> <li> <strong>U-Net:</strong> This specific type of neural network is absolutely crucial for Diffusion Models, especially in vision tasks. It’s an encoder-decoder architecture with “skip connections.” The encoder compresses the image, extracting high-level features, while the decoder reconstructs it. The skip connections directly link corresponding layers in the encoder and decoder, allowing fine-grained details from the earlier stages to be preserved during reconstruction. This is essential for accurately predicting noise across different scales.</li> <li> <strong>Time Step Embeddings:</strong> How does the U-Net know <em>which</em> time step $t$ it’s currently processing? We can’t just feed $t$ as a raw number. Instead, $t$ is typically converted into a high-dimensional vector using positional embeddings (similar to what Transformers use). This embedding is then added to the feature maps at various points in the U-Net, guiding its prediction.</li> </ul> <h3 id="applications-beyond-image-generation">Applications Beyond Image Generation</h3> <p>While Diffusion Models have gained fame for their breathtaking image generation capabilities (DALL-E 2, Stable Diffusion, Midjourney), their applications extend far beyond:</p> <ul> <li> <strong>Image Editing:</strong> Inpainting (filling holes), outpainting (extending borders), style transfer, super-resolution.</li> <li> <strong>Video Generation:</strong> Generating realistic video clips from text or other inputs.</li> <li> <strong>Audio Generation:</strong> Creating music, speech, or sound effects.</li> <li> <strong>Drug Discovery:</strong> Generating novel molecular structures with desired properties.</li> <li> <strong>3D Object Generation:</strong> Crafting 3D models from scratch or text prompts.</li> </ul> <h3 id="the-road-ahead">The Road Ahead</h3> <p>Despite their phenomenal success, Diffusion Models still have areas for improvement. The sampling process, while robust, can be computationally expensive and slow compared to GANs, as it requires hundreds or thousands of sequential denoising steps. Researchers are actively working on ways to speed this up, through techniques like “distillation” or using fewer, larger steps.</p> <p>Another critical consideration, as with all powerful AI models, is the ethical implications. The ability to generate hyper-realistic images raises questions about deepfakes, copyright, and bias embedded in training data. As we wield these powerful tools, understanding their limitations and potential for misuse is paramount.</p> <h3 id="my-personal-takeaway">My Personal Takeaway</h3> <p>Learning about Diffusion Models has been a truly enlightening journey. It’s a beautiful example of how a relatively simple, intuitive idea – reversing a known noisy process – can lead to such profound and powerful generative capabilities. It feels less like training a machine to “trick” another machine (as with GANs) and more like teaching it to understand the subtle degradation of information and then lovingly restore it. This “gentle art of crafting data from chaos” is not just technically brilliant, but also a poetic approach to artificial creativity. I can’t wait to see how these models continue to evolve and reshape the landscape of AI.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>