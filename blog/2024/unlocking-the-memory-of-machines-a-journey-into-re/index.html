<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Unlocking the Memory of Machines: A Journey into Recurrent Neural Networks | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/unlocking-the-memory-of-machines-a-journey-into-re/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Unlocking the Memory of Machines: A Journey into Recurrent Neural Networks</h1> <p class="post-meta"> Created on September 24, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/neural-networks"> <i class="fa-solid fa-hashtag fa-sm"></i> Neural Networks</a>   <a href="/blog/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> NLP</a>   <a href="/blog/blog/tag/rnns"> <i class="fa-solid fa-hashtag fa-sm"></i> RNNs</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey there, fellow explorers of the data universe!</p> <p>Today, I want to take you on a personal journey, one that started with a simple question: “How do we teach a computer to <em>remember</em>?” It sounds straightforward, right? We remember things all the time – the beginning of a sentence helps us understand the end, the previous scene in a movie gives context to the current one. But for traditional neural networks, this idea of “memory” or “sequence” was a real challenge.</p> <p>Let’s rewind a bit. If you’ve tinkered with neural networks before, you’re likely familiar with Feedforward Neural Networks (FFNNs). They’re fantastic for tasks where inputs are independent. Think about classifying an image: whether a picture contains a cat doesn’t really depend on what was in the previous picture. Each image is a standalone input.</p> <p>But what about sequential data? What about a sentence like “The quick brown fox jumps over the lazy dog.”? If I just gave a traditional neural network the word “dog” in isolation, it wouldn’t know if I was talking about a pet, a derogatory term, or a verb. The <em>context</em> provided by “the lazy” preceding it is crucial. Our human brains process information sequentially, building context as we go. We needed a neural network that could do the same.</p> <p>This is where the idea of <strong>Recurrent Neural Networks (RNNs)</strong> burst onto the scene, and it felt like a genuine “aha!” moment for me.</p> <h3 id="the-problem-when-order-matters">The Problem: When Order Matters</h3> <p>Imagine you’re trying to predict the next word in a sequence. If you’ve just seen the words “I went to the store and bought some…”, the next word is probably “milk,” “bread,” or “apples,” not “sky” or “car.” The entire history of words leading up to the current moment influences our prediction.</p> <p>Traditional FFNNs treat each input as independent. They process $x_1$, then $x_2$, then $x_3$, but they don’t have an internal mechanism to carry information from $x_1$ to $x_2$, or $x_2$ to $x_3$. It’s like having short-term amnesia after every single word! Clearly, for tasks like language modeling, machine translation, or even predicting stock prices (where past prices are highly indicative of future trends), this approach falls flat.</p> <h3 id="the-aha-moment-introducing-recurrence">The “Aha!” Moment: Introducing Recurrence</h3> <p>The core idea behind an RNN is brilliantly simple, yet profoundly powerful: give the network a memory. How do we do that? By introducing a loop!</p> <p>Instead of just feeding data forward, an RNN takes the output from a previous step and feeds it back into the current step. This feedback loop allows information to persist from one step to the next. It’s like the network is constantly whispering to itself, “Hey, remember what happened just a moment ago? Keep that in mind for what’s coming next!”</p> <p>Let’s visualize this conceptually. Imagine a single neural network layer. Now, imagine its output at time $t-1$ isn’t just passed to the next layer in depth, but also fed back <em>into itself</em> as an additional input for the next time step $t$. This internal state, this “memory” of past information, is often called the <strong>hidden state</strong> ($h_t$).</p> <h3 id="unrolling-the-loop-seeing-the-sequence">Unrolling the Loop: Seeing the Sequence</h3> <p>While the concept of a loop is great for understanding, when we actually implement and train RNNs, it’s often easier to think about them as an “unrolled” sequence of operations.</p> <p>Imagine we have a sequence of inputs $x_1, x_2, …, x_T$. We can “unroll” our recurrent network into a chain of identical modules, where each module passes a hidden state to the next.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>       x_1          x_2          x_3          ...          x_T
        |            |            |                          |
        V            V            V                          V
  [RNN Unit] -&gt; [RNN Unit] -&gt; [RNN Unit] -&gt; ... -&gt; [RNN Unit]
        |            |            |                          |
        V            V            V                          V
       h_1          h_2          h_3          ...          h_T
        |            |            |                          |
        V            V            V                          V
       y_1          y_2          y_3          ...          y_T
</code></pre></div></div> <p>In this unrolled view:</p> <ul> <li>Each <code class="language-plaintext highlighter-rouge">[RNN Unit]</code> represents the <em>same</em> recurrent layer, applying the <em>same</em> set of weights and biases at each time step. This is crucial for learning patterns across sequences.</li> <li>$x_t$ is the input at time step $t$.</li> <li>$h_t$ is the hidden state at time step $t$, computed using the current input $x_t$ and the <em>previous</em> hidden state $h_{t-1}$. This $h_t$ is the “memory.”</li> <li>$y_t$ is the output at time step $t$, derived from the current hidden state $h_t$. Not all RNNs produce an output at every time step; some might only output at the very end of a sequence (e.g., classifying a whole sentence).</li> </ul> <p>At the very first time step ($t=1$), there is no $h_0$, so we typically initialize it as a vector of zeros.</p> <h3 id="the-math-behind-the-memory">The Math Behind the Memory</h3> <p>Let’s peek under the hood at the core equations. For a simple RNN, the hidden state $h_t$ at time $t$ is calculated as:</p> \[h_t = \tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)\] <p>And the output $y_t$ at time $t$ (if an output is produced at each step) is:</p> \[y_t = W_{hy} h_t + b_y\] <p>Let’s break down that first, more complex equation:</p> <ul> <li>$x_t$: This is your current input vector (e.g., the word embedding for the current word).</li> <li>$h_{t-1}$: This is the hidden state (the “memory”) from the previous time step.</li> <li>$W_{xh}$: These are the weights connecting the current input $x_t$ to the hidden state $h_t$.</li> <li>$W_{hh}$: These are the weights connecting the previous hidden state $h_{t-1}$ to the current hidden state $h_t$. This is where the “recurrence” happens!</li> <li>$b_h$: This is the bias vector for the hidden layer.</li> <li>$\tanh$: This is the hyperbolic tangent activation function. It squashes values between -1 and 1, introducing non-linearity which is essential for learning complex patterns. Without it, stacking layers would just be a series of linear transformations, which limits what the network can learn.</li> </ul> <p>For the output equation:</p> <ul> <li>$W_{hy}$: These are the weights connecting the hidden state $h_t$ to the output $y_t$.</li> <li>$b_y$: This is the bias vector for the output layer.</li> <li>The output $y_t$ might then be passed through another activation function (like softmax for classification, to get probabilities).</li> </ul> <p>Notice that $W_{hh}$, $W_{xh}$, $b_h$, $W_{hy}$, and $b_y$ are the <em>same</em> matrices and vectors used at <em>every single time step</em>. This parameter sharing is incredibly efficient and allows the network to learn robust patterns that apply across different positions in a sequence.</p> <h3 id="where-rnns-shine-real-world-applications">Where RNNs Shine: Real-World Applications</h3> <p>The introduction of RNNs revolutionized how we handle sequential data across many domains:</p> <ol> <li> <strong>Natural Language Processing (NLP):</strong> <ul> <li> <strong>Machine Translation:</strong> Input a sentence in one language, output it in another.</li> <li> <strong>Text Generation:</strong> Given a starting phrase, generate coherent and contextually relevant text. This is how many AI text generators work!</li> <li> <strong>Sentiment Analysis:</strong> Read a review and determine if it’s positive or negative.</li> <li> <strong>Speech Recognition:</strong> Convert spoken audio into text.</li> </ul> </li> <li> <strong>Time Series Prediction:</strong> <ul> <li>Predicting stock prices, weather patterns, or energy consumption based on historical data.</li> </ul> </li> <li> <strong>Music Generation:</strong> <ul> <li>Generating new musical compositions by learning patterns from existing music.</li> </ul> </li> <li> <strong>Video Analysis:</strong> <ul> <li>Understanding actions in videos, where each frame is a step in the sequence.</li> </ul> </li> </ol> <h3 id="the-catch-the-long-term-dependency-problem">The Catch: The “Long-Term Dependency Problem”</h3> <p>As I delved deeper into RNNs, I encountered their Achilles’ heel: the dreaded <strong>vanishing and exploding gradient problem</strong>.</p> <p>During training, neural networks learn by adjusting their weights based on the “gradient” of the loss function. Think of the gradient as the slope of a hill – it tells you which way to step to reach the bottom (minimize loss). In RNNs, these gradients are calculated by backpropagating through time, essentially unwinding the unrolled network.</p> <ul> <li> <p><strong>Vanishing Gradients:</strong> When you multiply many small numbers together (which happens when gradients are less than 1.0, and they’re multiplied across many time steps), the result rapidly shrinks towards zero. This means gradients from early time steps become negligible by the time they reach the beginning of a long sequence. Consequently, the network “forgets” information from earlier parts of the sequence, making it hard to learn long-term dependencies (e.g., understanding the subject of a sentence that appeared 20 words ago).</p> </li> <li> <p><strong>Exploding Gradients:</strong> Conversely, if gradients are consistently greater than 1.0, they can grow exponentially, leading to extremely large updates to the weights. This makes the training process unstable, causing the network to diverge and fail to learn.</p> </li> </ul> <p>This problem meant that while RNNs were great for short sequences, they struggled significantly with very long ones, limiting their ability to truly capture complex, long-range context.</p> <h3 id="the-evolution-lstms-and-grus">The Evolution: LSTMs and GRUs</h3> <p>Fortunately, the brilliant minds in the deep learning community didn’t stop there. To address the gradient problem and enhance the RNN’s memory capabilities, more sophisticated architectures emerged. The most famous of these are <strong>Long Short-Term Memory (LSTM) networks</strong> and <strong>Gated Recurrent Units (GRUs)</strong>.</p> <p>While the details of LSTMs and GRUs deserve their own blog post, the key takeaway is that they introduce “gates” – special mechanisms that allow the network to selectively <em>remember</em>, <em>forget</em>, or <em>update</em> information in its hidden state. These gates, typically implemented using sigmoid activation functions, act like intelligent switches, controlling the flow of information and preventing gradients from vanishing or exploding. They essentially give the RNN a more sophisticated and explicit control over its memory.</p> <h3 id="my-takeaway-and-your-next-step">My Takeaway and Your Next Step</h3> <p>My journey with RNNs was a profound one. It showed me how a seemingly simple concept – a feedback loop – could unlock a whole new dimension of machine intelligence, allowing computers to process and understand the world in sequences, just like we do. From trying to predict the next word to generating entire paragraphs of text, RNNs and their more advanced siblings (LSTMs, GRUs, and now, the mighty Transformers) have fundamentally reshaped the landscape of AI.</p> <p>If you’re eager to dive deeper, I highly recommend exploring how LSTMs and GRUs work. Understanding the core RNN first, however, is absolutely foundational.</p> <p>So, go ahead, try to implement a simple RNN using your favorite deep learning library (TensorFlow or PyTorch are great!). You’ll find a whole new world of sequential data waiting to be explored. The ability to give machines memory is not just a technical achievement; it’s a step closer to building truly intelligent systems that can understand the rich, dynamic, and sequential nature of our world.</p> <p>Happy coding, and keep exploring!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>