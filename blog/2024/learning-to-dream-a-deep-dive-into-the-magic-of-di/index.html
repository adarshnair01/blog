<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Learning to Dream: A Deep Dive into the Magic of Diffusion Models | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/learning-to-dream-a-deep-dive-into-the-magic-of-di/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Learning to Dream: A Deep Dive into the Magic of Diffusion Models</h1> <p class="post-meta"> Created on March 16, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/generative-ai"> <i class="fa-solid fa-hashtag fa-sm"></i> Generative AI</a>   <a href="/blog/blog/tag/diffusion-models"> <i class="fa-solid fa-hashtag fa-sm"></i> Diffusion Models</a>   <a href="/blog/blog/tag/ai-art"> <i class="fa-solid fa-hashtag fa-sm"></i> AI Art</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>As a young student, I was always captivated by the idea of creation. Not just building things with my hands, but the very act of bringing something new into existence. Fast forward to today, and I find myself utterly spellbound by the creative power of Artificial Intelligence, particularly a groundbreaking family of models called <strong>Diffusion Models</strong>. You’ve probably seen their breathtaking work in tools like DALL-E 2, Midjourney, or Stable Diffusion – generating everything from photorealistic landscapes to fantastical creatures, all from a simple text prompt.</p> <p>When I first encountered these models, it felt like pure magic. How could a computer learn to “dream” and paint such intricate masterpieces? It seemed like an alchemist’s secret. But as I peeled back the layers, I discovered that the magic isn’t in some unknowable force, but in elegant mathematics and clever engineering. It’s a journey from pure static noise to coherent, beautiful imagery, step by careful step. And today, I want to share that journey with you.</p> <h3 id="what-even-are-generative-models">What Even <em>Are</em> Generative Models?</h3> <p>Before we dive into diffusion, let’s quickly frame what generative models are. In machine learning, we often talk about two main types of tasks:</p> <ol> <li> <strong>Discriminative Models:</strong> These are classifiers. They learn to <em>distinguish</em> between different types of data. Think of an AI that tells you if an image contains a cat or a dog. It discriminates.</li> <li> <strong>Generative Models:</strong> These are creators. They learn the underlying <em>distribution</em> of a dataset and then use that knowledge to <em>generate</em> new data points that resemble the original training data. An AI that can draw a new, never-before-seen cat or dog image is a generative model.</li> </ol> <p>For years, Generative Adversarial Networks (GANs) dominated this space, but they often struggled with training stability and generating diverse outputs. Then came Diffusion Models, and they completely changed the game.</p> <h3 id="the-core-idea-reverse-engineering-noise">The Core Idea: Reverse Engineering Noise</h3> <p>Imagine you have a beautiful painting. Now, imagine taking that painting and slowly, gently, sprinkling tiny grains of sand onto it. Then more, and more, until eventually, the painting is completely obscured by a thick layer of sand, becoming nothing but a field of random static.</p> <p>The core idea of Diffusion Models is to learn how to <strong>reverse this process</strong>. If you start with the pure static (noise) and you know how the sand was added, can you learn to <em>remove</em> the sand, grain by grain, until the original painting (or a new, similar one) emerges?</p> <p>This elegant concept is what makes Diffusion Models so powerful. They are trained to systematically denoise an image, transforming pure noise into meaningful data.</p> <p>Let’s break it down into two main processes:</p> <ol> <li> <strong>Forward Diffusion Process (Adding Noise):</strong> We take a clean image and gradually add Gaussian noise to it over many steps, until it becomes pure random noise. This process is fixed and easy to describe mathematically.</li> <li> <strong>Reverse Diffusion Process (Removing Noise):</strong> We learn to reverse the forward process. Starting from pure noise, we iteratively remove noise to generate a new, clean image. This is the magical part that the model learns.</li> </ol> <h3 id="the-forward-diffusion-process-the-gradual-destruction">The Forward Diffusion Process: The Gradual Destruction</h3> <p>Let’s start with a pristine image, $x_0$. This is our starting point. We then define a sequence of $T$ steps, where at each step $t$, we add a small amount of Gaussian noise to the image $x_{t-1}$ to get $x_t$.</p> <p>Mathematically, this looks like:</p> <table> <tbody> <tr> <td>$q(x_t</td> <td>x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t} x_{t-1}, \beta_t \mathbf{I})$</td> </tr> </tbody> </table> <p>Here:</p> <ul> <li>$x_t$ is the image at timestep $t$.</li> <li>$x_{t-1}$ is the image from the previous timestep.</li> <li>$\mathcal{N}$ denotes a normal (Gaussian) distribution.</li> <li>$\beta_t$ is a small, positive value (the “variance schedule”). It controls how much noise is added at each step. Typically, $\beta_t$ increases over time, meaning we add more noise in later steps.</li> <li>$\sqrt{1-\beta_t}$ determines how much of the previous image we retain.</li> <li>$\mathbf{I}$ is the identity matrix, meaning the noise is added independently to each pixel.</li> </ul> <p>A beautiful property of this process is that we can directly sample $x_t$ from $x_0$ using the following equation, which is derived by repeatedly applying the step-by-step definition:</p> <table> <tbody> <tr> <td>$q(x_t</td> <td>x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha_t}} x_0, (1-\bar{\alpha_t}) \mathbf{I})$</td> </tr> </tbody> </table> <p>where $\alpha_t = 1 - \beta_t$ and $\bar{\alpha_t} = \prod_{s=1}^t \alpha_s$.</p> <p>This equation is crucial for training, as it allows us to directly get a noisy version of $x_0$ at <em>any</em> timestep $t$, without having to simulate all previous steps. By the time we reach $x_T$, if $T$ is large enough and $\beta_t$ values are well-chosen, $x_T$ will be almost indistinguishable from pure Gaussian noise.</p> <p>Think of it like repeatedly blurring an image and then adding random sprinkles until it’s just a field of colorful static. This forward process is deterministic and known. The real challenge, and the magic, lies in reversing it.</p> <h3 id="the-reverse-diffusion-process-learning-to-denoise">The Reverse Diffusion Process: Learning to Denoise</h3> <table> <tbody> <tr> <td>Our goal is to learn the reverse of the forward process: how to go from $x_t$ back to $x_{t-1}$. This means we want to find the conditional probability $p_\theta(x_{t-1}</td> <td>x_t)$.</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>The <em>true</em> reverse transition $q(x_{t-1}</td> <td>x_t)$ is complex and depends on the initial image $x_0$. However, it turns out that if $\beta_t$ are small, $q(x_{t-1}</td> <td>x_t)$ is also a Gaussian distribution. Even more conveniently, the true posterior $q(x_{t-1}</td> <td>x_t, x_0)$ <em>is</em> tractable and also Gaussian!</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$q(x_{t-1}</td> <td>x_t, x_0) = \mathcal{N}(x_{t-1}; \tilde{\mu}(x_t, x_0), \tilde{\beta_t} \mathbf{I})$</td> </tr> </tbody> </table> <p>where $\tilde{\mu}(x_t, x_0) = \frac{\sqrt{\alpha_t}(1-\bar{\alpha}<em>{t-1})}{1-\bar{\alpha_t}} x_t + \frac{\sqrt{\bar{\alpha}</em>{t-1}}\beta_t}{1-\bar{\alpha_t}} x_0$ and $\tilde{\beta_t} = \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha_t}}\beta_t$.</p> <p>The brilliance here is that if we could predict $x_0$ (the original image) from $x_t$ (the noisy image), we could then perfectly compute the mean $\tilde{\mu}$ and reverse the process!</p> <p>But we don’t know $x_0$. So, what do we do? We train a neural network to estimate it! Or, even better, we train a neural network to estimate the noise $\epsilon$ that was added to $x_0$ to get $x_t$. This is a common simplification in modern Diffusion Models (like DDPMs).</p> <p>From the equation for $q(x_t | x_0)$, we know that $x_t = \sqrt{\bar{\alpha_t}} x_0 + \sqrt{1-\bar{\alpha_t}} \epsilon$, where $\epsilon \sim \mathcal{N}(0, \mathbf{I})$. We can rearrange this to express $x_0$ in terms of $x_t$ and $\epsilon$:</p> <p>$x_0 = \frac{1}{\sqrt{\bar{\alpha_t}}} (x_t - \sqrt{1-\bar{\alpha_t}} \epsilon)$</p> <p>Now, we train a neural network, often called $\epsilon_\theta(x_t, t)$, to predict this noise $\epsilon$. Once we have $\epsilon_\theta(x_t, t)$, we can substitute it back into the equation for $x_0$ to get an <em>estimate</em> of the original image, $\hat{x}_0$. With $\hat{x}_0$, we can then compute an estimate for $\tilde{\mu}(x_t, \hat{x}_0)$!</p> <table> <tbody> <tr> <td>So, the reverse process involves our neural network $p_\theta(x_{t-1}</td> <td>x_t)$, which approximates $q(x_{t-1}</td> <td>x_t)$, modeled as a Gaussian where its mean is estimated from $x_t$ and the predicted noise.</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$p_\theta(x_{t-1}</td> <td>x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t))$</td> </tr> </tbody> </table> <p>Here, $\mu_\theta(x_t, t)$ and $\Sigma_\theta(x_t, t)$ are learned functions (our neural network) that depend on the noisy image $x_t$ and the current timestep $t$. Typically, $\Sigma_\theta$ is fixed to one of the $\tilde{\beta_t}$ values, and the network focuses on learning $\mu_\theta$.</p> <h3 id="the-training-objective-learning-to-predict-noise">The Training Objective: Learning to Predict Noise</h3> <p>Training a Diffusion Model is surprisingly simple, especially compared to the complexities of GANs. Here’s the core idea:</p> <ol> <li> <strong>Pick an image:</strong> Take a clean image $x_0$ from your dataset.</li> <li> <strong>Pick a random timestep:</strong> Choose a random $t$ between 1 and $T$.</li> <li> <strong>Generate a noisy version:</strong> Use the forward process equation to directly generate $x_t$ from $x_0$ by adding a specific amount of noise $\epsilon$. $x_t = \sqrt{\bar{\alpha_t}} x_0 + \sqrt{1-\bar{\alpha_t}} \epsilon$</li> <li> <strong>Train the network:</strong> Feed $x_t$ and $t$ into your neural network, $\epsilon_\theta(x_t, t)$. The network’s job is to predict the noise $\epsilon$ that was added in step 3.</li> <li> <strong>Calculate the loss:</strong> The training objective is to minimize the difference between the <em>actual</em> noise $\epsilon$ (which we know because we added it) and the <em>predicted</em> noise $\epsilon_\theta(x_t, t)$. $L = ||\epsilon - \epsilon_\theta(x_t, t)||^2$</li> </ol> <p>This is a simple mean squared error loss. The neural network that performs this task is often a <strong>U-Net</strong> architecture. U-Nets are great for image-to-image tasks because they can capture both fine-grained details and high-level structure by using skip connections between encoder and decoder paths. This allows them to preserve spatial information while processing features at different scales.</p> <p>Repeat these steps millions of times with countless images, and your U-Net will become incredibly good at predicting the noise component in any noisy image $x_t$ at any given timestep $t$.</p> <h3 id="how-to-generate-a-new-image-the-reverse-magic">How to Generate a New Image: The Reverse Magic</h3> <p>Once our $\epsilon_\theta$ network is trained, generating a new image is like watching an artist bring a sculpture to life from a lump of clay:</p> <ol> <li> <strong>Start with random noise:</strong> Generate a sample $x_T$ from a standard Gaussian distribution (pure static). This is our “lump of clay.”</li> <li> <strong>Iterate backwards:</strong> For $t = T, T-1, …, 1$: <ul> <li> <strong>Predict the noise:</strong> Use our trained network $\epsilon_\theta(x_t, t)$ to predict the noise component in $x_t$.</li> <li> <strong>Denoise:</strong> Calculate $x_{t-1}$ by subtracting the predicted noise. The exact formula for this step is derived from the true reverse mean $\tilde{\mu}$ and the predicted noise. A common version is: $x_{t-1} = \frac{1}{\sqrt{\alpha_t}} \left(x_t - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha_t}}} \epsilon_\theta(x_t, t)\right) + \sigma_t z$ where $z \sim \mathcal{N}(0, \mathbf{I})$ (a small amount of <em>new</em> noise for stochasticity) and $\sigma_t^2$ is related to $\beta_t$.</li> <li>This step effectively nudges the noisy image towards a slightly less noisy version, predicting what the image <em>should</em> look like at the previous timestep.</li> </ul> </li> <li> <strong>Reveal the masterpiece:</strong> After $T$ steps, you will have $x_0$, a brand new, high-quality image generated by the model!</li> </ol> <p>This process is fascinating because the model doesn’t just “paint” directly. It sculpts by removing imperfections, guided by what it learned about how data becomes noise.</p> <h3 id="why-are-diffusion-models-so-good">Why Are Diffusion Models So Good?</h3> <ol> <li> <strong>Unparalleled Image Quality:</strong> Diffusion models consistently produce highly realistic and diverse images, often surpassing GANs in visual fidelity.</li> <li> <strong>Stable Training:</strong> Unlike GANs, which involve an adversarial dance between two networks, Diffusion Models have a simple, stable training objective (minimizing a straightforward loss function). No more tricky hyperparameter tuning to balance two competing networks!</li> <li> <strong>Mode Coverage:</strong> GANs often suffer from “mode collapse,” where they only generate a subset of the possible outputs. Diffusion models are much better at covering the entire data distribution, leading to more diverse and representative generations.</li> <li> <strong>Controllability:</strong> It’s relatively easy to condition Diffusion Models on other inputs, like text prompts (as in Stable Diffusion), image masks for inpainting, or even class labels, allowing for incredible control over the generated output.</li> <li> <strong>Flexible Sampling:</strong> While initial diffusion models were slow to sample, techniques like DDIM (Denoising Diffusion Implicit Models) and latent diffusion have drastically sped up generation times, making them practical for real-world applications.</li> </ol> <h3 id="beyond-images-the-versatility-of-diffusion">Beyond Images: The Versatility of Diffusion</h3> <p>While best known for stunning image generation, Diffusion Models are proving to be incredibly versatile across various domains:</p> <ul> <li> <strong>Image Editing:</strong> Inpainting (filling missing parts), outpainting (extending images), style transfer, super-resolution.</li> <li> <strong>Video Generation:</strong> Animating sequences from text prompts or existing images.</li> <li> <strong>Audio Synthesis:</strong> Generating realistic speech, music, or sound effects.</li> <li> <strong>3D Content Creation:</strong> Generating 3D models from text or 2D images.</li> <li> <strong>Drug Discovery:</strong> Designing new molecules with desired properties.</li> <li> <strong>Scientific Simulation:</strong> Generating realistic simulations for complex systems.</li> </ul> <h3 id="the-journey-continues">The Journey Continues</h3> <p>My journey into Diffusion Models has truly demystified the “magic” while deepening my appreciation for their elegance and power. They represent a significant leap forward in generative AI, pushing the boundaries of what machines can create.</p> <p>Of course, challenges remain. Generating high-resolution images can still be computationally intensive, and the speed of sampling, though improved, can still be a bottleneck for some applications. However, active research in areas like latent diffusion models (which perform diffusion in a compressed latent space rather than pixel space) and faster sampling schedules are continuously addressing these issues.</p> <p>If you’re a student fascinated by AI, I encourage you to dive deeper. Explore the papers, experiment with open-source models like Stable Diffusion, and perhaps even try implementing a simple diffusion model yourself. You’ll find that the “magic” of AI often reveals itself to be a beautiful interplay of mathematics, statistics, and brilliant computational design. And that, to me, is more captivating than any illusion.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>