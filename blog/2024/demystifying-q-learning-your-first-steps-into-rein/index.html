<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Demystifying Q-Learning: Your First Steps into Reinforcement Learning's Core | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/demystifying-q-learning-your-first-steps-into-rein/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Demystifying Q-Learning: Your First Steps into Reinforcement Learning's Core</h1> <p class="post-meta"> Created on March 14, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/reinforcement-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Reinforcement Learning</a>   <a href="/blog/blog/tag/q-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Q-Learning</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Welcome, curious minds, to a journey into one of the most fascinating corners of Artificial Intelligence: Reinforcement Learning (RL). If you’ve ever dreamt of building intelligent agents that can learn from experience, adapt, and make their own decisions, you’re in the right place. Today, we’re going to peel back the layers of a cornerstone algorithm that kickstarted much of modern RL: <strong>Q-Learning</strong>.</p> <p>When I first encountered RL, the idea that a machine could learn by trial and error, much like a child figuring out a new game, seemed almost magical. No datasets, no labels, just an agent interacting with an environment, getting feedback, and slowly but surely, becoming an expert. Q-Learning is one of the clearest and most intuitive ways to understand this magic. So, let’s embark on this adventure together!</p> <h3 id="what-even-is-reinforcement-learning">What Even <em>Is</em> Reinforcement Learning?</h3> <p>Before we dive into Q-Learning, let’s briefly set the stage. Imagine you have a pet puppy. You want to teach it a trick. You don’t give it a manual. Instead, you guide it, and when it does something right, you give it a treat (a <em>reward</em>). When it does something wrong, it gets no treat, or maybe a gentle “no” (a <em>negative reward</em>). Over time, the puppy learns to associate certain actions in certain situations with getting a treat.</p> <p>That’s Reinforcement Learning in a nutshell:</p> <ul> <li> <strong>Agent:</strong> Our puppy (or our learning algorithm).</li> <li> <strong>Environment:</strong> The living room, backyard, etc., where the puppy performs tricks.</li> <li> <strong>State ($S$):</strong> The current situation (e.g., puppy sitting, puppy standing, ball in front of puppy).</li> <li> <strong>Action ($A$):</strong> What the puppy does (e.g., sit, fetch, bark).</li> <li> <strong>Reward ($R$):</strong> The treat or lack thereof, immediate feedback.</li> <li> <strong>Policy ($\pi$):</strong> The strategy the puppy develops (e.g., “when ball is thrown, fetch it”).</li> </ul> <p>The agent’s goal is simple: <strong>maximize its cumulative reward over time.</strong> Q-Learning is a brilliant way to achieve this.</p> <h3 id="the-q-in-q-learning-quality-value-and-a-treasure-map">The “Q” in Q-Learning: Quality, Value, and a Treasure Map</h3> <p>The core idea of Q-Learning revolves around learning a “Q-value” for every possible <strong>state-action pair</strong>. Think of it this way: if you’re in a specific state (e.g., “I’m at a fork in the road”) and you take a specific action (e.g., “turn left”), what’s the <em>quality</em> or <em>value</em> of that decision in terms of future rewards?</p> <p>Imagine a treasure hunt. At each step, you’re in a certain location (a <em>state</em>). You have several paths you can take (an <em>action</em>). Some paths might lead directly to a small coin (an <em>immediate reward</em>), while others might seem less rewarding initially but eventually lead to the grand treasure chest (a <em>large future reward</em>).</p> <p>Q-Learning tries to estimate the <em>total discounted future reward</em> you can expect if you take a specific action from a specific state, and then continue optimally from that point onwards. We call this $Q(s, a)$.</p> <p>Essentially, our agent is building a “treasure map” where each point on the map (state, action) has a numerical value indicating how “good” it is to be there and perform that action.</p> <h3 id="building-our-agents-brain-the-q-table">Building Our Agent’s Brain: The Q-Table</h3> <p>For simple environments with a limited number of states and actions, our agent’s brain can be a literal table, called the <strong>Q-Table</strong>.</p> <p>Let’s consider a very simple grid world: a 3x3 maze. States: (0,0), (0,1), …, (2,2) — 9 possible states. Actions: Up, Down, Left, Right — 4 possible actions.</p> <p>Our Q-Table would look something like this (conceptual):</p> <table> <thead> <tr> <th>State \ Action</th> <th>Up</th> <th>Down</th> <th>Left</th> <th>Right</th> </tr> </thead> <tbody> <tr> <td>(0,0)</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td> </tr> <tr> <td>(0,1)</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td> </tr> <tr> <td>…</td> <td>…</td> <td>…</td> <td>…</td> <td>…</td> </tr> <tr> <td>(2,2)</td> <td>0</td> <td>0</td> <td>0</td> <td>0</td> </tr> </tbody> </table> <p>Initially, all Q-values are typically set to zero. As our agent explores the environment, interacts, and receives rewards, these values will be updated and refined.</p> <h3 id="the-heart-of-q-learning-the-update-rule">The Heart of Q-Learning: The Update Rule</h3> <p>This is where the magic happens. Every time our agent takes an action, moves to a new state, and receives a reward, it uses this experience to update its knowledge (the Q-table). The core of Q-Learning is the <strong>Bellman Equation for Q-values</strong>:</p> <p>$Q(s, a) \leftarrow Q(s, a) + \alpha \left[R + \gamma \max_{a’} Q(s’, a’) - Q(s, a)\right]$</p> <p>Don’t let the symbols intimidate you! Let’s break down each component, as I often found it helpful to understand each piece individually:</p> <ul> <li> <p><strong>$Q(s, a)$ (Old Estimate):</strong> This is the current Q-value for taking action $a$ in state $s$. It’s what our agent <em>currently believes</em> is the value of this state-action pair.</p> </li> <li> <p><strong>$\leftarrow$ (Assignment):</strong> We are updating the value of $Q(s, a)$.</p> </li> <li> <strong>$\alpha$ (Alpha - Learning Rate):</strong> This is a crucial hyperparameter, typically a small value between 0 and 1 (e.g., 0.1). <ul> <li>It determines <em>how much</em> we update our Q-value based on the new experience.</li> <li>A high $\alpha$ means the agent quickly adopts new information, potentially making it forget past experiences too fast.</li> <li>A low $\alpha$ means it learns slowly, making small adjustments.</li> <li>Think of it like how stubborn your puppy is: a high $\alpha$ puppy quickly changes its behavior, a low $\alpha$ puppy needs more repetition.</li> </ul> </li> <li> <p><strong>$R$ (Reward):</strong> This is the <em>immediate reward</em> the agent received for taking action $a$ in state $s$ and landing in state $s’$.</p> </li> <li> <strong>$\gamma$ (Gamma - Discount Factor):</strong> Another hyperparameter, also between 0 and 1 (e.g., 0.9). <ul> <li>It determines <em>how much</em> future rewards are valued compared to immediate rewards.</li> <li>A $\gamma$ closer to 0 makes the agent “myopic,” focusing only on immediate rewards.</li> <li>A $\gamma$ closer to 1 makes the agent “far-sighted,” considering future rewards more heavily.</li> <li>In our treasure hunt, a high $\gamma$ means you’re willing to take a longer, less immediately rewarding path if it leads to the grand treasure. A low $\gamma$ means you’d rather grab the small coin now.</li> </ul> </li> <li> <strong>$\max_{a’} Q(s’, a’)$ (Maximum Future Value):</strong> This is the most interesting part! <ul> <li>$s’$ is the <em>next state</em> the agent landed in after taking action $a$ from state $s$.</li> <li>$a’$ represents <em>all possible actions</em> the agent could take from this <em>new state</em> $s’$.</li> <li>$\max_{a’} Q(s’, a’)$ means we are looking at all possible actions from the <em>next state</em> $s’$ and picking the one that has the <em>highest Q-value</em> according to our current Q-table. This is what makes Q-Learning <em>optimistic</em> – it assumes the agent will always take the best possible action in the future.</li> </ul> </li> <li> <strong>$\left[R + \gamma \max_{a’} Q(s’, a’) - Q(s, a)\right]$ (Temporal Difference Error):</strong> This entire term represents the “surprise” or “error.” <ul> <li>$R + \gamma \max_{a’} Q(s’, a’)$ is our <em>new, better estimate</em> of the true value of $Q(s, a)$. It combines the immediate reward with the best possible discounted future reward from the next state.</li> <li>We subtract the $Q(s, a)$ (our old estimate) to see how much our prediction was off. If this error is positive, our old estimate was too low; if negative, too high.</li> </ul> </li> </ul> <p>So, in plain English, the update rule says: “The new Q-value for taking action ‘a’ in state ‘s’ should be updated by adding a fraction ($\alpha$) of the ‘surprise’ we just experienced. This ‘surprise’ is the difference between what we <em>just observed</em> (immediate reward plus the best possible discounted future reward from the next state) and what we <em>previously thought</em> ($Q(s, a)$) about the value of this state-action pair.”</p> <h3 id="the-full-q-learning-algorithm-step-by-step">The Full Q-Learning Algorithm (Step-by-Step)</h3> <ol> <li> <strong>Initialize the Q-Table:</strong> Fill all Q-values with zeros (or small random numbers).</li> <li> <strong>Set Hyperparameters:</strong> Choose values for $\alpha$, $\gamma$, and $\epsilon$ (we’ll discuss $\epsilon$ next).</li> <li> <strong>For Each Episode (e.g., one complete treasure hunt):</strong> <ul> <li> <strong>Reset the environment:</strong> Place the agent in an initial state ($s$).</li> <li> <strong>While the episode is not finished (agent hasn’t reached the goal or failed):</strong> <ul> <li> <strong>Choose an action ($a$):</strong> Based on the current Q-table values for state $s$. (More on this in the next section!)</li> <li> <strong>Execute action $a$:</strong> Interact with the environment.</li> <li> <strong>Observe outcome:</strong> Get the new state ($s’$), the immediate reward ($R$), and whether the episode is <code class="language-plaintext highlighter-rouge">done</code>.</li> <li> <strong>Update $Q(s, a)$:</strong> Apply the Bellman equation: $Q(s, a) \leftarrow Q(s, a) + \alpha \left[R + \gamma \max_{a’} Q(s’, a’) - Q(s, a)\right]$</li> <li> <strong>Transition to the next state:</strong> Set $s \leftarrow s’$.</li> </ul> </li> </ul> </li> </ol> <p>Repeat this process for thousands, even millions, of episodes. Slowly but surely, the Q-table will converge, and the values will reflect the true optimal expected future rewards.</p> <h3 id="the-balancing-act-exploration-vs-exploitation">The Balancing Act: Exploration vs. Exploitation</h3> <p>How does our agent choose an action $a$ from state $s$? This is a critical dilemma in RL:</p> <ul> <li> <strong>Exploitation:</strong> The agent uses its current knowledge (the Q-table) to choose the action that it <em>currently believes</em> will yield the maximum reward. It’s like always taking the path on the treasure map with the highest known value. This is “sticking to what you know.”</li> <li> <strong>Exploration:</strong> The agent tries a random action, even if its current knowledge suggests it might not be the best. It’s like trying a new, untrodden path on the map, hoping to discover an even better route or a hidden treasure. This is “trying new things.”</li> </ul> <p>If the agent only exploits, it might get stuck in a locally optimal solution, never finding the true grand treasure. If it only explores, it might never actually get to the treasure efficiently.</p> <p>The most common strategy to balance this is the <strong>$\epsilon$-greedy policy</strong>:</p> <ul> <li>With a small probability $\epsilon$ (epsilon, e.g., 0.1), the agent chooses a random action (explores).</li> <li>With probability $1 - \epsilon$, the agent chooses the action $a$ that has the highest $Q(s, a)$ value in its current state $s$ (exploits).</li> </ul> <p>Typically, $\epsilon$ starts high (e.g., 1.0) to encourage lots of exploration at the beginning when the agent knows nothing. As the agent learns more and the Q-table becomes more accurate, $\epsilon$ slowly decays over time towards a small value (e.g., 0.01), encouraging more exploitation.</p> <h3 id="limitations-and-the-road-ahead">Limitations and the Road Ahead</h3> <p>While incredibly powerful and foundational, Q-Learning with a Q-table has a significant limitation: the <strong>state space explosion</strong>.</p> <p>What if our “state” isn’t a simple 3x3 grid, but a complex image from a self-driving car’s camera? Or the precise joint angles of a robot arm? The number of possible states becomes astronomically large, making a simple Q-table impossible to store or update.</p> <p>This is where the magic of <strong>Deep Q-Networks (DQN)</strong> comes in. Instead of a table, we use a neural network to approximate the Q-values. The input to the neural network could be an image (the state), and the output would be the Q-values for each possible action. This combines the power of deep learning with reinforcement learning, leading to agents that can learn to play complex video games like Atari or control intricate robotic systems. But that, my friends, is a story for another day!</p> <h3 id="why-q-learning-matters-and-where-its-used">Why Q-Learning Matters (and Where It’s Used)</h3> <p>Q-Learning is a beautiful algorithm because it’s:</p> <ul> <li> <strong>Model-Free:</strong> It doesn’t need a pre-existing model of the environment (e.g., knowing exactly what happens if you take action ‘A’ in state ‘S’). It learns simply by observing.</li> <li> <strong>Off-Policy:</strong> It learns the optimal policy (what <em>should</em> be done) while potentially following a different, exploratory policy (what it’s <em>actually</em> doing). This is a powerful distinction.</li> <li> <strong>Foundational:</strong> It laid the groundwork for many advanced RL algorithms we see today, including Deep Q-Networks (DQNs).</li> </ul> <p>You can find Q-Learning (or its derivatives) at work in:</p> <ul> <li> <strong>Game AI:</strong> Creating agents that can play games from Pong to Go.</li> <li> <strong>Robotics:</strong> Teaching robots to grasp objects, navigate spaces, or perform intricate tasks.</li> <li> <strong>Resource Management:</strong> Optimizing energy consumption in data centers or managing traffic flow.</li> <li> <strong>Finance:</strong> Developing trading strategies.</li> </ul> <h3 id="your-turn-to-learn">Your Turn to Learn!</h3> <p>Q-Learning is a fantastic entry point into the world of Reinforcement Learning. It’s elegant, intuitive, and the mathematical backbone, once understood, makes perfect sense. My advice? Grab a simple environment (like a grid world or the classic “Frozen Lake” environment from OpenAI Gym), implement Q-Learning in Python, and watch your agent learn right before your eyes. The satisfaction of seeing an agent slowly but surely optimize its behavior is truly addictive.</p> <p>Happy learning, and may your agents always find the grand treasure!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>