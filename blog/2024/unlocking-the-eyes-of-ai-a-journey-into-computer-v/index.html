<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Unlocking the Eyes of AI: A Journey into Computer Vision | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/unlocking-the-eyes-of-ai-a-journey-into-computer-v/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Unlocking the Eyes of AI: A Journey into Computer Vision</h1> <p class="post-meta"> Created on November 07, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/computer-vision"> <i class="fa-solid fa-hashtag fa-sm"></i> Computer Vision</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/image-processing"> <i class="fa-solid fa-hashtag fa-sm"></i> Image Processing</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>My alarm blares, signaling the start of another day. I instinctively hit snooze, but not before my phone, with a quick glance, confirms it’s actually <em>me</em> trying to unlock it. Later, on my commute, a self-driving car effortlessly navigates traffic, identifying pedestrians, road signs, and other vehicles with astounding accuracy. At work, I might use an app that identifies plant species from a photo or a medical imaging tool that helps doctors spot anomalies.</p> <p>These aren’t scenes from a futuristic movie; they’re everyday realities, made possible by a field that continues to amaze and challenge me: <strong>Computer Vision</strong>.</p> <p>As a data scientist, I’ve spent countless hours wrestling with data, training models, and trying to get machines to understand the world. But there’s something uniquely captivating about Computer Vision. It’s the quest to replicate one of humanity’s most fundamental senses – sight – and bestow it upon machines. It’s about giving AI the ability to <em>see</em>, <em>interpret</em>, and <em>understand</em> the visual world around us.</p> <p>But what does “seeing” even mean for a computer? Let’s peel back the layers and embark on this journey together.</p> <h3 id="the-world-through-a-computers-eyes-pixels-and-numbers">The World Through a Computer’s “Eyes”: Pixels and Numbers</h3> <p>For humans, seeing is effortless. We glance at a cat and instantly recognize it as a cat, regardless of its color, size, or whether it’s sitting or running. For a computer, it’s a completely different story. An image isn’t a fluffy feline; it’s just a grid of numbers.</p> <p>Imagine a photograph. To a computer, that photo is a giant matrix (or an array, if you prefer) of tiny individual squares called <strong>pixels</strong>. Each pixel holds a numerical value representing its color and intensity. In a grayscale image, a pixel might be a single number from 0 (black) to 255 (white). For a color image, it’s usually three numbers – one for Red, one for Green, and one for Blue (RGB values) – each ranging from 0 to 255.</p> <p>So, when a computer “sees” a cat, it’s processing millions of these numbers. Its challenge is to look at patterns within these numbers and declare, “Aha! That specific arrangement of pixel values corresponds to a cat!”</p> <h3 id="from-simple-rules-to-smart-decisions-the-evolution-of-computer-vision">From Simple Rules to Smart Decisions: The Evolution of Computer Vision</h3> <p>Early attempts at Computer Vision were like trying to teach a toddler to recognize objects by giving them an exhaustive list of rigid rules: “If you see a perfectly straight line here, and another one exactly parallel there, and they’re this exact length, then it’s a table.” As you can imagine, this approach was incredibly fragile. What if the table was at an angle? What if the lighting was different? What if it had a tablecloth? The system would break.</p> <p>This was the era of <strong>classical image processing</strong>, where we hand-crafted algorithms to detect specific features:</p> <ul> <li> <strong>Edge Detection</strong>: Algorithms like Canny or Sobel filters would look for sudden changes in pixel intensity, which usually indicate an edge.</li> <li> <strong>Thresholding</strong>: Converting a color image into a black-and-white one based on pixel intensity.</li> <li> <strong>Feature Descriptors</strong>: Techniques like SIFT (Scale-Invariant Feature Transform) or HOG (Histogram of Oriented Gradients) were designed to extract unique, descriptive patterns from images that were somewhat robust to changes in scale or rotation.</li> </ul> <p>These methods were clever, but they required immense human effort to design and were often brittle. They struggled with the sheer variability of the real world. We needed something that could <em>learn</em> these features automatically, and adapt to different scenarios.</p> <h3 id="the-machine-learning-era-learning-from-data">The Machine Learning Era: Learning from Data</h3> <p>The next big leap came with <strong>Machine Learning</strong>. Instead of us defining all the rules, we started feeding computers vast amounts of data (images of cats, dogs, cars, etc.) and let algorithms like Support Vector Machines (SVMs) or Random Forests learn the patterns.</p> <p>Here, a critical step was still <strong>feature engineering</strong>. We’d use those classical image processing techniques (like SIFT or HOG) to extract “features” – those descriptive patterns – from the images. Then, we’d feed these hand-crafted features into our machine learning model. The model would then learn to associate certain combinations of these features with specific objects.</p> <p>This was better, but still constrained by our ability to design good features. What if the best features weren’t something we could easily conceptualize or hand-craft?</p> <h3 id="the-deep-learning-revolution-convolutional-neural-networks-cnns">The Deep Learning Revolution: Convolutional Neural Networks (CNNs)</h3> <p>Then came the game-changer: <strong>Deep Learning</strong>, specifically <strong>Convolutional Neural Networks (CNNs)</strong>. This is where computers truly began to learn to “see” in a more human-like way, by <em>automatically discovering</em> relevant features directly from the raw pixel data.</p> <p>Imagine you’re trying to identify a car. You don’t just look for four wheels; you recognize the overall shape, the windshield, the headlights, the doors. You build up this understanding hierarchically. CNNs do something very similar.</p> <p>The magic of CNNs lies in a few key operations:</p> <ol> <li> <p><strong>Convolutional Layers: The Feature Detectors</strong> At the heart of a CNN is the <strong>convolution operation</strong>. Think of it like a small “magnifying glass” or a “filter” (also called a kernel) that slides across your entire image. This filter is a small matrix of numbers, and its job is to detect a specific pattern, like an edge, a corner, or a particular texture.</p> <p>Let’s say we have an image $I$ and a filter $K$. The convolution operation involves multiplying the filter’s values by the corresponding pixel values in the image patch it’s currently covering, and then summing them up to get a single output pixel. This process is repeated as the filter slides across the entire image, creating a new “feature map.”</p> <p>Mathematically, for a 2D image and filter: $(I * K)(i, j) = \sum_m \sum_n I(i-m, j-n) K(m, n)$</p> <p>Initially, these filters are random. But during training, the CNN <em>learns</em> the optimal values for these filters, allowing them to detect increasingly sophisticated patterns.</p> <ul> <li> <strong>Early layers</strong> learn simple features: horizontal edges, vertical edges, diagonal lines, blobs of color.</li> <li> <strong>Middle layers</strong> combine these simple features to learn more complex patterns: corners, circles, textures, parts of objects (e.g., an eye, a wheel).</li> <li> <strong>Later layers</strong> combine these parts to recognize entire objects: faces, cars, animals.</li> </ul> </li> <li> <p><strong>Activation Functions: Adding Non-Linearity</strong> After convolution, an <strong>activation function</strong> (like ReLU - Rectified Linear Unit: $f(x) = \max(0, x)$) is applied. This introduces non-linearity, which is crucial for the network to learn complex patterns and relationships that aren’t just straight lines. Without non-linearity, a deep network would behave just like a single-layer network.</p> </li> <li> <p><strong>Pooling Layers: Downsampling and Robustness</strong> <strong>Pooling layers</strong> (most commonly Max Pooling) come after convolutional layers. Their job is to reduce the spatial dimensions (width and height) of the feature maps. Imagine taking a $2 \times 2$ window and selecting only the largest value from those four pixels.</p> <ul> <li> <strong>Why do this?</strong> <ul> <li> <strong>Reduces computation</strong>: Less data to process in subsequent layers.</li> <li> <strong>Reduces overfitting</strong>: Makes the model less sensitive to small variations or noise in the input image.</li> <li> <strong>Achieves spatial invariance</strong>: If a feature (like an edge) shifts slightly in the image, the pooling layer still likely picks up a strong activation, making the network more robust to minor positional changes.</li> </ul> </li> </ul> </li> <li> <p><strong>Fully Connected Layers: The Classifier</strong> After several alternating convolutional and pooling layers, the high-level features learned by the network are “flattened” into a single vector. This vector is then fed into one or more <strong>fully connected layers</strong> (like in a traditional neural network). These layers act as the final classifier, taking all the learned features and making a prediction about what’s in the image (e.g., “99% cat, 1% dog”).</p> </li> </ol> <h3 id="training-a-cnn-learning-by-example">Training a CNN: Learning by Example</h3> <p>The entire CNN architecture is trained using a process called <strong>backpropagation</strong> and <strong>gradient descent</strong>. We feed it thousands, even millions, of labeled images. The network makes a prediction, compares it to the correct answer (the label), calculates the “error,” and then adjusts its internal weights (the values in its filters and connections) to minimize that error. Over time, it gets incredibly good at recognizing patterns and making accurate predictions.</p> <h3 id="beyond-classification-the-diverse-tasks-of-computer-vision">Beyond Classification: The Diverse Tasks of Computer Vision</h3> <p>CNNs, and their more advanced variants, are the backbone of most modern Computer Vision applications, allowing us to tackle a wide array of tasks:</p> <ul> <li> <strong>Image Classification</strong>: “What is this object?” (e.g., Is this a hot dog or not a hot dog?)</li> <li> <strong>Object Detection</strong>: “What objects are in this image, and <em>where</em> are they?” (drawing bounding boxes around multiple objects, like in self-driving cars identifying pedestrians and other vehicles). Famous models include YOLO (You Only Look Once) and R-CNN (Region-based CNN).</li> <li> <strong>Semantic Segmentation</strong>: “What <em>category</em> does each pixel belong to?” (e.g., labeling every pixel as “sky,” “road,” or “building”). This provides a dense, pixel-level understanding of the scene.</li> <li> <strong>Instance Segmentation</strong>: Taking semantic segmentation a step further by distinguishing between individual instances of the same object class (e.g., identifying each individual person in a crowd, not just “people” generally).</li> <li> <strong>Pose Estimation</strong>: Locating key points on an object or person (e.g., identifying the joints of a human body to understand their posture or actions).</li> <li> <strong>Image Generation</strong>: Creating entirely new images (e.g., Generative Adversarial Networks (GANs) or Diffusion Models, which can create realistic faces, landscapes, or even artistic masterpieces from scratch).</li> </ul> <h3 id="the-road-ahead-challenges-and-ethical-considerations">The Road Ahead: Challenges and Ethical Considerations</h3> <p>While Computer Vision has made incredible strides, it’s not without its challenges:</p> <ul> <li> <strong>Data Bias</strong>: Models trained on biased datasets can perpetuate or amplify societal biases (e.g., facial recognition systems performing poorly on certain demographics).</li> <li> <strong>Robustness</strong>: Models can be surprisingly fragile to small, unnoticeable changes in input (known as adversarial attacks). Real-world conditions (varying lighting, weather, occlusions) still pose significant challenges.</li> <li> <strong>Explainability (XAI)</strong>: Often, we don’t fully understand <em>why</em> a deep learning model made a particular decision. For critical applications like medical diagnosis or autonomous driving, understanding the “why” is crucial.</li> <li> <strong>Ethical Implications</strong>: Privacy concerns with surveillance, the potential for misuse of facial recognition, and the impact of automation on jobs are all vital discussions that evolve with the technology.</li> </ul> <p>New research directions, such as <strong>self-supervised learning</strong> (learning from unlabeled data), <strong>transformers</strong> (architectures originally dominant in Natural Language Processing now showing powerful results in CV), and <strong>3D vision</strong>, promise to push the boundaries even further.</p> <h3 id="my-personal-takeaway">My Personal Takeaway</h3> <p>My journey through data science and machine learning has taught me that the most impactful technologies are often those that mimic human abilities in novel ways. Computer Vision is a prime example. It’s not just about getting computers to see; it’s about empowering them to understand, to interact, and to assist us in ways we’re only just beginning to imagine.</p> <p>Whether it’s enhancing medical diagnostics, making transportation safer, or simply helping my phone know it’s me, Computer Vision is a field brimming with innovation and purpose. If you’re passionate about problem-solving, enjoy a blend of mathematics and creativity, and want to build the future, I strongly encourage you to dive into this incredible world. The possibilities are truly limitless, and the future, seen through the eyes of AI, is looking brighter than ever.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>