<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Detective's Edge: Unveiling Truth with Bayesian Statistics | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/the-detectives-edge-unveiling-truth-with-bayesian/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Detective's Edge: Unveiling Truth with Bayesian Statistics</h1> <p class="post-meta"> Created on May 13, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/bayesian-statistics"> <i class="fa-solid fa-hashtag fa-sm"></i> Bayesian Statistics</a>   <a href="/blog/blog/tag/probability"> <i class="fa-solid fa-hashtag fa-sm"></i> Probability</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/statistical-inference"> <i class="fa-solid fa-hashtag fa-sm"></i> Statistical Inference</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hello fellow data explorers and curious minds!</p> <p>Today, I want to share a perspective on statistics that, for me, transformed how I approach uncertainty and learning from data. It’s a journey into the heart of “Bayesian Statistics,” and trust me, it’s less about intimidating formulas and more about developing an intuitive, powerful way of thinking.</p> <p>Imagine you’re a detective. You start with some initial hunches, maybe based on past cases or common sense. Then, a new piece of evidence surfaces. Do you ignore your initial hunches and only focus on the new clue? Or do you combine the new evidence with what you already suspected, refining your understanding and getting closer to the truth?</p> <p>Most of us would do the latter. We integrate new information into our existing mental model of the world. This natural human process of updating beliefs is precisely what Bayesian statistics formalizes.</p> <h3 id="the-philosophical-divide-frequentist-vs-bayesian-a-quick-peek">The Philosophical Divide: Frequentist vs. Bayesian (A Quick Peek)</h3> <p>Before we dive into the fun stuff, let’s quickly acknowledge the elephant in the room: there are generally two main schools of thought in statistics.</p> <ol> <li> <p><strong>Frequentist Statistics:</strong> This is what most of us encounter first. It views probability as the long-run frequency of an event. If you flip a fair coin an infinite number of times, the proportion of heads will tend towards 0.5. Parameters (like the true probability of heads) are considered fixed but unknown constants. Frequentist methods often focus on p-values and confidence intervals to make statements about the data <em>given</em> an assumed true parameter value.</p> </li> <li> <p><strong>Bayesian Statistics:</strong> This is our star today. It views probability as a <em>degree of belief</em>. Parameters are not fixed constants; they are quantities we are uncertain about, and we represent that uncertainty with probability distributions. As we gather new data, we <em>update</em> these beliefs. Instead of asking “What is the probability of observing this data given my hypothesis?”, Bayesians ask “What is the probability of my hypothesis being true given this data?”. It’s a subtle but crucial shift.</p> </li> </ol> <p>It’s like the difference between saying “If the coin were fair, how likely would it be to get 7 heads out of 10 flips?” (Frequentist) versus “Given that I got 7 heads out of 10 flips, how likely is it that the coin is fair?” (Bayesian). See the difference? Bayesian statistics often feels more aligned with how we intuitively reason.</p> <h3 id="the-heartbeat-of-bayesian-thinking-bayes-theorem">The Heartbeat of Bayesian Thinking: Bayes’ Theorem</h3> <p>At the core of all this lies a deceptively simple yet profoundly powerful formula: <strong>Bayes’ Theorem</strong>.</p> <p>Let’s write it down:</p> <table> <tbody> <tr> <td>$P(A</td> <td>B) = \frac{P(B</td> <td>A) P(A)}{P(B)}$</td> </tr> </tbody> </table> <p>Don’t let the symbols scare you! Let’s break down each component, giving them more intuitive names in our Bayesian context:</p> <ul> <li> <table> <tbody> <tr> <td>**$P(A</td> <td>B)$ - The Posterior Probability (What we want to know!)**</td> </tr> </tbody> </table> <ul> <li>This is our <em>updated belief</em> about hypothesis $A$ <em>after</em> observing data $B$. It’s the probability of our hypothesis being true, given the evidence. This is the gold we’re digging for!</li> </ul> </li> <li> <table> <tbody> <tr> <td>**$P(B</td> <td>A)$ - The Likelihood**</td> </tr> </tbody> </table> <ul> <li> <table> <tbody> <tr> <td>This tells us how probable our observed data $B$ would be if our hypothesis $A$ were true. It’s the “evidence strength” – how well the data supports hypothesis $A$. If $P(B</td> <td>A)$ is high, it means our data $B$ is quite consistent with $A$.</td> </tr> </tbody> </table> </li> </ul> </li> <li> <strong>$P(A)$ - The Prior Probability</strong> <ul> <li>This is our <em>initial belief</em> about hypothesis $A$ <em>before</em> observing any new data $B$. It represents our state of knowledge or ignorance. It could be based on previous experiments, expert opinion, or simply a broad range of possibilities if we have no strong initial feelings.</li> </ul> </li> <li> <strong>$P(B)$ - The Marginal Likelihood (or Evidence)</strong> <ul> <li>This is the total probability of observing the data $B$ across all possible hypotheses. It acts as a normalizing constant, ensuring that our posterior probabilities sum to 1. In many practical scenarios, especially when comparing hypotheses, we often don’t need to calculate $P(B)$ directly, as it’s just a scaling factor. We care about the <em>relative</em> probabilities.</li> </ul> </li> </ul> <p>So, in plain English, Bayes’ Theorem says:</p> <p><strong>Our updated belief about a hypothesis (Posterior) is proportional to how well the data supports it (Likelihood) multiplied by our initial belief in it (Prior).</strong></p> <h3 id="a-concrete-example-the-slightly-biased-coin">A Concrete Example: The Slightly Biased Coin</h3> <p>Let’s put this into action. Imagine you’re handed a coin, and you suspect it might be biased. You want to estimate the true probability of flipping a head, let’s call it $\theta$.</p> <p><strong>1. Formulating Our Prior ($P(\theta)$):</strong> Before you even flip the coin once, what’s your initial belief about $\theta$? Most coins are pretty fair, right? So, you might believe that $\theta$ is probably around 0.5, but it could range from 0 (always tails) to 1 (always heads).</p> <p>A common way to represent this belief for a probability like $\theta$ is using a <strong>Beta distribution</strong>. It’s super flexible and perfect for modeling probabilities. A Beta distribution is defined by two positive parameters, $\alpha$ and $\beta$.</p> <p>Let’s say we have a weak prior belief that the coin is fair, so we choose $Beta(2, 2)$. This distribution is centered at 0.5, but it’s quite wide, reflecting our mild uncertainty. It looks like a gentle hump around 0.5.</p> <p><strong>2. Gathering Data &amp; Calculating Likelihood ($P(\text{data}|\theta)$):</strong> You flip the coin 10 times and get 7 heads and 3 tails. This is our data!</p> <p>The probability of observing this specific sequence of heads and tails, given a true probability of heads $\theta$, follows a <strong>Binomial distribution</strong>.</p> <p>The likelihood function looks like this: $P(\text{data}|\theta) = \binom{n}{k} \theta^k (1-\theta)^{n-k}$ Where $n=10$ (total flips), $k=7$ (heads), and $(n-k)=3$ (tails). So, $P(7 \text{ heads in } 10 \text{ flips}|\theta) = \binom{10}{7} \theta^7 (1-\theta)^3$.</p> <p>This likelihood function tells us how “plausible” different values of $\theta$ are, given our observed data. If $\theta=0.5$, this probability is certain, but if $\theta=0.7$, it’s higher.</p> <p><strong>3. Updating Our Belief: The Posterior ($P(\theta|\text{data})$):</strong> Now, we combine our prior belief with the evidence using Bayes’ Theorem. $P(\theta|\text{data}) \propto P(\text{data}|\theta) P(\theta)$</p> <p>For our specific example: $P(\theta|\text{data}) \propto \left[ \binom{10}{7} \theta^7 (1-\theta)^3 \right] \times \left[ \frac{\theta^{2-1}(1-\theta)^{2-1}}{B(2,2)} \right]$</p> <p>Notice I used $\propto$ (proportional to) instead of $=$. This is because we’re omitting the $P(B)$ term for now, as it’s just a scaling factor. Simplifying, we combine the $\theta$ terms: $P(\theta|\text{data}) \propto \theta^{7+2-1} (1-\theta)^{3+2-1}$ $P(\theta|\text{data}) \propto \theta^8 (1-\theta)^4$</p> <p>This looks exactly like another Beta distribution! Specifically, a $Beta(\alpha_{new}, \beta_{new})$ where $\alpha_{new} = \alpha_{prior} + k_{heads}$ and $\beta_{new} = \beta_{prior} + k_{tails}$.</p> <p>So, our posterior distribution is $Beta(2+7, 2+3) = Beta(9, 5)$.</p> <p><strong>What does this mean?</strong> Our prior belief was $Beta(2,2)$, centered at 0.5. After observing 7 heads in 10 flips, our belief has shifted significantly to $Beta(9,5)$. This new distribution is centered at $\frac{9}{9+5} = \frac{9}{14} \approx 0.64$.</p> <p>Our belief about $\theta$ has moved from “probably fair (0.5)” towards “a bit biased towards heads (0.64)”. The distribution has also become narrower, reflecting that we are now <em>more confident</em> in this updated estimate of $\theta$. We’ve learned from the data!</p> <h3 id="why-is-this-so-powerful">Why is This So Powerful?</h3> <ol> <li> <p><strong>Incorporating Prior Knowledge:</strong> Unlike frequentist methods that often start from a “blank slate,” Bayesian statistics allows us to explicitly include existing knowledge, expert opinion, or results from previous studies. This is incredibly valuable, especially when data is scarce.</p> </li> <li> <p><strong>Sequential Learning:</strong> Bayes’ Theorem is perfectly designed for continuous learning. Every time you get new data, your current posterior distribution becomes the prior for the next update. This iterative process is how humans and intelligent systems learn effectively over time.</p> </li> <li> <p><strong>Direct Answers to Our Questions:</strong> We often want to know the probability of a hypothesis being true, or the range of probable values for a parameter. Bayesian methods directly provide these probabilities (e.g., “There’s a 95% probability that $\theta$ is between 0.5 and 0.75”), which are often more intuitive than frequentist p-values or confidence intervals.</p> </li> <li> <p><strong>Full Uncertainty Quantification:</strong> Instead of just a single “best estimate,” Bayesian analysis gives you a <em>distribution</em> over possible parameter values. This rich information allows you to understand the full range of uncertainty in your estimates.</p> </li> <li> <p><strong>Small Data Advantage:</strong> When you have very little data, frequentist methods can struggle. Bayesian methods, by leveraging prior information, can often provide more robust and sensible inferences.</p> </li> </ol> <h3 id="where-youll-see-bayesian-statistics-beyond-coins">Where You’ll See Bayesian Statistics (Beyond Coins!)</h3> <ul> <li> <strong>Machine Learning:</strong> Naive Bayes classifiers for spam detection and text classification. Gaussian Processes for flexible regression. Bayesian Neural Networks that quantify uncertainty in their predictions.</li> <li> <strong>A/B Testing:</strong> Deciding which website variant is better by continuously updating your belief in their performance.</li> <li> <strong>Medical Diagnosis:</strong> Updating the probability of a disease given test results (this is a classic example of Bayes’ Theorem in action!).</li> <li> <strong>Drug Discovery:</strong> Estimating the effectiveness of new treatments with limited trial data.</li> <li> <strong>Forecasting:</strong> Predicting future events (e.g., stock prices, weather) by incorporating new information.</li> <li> <strong>Astronomy:</strong> Estimating parameters of exoplanets or gravitational waves.</li> </ul> <h3 id="a-note-on-complexity-and-computation">A Note on Complexity and Computation</h3> <p>While the simple coin example worked out nicely with known distributions (called “conjugate priors”), many real-world problems don’t have such neat solutions. Calculating the posterior can involve complex integrals, especially for models with many parameters.</p> <p>This is where computational methods like <strong>Markov Chain Monte Carlo (MCMC)</strong> come into play. MCMC algorithms allow us to <em>sample</em> from complex posterior distributions, effectively bypassing the need for direct analytical calculation. Tools like PyMC3, Stan, and R’s <code class="language-plaintext highlighter-rouge">brms</code> package make these advanced computations accessible to data scientists.</p> <h3 id="embracing-the-bayesian-mindset">Embracing the Bayesian Mindset</h3> <p>Bayesian statistics isn’t just a set of formulas; it’s a paradigm shift. It encourages you to explicitly state your assumptions and beliefs, to be transparent about your uncertainty, and to continuously refine your understanding as new information comes to light.</p> <p>It’s about embracing uncertainty not as a weakness, but as a fundamental aspect of knowledge, and having a systematic way to reduce it.</p> <p>So, next time you encounter a problem involving uncertainty, ask yourself:</p> <ul> <li>What do I believe <em>before</em> seeing any new data? (Prior)</li> <li>How well does the new data align with different possibilities? (Likelihood)</li> <li>How should I update my beliefs given this new evidence? (Posterior)</li> </ul> <p>By doing so, you’ll not only be practicing Bayesian statistics, but you’ll also be thinking like a great detective, constantly refining your theories to get closer to the truth. Start exploring, and let the data guide your beliefs!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>