<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Alchemist's Secret: Turning Raw Data into Gold with Feature Engineering | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/the-alchemists-secret-turning-raw-data-into-gold-w/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Alchemist's Secret: Turning Raw Data into Gold with Feature Engineering</h1> <p class="post-meta"> Created on May 29, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/feature-engineering"> <i class="fa-solid fa-hashtag fa-sm"></i> Feature Engineering</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/data-preprocessing"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Preprocessing</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>Welcome back to my corner of the internet, where we unravel the mysteries of data science and machine learning, one concept at a time. Today, I want to talk about something that, in my early days, felt a bit like magic, but which I’ve come to realize is the cornerstone of almost every successful machine learning project: <strong>Feature Engineering</strong>.</p> <p>If you’ve ever felt like your data is just… sitting there, not quite telling the story you know it holds, then you’re in for a treat. Feature Engineering is all about taking that raw, inert data and crafting it into something meaningful, something that your models can actually <em>understand</em> and learn from. Think of it as giving your model glasses to see the world clearly, instead of a blurry mess.</p> <h3 id="what-is-feature-engineering-really">What is Feature Engineering, Really?</h3> <p>Imagine you’re trying to bake a cake. You have flour, sugar, eggs, butter – raw ingredients. But you can’t just throw them in an oven. You need to mix them, whisk them, maybe separate the egg whites, cream the butter and sugar. You’re transforming these basic ingredients into something the oven (your “model”) can work with to produce a delicious cake (your “prediction”).</p> <p>In data science, Feature Engineering is exactly that process of transforming raw data into features that represent the underlying problem to the predictive models, improving their accuracy and understanding. A “feature” is simply an input variable that your model uses to make a prediction.</p> <p>Why is this so crucial? Because machine learning models are fundamentally mathematical. They don’t “understand” concepts like “customer loyalty” or “peak traffic hour” directly. They understand numbers, patterns, and relationships <em>between numbers</em>. Our job, as data scientists, is to translate human understanding and domain knowledge into a language the model can speak.</p> <h3 id="the-why-why-cant-models-just-figure-it-out">The “Why”: Why Can’t Models Just Figure It Out?</h3> <p>You might wonder, “Can’t a super-smart AI just learn these relationships on its own?” While advanced deep learning models can indeed learn complex features, especially from images or text, for most tabular data problems (the kind you find in spreadsheets and databases), they still benefit immensely from well-engineered features.</p> <p>Here’s why:</p> <ol> <li> <strong>Models are “Dumb” without Context:</strong> A model sees a column of dates like <code class="language-plaintext highlighter-rouge">2023-10-26</code>. It doesn’t inherently know that October is a fall month, or that Friday might mean different purchasing behavior than Monday. We have to explicitly extract that “month” or “day of week” information for it.</li> <li> <strong>Highlighting Important Relationships:</strong> Sometimes, the most important information isn’t in a single column but in the <em>relationship</em> between two or more columns. For example, if you’re predicting house prices, the <code class="language-plaintext highlighter-rouge">total_area</code> and <code class="language-plaintext highlighter-rouge">number_of_rooms</code> might be useful individually, but <code class="language-plaintext highlighter-rouge">area_per_room</code> (a new feature derived from dividing total area by number of rooms) might be even more predictive!</li> <li> <strong>Improving Model Performance:</strong> Better features mean better performance. A model struggling with raw data can suddenly perform excellently once given well-crafted features. It reduces the “learning burden” on the model.</li> <li> <strong>Interpretability:</strong> Well-engineered features can often make your model’s decisions more interpretable. If your model predicts loan default because “debt-to-income ratio is high” (a engineered feature), that’s easier to understand than if it’s based on some vague combination of raw income and raw debt values.</li> </ol> <h3 id="the-toolbox-common-feature-engineering-techniques">The Toolbox: Common Feature Engineering Techniques</h3> <p>Let’s dive into some practical techniques you’ll use constantly.</p> <h4 id="1-numerical-feature-engineering">1. Numerical Feature Engineering</h4> <p>These techniques are for columns that contain numbers (e.g., age, price, temperature).</p> <ul> <li> <strong>Scaling and Normalization:</strong> <ul> <li> <strong>The Problem:</strong> Many machine learning algorithms (like Gradient Descent, Support Vector Machines, K-Nearest Neighbors) are sensitive to the scale of features. If one feature ranges from 0-1 and another from 0-1,000,000, the larger-scaled feature might dominate the calculation of distances or gradients.</li> <li> <strong>Min-Max Scaling:</strong> This rescales a feature to a fixed range, usually 0 to 1. $X_{scaled} = \frac{X - X_{min}}{X_{max} - X_{min}}$ This is great for algorithms that expect inputs in a bounded range.</li> <li> <strong>Standardization (Z-score Normalization):</strong> This rescales data to have a mean of 0 and a standard deviation of 1. $X_{scaled} = \frac{X - \mu}{\sigma}$ Where $\mu$ is the mean and $\sigma$ is the standard deviation. This is excellent for algorithms that assume a Gaussian distribution or are sensitive to feature means, like linear models.</li> </ul> </li> <li> <strong>Binning/Discretization:</strong> <ul> <li> <strong>The Idea:</strong> Grouping continuous numerical values into “bins” or categories. For example, turning ages (0-100) into <code class="language-plaintext highlighter-rouge">Child</code>, <code class="language-plaintext highlighter-rouge">Teenager</code>, <code class="language-plaintext highlighter-rouge">Adult</code>, <code class="language-plaintext highlighter-rouge">Senior</code>.</li> <li> <strong>Why:</strong> Can help capture non-linear relationships, reduce noise, or make models more robust to small variations. Sometimes, the <em>range</em> a value falls into is more important than the exact value itself.</li> </ul> </li> <li> <strong>Log Transformation:</strong> <ul> <li> <strong>The Idea:</strong> Applying a logarithmic function (e.g., $log(X)$ or $log(1+X)$) to a numerical feature.</li> <li> <strong>Why:</strong> Often used for highly skewed data (where values are heavily concentrated on one side, with a long tail on the other). It can make the distribution more symmetrical, which can help linear models perform better, as they often assume normally distributed errors.</li> </ul> </li> <li> <strong>Polynomial Features:</strong> <ul> <li> <strong>The Idea:</strong> Creating new features by raising existing features to a power or combining them multiplicatively. For example, from feature $X$, you can create $X^2$, $X^3$. From $X_1$ and $X_2$, you can create $X_1 X_2$.</li> <li> <strong>Why:</strong> To capture non-linear relationships. A linear model might struggle with data that curves, but by introducing $X^2$, it can effectively learn a parabolic relationship.</li> </ul> </li> </ul> <h4 id="2-categorical-feature-engineering">2. Categorical Feature Engineering</h4> <p>These techniques are for columns that contain categories (e.g., <code class="language-plaintext highlighter-rouge">City</code>, <code class="language-plaintext highlighter-rouge">Product_Type</code>, <code class="language-plaintext highlighter-rouge">Education_Level</code>). Models can’t work directly with text labels.</p> <ul> <li> <strong>One-Hot Encoding:</strong> <ul> <li> <strong>The Idea:</strong> Converts categorical variables into a set of new binary (0 or 1) features. For each unique category, a new column is created. If an observation belongs to that category, the value in the new column is 1; otherwise, it’s 0.</li> <li> <strong>Example:</strong> If you have a <code class="language-plaintext highlighter-rouge">Color</code> feature with values <code class="language-plaintext highlighter-rouge">Red</code>, <code class="language-plaintext highlighter-rouge">Blue</code>, <code class="language-plaintext highlighter-rouge">Green</code>: It becomes three new columns: <code class="language-plaintext highlighter-rouge">Color_Red</code>, <code class="language-plaintext highlighter-rouge">Color_Blue</code>, <code class="language-plaintext highlighter-rouge">Color_Green</code>. An entry that was <code class="language-plaintext highlighter-rouge">Red</code> would now be <code class="language-plaintext highlighter-rouge">Color_Red=1, Color_Blue=0, Color_Green=0</code>.</li> <li> <strong>Why:</strong> Prevents the model from incorrectly assuming an ordinal relationship between categories.</li> </ul> </li> <li> <strong>Label Encoding:</strong> <ul> <li> <strong>The Idea:</strong> Assigns a unique integer to each category. For example, <code class="language-plaintext highlighter-rouge">Red</code>=0, <code class="language-plaintext highlighter-rouge">Blue</code>=1, <code class="language-plaintext highlighter-rouge">Green</code>=2.</li> <li> <strong>Why:</strong> Use <em>only</em> when the categories have an inherent order (ordinal data), like <code class="language-plaintext highlighter-rouge">Small, Medium, Large</code> (which could be encoded as 0, 1, 2). If used on nominal data (like colors), the model might mistakenly infer that <code class="language-plaintext highlighter-rouge">Green</code> (2) is “greater” or “more important” than <code class="language-plaintext highlighter-rouge">Red</code> (0).</li> </ul> </li> <li> <strong>Frequency Encoding:</strong> <ul> <li> <strong>The Idea:</strong> Replaces each category with the frequency or count of its occurrence in the dataset.</li> <li> <strong>Why:</strong> Can be useful if the frequency of a category is predictive. For example, in a fraud detection system, if a particular city has a very high frequency of fraudulent transactions, its frequency count might be a good feature.</li> </ul> </li> </ul> <h4 id="3-date-and-time-feature-engineering">3. Date and Time Feature Engineering</h4> <p>Dates and times are a goldmine for features! Raw datetime objects are rarely useful directly.</p> <ul> <li> <strong>Extraction:</strong> <ul> <li> <strong>The Idea:</strong> Break down a datetime column into its components: <code class="language-plaintext highlighter-rouge">Year</code>, <code class="language-plaintext highlighter-rouge">Month</code>, <code class="language-plaintext highlighter-rouge">Day</code>, <code class="language-plaintext highlighter-rouge">Day of Week</code>, <code class="language-plaintext highlighter-rouge">Day of Year</code>, <code class="language-plaintext highlighter-rouge">Hour</code>, <code class="language-plaintext highlighter-rouge">Minute</code>, <code class="language-plaintext highlighter-rouge">Second</code>, <code class="language-plaintext highlighter-rouge">Is_Weekend</code>, <code class="language-plaintext highlighter-rouge">Is_Holiday</code>.</li> <li> <strong>Why:</strong> Different time components often have distinct patterns. For example, sales might peak on weekends or specific months.</li> </ul> </li> <li> <strong>Time Differences:</strong> <ul> <li> <strong>The Idea:</strong> Calculate the duration between two datetime columns. E.g., <code class="language-plaintext highlighter-rouge">time_to_delivery = delivery_date - order_date</code>.</li> <li> <strong>Why:</strong> The duration of an event can be highly predictive.</li> </ul> </li> <li> <strong>Cyclical Features:</strong> <ul> <li> <strong>The Idea:</strong> For features that repeat over a cycle (like hour of day, day of week, month of year), converting them to sine and cosine values can help models understand their cyclical nature without imposing artificial start/end points. For an hour <code class="language-plaintext highlighter-rouge">H</code> (0-23): $sin(2\pi \times H / 24)$ $cos(2\pi \times H / 24)$</li> <li> <strong>Why:</strong> A “day of week” encoded as 0-6 makes Monday (0) and Sunday (6) seem far apart, but they are actually adjacent in a week’s cycle. Sine/cosine transformation handles this.</li> </ul> </li> </ul> <h4 id="4-text-feature-engineering-briefly">4. Text Feature Engineering (Briefly)</h4> <p>While a whole field in itself, for basic understanding:</p> <ul> <li> <strong>Bag of Words/TF-IDF:</strong> Converts text into numerical vectors based on word counts or their importance (Term Frequency-Inverse Document Frequency).</li> <li> <strong>Word Embeddings:</strong> Represents words as dense vectors in a continuous vector space, capturing semantic relationships.</li> </ul> <h4 id="5-interaction-features--combinations">5. Interaction Features / Combinations</h4> <ul> <li> <strong>The Idea:</strong> Creating new features by combining existing ones, often through multiplication or division. <ul> <li><code class="language-plaintext highlighter-rouge">price_per_square_foot = total_price / total_area</code></li> <li><code class="language-plaintext highlighter-rouge">age_x_income = age * income</code></li> </ul> </li> <li> <strong>Why:</strong> To capture interactions that the model might not discover on its own. For example, the effect of <code class="language-plaintext highlighter-rouge">age</code> on an outcome might depend on <code class="language-plaintext highlighter-rouge">income</code>.</li> </ul> <h3 id="the-process-how-do-we-actually-do-it">The Process: How Do We Actually Do It?</h3> <p>Feature Engineering isn’t a magic formula you plug in. It’s an iterative process, often an art, driven by curiosity and domain knowledge.</p> <ol> <li> <strong>Understand Your Data (EDA is King!):</strong> Before doing anything, spend time with your data. Plot distributions, look at correlations, identify outliers, and understand missing values. This is your Exploratory Data Analysis (EDA) phase. Pandas in Python is your best friend here.</li> <li> <strong>Domain Knowledge is Gold:</strong> Talk to experts in the field the data comes from. If you’re predicting house prices, talk to real estate agents. They’ll tell you what truly drives value: school districts, crime rates, proximity to public transport, not just raw square footage. Your models will thank you.</li> <li> <strong>Hypothesis Generation:</strong> Based on your EDA and domain knowledge, hypothesize new features. “What if I combine these two columns?” “Could the <code class="language-plaintext highlighter-rouge">day of week</code> influence this outcome?”</li> <li> <strong>Experiment and Iterate:</strong> Create new features, train your model, evaluate its performance, and repeat. Keep refining. Sometimes a feature you thought was brilliant turns out to be useless, and a simple one makes all the difference.</li> <li> <strong>Feature Selection (A Quick Note):</strong> Just because you can create thousands of features doesn’t mean you should use them all. Too many features can lead to the “Curse of Dimensionality” and make your model slower and less accurate (overfitting). Techniques like L1 regularization (Lasso), tree-based feature importances, or Recursive Feature Elimination can help you select the best features.</li> </ol> <h3 id="challenges-and-pitfalls">Challenges and Pitfalls</h3> <p>Even with all its power, Feature Engineering isn’t without its challenges:</p> <ul> <li> <strong>Data Leakage:</strong> This is a big one! Accidentally including information from your target variable into your features. For example, if you’re predicting whether a customer will churn, and one of your features is “days since last complaint <em>after</em> churn,” that’s leakage. Your model will perform <em>amazingly</em> on your training data but miserably in the real world. Be very careful about the temporal order of events.</li> <li> <strong>Overfitting:</strong> Creating features that are too specific to your training data might make your model perform poorly on new, unseen data.</li> <li> <strong>Increased Complexity:</strong> More features can mean slower training, more memory usage, and harder-to-interpret models. Always aim for simplicity when possible.</li> </ul> <h3 id="conclusion-your-secret-superpower">Conclusion: Your Secret Superpower</h3> <p>Feature Engineering truly is the secret sauce in data science and machine learning. It’s where the raw ingredients are transformed into a gourmet meal. It’s the craft of turning numbers into insights, enabling models to not just process data, but to understand the underlying patterns of the real world.</p> <p>It’s a blend of technical skill, domain knowledge, creativity, and a dash of intuition. As you continue your journey in data science, you’ll find that mastering Feature Engineering will be one of your most valuable superpowers. Don’t just accept the data as it is; question it, explore it, and sculpt it into something truly magnificent.</p> <p>So, next time you’re faced with a dataset, don’t just jump to fitting a model. Take a moment. Put on your alchemist’s hat. What gold can you extract from those raw numbers?</p> <p>Keep experimenting, keep learning, and keep building!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>