<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Secret Ingredient: Unlocking Model Potential with Feature Engineering | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/the-secret-ingredient-unlocking-model-potential-wi/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Secret Ingredient: Unlocking Model Potential with Feature Engineering</h1> <p class="post-meta"> Created on November 06, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/feature-engineering"> <i class="fa-solid fa-hashtag fa-sm"></i> Feature Engineering</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/data-preprocessing"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Preprocessing</a>   <a href="/blog/blog/tag/model-performance"> <i class="fa-solid fa-hashtag fa-sm"></i> Model Performance</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey there, fellow explorers of the data universe!</p> <p>If you’re anything like me, when you first dive into the world of Data Science and Machine Learning, your head gets filled with fancy algorithms: Random Forests, Gradient Boosters, Neural Networks, oh my! We learn about training models, tuning hyperparameters, and evaluating performance metrics. It’s exciting, isn’t it? Like learning to wield a powerful new tool.</p> <p>But here’s a little secret I’ve learned along my journey: the most sophisticated algorithm in the world is only as good as the data you feed it. And more often than not, the raw data we start with isn’t quite ready for primetime. It’s like a master chef with incredible cooking skills but only given a pile of raw, unchopped vegetables and unopened cans. What if I told you there’s a crucial, often overlooked, step that can dramatically boost your model’s performance, sometimes even more than endless hyperparameter tuning?</p> <p>Welcome to the captivating world of <strong>Feature Engineering</strong>.</p> <h3 id="so-what-exactly-is-feature-engineering">So, What Exactly <em>Is</em> Feature Engineering?</h3> <p>At its core, Feature Engineering is the process of using <strong>domain knowledge</strong> to transform raw data into new, more meaningful features that better represent the underlying problem to a machine learning model. Think of it as telling a story to your model, but instead of just handing it a dictionary, you’re giving it well-crafted sentences, paragraphs, and even summaries.</p> <p>In simpler terms, it’s about making your data <em>smarter</em>. Machine learning models, regardless of their complexity, are essentially sophisticated pattern recognition machines. They work with numbers. If the patterns in your raw data are hidden or expressed in a way that’s hard for the model to discern, its performance will suffer. Feature engineering helps bring those hidden patterns to the surface.</p> <p>Let’s break down that formal definition:</p> <ul> <li> <strong>Domain Knowledge:</strong> This is the bedrock. It’s understanding <em>what</em> your data represents, the context behind it. If you’re predicting house prices, your domain knowledge tells you that square footage, number of bedrooms, and location are important. If you’re analyzing customer behavior, you know that things like “time since last purchase” or “average purchase value” might be key indicators. Without this context, you’re just manipulating numbers blindly.</li> <li> <strong>Transform Raw Data:</strong> This is the “engineering” part. We’re taking existing columns (features) and creating new ones, or modifying existing ones, to extract more predictive power.</li> </ul> <h3 id="the-why-behind-the-what-why-bother">The “Why” Behind the “What”: Why Bother?</h3> <p>You might be thinking, “Can’t the model just figure it out?” Sometimes, yes. Deep learning models, especially, are getting better at automatically learning features from raw data. But for a vast majority of practical problems, especially with tabular data, handcrafted features still reign supreme. Here’s why Feature Engineering is your secret weapon:</p> <ol> <li> <strong>Elevated Model Performance:</strong> This is the big one. Better features mean the model has clearer signals to learn from, leading to more accurate predictions, better classifications, and ultimately, a more performant solution. It’s the difference between a model getting 80% accuracy versus 95% accuracy.</li> <li> <strong>Bridging the Gap:</strong> Raw data is often messy and not in a model-ready format. For instance, a date string like “2023-10-26” is just text to a model. But features like “day of the week,” “month,” or “is_weekend” extracted from that date are highly informative.</li> <li> <strong>Reduced Overfitting &amp; Underfitting:</strong> Well-engineered features can simplify the problem for the model. If you provide strong, predictive features, the model doesn’t need to learn overly complex relationships, which can prevent overfitting (where the model learns the training data too well and fails on new data). Conversely, it helps prevent underfitting by giving the model enough meaningful information to learn anything at all.</li> <li> <strong>Enhanced Interpretability:</strong> Sometimes, a carefully constructed feature is more intuitive and explainable than a complex interaction learned by a black-box model. If you create a “loyalty_score” feature, it’s easy to explain its impact.</li> <li> <strong>Efficiency:</strong> With great features, you might even be able to use simpler, faster models (like linear regression) and still achieve excellent results, rather than relying on computationally expensive algorithms.</li> </ol> <p>Think of the “Garbage In, Garbage Out” (GIGO) principle. If you feed your model garbage (poor, uninformative features), you’ll get garbage predictions, no matter how powerful your algorithm is. Feature engineering transforms that “garbage” into gold.</p> <h3 id="hands-on-with-common-feature-engineering-techniques">Hands-On with Common Feature Engineering Techniques</h3> <p>Let’s get practical! Here are some common techniques you’ll encounter, categorized by data type, along with why they’re useful.</p> <h4 id="1-numerical-features">1. Numerical Features</h4> <p>These are your standard numbers: age, price, count, etc.</p> <ul> <li> <strong>Binning (or Discretization):</strong> Sometimes, the exact numerical value isn’t as important as the <em>range</em> it falls into. We can group continuous values into discrete bins. <ul> <li> <strong>Example:</strong> Instead of <code class="language-plaintext highlighter-rouge">Age</code> (25, 31, 47), create <code class="language-plaintext highlighter-rouge">Age Group</code> (Youth, Adult, Senior).</li> <li> <strong>Why:</strong> Can make models more robust to small fluctuations, can capture non-linear relationships, and sometimes make features more interpretable.</li> </ul> </li> <li> <strong>Polynomial Features:</strong> Sometimes, the relationship between a feature and the target isn’t linear. We can create higher-order terms. <ul> <li> <strong>Example:</strong> If <code class="language-plaintext highlighter-rouge">Area</code> predicts <code class="language-plaintext highlighter-rouge">Price</code>, perhaps $Area^2$ or $Area^3$ are also important. We’d create new features like <code class="language-plaintext highlighter-rouge">Area_squared</code> ($Area^2$) and <code class="language-plaintext highlighter-rouge">Area_cubed</code> ($Area^3$).</li> <li> <strong>Mathematical Representation:</strong> For a feature $x$, we can generate $x^2, x^3, \dots, x^n$.</li> <li> <strong>Why:</strong> Captures non-linear patterns, allowing linear models to fit curved relationships.</li> </ul> </li> <li> <strong>Interaction Features:</strong> The effect of one feature might depend on another. We can multiply or combine features. <ul> <li> <strong>Example:</strong> For <code class="language-plaintext highlighter-rouge">Price</code>, maybe <code class="language-plaintext highlighter-rouge">Rooms_per_sqft</code> (e.g., <code class="language-plaintext highlighter-rouge">Number of Rooms</code> / <code class="language-plaintext highlighter-rouge">Square Footage</code>) is more indicative than either alone. Or, <code class="language-plaintext highlighter-rouge">Age</code> * <code class="language-plaintext highlighter-rouge">Income</code> could represent wealth accumulation.</li> <li> <strong>Mathematical Representation:</strong> For features $x_1$ and $x_2$, an interaction term could be $x_1 \times x_2$.</li> <li> <strong>Why:</strong> Captures synergistic effects where the combination of features is more powerful than their individual contributions.</li> </ul> </li> <li> <strong>Transformations (Log, Square Root, etc.):</strong> Data can be skewed (e.g., income, website visits often have a long tail). Transformations can make distributions more symmetrical, which can help models that assume normal distributions. <ul> <li> <strong>Example:</strong> Taking the logarithm of <code class="language-plaintext highlighter-rouge">Income</code>: $\log(Income)$. We often use $\log(x+1)$ to handle zero values gracefully.</li> <li> <strong>Why:</strong> Reduces the impact of outliers, helps linear models perform better with skewed data, and stabilizes variance.</li> </ul> </li> </ul> <h4 id="2-categorical-features">2. Categorical Features</h4> <p>These represent categories or labels: colors, cities, product types.</p> <ul> <li> <strong>One-Hot Encoding:</strong> The most common way to handle nominal (unordered) categorical data. It converts each category value into a new binary (0 or 1) column. <ul> <li> <strong>Example:</strong> A <code class="language-plaintext highlighter-rouge">Color</code> feature with values <code class="language-plaintext highlighter-rouge">Red</code>, <code class="language-plaintext highlighter-rouge">Blue</code>, <code class="language-plaintext highlighter-rouge">Green</code> becomes three new features: <code class="language-plaintext highlighter-rouge">is_Red</code>, <code class="language-plaintext highlighter-rouge">is_Blue</code>, <code class="language-plaintext highlighter-rouge">is_Green</code>. If <code class="language-plaintext highlighter-rouge">Color</code> is <code class="language-plaintext highlighter-rouge">Red</code>, then <code class="language-plaintext highlighter-rouge">is_Red</code> is 1, and others are 0.</li> <li> <strong>Why:</strong> Models interpret numbers, not text. This prevents the model from assuming an arbitrary ordinal relationship (e.g., that <code class="language-plaintext highlighter-rouge">Blue</code> is “greater” than <code class="language-plaintext highlighter-rouge">Red</code> if you just assign 0, 1, 2).</li> </ul> </li> <li> <strong>Label Encoding (or Ordinal Encoding):</strong> Assigns a unique integer to each category. <ul> <li> <strong>Example:</strong> <code class="language-plaintext highlighter-rouge">Size</code> feature with <code class="language-plaintext highlighter-rouge">Small</code>, <code class="language-plaintext highlighter-rouge">Medium</code>, <code class="language-plaintext highlighter-rouge">Large</code> could become 0, 1, 2.</li> <li> <strong>Why:</strong> Useful when there’s an inherent order (ordinality) in the categories. Use with caution for nominal data, as the model might incorrectly infer order.</li> </ul> </li> <li> <strong>Frequency Encoding / Count Encoding:</strong> Replaces each category with the count or frequency of its occurrence in the dataset. <ul> <li> <strong>Example:</strong> If <code class="language-plaintext highlighter-rouge">City</code> “New York” appears 100 times, replace “New York” with 100.</li> <li> <strong>Why:</strong> Can capture the importance of a category based on its prevalence. Often works well for high-cardinality (many unique values) categorical features.</li> </ul> </li> </ul> <h4 id="3-date-and-time-features">3. Date and Time Features</h4> <p>Dates and times are rich sources of information, but models can’t understand them directly.</p> <ul> <li> <strong>Extracting Components:</strong> Break down a date-time stamp into its constituent parts. <ul> <li> <strong>Example:</strong> From <code class="language-plaintext highlighter-rouge">2023-10-26 14:30:00</code>, extract <code class="language-plaintext highlighter-rouge">Year</code> (2023), <code class="language-plaintext highlighter-rouge">Month</code> (10), <code class="language-plaintext highlighter-rouge">Day</code> (26), <code class="language-plaintext highlighter-rouge">Day of Week</code> (4 for Thursday), <code class="language-plaintext highlighter-rouge">Hour</code> (14), <code class="language-plaintext highlighter-rouge">Minute</code> (30), <code class="language-plaintext highlighter-rouge">Is_Weekend</code> (False).</li> <li> <strong>Why:</strong> Seasonality, daily patterns, or specific event dates are often highly predictive.</li> </ul> </li> <li> <strong>Time Since Event / Time Until Event:</strong> Calculate durations relevant to your problem. <ul> <li> <strong>Example:</strong> <code class="language-plaintext highlighter-rouge">Days since last login</code>, <code class="language-plaintext highlighter-rouge">Time until next renewal</code>, <code class="language-plaintext highlighter-rouge">Elapsed time since registration</code>.</li> <li> <strong>Why:</strong> Captures recency, dormancy, or upcoming deadlines which are powerful behavioral indicators.</li> </ul> </li> <li> <strong>Cyclical Features:</strong> For features like <code class="language-plaintext highlighter-rouge">month</code>, <code class="language-plaintext highlighter-rouge">day of week</code>, or <code class="language-plaintext highlighter-rouge">hour</code>, the “end” wraps around to the “beginning” (December follows November, but January follows December). Simple integer encoding creates an artificial jump. We can use sine and cosine transformations to represent these cyclical relationships. <ul> <li> <strong>Example:</strong> For <code class="language-plaintext highlighter-rouge">Month</code> (1-12): $Month_{sin} = \sin(\frac{2\pi \cdot Month}{12})$ $Month_{cos} = \cos(\frac{2\pi \cdot Month}{12})$</li> <li> <strong>Why:</strong> Represents cyclical nature without creating artificial boundaries, ensuring the model understands that month 12 is “close” to month 1.</li> </ul> </li> </ul> <h4 id="4-text-features-a-quick-glimpse">4. Text Features (A Quick Glimpse)</h4> <p>Text data is its own beast, but Feature Engineering is paramount here too.</p> <ul> <li> <strong>Bag-of-Words (BoW):</strong> Represents text as a bag (multiset) of its words, disregarding grammar and word order, but keeping multiplicity. Each unique word becomes a feature, and its value is its frequency.</li> <li> <strong>TF-IDF (Term Frequency-Inverse Document Frequency):</strong> Weights words by how often they appear in a document (TF) and how unique they are across all documents (IDF). This highlights important, distinguishing words.</li> <li> <strong>Word Embeddings (Advanced):</strong> These convert words into dense numerical vectors, capturing semantic relationships between words. Words with similar meanings have similar vectors. (e.g., Word2Vec, GloVe).</li> </ul> <h3 id="the-art-and-science-an-iterative-process">The Art and Science: An Iterative Process</h3> <p>Feature Engineering isn’t a one-and-done task. It’s an <strong>iterative process</strong> that blends creativity (the “art”) with rigorous experimentation (the “science”):</p> <ol> <li> <strong>Brainstorm:</strong> Based on your domain knowledge, what new features <em>could</em> be useful? What relationships might be hidden?</li> <li> <strong>Create:</strong> Use tools like Pandas and Scikit-learn to implement these features.</li> <li> <strong>Evaluate:</strong> Train your model with the new features and see if performance improves. Compare different feature sets.</li> <li> <strong>Refine:</strong> If a feature helps, try to refine it further. If it doesn’t, discard it or try a different approach.</li> </ol> <p>This cycle continues until you’re satisfied with your model’s performance. It often goes hand-in-hand with <strong>Feature Selection</strong>, where you identify and remove redundant or irrelevant features to simplify your model and improve generalization.</p> <h3 id="your-toolkit">Your Toolkit</h3> <p>You don’t need exotic tools to be a feature engineer. Your best friends will be:</p> <ul> <li> <strong>Pandas:</strong> For powerful data manipulation, aggregation, and transformation.</li> <li> <strong>Numpy:</strong> For numerical operations, especially when working with arrays.</li> <li> <strong>Scikit-learn’s <code class="language-plaintext highlighter-rouge">preprocessing</code> module:</strong> Contains handy functions for one-hot encoding, polynomial features, scaling, and more.</li> </ul> <h3 id="conclusion-embrace-the-creativity">Conclusion: Embrace the Creativity!</h3> <p>Feature Engineering is where the real magic happens in many data science projects. It’s not just a technical step; it’s a creative endeavor. It forces you to think deeply about your data, to understand the problem you’re trying to solve, and to come up with clever ways to present that information to your model.</p> <p>Don’t be afraid to get your hands dirty! Experiment, hypothesize, and create. You’ll find that by transforming your raw data into insightful, well-crafted features, you’re not just improving your models; you’re developing a deeper understanding of the world your data represents. And that, my friends, is truly empowering.</p> <p>So next time you’re faced with a dataset, before you even think about the fanciest algorithms, take a moment. Ask yourself: “What story is this data trying to tell? And how can I help my model understand it better?” The answer often lies in the art of Feature Engineering. Happy creating!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>