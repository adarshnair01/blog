<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Demystifying the Giants: A Journey into the World of Large Language Models | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/demystifying-the-giants-a-journey-into-the-world-o/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Demystifying the Giants: A Journey into the World of Large Language Models</h1> <p class="post-meta"> Created on May 16, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> NLP</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/transformers"> <i class="fa-solid fa-hashtag fa-sm"></i> Transformers</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>As a budding data scientist and machine learning engineer, few areas in artificial intelligence have captivated my imagination quite like Large Language Models (LLMs). From the moment I first interacted with a model that could generate coherent, contextually relevant text, I felt like I was peeking into a future where human-computer interaction takes on a whole new dimension. It felt less like a tool and more like a nascent digital collaborator.</p> <p>But what <em>are</em> these LLMs that seem to be popping up everywhere, effortlessly writing essays, generating code, and answering complex questions? Are they truly “thinking,” or is there a clever statistical trick at play? In this post, I want to take you on a journey through the core concepts, the ingenious architecture, and the sheer scale that underpins these phenomenal models. My goal isn’t just to inform, but to spark that same sense of wonder and understanding I felt when I first delved into their inner workings.</p> <h3 id="what-exactly-is-a-large-language-model">What Exactly is a Large Language Model?</h3> <p>At its heart, a Large Language Model is a sophisticated statistical tool designed to understand, generate, and manipulate human language. Think of it as an incredibly advanced autocomplete system. Given a sequence of words, its primary task is to predict the <em>next most probable word</em> (or “token,” which could be a word, part of a word, or even a punctuation mark).</p> <p>The “Large” in LLM refers to several things:</p> <ol> <li> <strong>Parameters:</strong> These models boast an astonishing number of adjustable parameters – often in the billions, sometimes even trillions. These parameters are the weights and biases that the model learns during its training process, determining how it processes information.</li> <li> <strong>Training Data:</strong> LLMs are trained on truly colossal datasets, often comprising vast swathes of the internet – books, articles, websites, code repositories, and more. This exposure to diverse text allows them to grasp grammar, facts, reasoning patterns, and even stylistic nuances across countless topics.</li> <li> <strong>Computational Power:</strong> Training these behemoths requires immense computational resources, typically massive clusters of GPUs running for weeks or months.</li> </ol> <p>So, when an LLM writes a poem, it’s not “creative” in the human sense. It’s masterfully stitching together patterns, styles, and vocabulary it learned from countless poems in its training data, predicting the most plausible next word that fits the context and desired style. It’s an act of statistical generation, but one so complex and nuanced that it <em>appears</em> intelligent.</p> <h3 id="the-brain-behind-the-magic-the-transformer-architecture">The Brain Behind the Magic: The Transformer Architecture</h3> <p>For decades, sequential data like language was primarily handled by Recurrent Neural Networks (RNNs) and their variants (LSTMs, GRUs). While groundbreaking, they struggled with very long sequences, often forgetting information from earlier parts of a sentence. Then, in 2017, a paper titled “Attention Is All You Need” introduced the Transformer architecture, and it changed everything.</p> <p>The Transformer is the foundational architecture for almost all modern LLMs. Its brilliance lies in a mechanism called <strong>self-attention</strong>, which allows the model to weigh the importance of different words in an input sequence when processing each word.</p> <p>Let’s break down the key ideas:</p> <h4 id="1-self-attention-focusing-on-what-matters">1. Self-Attention: Focusing on What Matters</h4> <p>Imagine you’re reading a sentence like, “The animal didn’t cross the street because <strong>it</strong> was too tired.” To understand what “it” refers to, you intuitively know to look back at “animal.” Self-attention mimics this. For each word the model processes, it looks at <em>all</em> other words in the input sequence and calculates how relevant they are to the current word.</p> <p>How does it do this? Through a clever use of three vectors for each word:</p> <ul> <li> <strong>Query (Q):</strong> What am I looking for? (The current word’s “question”).</li> <li> <strong>Key (K):</strong> What do I have? (The current word’s “label” or “index”).</li> <li> <strong>Value (V):</strong> What information do I carry? (The current word’s actual content).</li> </ul> <p>The attention mechanism essentially computes a “score” between the Query of the current word and the Key of every other word. These scores are then normalized (using a softmax function) to get attention weights, indicating how much “attention” each word should pay to others. Finally, these weights are multiplied by the Value vectors, summing up a context-aware representation for the current word.</p> <p>Mathematically, for a single attention head, this process can be elegantly summarized as:</p> \[Attention(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\] <p>Here:</p> <ul> <li>$Q$, $K$, $V$ are matrices stacked with query, key, and value vectors for all words in the sequence.</li> <li>$K^T$ is the transpose of the key matrix.</li> <li>$\frac{1}{\sqrt{d_k}}$ is a scaling factor (where $d_k$ is the dimension of the key vectors) to prevent very large dot products from pushing the softmax into regions with extremely small gradients.</li> <li>$\text{softmax}$ normalizes the scores so they sum to 1, effectively turning them into probability distributions (attention weights).</li> </ul> <p>The result is a new representation for each word that incorporates information from all other words, weighted by their relevance. It’s like having a dedicated editor for each word, constantly cross-referencing it with the entire document.</p> <h4 id="2-multi-head-attention-diverse-perspectives">2. Multi-Head Attention: Diverse Perspectives</h4> <p>Instead of just one set of $Q, K, V$ vectors, Transformers use <em>multiple</em> “attention heads” in parallel. Each head learns to focus on different aspects of relationships between words. One head might focus on grammatical dependencies (e.g., subject-verb agreement), while another might focus on semantic relationships (e.g., synonyms or related concepts). The outputs from these heads are then concatenated and linearly transformed, providing a richer, multi-faceted understanding of the input.</p> <h4 id="3-positional-encoding-preserving-order">3. Positional Encoding: Preserving Order</h4> <p>One interesting property of self-attention is that it’s “permutation-invariant.” This means it treats the words in a sentence as a “bag of words” without inherently knowing their order. But word order is crucial for language! “Dog bites man” is different from “Man bites dog.”</p> <p>To solve this, Transformers inject <strong>positional encodings</strong> into the input embeddings. These are special vectors added to the word embeddings that provide information about the absolute or relative position of each token in the sequence. This way, the model knows where each word stands in the sentence without losing the parallel processing benefits of attention.</p> <h4 id="4-feed-forward-networks-and-residual-connections">4. Feed-Forward Networks and Residual Connections</h4> <p>Beyond attention, each Transformer block also includes a simple feed-forward neural network for further processing the attention output, and crucial <strong>residual connections</strong> (also known as skip connections) that help with training very deep networks by allowing gradients to flow more easily.</p> <h3 id="training-llms-a-herculean-feat">Training LLMs: A Herculean Feat</h3> <p>Building an LLM isn’t just about the architecture; it’s about the monumental effort required for training.</p> <ol> <li> <p><strong>Pre-training:</strong> This is the most computationally intensive phase. The model is fed vast amounts of text data (trillions of tokens!) and trained on a simple objective: predicting the next token. It learns to minimize a loss function, typically cross-entropy loss, which penalizes incorrect predictions of the next word. This unsupervised learning phase allows the model to absorb grammar, syntax, factual knowledge, and common-sense reasoning directly from the raw text.</p> </li> <li> <p><strong>Fine-tuning (and Instruction Tuning / RLHF):</strong> After pre-training, the general-purpose model is often specialized.</p> <ul> <li> <strong>Instruction Tuning:</strong> The model is further trained on datasets of instructions and desired responses (e.g., “Summarize this article:”, “Write a poem about:”). This teaches the model to follow instructions and generate helpful outputs.</li> <li> <strong>Reinforcement Learning from Human Feedback (RLHF):</strong> This is a critical step for models like ChatGPT. Human annotators rank or score different model outputs based on helpfulness, harmlessness, and honesty. This human feedback is then used to train a “reward model,” which in turn is used to further fine-tune the LLM, making it better aligned with human preferences. This process is what often gives LLMs their conversational, safety-aware qualities.</li> </ul> </li> </ol> <h3 id="emergent-abilities-and-the-scaling-laws">Emergent Abilities and the Scaling Laws</h3> <p>One of the most fascinating discoveries in the LLM landscape is the concept of “emergent abilities.” As models scale up in size (parameters), data, and compute, they don’t just get incrementally better; they sometimes develop entirely new capabilities that weren’t present in smaller models and weren’t explicitly programmed.</p> <p>These emergent abilities include:</p> <ul> <li> <strong>In-context learning:</strong> The ability to learn from examples provided directly in the prompt, without explicit fine-tuning.</li> <li> <strong>Chain-of-thought reasoning:</strong> When prompted to “think step by step,” larger models can often break down complex problems and show their intermediate reasoning, leading to more accurate answers.</li> <li><strong>Mathematical problem-solving and coding.</strong></li> </ul> <p>This phenomenon has given rise to the idea of <strong>scaling laws</strong>, which suggest that the performance of LLMs often improves predictably with increased training data, model parameters, and computational budget. It implies that simply making models bigger, within certain architectural constraints, unlocks new levels of capability.</p> <h3 id="beyond-the-hype-limitations-and-ethical-considerations">Beyond the Hype: Limitations and Ethical Considerations</h3> <p>While LLMs are incredibly powerful, it’s crucial to understand their limitations and the ethical considerations they bring:</p> <ul> <li> <strong>Hallucinations:</strong> LLMs can confidently generate factually incorrect information or make things up. They are designed to generate <em>plausible</em> text, not necessarily <em>truthful</em> text.</li> <li> <strong>Bias:</strong> Because they are trained on vast amounts of internet data, LLMs inevitably absorb and can propagate biases present in that data – whether it’s gender bias, racial bias, or stereotypes.</li> <li> <strong>Lack of True Understanding:</strong> Despite their impressive conversational abilities, LLMs do not possess consciousness, true reasoning, or understanding in the human sense. They are highly sophisticated pattern matchers.</li> <li> <strong>Environmental Impact:</strong> The sheer energy required to train and run these models is substantial, raising concerns about their carbon footprint.</li> <li> <strong>Ethical Dilemmas:</strong> Concerns around misinformation, misuse (e.g., deepfakes, automated spam), job displacement, and the concentration of AI power are real and require careful consideration.</li> </ul> <h3 id="the-road-ahead-my-perspective">The Road Ahead: My Perspective</h3> <p>Delving into Large Language Models has been an incredible journey. From the mathematical elegance of the attention mechanism to the mind-boggling scale of their training, it’s a field brimming with innovation. As a data scientist and MLE, I’m particularly excited by the potential for these models to augment human capabilities – assisting in research, automating tedious tasks, and unlocking new forms of creativity.</p> <p>The development of LLMs is still in its early stages, and the challenges are as significant as the opportunities. My personal conviction is that the future of LLMs lies not just in building bigger, more capable models, but in developing them responsibly, with a strong focus on transparency, fairness, and safety. Understanding <em>how</em> they work is the first step towards shaping that future ethically and effectively.</p> <p>I hope this deep dive has shed some light on the fascinating world of Large Language Models and inspired you to explore this rapidly evolving frontier further. The possibilities are truly immense, and I’m thrilled to be a part of the generation that gets to help shape them.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>