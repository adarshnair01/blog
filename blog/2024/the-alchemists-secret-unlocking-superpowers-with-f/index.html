<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Alchemist's Secret: Unlocking Superpowers with Feature Engineering | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/the-alchemists-secret-unlocking-superpowers-with-f/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Alchemist's Secret: Unlocking Superpowers with Feature Engineering</h1> <p class="post-meta"> Created on December 23, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/feature-engineering"> <i class="fa-solid fa-hashtag fa-sm"></i> Feature Engineering</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   <a href="/blog/blog/tag/data-preprocessing"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Preprocessing</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey there, fellow data adventurer!</p> <p>If you’ve spent any time diving into the world of machine learning, you’ve probably heard about fancy algorithms like Random Forests, Gradient Boosters, or deep neural networks. We often focus on picking the “best” model, tuning its hyperparameters until our fingers hurt, and then celebrating a tiny bump in accuracy. But what if I told you there’s an often-overlooked, yet incredibly powerful, stage in the machine learning pipeline that can deliver leaps in performance, sometimes far exceeding what a new algorithm or hyperparameter tweak ever could?</p> <p>Welcome to the captivating realm of <strong>Feature Engineering</strong>.</p> <p>Think of yourself as a master chef. You have all sorts of raw ingredients: carrots, potatoes, spices, meat. You could just throw them all in a pot and hope for the best, but a true chef doesn’t do that. They peel the carrots, dice the potatoes, mince the garlic, marinate the meat, and combine flavors in a way that elevates each component.</p> <p>In data science, your “raw ingredients” are the columns in your dataset. And <em>Feature Engineering</em> is our way of being that master chef. It’s the art and science of transforming raw data into meaningful inputs (features) that machine learning models can understand and learn from more effectively. It’s about taking the mundane and making it magnificent, giving our models the “superpowers” they need to solve complex problems.</p> <h3 id="why-bother-the-garbage-in-garbage-out-truth">Why Bother? The “Garbage In, Garbage Out” Truth</h3> <p>You might ask, “Why can’t the model just figure it out from the raw data?” That’s a great question! And sometimes, for very simple problems or incredibly powerful models (like deep learning with massive datasets), it <em>can</em>. But for the vast majority of real-world scenarios, raw data is like unpolished gems: beautiful underneath, but needing a bit of work to truly shine.</p> <p>Here’s why Feature Engineering is so crucial:</p> <ol> <li> <strong>Models Speak Numbers:</strong> Machine learning models, at their core, are mathematical equations. They understand numbers. Text, dates, categories – these all need to be converted into a numerical format for a model to even process them.</li> <li> <strong>Uncovering Hidden Patterns:</strong> Raw features often don’t explicitly capture the relationships that are most predictive of your target variable. For example, knowing someone’s height and weight separately might be useful, but knowing their <em>Body Mass Index (BMI)</em> (weight / height$^2$) is often a far more powerful predictor for health outcomes. This is a created feature that combines existing ones in a meaningful way.</li> <li> <strong>Reducing Noise, Amplifying Signal:</strong> Raw data can be noisy, redundant, or irrelevant. Feature engineering helps us distill the most important information, making it easier for the model to learn the true underlying patterns rather than getting distracted by noise.</li> <li> <strong>Meeting Model Assumptions:</strong> Many statistical models (like linear regression) make assumptions about the data’s distribution (e.g., normality, linearity). Transforming features can help meet these assumptions, leading to more robust and accurate models.</li> <li> <strong>Beating the “Curse of Dimensionality”:</strong> While creating features, we also need to be mindful of not creating <em>too many</em> irrelevant features, which can confuse models and increase computational cost. Feature engineering isn’t just about <em>creating</em> features, but <em>optimizing</em> them.</li> </ol> <p>In essence, Feature Engineering is about translating human domain knowledge and intuition into a language that algorithms can understand and leverage. It’s the bridge between raw data and model intelligence.</p> <h3 id="the-toolbox-common-feature-engineering-techniques">The Toolbox: Common Feature Engineering Techniques</h3> <p>Let’s peek inside our chef’s toolbox and explore some common techniques to transform different types of data.</p> <h4 id="1-numerical-features-shaping-the-numbers">1. Numerical Features: Shaping the Numbers</h4> <p>Numerical data is often the easiest to work with, but even here, we can find opportunities for improvement.</p> <ul> <li> <strong>Binning (Discretization):</strong> Sometimes, continuous numerical data is more useful when grouped into discrete categories or “bins.” <ul> <li> <em>Example:</em> Instead of <code class="language-plaintext highlighter-rouge">age</code> (e.g., 23, 35, 48), we might create <code class="language-plaintext highlighter-rouge">age_group</code> (e.g., ‘Child’, ‘Young Adult’, ‘Middle-Aged’, ‘Senior’). This can help capture non-linear relationships or reduce sensitivity to small variations.</li> </ul> </li> <li> <strong>Transformations:</strong> Many models perform better when features follow a more normal (bell-curve-like) distribution. Skewed data (where values are concentrated on one side) can be problematic. <ul> <li> <em>Log Transformation:</em> Very common for positively skewed data (e.g., income, house prices). Applying a logarithm, like $\log(x)$, can compress large values and expand small ones, making the distribution more symmetrical.</li> <li> <em>Square Root Transformation:</em> Similar to log, $\sqrt{x}$ can also help reduce skewness.</li> <li> <em>Reciprocal Transformation:</em> $1/x$ can be useful for data where smaller values indicate higher importance.</li> </ul> </li> <li> <strong>Interaction Features:</strong> This is where we combine two or more existing features to create a new one that represents their relationship. <ul> <li> <em>Example:</em> If you’re predicting house prices, <code class="language-plaintext highlighter-rouge">price_per_sq_ft</code> (calculated as <code class="language-plaintext highlighter-rouge">price / square_footage</code>) might be a much stronger predictor than price and square footage alone.</li> <li> <em>Example:</em> In an e-commerce context, <code class="language-plaintext highlighter-rouge">click_rate</code> (<code class="language-plaintext highlighter-rouge">clicks / impressions</code>) or <code class="language-plaintext highlighter-rouge">total_spend</code> (<code class="language-plaintext highlighter-rouge">quantity * price</code>) can be powerful.</li> <li> <em>Polynomial Features:</em> Sometimes, the relationship isn’t linear. We can create polynomial features like $x^2$, $x^3$, etc., to capture these non-linear patterns. For example, if predicting a car’s stopping distance, both speed and speed squared might be important.</li> </ul> </li> </ul> <h4 id="2-categorical-features-giving-labels-a-voice">2. Categorical Features: Giving Labels a Voice</h4> <p>Categorical data represents types or groups (e.g., ‘Red’, ‘Blue’, ‘Green’; ‘Male’, ‘Female’). Models can’t directly understand these labels, so we need to convert them to numbers.</p> <ul> <li> <strong>One-Hot Encoding:</strong> This is one of the most common methods for <em>nominal</em> (unordered) categorical data. For each unique category, we create a new binary (0 or 1) column. <ul> <li> <em>Example:</em> If we have a <code class="language-plaintext highlighter-rouge">color</code> feature with values ‘Red’, ‘Green’, ‘Blue’: <ul> <li> <code class="language-plaintext highlighter-rouge">color_Red</code>: 1 if Red, 0 otherwise</li> <li> <code class="language-plaintext highlighter-rouge">color_Green</code>: 1 if Green, 0 otherwise</li> <li> <code class="language-plaintext highlighter-rouge">color_Blue</code>: 1 if Blue, 0 otherwise</li> </ul> </li> <li> <em>Why:</em> It prevents the model from assuming an artificial ordinal relationship between categories (e.g., thinking ‘Red’ is “greater” than ‘Blue’).</li> </ul> </li> <li> <strong>Label Encoding (Ordinal Encoding):</strong> For <em>ordinal</em> (ordered) categorical data, we can assign an integer to each category based on its rank. <ul> <li> <em>Example:</em> A <code class="language-plaintext highlighter-rouge">t-shirt_size</code> feature with ‘Small’, ‘Medium’, ‘Large’: <ul> <li>‘Small’ = 0</li> <li>‘Medium’ = 1</li> <li>‘Large’ = 2</li> </ul> </li> <li> <em>Caution:</em> Use this only when there’s a clear, inherent order. Applying it to nominal data can mislead models into thinking there’s a numerical hierarchy that doesn’t exist (e.g., ‘Red’ = 0, ‘Green’ = 1, ‘Blue’ = 2 implies Blue &gt; Green &gt; Red, which is meaningless).</li> </ul> </li> <li> <strong>Target Encoding (Mean Encoding):</strong> This advanced technique replaces a category with the mean of the target variable for that category. <ul> <li> <em>Example:</em> If predicting housing price, ‘Suburb_A’ might be replaced by the average house price in Suburb A.</li> <li> <em>Pros:</em> Can capture complex relationships and reduce dimensionality compared to one-hot encoding for high-cardinality features (many unique categories).</li> <li> <em>Cons:</em> Prone to data leakage if not handled carefully (e.g., using the target mean calculated from the entire dataset, including the validation/test set). Requires careful validation.</li> </ul> </li> </ul> <h4 id="3-date-and-time-features-unpacking-temporal-insights">3. Date and Time Features: Unpacking Temporal Insights</h4> <p>Dates and times are a goldmine for features, often containing rich cyclical and sequential patterns.</p> <ul> <li> <strong>Extracting Components:</strong> Break down a timestamp into its constituent parts: <ul> <li> <code class="language-plaintext highlighter-rouge">year</code>, <code class="language-plaintext highlighter-rouge">month</code>, <code class="language-plaintext highlighter-rouge">day</code>, <code class="language-plaintext highlighter-rouge">day_of_week</code>, <code class="language-plaintext highlighter-rouge">hour</code>, <code class="language-plaintext highlighter-rouge">minute</code>, <code class="language-plaintext highlighter-rouge">second</code>.</li> <li> <code class="language-plaintext highlighter-rouge">is_weekend</code> (boolean).</li> <li> <code class="language-plaintext highlighter-rouge">quarter</code>, <code class="language-plaintext highlighter-rouge">week_of_year</code>.</li> </ul> </li> <li> <strong>Cyclical Features:</strong> For features like <code class="language-plaintext highlighter-rouge">month</code> (1-12) or <code class="language-plaintext highlighter-rouge">day_of_week</code> (0-6), simply encoding them as numbers can mislead the model into thinking that 12 is “far” from 1, when in reality, December is followed by January. We can use sine and cosine transformations to represent these cyclical patterns: <ul> <li>For a feature $F$ with a maximum value $M$ (e.g., $M=12$ for months, $M=24$ for hours): <ul> <li>$\text{sin}(2 \pi F / M)$</li> <li>$\text{cos}(2 \pi F / M)$</li> </ul> </li> <li>This maps the values onto a circle, so January (1) and December (12) are close in the cyclical space.</li> </ul> </li> <li> <strong>Time Differences:</strong> Calculate durations or time since an event. <ul> <li> <em>Example:</em> <code class="language-plaintext highlighter-rouge">time_since_last_purchase</code>, <code class="language-plaintext highlighter-rouge">age_of_account_in_days</code>.</li> </ul> </li> </ul> <h4 id="4-text-features-making-sense-of-words">4. Text Features: Making Sense of Words</h4> <p>When dealing with text, we need to convert words into numerical representations.</p> <ul> <li> <strong>Bag-of-Words (BoW):</strong> A simple but effective method where each document or piece of text is represented as a “bag” of its words, disregarding grammar and word order. We count the frequency of each word. <ul> <li> <em>Example:</em> “The cat sat on the mat” -&gt; {‘the’: 2, ‘cat’: 1, ‘sat’: 1, ‘on’: 1, ‘mat’: 1}</li> </ul> </li> <li> <strong>TF-IDF (Term Frequency-Inverse Document Frequency):</strong> A more advanced method that weights words based on how frequently they appear in a document (Term Frequency) and how rare they are across all documents (Inverse Document Frequency). This gives more importance to unique, meaningful words.</li> <li> <strong>Text Length/Word Count:</strong> Simple features like the number of characters, words, or sentences can sometimes be surprisingly predictive.</li> </ul> <h4 id="5-domain-specific-features-the-art-of-knowing-your-data">5. Domain-Specific Features: The Art of Knowing Your Data</h4> <p>This is where true data science creativity shines. Domain-specific features come from deep knowledge of the problem area.</p> <ul> <li> <em>Example (Retail):</em> If predicting sales, knowing about <code class="language-plaintext highlighter-rouge">promotional_events</code>, <code class="language-plaintext highlighter-rouge">holidays</code>, or <code class="language-plaintext highlighter-rouge">competitor_actions</code> can be incredibly valuable.</li> <li> <em>Example (Finance):</em> For fraud detection, <code class="language-plaintext highlighter-rouge">transaction_amount_to_avg_amount_ratio</code> or <code class="language-plaintext highlighter-rouge">time_since_last_transaction</code> are common.</li> <li> <em>Example (Healthcare):</em> Combining <code class="language-plaintext highlighter-rouge">height</code> and <code class="language-plaintext highlighter-rouge">weight</code> to get <code class="language-plaintext highlighter-rouge">BMI</code>.</li> </ul> <p>These features aren’t generic; they’re tailored to the specific problem and often require collaboration with domain experts.</p> <h3 id="the-workflow-an-iterative-dance">The Workflow: An Iterative Dance</h3> <p>Feature Engineering isn’t a one-and-done step. It’s an iterative process, much like a detective slowly piecing together clues:</p> <ol> <li> <strong>Exploratory Data Analysis (EDA):</strong> Start by deeply understanding your data. Visualize distributions, identify relationships, spot outliers, and discover missing values. This is where you generate hypotheses for new features. “Aha!” moments often happen here.</li> <li> <strong>Brainstorm &amp; Create:</strong> Based on your EDA and domain knowledge, brainstorm potential new features. Write down your ideas and then implement them.</li> <li> <strong>Model &amp; Evaluate:</strong> Integrate your new features into your model, train it, and evaluate its performance. Did it improve? Did it get worse?</li> <li> <strong>Feature Selection:</strong> Not all created features are good features. Some might be redundant, noisy, or cause overfitting. Techniques like correlation analysis, mutual information, or model-based selection (e.g., using <code class="language-plaintext highlighter-rouge">feature_importances_</code> from tree models) help you pick the best ones.</li> <li> <strong>Iterate:</strong> Go back to step 1! Refine existing features, create new ones, discard ineffective ones. This cycle continues until you’re satisfied with your model’s performance.</li> </ol> <h3 id="tools-of-the-trade">Tools of the Trade</h3> <p>Thankfully, we don’t have to build these transformations from scratch every time. Libraries like <strong>Pandas</strong> in Python are invaluable for data manipulation and creating new columns. <strong>Scikit-learn</strong> offers many pre-built transformers and encoders (e.g., <code class="language-plaintext highlighter-rouge">StandardScaler</code>, <code class="language-plaintext highlighter-rouge">MinMaxScaler</code>, <code class="language-plaintext highlighter-rouge">OneHotEncoder</code>, <code class="language-plaintext highlighter-rouge">LabelEncoder</code>, <code class="language-plaintext highlighter-rouge">PolynomialFeatures</code>) that make the process efficient.</p> <h3 id="the-human-touch-why-feature-engineering-matters">The Human Touch: Why Feature Engineering Matters</h3> <p>In an era increasingly dominated by “autoML” and powerful deep learning, it might seem like the human touch is becoming less important. However, for many practical problems, especially with smaller datasets or when interpretability is key, Feature Engineering remains a critical differentiator. It’s where your creativity, intuition, and understanding of the problem space truly shine.</p> <p>You’re not just feeding numbers to a machine; you’re teaching it how to see the world more clearly, giving it the context and insights it needs to make smarter decisions. So, next time you’re working on a machine learning project, don’t just jump to model selection. Take a moment, channel your inner alchemist, and see what magic you can brew with Feature Engineering. The results might just surprise you!</p> <p>Happy engineering!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>