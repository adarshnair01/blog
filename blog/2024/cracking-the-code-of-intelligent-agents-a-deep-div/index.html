<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Cracking the Code of Intelligent Agents: A Deep Dive into Q-Learning | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/cracking-the-code-of-intelligent-agents-a-deep-div/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Cracking the Code of Intelligent Agents: A Deep Dive into Q-Learning</h1> <p class="post-meta"> Created on December 24, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/reinforcement-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Reinforcement Learning</a>   <a href="/blog/blog/tag/q-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Q-Learning</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   <a href="/blog/blog/tag/python"> <i class="fa-solid fa-hashtag fa-sm"></i> Python</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey there, fellow explorers of the digital frontier! Today, I want to pull back the curtain on one of the most foundational and intuitively brilliant algorithms in the realm of Reinforcement Learning: <strong>Q-Learning</strong>. If you’ve ever imagined building an AI that learns from experience, navigating a maze, or mastering a game, Q-Learning is often where that journey begins. It’s a stepping stone to understanding far more complex systems, and honestly, it’s just plain cool.</p> <h3 id="whats-the-big-idea-with-reinforcement-learning">What’s the Big Idea with Reinforcement Learning?</h3> <p>Before we dive into the ‘Q’, let’s set the stage. Imagine you’re trying to teach a robot to make a perfect cup of coffee. You wouldn’t write down every single instruction for every possible scenario (what if the cup is missing? what if the milk is empty?). Instead, you’d give it some general guidelines and a way to know if it’s doing well or poorly.</p> <p>This is the essence of <strong>Reinforcement Learning (RL)</strong>. We have:</p> <ul> <li> <strong>An Agent:</strong> Our robot, our AI, our learning entity.</li> <li> <strong>An Environment:</strong> The coffee machine, the kitchen, the world it operates in.</li> <li> <strong>States ($s$):</strong> The current situation (e.g., “coffee machine off, cup ready”).</li> <li> <strong>Actions ($a$):</strong> What the agent can do (e.g., “turn on machine,” “add water”).</li> <li> <strong>Rewards ($r$):</strong> Feedback from the environment – positive for good actions (e.g., “coffee brewed!”), negative for bad ones (e.g., “spilled hot water!”).</li> </ul> <p>The agent’s goal? To learn a <strong>policy</strong> – a strategy that tells it which action to take in any given state – to maximize its total cumulative reward over time. It’s a journey of trial and error, much like how a child learns to ride a bike: falling down is a negative reward, staying upright is a positive one.</p> <h3 id="enter-q-learning-valuing-your-choices">Enter Q-Learning: Valuing Your Choices</h3> <p>So, how does an agent figure out the <em>best</em> action? This is where Q-Learning shines. Q-Learning is a <strong>value-based</strong>, <strong>model-free</strong> RL algorithm.</p> <ul> <li> <strong>Value-based:</strong> It focuses on learning the “value” or “quality” of taking a certain action in a certain state.</li> <li> <strong>Model-free:</strong> It doesn’t need to know the internal workings or dynamics of the environment (like predicting what state it will land in after an action). It learns purely from experience.</li> </ul> <p>Think of it like this: If you’re trying to navigate a new city, you start by exploring. You learn that taking the bus on Main Street is usually good (high value) because it gets you to your destination, but walking down a dark alley is usually bad (low value). Q-Learning builds up a “map” of these values.</p> <p>The “Q” in Q-Learning stands for “Quality.” We’re trying to learn a function, $Q(s, a)$, which tells us the <strong>maximum expected future reward</strong> we can get by taking action $a$ in state $s$, and then following an optimal policy thereafter.</p> <p>Imagine a giant spreadsheet, a <strong>Q-table</strong>, where rows are states and columns are actions. Each cell $(s, a)$ holds a numerical value – the Q-value – representing how “good” it is to take action $a$ from state $s$.</p> <table> <thead> <tr> <th style="text-align: left">State \ Action</th> <th style="text-align: left">Go Left</th> <th style="text-align: left">Go Right</th> <th style="text-align: left">Go Up</th> <th style="text-align: left">Go Down</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Start (0,0)</td> <td style="text-align: left">?</td> <td style="text-align: left">?</td> <td style="text-align: left">?</td> <td style="text-align: left">?</td> </tr> <tr> <td style="text-align: left">Corridor (0,1)</td> <td style="text-align: left">?</td> <td style="text-align: left">?</td> <td style="text-align: left">?</td> <td style="text-align: left">?</td> </tr> <tr> <td style="text-align: left">Goal (2,2)</td> <td style="text-align: left">?</td> <td style="text-align: left">?</td> <td style="text-align: left">?</td> <td style="text-align: left">?</td> </tr> <tr> <td style="text-align: left">…</td> <td style="text-align: left">…</td> <td style="text-align: left">…</td> <td style="text-align: left">…</td> <td style="text-align: left">…</td> </tr> </tbody> </table> <p>Initially, all these Q-values are unknown (maybe zero or random). Through interaction with the environment, the agent updates these values, gradually learning which paths lead to the biggest rewards.</p> <h3 id="the-heart-of-q-learning-the-update-rule">The Heart of Q-Learning: The Update Rule</h3> <p>This is where the magic happens! The Q-Learning algorithm iteratively updates the Q-values based on experience. Each time the agent takes an action, observes a reward, and lands in a new state, it refines its estimate of the Q-value for the state-action pair it just left.</p> <p>Here’s the iconic Q-Learning update rule, often referred to as the Bellman equation for optimal control:</p> \[Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s, a)]\] <p>Let’s break this down piece by piece – it’s less intimidating than it looks, I promise!</p> <ol> <li> <p><strong>$Q(s, a)$:</strong> This is our <em>current</em> estimate of the quality of taking action $a$ in state $s$. We’re going to update this value.</p> </li> <li> <strong>$\alpha$ (alpha) - The Learning Rate:</strong> <ul> <li>This is a hyperparameter, usually between 0 and 1.</li> <li>It determines <em>how much</em> we trust new information versus our old estimate.</li> <li>If $\alpha = 1$, the agent completely replaces its old estimate with the new experience.</li> <li>If $\alpha = 0$, the agent learns nothing.</li> <li>A common value is $0.1$ or $0.2$, meaning we slightly adjust our current estimate with new evidence.</li> </ul> </li> <li> <strong>$[r + \gamma \max_{a’} Q(s’, a’) - Q(s, a)]$ - The “Temporal Difference” Error:</strong> <ul> <li>This whole bracketed term is the <strong>temporal difference (TD) error</strong>. It represents the difference between what we <em>expected</em> to happen ($Q(s, a)$) and what actually <em>did</em> happen, combined with our best estimate of the future.</li> <li>If this error is positive, our action was better than expected. If negative, it was worse.</li> </ul> <p>Let’s dissect the components <em>inside</em> the error:</p> <ul> <li> <p><strong>$r$ - Immediate Reward:</strong> This is the immediate reward the agent received after taking action $a$ from state $s$ and landing in state $s’$. This is the tangible “pat on the back” or “slap on the wrist.”</p> </li> <li> <strong>$\gamma$ (gamma) - The Discount Factor:</strong> <ul> <li>Another hyperparameter, also between 0 and 1.</li> <li>It determines the importance of <em>future</em> rewards versus <em>immediate</em> rewards.</li> <li>If $\gamma = 0$, the agent is “myopic” – it only cares about the immediate reward.</li> <li>If $\gamma = 1$, the agent is “far-sighted” – it values future rewards just as much as immediate ones (this can sometimes lead to infinite value in environments without terminal states).</li> <li>Typical values are $0.9$ or $0.99$, meaning future rewards are important but slightly less valuable than immediate ones.</li> </ul> </li> <li> <strong>$\max_{a’} Q(s’, a’)$ - The Maximum Future Q-value:</strong> <ul> <li>This is the clever part! After landing in the <em>new state</em> $s’$, the agent looks ahead and imagines what the <em>best possible action</em> $a’$ would be from <em>that new state</em>, based on its <em>current knowledge</em> (i.e., its current Q-table).</li> <li>This term represents the optimal future reward the agent <em>anticipates</em> receiving.</li> </ul> </li> </ul> </li> </ol> <p>So, in plain English, the update rule says:</p> <p>“Update your current estimate of $Q(s, a)$ by adding a fraction ($\alpha$) of the difference between what you <em>just experienced</em> (immediate reward $r$ plus the best discounted future reward from the next state $\gamma \max_{a’} Q(s’, a’)$) and what you <em>thought you would get</em> ($Q(s, a)$).”</p> <p>This iterative process, repeated over countless interactions, gradually allows the Q-values to converge towards their true optimal values.</p> <h3 id="the-exploration-vs-exploitation-dilemma">The Exploration vs. Exploitation Dilemma</h3> <p>How does the agent choose an action $a$ from state $s$? This is critical. If it always picks the action with the highest current Q-value, it’s <strong>exploiting</strong> its current knowledge. This sounds good, but what if its initial estimates were wrong, or there’s a better path it hasn’t discovered yet? It might get stuck in a locally optimal, but globally suboptimal, solution.</p> <p>This is why we need <strong>exploration</strong> – trying out new, seemingly non-optimal actions to discover better rewards.</p> <p>The most common strategy to balance these two is the <strong>$\epsilon$-greedy policy</strong>:</p> <ul> <li>With a small probability $\epsilon$ (epsilon), the agent chooses a random action (exploration).</li> <li>With probability $1 - \epsilon$, the agent chooses the action with the highest Q-value for the current state (exploitation).</li> </ul> <p>Typically, $\epsilon$ starts high (e.g., $0.9$ or $1.0$) to encourage lots of exploration early on, and then slowly <em>decays</em> over time. As the agent learns more, $\epsilon$ becomes very small (e.g., $0.01$), making the agent mostly exploit its learned knowledge.</p> <h3 id="a-simple-walkthrough-the-grid-world-example">A Simple Walkthrough: The Grid World Example</h3> <p>Let’s quickly visualize this with a tiny grid world. Imagine a 3x3 grid, where (0,0) is Start, and (2,2) is a Goal with a reward of +10. All other actions give -1 reward.</p> <table> <thead> <tr> <th>S</th> <th> </th> <th> </th> </tr> </thead> <tbody> <tr> <td> </td> <td> </td> <td> </td> </tr> <tr> <td> </td> <td> </td> <td>G</td> </tr> </tbody> </table> <p>Initial Q-table (all zeros):</p> <table> <thead> <tr> <th style="text-align: left">State \ Action</th> <th style="text-align: left">Left</th> <th style="text-align: left">Right</th> <th style="text-align: left">Up</th> <th style="text-align: left">Down</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">(0,0)</td> <td style="text-align: left">0</td> <td style="text-align: left">0</td> <td style="text-align: left">0</td> <td style="text-align: left">0</td> </tr> <tr> <td style="text-align: left">(0,1)</td> <td style="text-align: left">0</td> <td style="text-align: left">0</td> <td style="text-align: left">0</td> <td style="text-align: left">0</td> </tr> <tr> <td style="text-align: left">…</td> <td style="text-align: left">…</td> <td style="text-align: left">…</td> <td style="text-align: left">…</td> <td style="text-align: left">…</td> </tr> </tbody> </table> <p>Let’s say our agent is at state $s = (2,1)$ (the cell directly left of the Goal). It takes action $a = \text{Right}$. It lands in state $s’ = (2,2)$ (the Goal!). It receives reward $r = +10$.</p> <p>Now, it applies the update rule for $Q((2,1), \text{Right})$:</p> <p>$Q((2,1), \text{Right}) \leftarrow Q((2,1), \text{Right}) + \alpha [r + \gamma \max_{a’} Q(s’, a’) - Q((2,1), \text{Right})]$</p> <p>Let $\alpha = 0.1$ and $\gamma = 0.9$. Current $Q((2,1), \text{Right}) = 0$. Immediate $r = +10$. From the Goal state $s’ = (2,2)$, there are no more actions, so $\max_{a’} Q((2,2), a’)$ is $0$.</p> <p>$Q((2,1), \text{Right}) \leftarrow 0 + 0.1 [10 + 0.9 * 0 - 0]$ $Q((2,1), \text{Right}) \leftarrow 0.1 [10]$ $Q((2,1), \text{Right}) \leftarrow 1$</p> <p>So, the Q-value for moving Right from (2,1) becomes 1. This is a positive update! The agent now “knows” this action is somewhat good.</p> <p>As the agent continues exploring, eventually it will reach the goal from (2,1) again, strengthening that Q-value. Then, it might take an action to get to (2,1) from (1,1). When it calculates the $Q((1,1), \text{Down})$ update, the $\max_{a’} Q((2,1), a’)$ term will now include that positive Q-value of 1, propagating the reward backwards through the state space! This is the core idea of <strong>temporal difference learning</strong> – learning from the difference between temporally successive predictions.</p> <h3 id="advantages-and-disadvantages-of-q-learning">Advantages and Disadvantages of Q-Learning</h3> <p>Like any tool, Q-Learning has its strengths and weaknesses:</p> <p><strong>Pros:</strong></p> <ul> <li> <strong>Model-Free:</strong> It doesn’t need prior knowledge of the environment’s dynamics, making it highly adaptable.</li> <li> <strong>Simple to Understand &amp; Implement:</strong> For discrete state and action spaces, it’s relatively straightforward.</li> <li> <strong>Guaranteed Convergence:</strong> Under certain conditions (e.g., all state-action pairs are visited infinitely often, appropriate learning rate decay), Q-values will converge to optimal values.</li> </ul> <p><strong>Cons:</strong></p> <ul> <li> <strong>Scalability Issues (Curse of Dimensionality):</strong> The Q-table can grow astronomically large for environments with many states or actions. Imagine a robotic arm with continuous joint angles – the “states” are infinite! This is why basic Q-Learning isn’t used for complex tasks like playing StarCraft.</li> <li> <strong>Slow Learning:</strong> Requires many, many interactions to explore and converge, especially in sparse reward environments (where rewards are rare).</li> </ul> <h3 id="beyond-basic-q-learning-the-path-to-deep-reinforcement-learning">Beyond Basic Q-Learning: The Path to Deep Reinforcement Learning</h3> <p>The scalability issue led to a revolution. What if, instead of a giant table, we could <em>approximate</em> the Q-function using a neural network? This is the core idea behind <strong>Deep Q-Networks (DQN)</strong>, a landmark innovation that combines Q-Learning with deep learning. DQNs allowed agents to play Atari games directly from raw pixel data, showing the incredible power of scaling up these foundational RL concepts.</p> <p>Q-Learning, in its pure tabular form, might not conquer the world, but it lays the essential groundwork for understanding more advanced algorithms. It teaches us the fundamental principles of value iteration, temporal difference learning, and the crucial balance between exploration and exploitation.</p> <h3 id="conclusion">Conclusion</h3> <p>So there you have it – a journey into the heart of Q-Learning! It’s a testament to how simple yet powerful concepts can lead to intelligent behavior. From navigating a tiny grid to inspiring the deep learning revolution, Q-Learning stands as a pillar of Reinforcement Learning. It shows us that learning from experience, making mistakes, and continually refining our understanding is a potent recipe for achieving goals, whether you’re a human, a robot, or a piece of code.</p> <p>I encourage you to play with Q-Learning yourself! Implement a simple grid-world solver in Python, tweak the hyperparameters, and watch your agent learn. It’s an incredibly rewarding experience that truly demystifies the magic of AI. Happy learning!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>