<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Unveiling the Wisdom of the Crowd: A Deep Dive into Random Forests | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/unveiling-the-wisdom-of-the-crowd-a-deep-dive-into/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Unveiling the Wisdom of the Crowd: A Deep Dive into Random Forests</h1> <p class="post-meta"> Created on March 20, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/random-forests"> <i class="fa-solid fa-hashtag fa-sm"></i> Random Forests</a>   <a href="/blog/blog/tag/ensemble-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Ensemble Learning</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/decision-trees"> <i class="fa-solid fa-hashtag fa-sm"></i> Decision Trees</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>It’s [Your Name/Persona] here, and today we’re diving into one of my absolute favorite machine learning algorithms: <strong>Random Forests</strong>. When I first encountered them, it felt like magic. How could simply putting a bunch of “random” decision trees together create something so powerful? But as I dug deeper, the elegance and ingenuity behind it truly clicked. Whether you’re a seasoned data scientist or a high school student just curious about AI, I promise you’ll find something fascinating here.</p> <h3 id="the-lone-tree-a-simple-start">The Lone Tree: A Simple Start</h3> <p>Before we can build a forest, let’s talk about a single tree: a <strong>Decision Tree</strong>. Imagine you’re trying to decide if you should go to the park. Your thought process might look like a flowchart:</p> <ul> <li>Is it raining? <ul> <li>Yes -&gt; Don’t go to the park.</li> <li>No -&gt; Is it sunny? <ul> <li>Yes -&gt; Go to the park!</li> <li>No -&gt; Is it windy? <ul> <li>Yes -&gt; Don’t go to the park.</li> <li>No -&gt; Go to the park!</li> </ul> </li> </ul> </li> </ul> </li> </ul> <p>That’s essentially what a decision tree does. It asks a series of questions (features) about your data, and based on the answers, it funnels you down a path to a prediction (a “leaf node”). These trees are incredibly intuitive and easy to understand.</p> <p><strong>The Good:</strong></p> <ul> <li> <strong>Interpretable:</strong> You can literally see the logic.</li> <li> <strong>Simple:</strong> Easy to explain how a decision is made.</li> </ul> <p><strong>The Bad (and the Ugly):</strong> The problem with a single, deep decision tree is that it’s often too good at memorizing its training data. This is called <strong>overfitting</strong>. Imagine our park-goer, who, after a single bad experience with a slightly damp bench, decides to <em>never</em> go to the park again if there’s even a 1% chance of rain. It becomes overly specific to past events and can’t generalize well to new situations.</p> <p>A single decision tree can be unstable. A small change in the training data can lead to a completely different tree structure, drastically altering predictions. It’s like having a single, easily swayed expert.</p> <h3 id="the-power-of-the-crowd-ensemble-learning">The Power of the Crowd: Ensemble Learning</h3> <p>This is where the magic of <strong>Ensemble Learning</strong> comes in. Instead of relying on one “expert” (a single decision tree), what if we could gather an entire committee of experts? Each expert might have their own biases or blind spots, but by combining their opinions, we can arrive at a much more robust and accurate decision.</p> <p>Random Forests are a prime example of an ensemble method, specifically using a technique called <strong>Bagging</strong> (Bootstrap Aggregating).</p> <h3 id="building-a-forest-the-random-in-random-forest">Building a Forest: The “Random” in Random Forest</h3> <p>A Random Forest, as the name suggests, is a collection of many decision trees. But it’s not just any collection; it’s a <em>diverse</em> collection, and this diversity is achieved through two key sources of randomness:</p> <ol> <li> <p><strong>Bagging (Random Sampling of Data):</strong> Imagine you have your original dataset. Instead of training all trees on this single dataset, we create multiple <em>new</em> datasets for each tree. How? By <strong>bootstrapping</strong>. We randomly sample observations from our original dataset <em>with replacement</em>.</p> <p>What does “with replacement” mean? It means an observation that’s chosen for one new dataset can be chosen again for the <em>same</em> dataset, or for another one. This results in each new dataset being roughly the same size as the original but containing a slightly different mix of observations, some repeated, some left out.</p> <p>Think of it like this: You have 100 students. You want to pick 10 different committees of 100 students each. For each committee, you randomly pick students, and after picking one, you put their name back in the hat so they can be picked again for the same committee. This ensures each tree sees a slightly different “view” of the data, making them diverse.</p> </li> <li> <p><strong>Feature Randomness (Random Sampling of Features):</strong> This is the second crucial layer of randomness. When a decision tree is being built, at <em>each split</em> (each question it asks), it doesn’t consider all possible features to find the best split. Instead, it only considers a random subset of the available features.</p> <p>For example, if you have 100 features, a typical Random Forest might only consider 10 features at each split point. This prevents one or two very strong features from dominating every tree in the forest. If one feature is always the best splitter, then all trees would look very similar, defeating the purpose of diversity. By forcing trees to consider different features, we ensure they learn different aspects of the data.</p> </li> </ol> <p>So, in summary, each tree in a Random Forest is trained on:</p> <ul> <li>A different subset of the data (thanks to bootstrapping).</li> <li>Considering a different subset of features at each split.</li> </ul> <p>This dual randomness ensures that each tree is unique, somewhat biased, but when combined, they form a powerful, unbiased predictor.</p> <h3 id="the-forests-verdict-aggregating-predictions">The Forest’s Verdict: Aggregating Predictions</h3> <p>Once all the individual trees are grown (and usually, they are grown very deep, even to the point of overfitting their specific bootstrapped dataset), it’s time for the “committee” to make a decision.</p> <ul> <li> <strong>For Classification Tasks (e.g., predicting ‘cat’ or ‘dog’):</strong> Each tree makes its prediction. The forest then takes a <strong>majority vote</strong>. If 70 out of 100 trees predict ‘cat’, then the forest predicts ‘cat’.</li> <li> <strong>For Regression Tasks (e.g., predicting house price):</strong> Each tree predicts a numerical value. The forest then takes the <strong>average</strong> of all the individual tree predictions.</li> </ul> <p>Mathematically, for regression, if we have $B$ trees, and $T_b(x)$ is the prediction of the $b$-th tree for input $x$, the final prediction is: \(\hat{y} = \frac{1}{B} \sum\_{b=1}^B T_b(x)\)</p> <p>For classification, the final prediction $\hat{c}$ is the mode (most frequent class) of the individual tree predictions: \(\hat{c} = \text{mode} \{ T*b(x) \}*{b=1}^B\)</p> <h3 id="why-random-forests-are-so-effective">Why Random Forests Are So Effective</h3> <ol> <li> <p><strong>Reduced Variance (Less Overfitting):</strong> This is the core benefit. While individual decision trees might overfit their specific bootstrapped dataset, the aggregation process (averaging or majority voting) significantly reduces the overall variance of the model. Think of it like this: if one expert is wrong in one direction, another expert might be wrong in the opposite direction, and their errors cancel out when averaged. This makes the forest much more stable and robust to noise in the data.</p> </li> <li> <p><strong>Bias-Variance Tradeoff:</strong> Decision trees typically have low bias (they can fit complex patterns well) but high variance (they’re unstable). Random Forests reduce the variance substantially <em>without</em> significantly increasing the bias, leading to a much better balance and overall higher accuracy.</p> </li> <li> <p><strong>Robustness to Outliers and Noise:</strong> Because each tree sees a slightly different dataset, a few outliers or noisy data points won’t drastically affect the entire forest’s decision, unlike a single tree.</p> </li> <li> <p><strong>Implicit Feature Importance:</strong> Random Forests can tell you which features were most useful in making predictions. They do this by measuring how much each feature reduces impurity (like Gini impurity or information gain) across all trees in the forest. Features that consistently lead to better splits are deemed more important. This is incredibly valuable for understanding your data!</p> </li> </ol> <h3 id="key-parameters-to-consider">Key Parameters to Consider</h3> <p>When you’re building a Random Forest (e.g., using <code class="language-plaintext highlighter-rouge">scikit-learn</code> in Python), there are a few important knobs to turn:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">n_estimators</code>: The number of trees in the forest. More trees generally lead to better performance but also take longer to train. A good starting point is often 100-500.</li> <li> <code class="language-plaintext highlighter-rouge">max_features</code>: The number of features to consider at each split. This is often set to $\sqrt{\text{total features}}$ for classification and $\text{total features}/3$ for regression.</li> <li> <code class="language-plaintext highlighter-rouge">max_depth</code>: The maximum depth of individual trees. Often left <code class="language-plaintext highlighter-rouge">None</code> to allow trees to grow fully, as the bagging process handles overfitting.</li> <li> <code class="language-plaintext highlighter-rouge">min_samples_leaf</code>: The minimum number of samples required to be at a leaf node. This helps to prevent individual trees from becoming too specific.</li> </ul> <h3 id="when-to-use-and-not-use-random-forests">When to Use (and Not Use) Random Forests</h3> <p><strong>Pros:</strong></p> <ul> <li> <strong>High Accuracy:</strong> Often among the most accurate algorithms.</li> <li> <strong>Handles High Dimensionality:</strong> Can work with thousands of input features.</li> <li><strong>Handles Missing Values (with some imputation strategies) and Categorical Features.</strong></li> <li> <strong>Less Prone to Overfitting</strong> than a single decision tree.</li> <li> <strong>Robust:</strong> Less sensitive to noisy data or outliers.</li> <li> <strong>Provides Feature Importance:</strong> A great way to understand your data better.</li> </ul> <p><strong>Cons:</strong></p> <ul> <li> <strong>Less Interpretable:</strong> While individual trees are interpretable, a forest of hundreds of trees is not. It’s often considered a “black box” model.</li> <li> <strong>Computationally Intensive:</strong> Training many trees can take time and memory, especially with very large datasets or many trees.</li> <li> <strong>Can Still Overfit:</strong> While generally robust, a very high number of features or too deep trees <em>can</em> still lead to some overfitting if not carefully tuned.</li> </ul> <h3 id="real-world-applications">Real-World Applications</h3> <p>Random Forests are everywhere!</p> <ul> <li> <strong>Finance:</strong> Predicting stock prices, identifying fraudulent transactions.</li> <li> <strong>Healthcare:</strong> Diagnosing diseases, predicting patient outcomes.</li> <li> <strong>E-commerce:</strong> Recommending products, predicting customer churn.</li> <li> <strong>Image Recognition:</strong> Object detection and classification.</li> <li> <strong>Environmental Science:</strong> Predicting weather patterns, identifying forest fires.</li> </ul> <h3 id="my-takeaway">My Takeaway</h3> <p>Random Forests are a testament to the idea that diversity and collaboration often lead to better outcomes. What seems like a simple concept – just combine many decision trees – becomes incredibly powerful and robust when you introduce strategic randomness. It’s an algorithm that perfectly balances simplicity in its building blocks with incredible complexity and accuracy in its final form.</p> <p>If you’re just starting in machine learning, understanding Random Forests is a huge step. They are a foundational algorithm that often serves as a strong baseline for many predictive tasks. So go ahead, experiment, build your own forest, and marvel at the wisdom it uncovers!</p> <p>Happy learning!</p> <p>[Your Name/Persona]</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>