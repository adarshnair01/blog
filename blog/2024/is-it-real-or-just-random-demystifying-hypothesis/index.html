<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Is It Real, Or Just Random? Demystifying Hypothesis Testing | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/is-it-real-or-just-random-demystifying-hypothesis/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Is It Real, Or Just Random? Demystifying Hypothesis Testing</h1> <p class="post-meta"> Created on June 07, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/hypothesis-testing"> <i class="fa-solid fa-hashtag fa-sm"></i> Hypothesis Testing</a>   <a href="/blog/blog/tag/statistics"> <i class="fa-solid fa-hashtag fa-sm"></i> Statistics</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/p-value"> <i class="fa-solid fa-hashtag fa-sm"></i> P-value</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>Welcome back to my journal, where we tackle the fascinating world of data and machine learning. Today, I want to share a concept that, for a long time, felt like a secret handshake among statisticians: <strong>Hypothesis Testing</strong>.</p> <p>It sounds intimidating, right? “Hypothesis Testing.” My first encounter with it felt like I was being asked to solve a riddle in a foreign language. But once I truly grasped its essence, I realized it’s less about complex math and more about a structured way of thinking – a detective’s approach to data. And believe me, whether you’re building predictive models or optimizing a website, this skill is absolutely indispensable.</p> <p>So, grab a coffee (or your favorite beverage), and let’s pull back the curtain on this statistical superpower together.</p> <h3 id="the-quest-for-certainty-beyond-the-hunch">The Quest for Certainty: Beyond the Hunch</h3> <p>Imagine this: My friend, a product manager, just launched a fancy new feature on their e-commerce website. They’re convinced it’s going to boost conversion rates by at least 10%. They show me a dashboard, and sure enough, conversions <em>look</em> higher. But is it <em>really</em> the feature, or just a lucky streak? Could it be random fluctuation, or did the timing of a big sale coincidentally align?</p> <p>This is where intuition, while valuable, falls short. We need a systematic way to differentiate between genuine effects and mere chance. That’s precisely what Hypothesis Testing provides: a formal procedure to evaluate a claim about a population using evidence from a sample of data. It’s how we move from “I think so” to “The data suggests…”</p> <h3 id="the-foundation-null-and-alternative-hypotheses">The Foundation: Null and Alternative Hypotheses</h3> <p>Every good detective story starts with a clear set of possibilities. In Hypothesis Testing, these are our two competing statements:</p> <ol> <li> <strong>The Null Hypothesis ($H_0$)</strong>: This is our default assumption, the status quo. It’s the “nothing to see here,” “no effect,” or “no difference” statement. Think of it as the “innocent until proven guilty” in a courtroom. We assume it’s true until we have strong evidence to the contrary. <ul> <li> <em>My friend’s example:</em> $H_0$: “The new feature has <strong>no effect</strong> on the conversion rate.” (Or, the conversion rate with the new feature is the same as without it).</li> </ul> </li> <li> <strong>The Alternative Hypothesis ($H_A$ or $H_1$)</strong>: This is what we’re trying to prove, the challenger to the status quo. It’s the “guilty” verdict, the “there <em>is</em> an effect,” or “there <em>is</em> a difference.” This is often what the researcher or product manager hopes to demonstrate. <ul> <li> <em>My friend’s example:</em> $H_A$: “The new feature <strong>increases</strong> the conversion rate.” (Or, the conversion rate with the new feature is greater than without it).</li> </ul> </li> </ol> <p>It’s crucial that $H_0$ and $H_A$ are mutually exclusive and collectively exhaustive (they cover all possibilities). Notice how I used “increases” in $H_A$. This is a <strong>one-tailed test</strong>, meaning we’re only interested if the effect is in one specific direction. If we were simply looking for <em>any</em> change (increase or decrease), it would be a <strong>two-tailed test</strong>.</p> <h3 id="the-gatekeeper-significance-level-alpha">The Gatekeeper: Significance Level ($\alpha$)</h3> <p>Before we even look at the data, we need to set our standards. How strong does the evidence need to be for us to reject the null hypothesis? This standard is called the <strong>Significance Level</strong>, denoted by $\alpha$ (alpha).</p> <p>Think of $\alpha$ as our “risk tolerance.” It’s the probability of mistakenly rejecting a true null hypothesis. In simpler terms, it’s the maximum chance we’re willing to take of saying there’s an effect when there isn’t one (a “false positive”).</p> <p>Common values for $\alpha$ are 0.05 (5%) or 0.01 (1%). If we choose $\alpha = 0.05$, we’re saying: “I’m only willing to reject the null hypothesis if there’s less than a 5% chance that I’d see this data (or more extreme) if the null hypothesis were actually true.”</p> <p>Setting $\alpha$ beforehand is important to prevent cherry-picking results!</p> <h3 id="measuring-the-evidence-the-test-statistic">Measuring the Evidence: The Test Statistic</h3> <p>Okay, we have our hypotheses and our risk tolerance. Now it’s time to gather evidence from our sample data. We calculate something called a <strong>Test Statistic</strong>.</p> <p>A test statistic is a single number that summarizes how far our sample data deviates from what the null hypothesis predicts, taking into account the variability in our data. It’s like a standardized score for the difference we observe.</p> <p>Different types of data and different scenarios call for different test statistics:</p> <ul> <li> <strong>Z-score</strong>: Often used when we know the population standard deviation or have a large sample size.</li> <li> <strong>T-score</strong>: Used when we don’t know the population standard deviation and are working with smaller samples.</li> <li> <strong>Chi-square ($\chi^2$)</strong>: For analyzing categorical data, like counts or frequencies.</li> <li> <strong>F-statistic</strong>: Used in ANOVA to compare means across more than two groups.</li> </ul> <p>The specific formula for a test statistic will vary, but the <em>purpose</em> remains the same: quantify the observed effect in a standardized way. For instance, comparing two proportions (like in our conversion rate example), you might calculate a Z-score like this:</p> <p>$Z = \frac{(\hat{p_1} - \hat{p_2})}{\sqrt{\hat{p}(1-\hat{p})(\frac{1}{n_1} + \frac{1}{n_2})}}$</p> <p>Where $\hat{p_1}$ and $\hat{p_2}$ are the sample proportions, $n_1$ and $n_2$ are the sample sizes, and $\hat{p}$ is the pooled proportion. Don’t worry about memorizing this specific formula now, just understand that it boils down our observations into a single, comparable number.</p> <h3 id="the-probability-puzzle-the-p-value">The Probability Puzzle: The P-value</h3> <p>This is often the trickiest part for newcomers, but it’s the heart of the decision-making process. Once we have our test statistic, we use it to calculate the <strong>P-value</strong>.</p> <p>My personal “Aha!” moment came when I understood the P-value’s true definition:</p> <blockquote> <p>The <strong>P-value</strong> is the probability of observing a test statistic as extreme as, or more extreme than, the one calculated from our sample data, <em>assuming that the null hypothesis ($H_0$) is true</em>.</p> </blockquote> <p>Let’s break that down. If the null hypothesis (e.g., “no effect of the new feature”) were truly correct, how likely would it be for us to see the kind of conversion rate difference we just observed purely by chance?</p> <ul> <li> <strong>A small P-value (e.g., $P = 0.01$)</strong> suggests that if $H_0$ were true, our observed data would be very, very unlikely. This makes us question the validity of $H_0$. “Wow, if the feature had no effect, getting this much of an uplift would be almost a miracle!”</li> <li> <strong>A large P-value (e.g., $P = 0.30$)</strong> suggests that if $H_0$ were true, our observed data would be quite common. This means our data doesn’t provide strong evidence against $H_0$. “Meh, seeing this kind of change by random chance isn’t that unusual if the feature had no effect.”</li> </ul> <p>It’s critical to understand what the P-value <em>is not</em>:</p> <ul> <li>It is <strong>not</strong> the probability that the null hypothesis is true.</li> <li>It is <strong>not</strong> the probability that the alternative hypothesis is false.</li> <li>It is <strong>not</strong> the probability of making a mistake.</li> </ul> <p>It’s solely a measure of the evidence against the null hypothesis <em>from our sample data</em>, interpreted under the assumption that the null hypothesis is true.</p> <h3 id="making-the-call-decision-time">Making the Call: Decision Time!</h3> <p>With our P-value in hand, we compare it to our pre-determined significance level, $\alpha$. This is where we make our decision:</p> <ul> <li> <strong>If P-value $\le \alpha$</strong>: We <strong>reject the null hypothesis ($H_0$)</strong>. <ul> <li> <em>Interpretation</em>: Our observed data is statistically significant. We have enough evidence to conclude that our alternative hypothesis ($H_A$) is likely true. In my friend’s case: “We have statistically significant evidence that the new feature <em>does</em> increase the conversion rate.”</li> </ul> </li> <li> <strong>If P-value $&gt; \alpha$</strong>: We <strong>fail to reject the null hypothesis ($H_0$)</strong>. <ul> <li> <em>Interpretation</em>: Our observed data is <em>not</em> statistically significant. We do not have enough evidence to conclude that the alternative hypothesis ($H_A$) is true. <em>Crucially, this does not mean we accept the null hypothesis!</em> It simply means our data isn’t strong enough to overturn the status quo. In my friend’s case: “We do not have enough statistically significant evidence to conclude that the new feature increases the conversion rate based on this experiment.” Perhaps there <em>is</em> an effect, but our test wasn’t powerful enough to detect it, or the effect is smaller than we hoped.</li> </ul> </li> </ul> <p>This distinction between “reject $H_0$” and “fail to reject $H_0$” is a cornerstone of responsible statistical reporting.</p> <h3 id="the-inevitable-types-of-errors">The Inevitable: Types of Errors</h3> <p>No decision-making process is perfect, and Hypothesis Testing is no exception. There are two types of errors we can make:</p> <ol> <li> <strong>Type I Error (False Positive)</strong>: This occurs when we <strong>reject the null hypothesis ($H_0$) when it is actually true</strong>. <ul> <li> <em>Probability of Type I Error</em>: This is exactly our significance level, $\alpha$.</li> <li> <em>My friend’s example</em>: We conclude the new feature increases conversion, but in reality, it doesn’t. We might invest more resources into a feature that provides no real benefit.</li> </ul> </li> <li> <strong>Type II Error (False Negative)</strong>: This occurs when we <strong>fail to reject the null hypothesis ($H_0$) when it is actually false</strong>. <ul> <li> <em>Probability of Type II Error</em>: This is denoted by $\beta$ (beta).</li> <li> <em>My friend’s example</em>: We conclude the new feature <em>doesn’t</em> increase conversion, but in reality, it actually does! We might miss out on a valuable improvement.</li> </ul> </li> </ol> <p>There’s a trade-off between these two errors. Decreasing $\alpha$ (making it harder to reject $H_0$) increases the chance of a Type II error ($\beta$), and vice-versa. Understanding the consequences of each error in your specific context helps you choose an appropriate $\alpha$ level. For example, in drug trials, a Type I error (saying a drug works when it doesn’t) can be very dangerous, so a much smaller $\alpha$ might be used.</p> <h3 id="a-walkthrough-the-ab-test-example">A Walkthrough: The A/B Test Example</h3> <p>Let’s put it all together with a slightly more detailed example. My friend is running an A/B test comparing their current website (Control A) to a new design (Variant B) to see if Variant B leads to a higher click-through rate (CTR) on a specific call to action.</p> <ol> <li> <strong>Define Hypotheses</strong>: <ul> <li>$H_0$: The CTR of Variant B is the same as or less than the CTR of Control A. ($CTR_B \le CTR_A$)</li> <li>$H_A$: The CTR of Variant B is greater than the CTR of Control A. ($CTR_B &gt; CTR_A$) - This is a one-tailed test.</li> </ul> </li> <li> <strong>Set Significance Level ($\alpha$)</strong>: <ul> <li>My friend decides that they are willing to accept a 5% chance of a false positive, so $\alpha = 0.05$.</li> </ul> </li> <li> <strong>Collect Data</strong>: <ul> <li>They run the A/B test for two weeks.</li> <li>Control A: 10,000 visitors, 2,000 clicks. ($CTR_A = 2000/10000 = 0.20$)</li> <li>Variant B: 10,000 visitors, 2,200 clicks. ($CTR_B = 2200/10000 = 0.22$)</li> </ul> </li> <li> <strong>Calculate Test Statistic</strong>: <ul> <li>Since we’re comparing proportions of two independent samples, a Z-test is appropriate. We’d calculate the pooled proportion $\hat{p} = (2000+2200)/(10000+10000) = 4200/20000 = 0.21$.</li> <li>The Z-score formula (as introduced earlier) would be applied. Let’s <em>assume</em> for simplicity that our calculated Z-score comes out to be $Z_{calc} = 2.5$. (In a real scenario, you’d use statistical software or a calculator for this).</li> </ul> </li> <li> <strong>Find P-value</strong>: <ul> <li>Using statistical tables or software, we find the probability of getting a Z-score of 2.5 or higher (since it’s a one-tailed test for “greater than”).</li> <li>Let’s <em>assume</em> this P-value comes out to be $P = 0.0062$.</li> </ul> </li> <li> <strong>Make a Decision</strong>: <ul> <li>We compare $P = 0.0062$ with $\alpha = 0.05$.</li> <li>Since $0.0062 \le 0.05$, the P-value is less than or equal to our significance level.</li> <li>Therefore, we <strong>reject the null hypothesis ($H_0$)</strong>.</li> </ul> </li> <li> <strong>Conclusion</strong>: <ul> <li>“Based on our A/B test, with a significance level of 0.05, we have statistically significant evidence to conclude that Variant B’s new design leads to a higher click-through rate compared to Control A. We can be reasonably confident in rolling out Variant B.”</li> </ul> </li> </ol> <p>This systematic approach removes the guesswork and provides a data-driven justification for our decisions.</p> <h3 id="hypothesis-testing-in-the-data-science--mle-world">Hypothesis Testing in the Data Science &amp; MLE World</h3> <p>Why is this so crucial for us? Hypothesis Testing is the bedrock for many data science and machine learning applications:</p> <ul> <li> <strong>A/B Testing &amp; Experimentation</strong>: As shown above, this is fundamental for product development, marketing, and UX design. Did that new model variant <em>really</em> improve performance, or was it just noise?</li> <li> <strong>Model Comparison</strong>: Are the performance differences between two models (e.g., a new neural network vs. a baseline logistic regression) statistically significant? This helps prevent over-optimizing for tiny, random fluctuations.</li> <li> <strong>Feature Selection</strong>: Does a particular feature have a statistically significant relationship with the target variable, making it valuable for our model?</li> <li> <strong>Anomaly Detection</strong>: Is a new data point significantly different from the expected distribution, indicating a potential anomaly or fraud?</li> <li> <strong>Statistical Process Control</strong>: Monitoring if a process is “in control” or if a deviation is statistically significant, requiring intervention.</li> <li> <strong>Causal Inference</strong>: While correlation isn’t causation, hypothesis testing helps us rigorously test specific causal claims under experimental conditions.</li> </ul> <h3 id="conclusion-your-statistical-compass">Conclusion: Your Statistical Compass</h3> <p>And there you have it! Hypothesis Testing, once a formidable mountain, is actually a powerful, logical framework that empowers you to make informed, evidence-based decisions from your data. It’s not about being “100% certain” (that’s rare in data!), but about quantifying uncertainty and making the best decision given the evidence at hand and the risks involved.</p> <p>By understanding $H_0$, $H_A$, $\alpha$, test statistics, P-values, and the types of errors, you’re now equipped with a statistical compass to navigate the often-murky waters of data analysis. So next time someone says, “I think this works,” you can confidently ask, “And what does the data <em>statistically</em> say?”</p> <p>Keep exploring, keep questioning, and let the data guide you!</p> <p>Cheers,</p> <p>[Your Name/Alias]</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>