<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Learning by Doing: My Deep Dive into Reinforcement Learning (and Why It's So Cool!) | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/learning-by-doing-my-deep-dive-into-reinforcement/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Learning by Doing: My Deep Dive into Reinforcement Learning (and Why It's So Cool!)</h1> <p class="post-meta"> Created on April 25, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/reinforcement-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Reinforcement Learning</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/decision-making"> <i class="fa-solid fa-hashtag fa-sm"></i> Decision Making</a>   <a href="/blog/blog/tag/robotics"> <i class="fa-solid fa-hashtag fa-sm"></i> Robotics</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>As someone deeply fascinated by how intelligence works, both biological and artificial, I’ve spent a lot of time pondering one of the most fundamental aspects of learning: <strong>experience</strong>. Think about it – from a baby learning to walk to a seasoned chess player mastering new strategies, we all learn by trying things out, seeing what happens, and adjusting our behavior based on the outcomes. We get “rewards” (like successfully taking a step or winning a game) and “punishments” (falling over, losing). This intuitive process of “learning by doing” isn’t just for humans; it’s also at the heart of one of the most exciting branches of Artificial Intelligence: <strong>Reinforcement Learning (RL)</strong>.</p> <p>Today, I want to take you on a journey into the world of RL. We’ll strip away the jargon and explore its core ideas, the math that makes it tick, and why it’s shaping the future of AI in incredible ways. If you’ve ever been curious about how AI agents can seemingly learn on their own, you’re in for a treat!</p> <h3 id="the-fundamental-idea-learning-through-interaction">The Fundamental Idea: Learning Through Interaction</h3> <p>At its core, Reinforcement Learning is about an <strong>agent</strong> learning to make sequences of decisions in an <strong>environment</strong> to maximize some notion of cumulative <strong>reward</strong>. It’s a bit like training a pet: you reward good behavior and ignore (or mildly discourage) bad behavior, and over time, the pet learns what to do.</p> <p>Let’s break down the key players in any RL scenario:</p> <ol> <li> <strong>Agent</strong>: This is our AI – the decision-maker, the learner. Think of it as the student.</li> <li> <strong>Environment</strong>: This is the world the agent interacts with. It could be a video game, a physical maze for a robot, or even a simulation of a stock market. Think of it as the classroom.</li> <li> <strong>State ($S_t$)</strong>: At any given moment $t$, the environment is in a particular state. This is a snapshot of everything the agent needs to know. For a robot in a maze, the state might be its current position. For a game AI, it could be the entire game board.</li> <li> <strong>Action ($A_t$)</strong>: Based on the current state, the agent chooses an action to take. The robot might move left, right, up, or down. The game AI might move a piece or cast a spell.</li> <li> <strong>Reward ($R_t$)</strong>: After taking an action in a state, the environment provides a numerical reward. This is the feedback signal. A positive reward encourages the agent to repeat the action; a negative reward discourages it. The robot might get a +10 reward for reaching the exit and a -1 reward for hitting a wall.</li> </ol> <p>This interaction happens in a loop:</p> <p><strong>Agent observes state ($S_t$) $\rightarrow$ Agent chooses action ($A_t$) $\rightarrow$ Environment transitions to new state ($S_{t+1}$) and gives reward ($R_{t+1}$) $\rightarrow$ Loop repeats.</strong></p> <p>The ultimate goal of the agent is to learn a <strong>policy</strong> ($\pi$). A policy is essentially a strategy: it tells the agent what action to take in any given state. Our agent wants to find the <em>optimal policy</em> ($\pi^*$) – the policy that maximizes its total expected reward over the long run.</p> <h3 id="the-challenge-delayed-gratification-and-the-value-of-future-rewards">The Challenge: Delayed Gratification and The Value of Future Rewards</h3> <p>One of the trickiest parts of RL is that rewards aren’t always immediate. Imagine teaching a robot to make a cup of coffee. It might perform many steps correctly (picking up the cup, pouring water) before it finally gets the “reward” of a complete, delicious cup of coffee. How does it know which of its earlier actions contributed to that final success? This is the problem of <strong>credit assignment</strong>.</p> <p>To solve this, RL introduces the concept of <strong>value functions</strong>. Instead of just looking at immediate rewards, value functions estimate the <em>total future reward</em> an agent can expect to receive starting from a particular state or taking a particular action.</p> <p>There are two main types:</p> <ol> <li> <p><strong>State-Value Function ($V(s)$)</strong>: This tells us “how good” it is to be in a particular state $s$. Specifically, it’s the expected return (sum of future rewards) if the agent starts in state $s$ and then follows a specific policy $\pi$. $V^\pi(s) = E[G_t | S_t = s]$ Where $G_t$ is the total discounted future reward from time $t$: $G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + …$</p> </li> <li> <p><strong>Action-Value Function ($Q(s,a)$)</strong>: This is even more useful! It tells us “how good” it is to take a specific action $a$ in a specific state $s$, and then follow policy $\pi$ afterwards. $Q^\pi(s,a) = E[G_t | S_t = s, A_t = a]$</p> </li> </ol> <h3 id="the-magic-of-the-discount-factor-gamma-and-the-bellman-equation">The Magic of the Discount Factor ($\gamma$) and the Bellman Equation</h3> <p>Notice that weird symbol $\gamma$ in the equation for $G_t$? That’s the <strong>discount factor</strong>, a number between 0 and 1. It determines how much the agent cares about immediate rewards versus future rewards.</p> <ul> <li>If $\gamma$ is close to 0, the agent is “myopic” and only cares about immediate rewards.</li> <li>If $\gamma$ is close to 1, the agent is “farsighted” and values future rewards almost as much as immediate ones.</li> </ul> <p>Why do we need it?</p> <ol> <li> <strong>Mathematical Convergence</strong>: It ensures that the sum of infinite future rewards doesn’t explode.</li> <li> <strong>Realistic Preference</strong>: Often, immediate rewards <em>are</em> more certain or desirable than far-off rewards.</li> </ol> <p>Now, for the really clever part: the <strong>Bellman Equation</strong>. It’s the backbone of many RL algorithms and allows us to break down the complex problem of estimating long-term rewards into smaller, manageable pieces.</p> <p>The intuition behind the Bellman Equation is simple: <strong>the value of a state (or state-action pair) can be expressed in terms of the immediate reward plus the discounted value of the <em>next</em> state (or state-action pair).</strong></p> <p>For the optimal state-value function $V^<em>(s)$, it looks like this: $V^</em>(s) = \max_a E[R_{t+1} + \gamma V^*(S_{t+1}) | S_t = s, A_t = a]$</p> <p>And for the optimal action-value function $Q^<em>(s,a)$: $Q^</em>(s,a) = E[R_{t+1} + \gamma \max_{a’} Q^*(S_{t+1}, a’) | S_t = s, A_t = a]$</p> <p>Don’t let the math scare you! What this means is: <em>the best possible value you can get from being in a state $s$ (or taking action $a$ in state $s$) is the reward you get immediately, plus the best possible value you can get from wherever you end up next.</em> This recursive definition allows algorithms to iteratively calculate these values.</p> <h3 id="exploration-vs-exploitation-the-age-old-dilemma">Exploration vs. Exploitation: The Age-Old Dilemma</h3> <p>Imagine our robot in a maze. It needs to <em>explore</em> different paths to discover where the exit is and where the walls are. But once it finds a path that leads to a reward, it should <em>exploit</em> that knowledge to reach the reward efficiently.</p> <p>This <strong>exploration-exploitation trade-off</strong> is critical in RL.</p> <ul> <li> <strong>Exploration</strong>: Trying new, potentially suboptimal actions to discover more about the environment and potentially find better rewards.</li> <li> <strong>Exploitation</strong>: Choosing actions that are known to yield high rewards based on current knowledge.</li> </ul> <p>If an agent only explores, it might never settle on an optimal path. If it only exploits, it might get stuck in a suboptimal local maximum, never discovering the truly best path.</p> <p>A common strategy is <strong>$\epsilon$-greedy</strong>: With probability $\epsilon$ (a small number, e.g., 0.1), the agent chooses a random action (explores). With probability $1-\epsilon$, the agent chooses the action it currently believes is best (exploits). Often, $\epsilon$ starts high and slowly decays over time, allowing for more exploration early on and more exploitation as the agent gains knowledge.</p> <h3 id="famous-algorithms-q-learning-and-deep-reinforcement-learning">Famous Algorithms: Q-Learning and Deep Reinforcement Learning</h3> <p>With the core concepts in place, let’s look at how agents actually <em>learn</em>.</p> <h4 id="q-learning">Q-Learning</h4> <p>Q-Learning is a very popular, <strong>model-free</strong> (meaning it doesn’t need to know how the environment works beforehand) and <strong>off-policy</strong> (it learns the optimal Q-values regardless of the policy being followed) algorithm. It directly learns the optimal action-value function $Q^*(s,a)$.</p> <p>The update rule for Q-learning is: $Q(s,a) \leftarrow Q(s,a) + \alpha [R_{t+1} + \gamma \max_{a’} Q(S_{t+1}, a’) - Q(s,a)]$</p> <p>Let’s break it down:</p> <ul> <li>$Q(s,a)$: The current estimated value of taking action $a$ in state $s$.</li> <li>$\alpha$ (alpha): The <strong>learning rate</strong> (between 0 and 1). It dictates how much new information overrides old information. A high $\alpha$ means the agent quickly adapts, while a low $\alpha$ means it’s more cautious.</li> <li>$R_{t+1}$: The immediate reward received.</li> <li>$\gamma \max_{a’} Q(S_{t+1}, a’)$: This is the estimated optimal future value from the <em>next</em> state $S_{t+1}$. The <code class="language-plaintext highlighter-rouge">max</code> indicates that the agent assumes it will take the best possible action in the next state.</li> <li>$R_{t+1} + \gamma \max_{a’} Q(S_{t+1}, a’) - Q(s,a)$: This entire term is the <strong>temporal difference (TD) error</strong>. It’s the difference between the new, improved estimate of the Q-value and the old estimate. If this difference is positive, it means our current action was better than expected; if negative, worse.</li> </ul> <p>Q-Learning essentially allows the agent to iteratively refine its understanding of which actions are best in which states, eventually converging on the optimal $Q^*(s,a)$ function.</p> <h4 id="deep-reinforcement-learning-drl">Deep Reinforcement Learning (DRL)</h4> <p>What happens when the number of states and actions becomes astronomically large? Imagine an Atari game – the state is literally every pixel on the screen. It’s impossible to store a Q-value for every possible pixel configuration in a table.</p> <p>This is where <strong>Deep Reinforcement Learning</strong> comes in. It combines the principles of RL with the power of <strong>deep neural networks</strong>. Instead of a table, a neural network is used to <em>approximate</em> the Q-function (or the policy itself).</p> <p>For example, in <strong>Deep Q-Networks (DQNs)</strong>, a neural network takes the state (e.g., raw pixels from a game screen) as input and outputs the Q-values for all possible actions. The network learns by trying to minimize the TD error we saw earlier. This allows agents to generalize across similar states and handle extremely complex environments, leading to groundbreaking successes like AlphaGo and AI mastering Atari games.</p> <h3 id="real-world-impact-and-the-road-ahead">Real-World Impact and the Road Ahead</h3> <p>Reinforcement Learning isn’t just for games and theoretical mazes. Its applications are rapidly expanding:</p> <ul> <li> <strong>Robotics</strong>: Teaching robots to grasp objects, navigate complex terrains, or perform intricate tasks.</li> <li> <strong>Autonomous Driving</strong>: Training self-driving cars to make safe and efficient decisions on the road.</li> <li> <strong>Healthcare</strong>: Optimizing treatment plans, drug discovery, and medical diagnoses.</li> <li> <strong>Financial Trading</strong>: Developing agents that can make investment decisions.</li> <li> <strong>Personalized Recommendations</strong>: Improving recommender systems by learning user preferences over time.</li> <li> <strong>Resource Management</strong>: Optimizing energy consumption in data centers or managing traffic flow.</li> </ul> <p>The challenges in RL are still significant:</p> <ul> <li> <strong>Sample Efficiency</strong>: RL agents often require enormous amounts of data (experience) to learn.</li> <li> <strong>Safety</strong>: Ensuring that agents don’t learn unsafe behaviors, especially in real-world scenarios.</li> <li> <strong>Reward Design</strong>: Crafting an effective reward function that truly reflects the desired behavior can be difficult.</li> <li> <strong>High-Dimensionality</strong>: Dealing with extremely complex state and action spaces remains an active research area.</li> </ul> <h3 id="conclusion-the-future-is-learning">Conclusion: The Future is Learning!</h3> <p>Reinforcement Learning is truly a fascinating field that mirrors how we, as humans, learn and adapt. The idea of an agent exploring an environment, making mistakes, receiving feedback, and incrementally improving its strategy is not just powerful – it’s profoundly intelligent.</p> <p>From simple maze-solving bots to sophisticated systems that can beat world champions in complex games, RL is pushing the boundaries of what AI can achieve. As we continue to develop more robust algorithms and harness greater computational power, I believe RL will play an even more pivotal role in creating intelligent systems that can truly learn by doing, solving some of humanity’s most complex challenges.</p> <p>It’s an exciting time to be involved in Data Science and Machine Learning, and diving deeper into RL is definitely a journey worth taking. Keep learning, keep experimenting, and who knows what incredible AI agents you might build!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>