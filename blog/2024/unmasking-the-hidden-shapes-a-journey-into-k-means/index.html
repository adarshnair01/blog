<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Unmasking the Hidden Shapes: A Journey into K-Means Clustering | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/unmasking-the-hidden-shapes-a-journey-into-k-means/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Unmasking the Hidden Shapes: A Journey into K-Means Clustering</h1> <p class="post-meta"> Created on July 17, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/k-means"> <i class="fa-solid fa-hashtag fa-sm"></i> K-Means</a>   <a href="/blog/blog/tag/clustering"> <i class="fa-solid fa-hashtag fa-sm"></i> Clustering</a>   <a href="/blog/blog/tag/unsupervised-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Unsupervised Learning</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>Welcome back to the lab. Today, we’re not just learning about an algorithm; we’re peeling back the curtain on one of the most fundamental and intuitive concepts in machine learning: <em>clustering</em>. Imagine you have a giant pile of LEGO bricks, all mixed up. Some are red, some are blue, some are flat, some are tall. Your goal? To sort them into groups that “make sense,” even if no one told you beforehand what “red” or “flat” meant. You’re just trying to find natural groupings.</p> <p>That’s the essence of clustering. It’s a journey into the heart of <strong>unsupervised learning</strong>, where our data has no labels, no right answers we’re trying to predict. Instead, we’re explorers, seeking patterns, structures, and hidden communities within the raw data itself. And at the forefront of this exploration, shining bright with its simplicity and effectiveness, is an algorithm called <strong>K-Means Clustering</strong>.</p> <h3 id="the-intuition-finding-neighbors-in-a-crowded-room">The Intuition: Finding Neighbors in a Crowded Room</h3> <p>Let’s ground this with an analogy. Picture a school dance. Kids are just milling about. The principal wants to form study groups, but they don’t know who likes math, who likes history, or who likes poetry. What’s a simple way to start?</p> <ol> <li> <strong>Pick some initial “leaders”</strong>: Randomly select a few students to be the “center” of a potential group. Let’s say we pick <code class="language-plaintext highlighter-rouge">k=3</code> leaders.</li> <li> <strong>Gather their followers</strong>: Each student in the room looks around and decides which of the <code class="language-plaintext highlighter-rouge">k</code> leaders they are <em>closest</em> to, maybe based on where they’re standing. They then move to join that leader’s group.</li> <li> <strong>Refine the leadership</strong>: Once everyone has joined a group, the “leader” of each group might not be the actual geographic center anymore. So, each group’s <em>true</em> center is recalculated based on the average position of all the students now in it. A new “leader” (or centroid) emerges.</li> <li> <strong>Repeat</strong>: Students then look at these <em>new</em> leaders and decide again who they are closest to. They might switch groups! This continues until the groups stabilize – no one wants to switch groups anymore, and the leaders are truly at the center of their followers.</li> </ol> <p>That “guess, check, refine” loop is the heart of K-Means. It’s surprisingly powerful for such a simple idea.</p> <h3 id="k-means-a-step-by-step-breakdown">K-Means: A Step-by-Step Breakdown</h3> <p>Let’s translate our dance analogy into the language of data and algorithms.</p> <p>Imagine our data points are scattered across a 2D graph (though K-Means works in any number of dimensions!). Each point represents an observation, and its coordinates are its features (e.g., age and income for customer data).</p> <p><strong>Step 1: Choose the number of clusters, <code class="language-plaintext highlighter-rouge">k</code></strong> This is the “K” in K-Means. It’s the most critical decision we make upfront: <em>how many groups do we want to find?</em> Do we want 3 customer segments, or 5? We’ll discuss how to pick <code class="language-plaintext highlighter-rouge">k</code> later, but for now, let’s assume we’ve picked a value.</p> <p><strong>Step 2: Initialize <code class="language-plaintext highlighter-rouge">k</code> centroids</strong> We randomly select <code class="language-plaintext highlighter-rouge">k</code> data points from our dataset to be our initial “centroids” (our group leaders). These aren’t actual data points anymore; they’re just starting guesses for the <em>centers</em> of our clusters.</p> <p><strong>Step 3: Assign each data point to the closest centroid</strong> Now, every single data point in our dataset looks at all <code class="language-plaintext highlighter-rouge">k</code> centroids and decides which one it is <em>closest</em> to. But what does “closest” mean?</p> <p>Here, we use a concept called <strong>Euclidean distance</strong>. If you remember your high school geometry, it’s just the straight-line distance between two points. For two points $\mathbf{x} = (x_1, x_2, \ldots, x_n)$ and $\mathbf{y} = (y_1, y_2, \ldots, y_n)$ in $n$-dimensional space, the Euclidean distance is:</p> <p>$d(\mathbf{x}, \mathbf{y}) = \sqrt{\sum_{i=1}^n (x_i - y_i)^2}$</p> <p>Intuitively, it’s just how far apart two points are in space. So, each data point calculates its distance to every centroid and then joins the cluster whose centroid is the shortest distance away.</p> <p><strong>Step 4: Update the centroids</strong> Once all data points have been assigned to one of the <code class="language-plaintext highlighter-rouge">k</code> clusters, our initial random centroids probably aren’t in the “middle” of their assigned points anymore. So, we recalculate the position of each centroid. The new centroid for a cluster is simply the <strong>mean</strong> (average) of all the data points currently assigned to that cluster.</p> <table> <tbody> <tr> <td>If $S_j$ is the set of data points assigned to cluster $j$, and $</td> <td>\mathbf{S_j}</td> <td>$ is the number of points in that cluster, the new centroid $\mu_j$ is:</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$\mu_j = \frac{1}{</td> <td>S_j</td> <td>} \sum_{\mathbf{x} \in S_j} \mathbf{x}$</td> </tr> </tbody> </table> <p>This step ensures our “leaders” truly represent the center of their groups.</p> <p><strong>Step 5: Repeat Steps 3 and 4 until convergence</strong> Steps 3 and 4 are repeated iteratively. What does “convergence” mean here? It means one of two things usually happens:</p> <ul> <li>The centroids no longer move significantly between iterations.</li> <li>The assignment of data points to clusters no longer changes significantly.</li> </ul> <p>When either of these conditions is met, the algorithm has “converged,” and we have our final <code class="language-plaintext highlighter-rouge">k</code> clusters.</p> <h3 id="the-math-behind-the-magic-what-k-means-tries-to-optimize">The Math Behind the Magic: What K-Means Tries to Optimize</h3> <p>While the iterative process feels intuitive, there’s a mathematical objective function that K-Means implicitly tries to minimize. This function is called the <strong>Within-Cluster Sum of Squares (WCSS)</strong>, also sometimes referred to as <em>inertia</em>.</p> <p>The goal of K-Means is to make the clusters as “tight” as possible. We want data points within a cluster to be very close to their own centroid, and thus relatively far from other centroids. WCSS quantifies this “tightness.” It’s the sum of the squared distances between each data point and the centroid of the cluster it belongs to.</p> <p>The formula for WCSS (or the objective function $J$) is:</p> <table> <tbody> <tr> <td>$J = \sum_{j=1}^k \sum_{\mathbf{x} \in S_j}</td> <td> </td> <td>\mathbf{x} - \mu_j</td> <td> </td> <td>^2$</td> </tr> </tbody> </table> <p>Let’s break it down:</p> <ul> <li>$\sum_{j=1}^k$: This means we sum across all <code class="language-plaintext highlighter-rouge">k</code> clusters.</li> <li>$\sum_{\mathbf{x} \in S_j}$: For each cluster $j$, we sum across all data points $\mathbf{x}$ that belong to that cluster ($S_j$).</li> <li> <table> <tbody> <tr> <td>$</td> <td> </td> <td>\mathbf{x} - \mu_j</td> <td> </td> <td>^2$: This is the squared Euclidean distance between a data point $\mathbf{x}$ and its assigned centroid $\mu_j$. We square it to emphasize larger distances and to make the math easier (derivatives are nicer without the square root).</td> </tr> </tbody> </table> </li> </ul> <p>So, K-Means is essentially trying to find a configuration of <code class="language-plaintext highlighter-rouge">k</code> centroids and <code class="language-plaintext highlighter-rouge">k</code> cluster assignments such that this total sum of squared distances is as small as possible. It’s like trying to put all your LEGO bricks into <code class="language-plaintext highlighter-rouge">k</code> boxes, making sure each brick is as close as possible to the center of its own box.</p> <h3 id="when-to-embrace-k-means-and-when-to-be-cautious">When to Embrace K-Means (and When to Be Cautious)</h3> <p>K-Means is a fantastic algorithm, but like any tool, it has its strengths and weaknesses.</p> <p><strong>Strengths (Why we love it):</strong></p> <ul> <li> <strong>Simplicity and Speed</strong>: It’s easy to understand, implement, and computationally efficient, especially for large datasets.</li> <li> <strong>Scalability</strong>: It scales relatively well to a large number of samples and dimensions.</li> <li> <strong>Easy to Interpret</strong>: The centroids can often be interpreted as representatives or prototypes of their respective clusters.</li> </ul> <p><strong>Weaknesses (Things to watch out for):</strong></p> <ul> <li> <strong>The <code class="language-plaintext highlighter-rouge">k</code> Problem</strong>: You <em>must</em> specify <code class="language-plaintext highlighter-rouge">k</code> upfront. This is often the hardest part, as the “right” number of clusters isn’t always obvious.</li> <li> <strong>Sensitivity to Initial Centroids</strong>: Because it’s an iterative algorithm, K-Means can get stuck in “local optima.” If you start with a bad set of random centroids, you might end up with suboptimal clusters. The common practice is to run K-Means multiple times with different random initializations and pick the best result (lowest WCSS).</li> <li> <strong>Assumes Spherical Clusters of Similar Size</strong>: K-Means defines clusters by their centroids and the points closest to them, which inherently means it tends to form roughly spherical clusters. It struggles with clusters of irregular shapes (like crescent moons) or widely varying densities.</li> <li> <strong>Sensitivity to Outliers</strong>: Outliers can disproportionately affect centroid positions, “pulling” them away from the true center of a cluster and distorting the results.</li> <li> <strong>Requires Numeric Data</strong>: K-Means works with distances, so your data needs to be numerical. Categorical features often require special encoding.</li> </ul> <h3 id="the-million-dollar-question-how-do-you-choose-k">The Million-Dollar Question: How Do You Choose <code class="language-plaintext highlighter-rouge">k</code>?</h3> <p>Since choosing <code class="language-plaintext highlighter-rouge">k</code> is so crucial, data scientists have developed techniques to help.</p> <ol> <li> <p><strong>The Elbow Method</strong>: This is perhaps the most common heuristic. You run K-Means for a range of <code class="language-plaintext highlighter-rouge">k</code> values (e.g., from 1 to 10) and calculate the WCSS for each <code class="language-plaintext highlighter-rouge">k</code>. Then, you plot <code class="language-plaintext highlighter-rouge">k</code> against the WCSS.</p> <p>As <code class="language-plaintext highlighter-rouge">k</code> increases, the WCSS will always decrease (because with more clusters, points will generally be closer to their centroids). However, at some point, adding more clusters doesn’t significantly reduce the WCSS. This point, where the WCSS curve starts to flatten out, often resembles an “elbow” – and that’s usually a good candidate for the optimal <code class="language-plaintext highlighter-rouge">k</code>.</p> <p>Visually, you’re looking for the point of diminishing returns.</p> </li> <li> <p><strong>Silhouette Score</strong>: This is a more sophisticated metric that measures how similar a data point is to its own cluster compared to other clusters. The silhouette score for a point ranges from -1 to +1, where:</p> <ul> <li>+1 indicates the point is far away from neighboring clusters.</li> <li>0 indicates the point is on or very close to the decision boundary between two clusters.</li> <li>-1 indicates the point might be assigned to the wrong cluster.</li> </ul> <p>You calculate the average silhouette score for various <code class="language-plaintext highlighter-rouge">k</code> values, and the <code class="language-plaintext highlighter-rouge">k</code> that yields the highest average silhouette score is often considered the best choice.</p> </li> </ol> <h3 id="a-personal-reflection-my-first-brush-with-clarity">A Personal Reflection: My First Brush with Clarity</h3> <p>I remember the first time I truly grasped K-Means. It was during a university project, trying to segment customer reviews. The data was a mess – thousands of reviews, no labels, just raw text. My initial thought was, “How on Earth do I find groups in <em>this</em>?”</p> <p>Then K-Means was introduced. The idea that you could just <em>guess</em> centers, assign points, and then <em>refine</em> until things settled felt almost too simple to be powerful. But when I ran it, and actual, coherent topics started emerging in the clusters (e.g., one cluster had all the complaints about shipping, another about product quality, another about customer service), it felt like magic. It was a tangible demonstration of how structure can emerge from chaos through an elegant, iterative process. It solidified my appreciation for the beauty of algorithms that mirror human intuition.</p> <h3 id="wrapping-up-the-unsung-hero-of-data-exploration">Wrapping Up: The Unsung Hero of Data Exploration</h3> <p>K-Means Clustering is a cornerstone of unsupervised learning. It’s not always the flashiest algorithm, and it has its quirks, but its simplicity, efficiency, and effectiveness make it an invaluable tool in any data scientist’s toolkit. From customer segmentation and document clustering to image compression and anomaly detection, K-Means helps us make sense of the vast, unlabeled datasets that populate our world.</p> <p>So next time you encounter a dataset without labels, remember K-Means. It’s waiting to help you uncover the hidden shapes and patterns that lie within, turning data chaos into meaningful insights.</p> <p>Keep exploring, keep learning, and remember that sometimes, the simplest ideas hold the most profound power.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>