<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Beyond Accuracy: Charting Your Model's True Performance with ROC &amp; AUC | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/beyond-accuracy-charting-your-models-true-performa/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Beyond Accuracy: Charting Your Model's True Performance with ROC &amp; AUC</h1> <p class="post-meta"> Created on March 08, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/classification"> <i class="fa-solid fa-hashtag fa-sm"></i> Classification</a>   <a href="/blog/blog/tag/model-evaluation"> <i class="fa-solid fa-hashtag fa-sm"></i> Model Evaluation</a>   <a href="/blog/blog/tag/roc"> <i class="fa-solid fa-hashtag fa-sm"></i> ROC</a>   <a href="/blog/blog/tag/auc"> <i class="fa-solid fa-hashtag fa-sm"></i> AUC</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>Remember those early days in data science, fresh-faced and eager, when the holy grail of model evaluation felt like achieving that elusive 99% accuracy? I certainly do. I’d train a classification model, see a high accuracy score, and beam with pride. “Look at this masterpiece!” I’d think. But then, reality, as it often does, came knocking.</p> <p>What if my model was predicting something like a rare disease, where only 1% of the population has it? A model that <em>always</em> predicts “no disease” would achieve 99% accuracy, but it would be utterly useless – missing every single positive case! This experience taught me a crucial lesson: <strong>accuracy, by itself, is often not enough, and sometimes, it can be downright misleading.</strong></p> <p>This is where the real heroes of classification model evaluation step in: the <strong>Receiver Operating Characteristic (ROC) curve</strong> and the <strong>Area Under the ROC Curve (AUC)</strong>. These tools don’t just give you a single snapshot; they paint a comprehensive picture of your model’s performance across all possible scenarios, helping you understand its true potential.</p> <p>Ready to dive deep? Let’s chart this course together!</p> <h3 id="the-foundation-your-models-best-guess-and-the-confusion-matrix">The Foundation: Your Model’s Best Guess and the Confusion Matrix</h3> <p>Before we unleash the power of ROC and AUC, let’s quickly recap how a binary classification model works under the hood and what foundational metrics we derive from its predictions.</p> <p>Most classification models don’t just spit out a ‘0’ or ‘1’. Instead, they output a <strong>probability score</strong> (e.g., 0.7, 0.25) that a data point belongs to the positive class. To convert this probability into a definitive ‘0’ or ‘1’ prediction, we use a <strong>threshold</strong>. Typically, this threshold is 0.5: if the probability is $\geq 0.5$, it’s positive; otherwise, it’s negative. But – and this is a big “but” – this threshold is adjustable!</p> <p>Once we have actual labels and our model’s predictions (based on a chosen threshold), we can construct the legendary <strong>Confusion Matrix</strong>. This 2x2 table is the Rosetta Stone for understanding individual classification errors.</p> <table> <thead> <tr> <th style="text-align: left"> </th> <th style="text-align: left">Predicted Positive</th> <th style="text-align: left">Predicted Negative</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Actual Positive</strong></td> <td style="text-align: left">True Positive (TP)</td> <td style="text-align: left">False Negative (FN)</td> </tr> <tr> <td style="text-align: left"><strong>Actual Negative</strong></td> <td style="text-align: left">False Positive (FP)</td> <td style="text-align: left">True Negative (TN)</td> </tr> </tbody> </table> <p>Let’s quickly define these:</p> <ul> <li> <strong>True Positive (TP):</strong> We predicted positive, and it was actually positive. (Good!)</li> <li> <strong>True Negative (TN):</strong> We predicted negative, and it was actually negative. (Good!)</li> <li> <strong>False Positive (FP):</strong> We predicted positive, but it was actually negative. (Type I error, “false alarm”)</li> <li> <strong>False Negative (FN):</strong> We predicted negative, but it was actually positive. (Type II error, “miss”)</li> </ul> <p>The goal, of course, is to maximize TP and TN, while minimizing FP and FN. But as you’ll see, these are often in a delicate balance.</p> <h3 id="two-sides-of-the-same-coin-tpr-and-fpr">Two Sides of the Same Coin: TPR and FPR</h3> <p>From the Confusion Matrix, we derive several critical ratios, but for ROC and AUC, two stand out:</p> <ol> <li> <p><strong>True Positive Rate (TPR)</strong>: Also known as <strong>Sensitivity</strong> or <strong>Recall</strong>. \(TPR = \frac{TP}{TP + FN}\) This tells us: “Out of all the <em>actual positive</em> cases, how many did our model correctly identify?” A high TPR means our model is good at catching positives. In our disease example, it means catching most people who actually have the disease.</p> </li> <li> <p><strong>False Positive Rate (FPR)</strong>: \(FPR = \frac{FP}{FP + TN}\) This tells us: “Out of all the <em>actual negative</em> cases, how many did our model <em>incorrectly</em> identify as positive?” A low FPR means our model doesn’t cry “wolf!” too often. In the disease example, it means not telling too many healthy people they have the disease.</p> </li> </ol> <p>Now, here’s the kicker: <strong>TPR and FPR are often inversely related.</strong> If you make your model extremely sensitive (e.g., by lowering the probability threshold to catch almost <em>any</em> hint of a positive case), your TPR will likely go up. But what happens? You’ll also start misclassifying a lot more actual negative cases as positive, driving your FPR up. Conversely, if you make your model very conservative (e.g., by raising the threshold, only predicting positive when it’s <em>absolutely sure</em>), your FPR will likely drop, but you’ll probably miss a lot of actual positive cases, causing your TPR to fall.</p> <p>This trade-off is the heart of why a single accuracy score or a single set of TPR/FPR values based on one threshold isn’t enough.</p> <h3 id="the-roc-curve-a-visual-tale-of-trade-offs">The ROC Curve: A Visual Tale of Trade-offs</h3> <p>Imagine you’re running an experiment. You train your model and get a set of probability scores for your test data. Now, instead of just picking one threshold (like 0.5), what if you tried <em>every single possible threshold</em>?</p> <p>For each unique probability score output by your model, you could treat it as a potential threshold:</p> <ol> <li>Classify all data points based on this threshold.</li> <li>Build the confusion matrix.</li> <li>Calculate the TPR and FPR.</li> <li>Plot this (FPR, TPR) pair as a single point on a graph.</li> </ol> <p>If you connect all these points, you get the <strong>ROC Curve</strong>!</p> <ul> <li>The <strong>x-axis</strong> is the <strong>False Positive Rate (FPR)</strong>.</li> <li>The <strong>y-axis</strong> is the <strong>True Positive Rate (TPR)</strong>.</li> </ul> <p>Let’s visualize what different curves mean:</p> <ul> <li> <strong>The Perfect Classifier (Top-Left Corner: 0, 1):</strong> A magical model would achieve 100% TPR (all positives caught) with 0% FPR (no false alarms). Its ROC curve would shoot straight up the y-axis to (0,1) and then across to (1,1). We all dream of this model!</li> <li> <strong>The Random Classifier (Diagonal Line: y=x):</strong> A model that performs no better than random guessing would produce an ROC curve that follows the diagonal line from (0,0) to (1,1). For example, if it correctly identifies 50% of positives, it will also incorrectly identify 50% of negatives as positives. Not very useful.</li> <li> <strong>A Good Classifier:</strong> Its curve will generally bulge towards the top-left corner, staying as far away from the diagonal line as possible. The further it bows towards (0,1), the better its performance.</li> </ul> <p>Each point on the ROC curve represents a different threshold setting. If you want to be very cautious and minimize false alarms (low FPR), you might operate at a point towards the bottom-left of the curve. If missing a positive case is catastrophic, and you’re willing to accept more false alarms, you might operate at a point towards the top-right.</p> <p><strong>Why is this powerful?</strong> The ROC curve helps you visualize the entire spectrum of your model’s performance. It allows you to select an optimal threshold based on the specific costs of false positives and false negatives for <em>your particular problem</em>.</p> <h3 id="auc-the-single-number-to-rule-them-all-almost">AUC: The Single Number to Rule Them All (Almost!)</h3> <p>While the ROC curve is fantastic for visual analysis and understanding trade-offs, sometimes you need a single metric to compare models or summarize overall performance. Enter <strong>AUC</strong>, the <strong>Area Under the ROC Curve</strong>.</p> <p>As its name suggests, AUC is simply the area underneath the ROC curve. It condenses the entire curve into a single scalar value, ranging from 0 to 1.</p> <p><strong>What does AUC mean intuitively?</strong> The AUC score represents the probability that the classifier will rank a randomly chosen positive instance higher than a randomly chosen negative instance. In simpler terms, if you randomly pick one positive example and one negative example, AUC tells you the probability that your model will assign a higher probability score to the positive example than to the negative one.</p> <p>Let’s break down AUC values:</p> <ul> <li> <strong>AUC = 0.5:</strong> This is equivalent to the diagonal line, meaning your model performs no better than random guessing. It’s as good as flipping a coin.</li> <li> <strong>AUC = 1.0:</strong> This indicates a perfect classifier. It means the model can perfectly distinguish between positive and negative classes without any overlap in their probability distributions.</li> <li> <strong>AUC between 0.5 and 1.0:</strong> This indicates how well the model is distinguishing between the positive and negative classes. <ul> <li> <strong>0.7 - 0.8:</strong> Often considered an acceptable or fair model.</li> <li> <strong>0.8 - 0.9:</strong> Generally considered a good model.</li> <li> <strong>0.9 - 1.0:</strong> An excellent model.</li> </ul> </li> </ul> <p><strong>Why is AUC a superior metric to simple accuracy in many cases?</strong></p> <ol> <li> <strong>Threshold-Independence:</strong> Unlike accuracy, which depends on a single threshold, AUC evaluates the model’s performance across <em>all possible thresholds</em>. This means it assesses the model’s intrinsic ability to rank instances correctly, regardless of where you decide to cut off your predictions.</li> <li> <strong>Robust to Class Imbalance:</strong> Remember our rare disease example? A model achieving 99% accuracy by predicting ‘no disease’ for everyone would have an AUC of 0.5 (random guessing). AUC is not swayed by the proportion of positive to negative classes because TPR and FPR are calculated independently within their respective actual classes ($TP/(TP+FN)$ and $FP/(FP+TN)$).</li> <li> <strong>Compares Model Ranking Power:</strong> If Model A has a higher AUC than Model B, it generally means Model A is better at distinguishing between positive and negative classes across the board.</li> </ol> <h3 id="practical-application-a-quick-code-glimpse">Practical Application: A Quick Code Glimpse</h3> <p>In Python, using <code class="language-plaintext highlighter-rouge">scikit-learn</code>, calculating ROC and AUC is straightforward:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_curve</span><span class="p">,</span> <span class="n">auc</span>
<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="n">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Let's imagine we have some data and labels
# X, y are your features and target variable
# For demonstration, let's create some dummy data
</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">2.5</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span> <span class="c1"># A simple classification rule
</span>
<span class="c1"># Split data
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Train a simple classifier (e.g., Logistic Regression)
</span><span class="n">model</span> <span class="o">=</span> <span class="nc">LogisticRegression</span><span class="p">(</span><span class="n">solver</span><span class="o">=</span><span class="sh">'</span><span class="s">liblinear</span><span class="sh">'</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Get the probability scores for the positive class (class 1)
# model.predict_proba returns probabilities for [class 0, class 1]
</span><span class="n">y_scores</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>

<span class="c1"># Calculate FPR, TPR, and thresholds
</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> <span class="nf">roc_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_scores</span><span class="p">)</span>

<span class="c1"># Calculate AUC
</span><span class="n">roc_auc</span> <span class="o">=</span> <span class="nf">auc</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Model</span><span class="sh">'</span><span class="s">s AUC: </span><span class="si">{</span><span class="n">roc_auc</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Plotting the ROC curve
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">darkorange</span><span class="sh">'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">'</span><span class="s">ROC curve (AUC = </span><span class="si">{</span><span class="n">roc_auc</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">navy</span><span class="sh">'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Random Classifier (AUC = 0.5)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">False Positive Rate</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">True Positive Rate</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Receiver Operating Characteristic (ROC) Curve</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="sh">"</span><span class="s">lower right</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <p><em>(Note: The actual plot would appear here in a real blog post.)</em></p> <p>This code snippet gives you both the calculated AUC score and a visual representation of your model’s performance across different operating points. You can then analyze the curve to pick a threshold that aligns with your specific business needs. For example, if false positives are extremely costly (e.g., misdiagnosing a healthy patient with a severe illness), you might choose a threshold that yields a very low FPR, even if it means a slightly lower TPR.</p> <h3 id="when-to-think-beyond-auc">When to Think Beyond AUC</h3> <p>While ROC and AUC are incredibly powerful, no metric is a silver bullet. Here are a couple of situations where you might want to consider additional perspectives:</p> <ul> <li> <strong>Extreme Class Imbalance (especially very rare positive class):</strong> While AUC is robust to imbalance, if the positive class is exceedingly rare (e.g., 0.001%), even a very small number of false positives can dominate the performance metrics. In such scenarios, the <strong>Precision-Recall (PR) curve</strong> can often provide a more informative and pessimistic view, focusing directly on the performance for the positive class.</li> <li> <strong>Probability Calibration:</strong> AUC tells you about the <em>ranking</em> ability of your model, but it doesn’t tell you how well-calibrated your probabilities are. If your model predicts a probability of 0.8, is it truly correct 80% of the time? For applications where the predicted probability itself matters (e.g., risk assessment, financial modeling), you might need to look at calibration curves.</li> </ul> <h3 id="wrapping-up">Wrapping Up</h3> <p>ROC and AUC have truly transformed how I, and many data scientists, evaluate classification models. They push us past the simplistic notion of “accuracy” and encourage a deeper, more nuanced understanding of our models’ strengths and weaknesses.</p> <p>By understanding the trade-offs between True Positive Rate and False Positive Rate, visualized through the elegant ROC curve, and summarized by the robust AUC score, you gain an invaluable perspective on your model’s ability to discriminate between classes. This knowledge empowers you to choose the right model, set the appropriate thresholds, and ultimately, make better, more informed decisions.</p> <p>So, the next time you build a classifier, don’t just stop at accuracy. Unleash the power of ROC and AUC. Your models (and your stakeholders) will thank you!</p> <p>Happy modeling!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>