<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Predictable Random: My Journey into Markov Chains | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/the-predictable-random-my-journey-into-markov-chai/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Predictable Random: My Journey into Markov Chains</h1> <p class="post-meta"> Created on November 11, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/markov-chains"> <i class="fa-solid fa-hashtag fa-sm"></i> Markov Chains</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/probability"> <i class="fa-solid fa-hashtag fa-sm"></i> Probability</a>   <a href="/blog/blog/tag/sequential-data"> <i class="fa-solid fa-hashtag fa-sm"></i> Sequential Data</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>As a budding data scientist, I’m often struck by the elegant simplicity underlying some of the most profound concepts in our field. One such concept that captivated me early on, bridging the gap between abstract mathematics and real-world applications, is the <strong>Markov Chain</strong>. It’s a tool so versatile, it powers everything from Google’s PageRank algorithm to the predictive text on your phone.</p> <p>But what exactly <em>is</em> a Markov Chain? When I first encountered the term, it sounded intimidating, almost like something out of a dense theoretical physics textbook. Yet, as I dug deeper, I realized its core idea is beautifully intuitive. It’s about making predictions based on <em>just</em> the current situation, forgetting everything that happened before.</p> <p>Let’s dive in.</p> <h3 id="the-heart-of-the-matter-memorylessness">The Heart of the Matter: Memorylessness</h3> <p>Imagine you’re playing a board game. Your next move depends entirely on where your piece is <em>right now</em> and the roll of the dice. It doesn’t matter if you landed on that square two turns ago, or if your opponent moved five spaces forward in the last round. Your past moves are irrelevant to your <em>next</em> move; only your current position matters.</p> <p>This, in essence, is the <strong>memoryless property</strong>, the defining characteristic of a Markov Chain. Formally, it states that the probability of transitioning to any particular state depends solely on the current state and not on the sequence of events that preceded it.</p> <p>Let’s denote the state of our system at time $n$ as $X_n$. The memoryless property can be written as:</p> <table> <tbody> <tr> <td>$P(X_{n+1} = j</td> <td>X_n = i, X_{n-1} = i_{n-1}, \dots, X_0 = i_0) = P(X_{n+1} = j</td> <td>X_n = i)$</td> </tr> </tbody> </table> <p>This equation simply means: the probability of being in state $j$ at the next time step ($n+1$), given all past states up to the current state ($n$), is the same as the probability of being in state $j$ given <em>only</em> the current state ($i$). It’s a powerful simplification that makes complex systems tractable.</p> <h3 id="building-blocks-states-and-transitions">Building Blocks: States and Transitions</h3> <p>To really grasp Markov Chains, we need two fundamental components:</p> <ol> <li> <strong>States</strong>: These are the possible situations or conditions our system can be in. Think of them as discrete categories. <ul> <li> <strong>Example</strong>: If we’re modeling daily weather, our states might be {Sunny, Cloudy, Rainy}.</li> <li> <strong>Example</strong>: For a simple text generator, states could be individual words {the, cat, sat, on, mat}.</li> </ul> </li> <li> <strong>Transitions</strong>: These are the movements or changes from one state to another. Crucially, each transition has an associated probability. <ul> <li> <strong>Example (Weather)</strong>: If it’s Sunny today, there’s a certain probability it will be Sunny tomorrow, another probability it will be Cloudy, and another it will be Rainy.</li> </ul> </li> </ol> <p>Let’s consider a simple weather example, which is often the first illustration I encountered and found most helpful.</p> <p><strong>Our Weather Model:</strong> Assume we have three states: $S_1$ (Sunny), $S_2$ (Cloudy), $S_3$ (Rainy). The probabilities of transitioning from one state to another might look like this:</p> <ul> <li> <strong>If today is Sunny:</strong> <ul> <li>Tomorrow is Sunny: 70% chance</li> <li>Tomorrow is Cloudy: 20% chance</li> <li>Tomorrow is Rainy: 10% chance</li> </ul> </li> <li> <strong>If today is Cloudy:</strong> <ul> <li>Tomorrow is Sunny: 30% chance</li> <li>Tomorrow is Cloudy: 40% chance</li> <li>Tomorrow is Rainy: 30% chance</li> </ul> </li> <li> <strong>If today is Rainy:</strong> <ul> <li>Tomorrow is Sunny: 10% chance</li> <li>Tomorrow is Cloudy: 40% chance</li> <li>Tomorrow is Rainy: 50% chance</li> </ul> </li> </ul> <p>Notice an important detail: for each current state, the probabilities of transitioning to <em>all possible next states</em> must sum up to 1 (or 100%). You can’t just vanish into thin air, you <em>must</em> transition to one of the defined states.</p> <h3 id="the-transition-matrix-our-map-of-possibilities">The Transition Matrix: Our Map of Possibilities</h3> <p>These transition probabilities can be elegantly represented in a square matrix called the <strong>transition matrix</strong>, often denoted by $P$. Each element $P_{ij}$ represents the probability of moving from state $i$ to state $j$.</p> <p>For our weather example, the transition matrix $P$ would be:</p> <p>$P = \begin{pmatrix} 0.7 &amp; 0.2 &amp; 0.1 <br> 0.3 &amp; 0.4 &amp; 0.3 <br> 0.1 &amp; 0.4 &amp; 0.5 \end{pmatrix}$</p> <p>Here:</p> <ul> <li>Row 1: Probabilities of transitioning <em>from</em> Sunny (to Sunny, Cloudy, Rainy).</li> <li>Row 2: Probabilities of transitioning <em>from</em> Cloudy (to Sunny, Cloudy, Rainy).</li> <li>Row 3: Probabilities of transitioning <em>from</em> Rainy (to Sunny, Cloudy, Rainy).</li> </ul> <p>Each row sums to 1. This matrix is the entire “memory” of our Markov Chain; it’s all the information we need to predict the future.</p> <h3 id="simulating-the-future-one-step-at-a-time">Simulating the Future (One Step at a Time)</h3> <p>With our transition matrix, we can simulate the weather day by day. Let’s say today is Sunny. We’d pick a random number between 0 and 1.</p> <ul> <li>If the number is between 0 and 0.7, tomorrow is Sunny.</li> <li>If between 0.7 and 0.9 (0.7+0.2), tomorrow is Cloudy.</li> <li>If between 0.9 and 1.0 (0.9+0.1), tomorrow is Rainy.</li> </ul> <p>This process gives us a sequence of states. If we start Sunny, we might get: Sunny -&gt; Sunny -&gt; Cloudy -&gt; Rainy -&gt; Cloudy -&gt; …</p> <p>What if we want to know the probability of the weather being Sunny, Cloudy, or Rainy <em>two</em> days from now, given it’s Sunny today? We can achieve this by multiplying the transition matrix by itself. If $P$ gives us the probabilities for 1 step, $P^2$ gives us the probabilities for 2 steps, $P^3$ for 3 steps, and so on.</p> <p>Let $p^{(n)}_i$ be the probability of being in state $i$ after $n$ steps. If we start in a specific state, say Sunny (represented as a row vector $[1, 0, 0]$), then the probability distribution after $n$ steps would be:</p> <p>$p^{(n)} = p^{(0)} P^n$</p> <p>Where $p^{(0)}$ is our initial state distribution (e.g., $[1, 0, 0]$ if we <em>know</em> it’s Sunny today).</p> <h3 id="the-long-run-reaching-a-steady-state">The Long Run: Reaching a Steady State</h3> <p>One of the most fascinating aspects of Markov Chains is their long-term behavior. If you run a Markov Chain for a very long time, through many, many transitions, the probability of being in any particular state often stabilizes. It reaches a <strong>stationary distribution</strong> (also called an equilibrium or steady-state distribution).</p> <p>Imagine running our weather simulation for a year, or even ten years. After a while, the percentage of days that are Sunny, Cloudy, or Rainy will tend to settle into a fixed proportion. It won’t matter what the weather was on day one; the influence of the initial state fades over time.</p> <p>We can find this stationary distribution, denoted by $\pi = [\pi_1, \pi_2, \pi_3, \dots, \pi_k]$ (where $\pi_i$ is the long-run probability of being in state $i$), by solving the following equation:</p> <p>$\pi P = \pi$</p> <p>This means that if we are in the stationary distribution $\pi$, applying one more transition (multiplying by $P$) does not change the distribution. Additionally, as with any probability distribution, the sum of all probabilities must be 1:</p> <p>$\sum_{i} \pi_i = 1$</p> <p>Solving these equations (a system of linear equations) yields the long-term probabilities for each state. For our weather example, if we solved for $\pi$, we might find something like $\pi = [0.4, 0.35, 0.25]$. This would imply that in the long run, 40% of days are Sunny, 35% are Cloudy, and 25% are Rainy, regardless of today’s weather.</p> <h3 id="where-markov-chains-shine-real-world-applications">Where Markov Chains Shine: Real-World Applications</h3> <p>The simplicity and power of Markov Chains make them invaluable in a surprising array of fields:</p> <ol> <li> <p><strong>Google’s PageRank Algorithm</strong>: This is perhaps the most famous application. PageRank models a “random surfer” who clicks on links. Each webpage is a “state,” and the probability of transitioning from one page to another is based on the links present. The stationary distribution of this Markov Chain represents the long-term probability of the random surfer being on any given page, effectively determining the page’s importance or “rank.”</p> </li> <li> <strong>Natural Language Processing (NLP)</strong>: <ul> <li> <strong>Text Generation</strong>: Markov Chains can be used to generate text that mimics the style of a given corpus. Each word (or character) is a state, and the transition probabilities are learned from how often one word follows another. This is how early predictive text systems or “Markov text generators” work.</li> <li> <strong>Speech Recognition</strong>: More advanced versions, like Hidden Markov Models (HMMs), are fundamental in speech recognition, where the underlying “states” (e.g., phonemes, words) are hidden, and we observe only the output (audio signals).</li> </ul> </li> <li> <p><strong>Genetics and Biology</strong>: Markov Chains are used to model DNA sequences, protein folding, and population dynamics. For example, modeling mutations or the spread of diseases.</p> </li> <li> <p><strong>Finance</strong>: While financial markets are notoriously complex, simple Markov Chains can be used to model stock price movements (e.g., going up, down, or staying flat), though often more sophisticated models are employed.</p> </li> <li> <strong>Queueing Theory</strong>: Analyzing waiting lines and customer service systems often involves Markov Chains, modeling the number of customers in a queue as a state.</li> </ol> <h3 id="beyond-the-basics-limitations-and-extensions">Beyond the Basics: Limitations and Extensions</h3> <p>While incredibly powerful, the memoryless property is also the main limitation of a simple Markov Chain. In many real-world scenarios, the past <em>does</em> matter beyond just the immediate previous state.</p> <p>This is where more advanced concepts come into play:</p> <ul> <li> <strong>Higher-Order Markov Chains</strong>: These models consider more than just the immediate previous state. For example, a second-order Markov Chain would base the next state on the <em>two</em> previous states.</li> <li> <strong>Hidden Markov Models (HMMs)</strong>: As mentioned earlier, HMMs are used when the states themselves are not directly observable, but we can observe emissions or outputs that depend on those states. Think of trying to infer someone’s mood (hidden state) based on their tone of voice and facial expressions (observations).</li> <li> <strong>Markov Decision Processes (MDPs)</strong>: These extend Markov Chains by adding decisions or actions that an agent can take, influencing the transitions and leading to rewards. This forms the foundation of Reinforcement Learning, where an agent learns optimal policies by interacting with an environment.</li> </ul> <h3 id="my-ongoing-journey">My Ongoing Journey</h3> <p>My exploration of Markov Chains continues. What started as a simple concept of “memorylessness” has unfolded into a rich tapestry of applications and theoretical depth. From understanding how my favorite search engine works to appreciating the nuances of natural language, Markov Chains offer a robust framework for modeling sequential data.</p> <p>If you’re just starting out in data science or machine learning, I highly encourage you to spend some time with Markov Chains. They provide a fantastic stepping stone into understanding stochastic processes, dynamic systems, and the elegant ways mathematics can model the seemingly random patterns of our world. It’s a journey well worth taking!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>