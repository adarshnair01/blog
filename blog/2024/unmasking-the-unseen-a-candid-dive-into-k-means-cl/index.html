<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Unmasking the Unseen: A Candid Dive into K-Means Clustering | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/unmasking-the-unseen-a-candid-dive-into-k-means-cl/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Unmasking the Unseen: A Candid Dive into K-Means Clustering</h1> <p class="post-meta"> Created on March 25, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/k-means"> <i class="fa-solid fa-hashtag fa-sm"></i> K-Means</a>   <a href="/blog/blog/tag/clustering"> <i class="fa-solid fa-hashtag fa-sm"></i> Clustering</a>   <a href="/blog/blog/tag/unsupervised-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Unsupervised Learning</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>Welcome back to the lab – or rather, my little corner of the internet where we dissect fascinating bits of data science. Today, we’re tackling a concept that, despite its simplicity, underpins countless real-world applications: <strong>K-Means Clustering</strong>.</p> <p>Imagine you’ve just inherited a massive pile of LEGO bricks. They’re all mixed up – different colors, different shapes, different sizes. Your task? To sort them into meaningful groups. But here’s the catch: nobody told you what the “right” groups are. You just have to figure it out.</p> <p>Sound familiar? This is the core challenge of <strong>unsupervised learning</strong>, a branch of machine learning where we deal with unlabeled data. We don’t have answers or target variables; instead, we’re looking for inherent patterns, structures, or groupings within the data itself. And that, my friends, is precisely where K-Means Clustering shines.</p> <h3 id="the-big-idea-finding-natural-neighborhoods">The Big Idea: Finding Natural Neighborhoods</h3> <p>At its heart, K-Means is an algorithm that partitions <code class="language-plaintext highlighter-rouge">n</code> observations into <code class="language-plaintext highlighter-rouge">k</code> clusters. The goal is simple: each observation belongs to the cluster whose mean (or “centroid”) is closest to it. Think of it like a game of musical chairs, but for data points, with the centroids being the chairs.</p> <p>Why “K-Means”?</p> <ul> <li> <strong>K</strong> represents the number of clusters we want to find.</li> <li> <strong>Means</strong> refers to the average position (centroid) of the data points within each cluster.</li> </ul> <p>It’s like saying, “Hey, I believe there are <code class="language-plaintext highlighter-rouge">K</code> natural groups here. Let’s find the ‘center’ of each group and then assign every data point to its closest center.” Elegant, right?</p> <h3 id="how-k-means-works-a-step-by-step-journey">How K-Means Works: A Step-by-Step Journey</h3> <p>Let’s break down the K-Means algorithm into its core components. Picture yourself as the conductor of an orchestra, guiding each data point to its harmonious section.</p> <h4 id="step-1-choose-your-k-the-number-of-clusters">Step 1: Choose Your <code class="language-plaintext highlighter-rouge">K</code> (The Number of Clusters)</h4> <p>This is perhaps the most crucial initial decision. Before K-Means can do its magic, you, the data scientist, must tell it how many clusters (<code class="language-plaintext highlighter-rouge">K</code>) you want to find. There’s no single “right” way to choose <code class="language-plaintext highlighter-rouge">K</code> beforehand; it’s often a blend of domain knowledge and empirical methods (which we’ll touch on later).</p> <p>For now, let’s assume we’ve decided on a <code class="language-plaintext highlighter-rouge">K</code>. For instance, if we’re segmenting customers, we might decide <code class="language-plaintext highlighter-rouge">K=3</code> for “high-value,” “medium-value,” and “low-value” groups.</p> <h4 id="step-2-initialize-centroids">Step 2: Initialize Centroids</h4> <p>With <code class="language-plaintext highlighter-rouge">K</code> chosen, the algorithm needs a starting point. It randomly selects <code class="language-plaintext highlighter-rouge">K</code> data points from your dataset to serve as the initial centroids (the “centers” of your clusters). These initial centroids are just educated guesses; they’ll move around quite a bit as the algorithm progresses.</p> <p><strong>Why random?</strong> Well, it’s simple, but it also means that different initializations can lead to slightly different final clusterings. More on that later!</p> <h4 id="step-3-assign-points-to-clusters-the-e-for-expectation-step">Step 3: Assign Points to Clusters (The “E” for Expectation Step)</h4> <p>Now the real work begins! For every single data point in your dataset, the algorithm calculates its distance to <em>each</em> of the <code class="language-plaintext highlighter-rouge">K</code> centroids. Whichever centroid is closest, that’s the cluster the data point is assigned to.</p> <p>How do we measure “closeness”? The most common method is the <strong>Euclidean distance</strong>, which you might remember from geometry class. For two points, $\mathbf{x} = (x_1, x_2, \dots, x_D)$ and $\mathbf{c} = (c_1, c_2, \dots, c_D)$ in D-dimensional space, the Euclidean distance is:</p> \[d(\mathbf{x}, \mathbf{c}) = \sqrt{\sum_{i=1}^D (x_i - c_i)^2}\] <p>Think of it as the straight-line distance between two points. Every data point essentially “votes” for its nearest centroid, establishing its initial cluster membership.</p> <h4 id="step-4-update-centroids-the-m-for-maximization-step">Step 4: Update Centroids (The “M” for Maximization Step)</h4> <p>Once all data points have been assigned to their nearest cluster, the centroids themselves need to move. Each centroid is recalculated by taking the <em>mean</em> (average) of all the data points currently assigned to its cluster.</p> <p>If $C_j$ represents the set of data points assigned to cluster $j$, then the new centroid $\mathbf{c}_j$ for that cluster is:</p> \[\mathbf{c}_j = \frac{1}{|C_j|} \sum_{\mathbf{x} \in C_j} \mathbf{x}\] <p>This step is crucial because it ensures that each centroid is truly at the “center” of its assigned points, reflecting the current best guess for the cluster’s location.</p> <h4 id="step-5-repeat-until-convergence">Step 5: Repeat Until Convergence</h4> <p>Steps 3 and 4 are repeated iteratively. Data points are reassigned to their nearest <em>newly moved</em> centroids, and then the centroids are recalculated again. This process continues until one of two conditions is met:</p> <ol> <li> <strong>Convergence:</strong> The centroids no longer move significantly between iterations, meaning the cluster assignments have stabilized. The clusters have found their “happy places.”</li> <li> <strong>Maximum Iterations:</strong> A predefined maximum number of iterations is reached, preventing the algorithm from running indefinitely.</li> </ol> <p>When the algorithm converges, you’re left with <code class="language-plaintext highlighter-rouge">K</code> distinct clusters, each with its own centroid, and every data point neatly assigned to one of them.</p> <h3 id="the-math-behind-the-magic-the-objective-function">The Math Behind the Magic: The Objective Function</h3> <p>While K-Means seems intuitive, it’s actually solving an optimization problem. The algorithm strives to minimize something called the <strong>Within-Cluster Sum of Squares (WCSS)</strong>, also known as <strong>Inertia</strong>.</p> <p>WCSS is the sum of the squared distances between each data point and the centroid of the cluster it belongs to.</p> \[J = \sum_{j=1}^K \sum_{\mathbf{x} \in C_j} \| \mathbf{x} - \mathbf{c}_j \|^2\] <p>Here:</p> <ul> <li>$K$ is the number of clusters.</li> <li>$C_j$ is the set of points in cluster $j$.</li> <li>$\mathbf{x}$ is a data point.</li> <li>$\mathbf{c}_j$ is the centroid of cluster $j$.</li> <li>$| \mathbf{x} - \mathbf{c}_j |^2$ is the squared Euclidean distance between point $\mathbf{x}$ and centroid $\mathbf{c}_j$.</li> </ul> <p>Minimizing WCSS means we want to make the clusters as “tight” and compact as possible. We want points within a cluster to be very close to their centroid, implying a strong similarity. K-Means guarantees that it will converge to a local minimum of this objective function.</p> <h3 id="choosing-your-k-the-elbow-method">Choosing Your <code class="language-plaintext highlighter-rouge">K</code>: The Elbow Method</h3> <p>“But wait,” you might say, “how do I pick that initial <code class="language-plaintext highlighter-rouge">K</code>?” Great question! While domain knowledge is often king, a common heuristic is the <strong>Elbow Method</strong>.</p> <p>The idea is to run K-Means for a range of <code class="language-plaintext highlighter-rouge">K</code> values (e.g., from 1 to 10) and calculate the WCSS for each <code class="language-plaintext highlighter-rouge">K</code>. Then, you plot the WCSS values against the corresponding <code class="language-plaintext highlighter-rouge">K</code> values.</p> <p>As <code class="language-plaintext highlighter-rouge">K</code> increases, the WCSS will generally decrease because having more clusters means points can be closer to their respective centroids. However, at some point, adding more clusters provides diminishing returns – the decrease in WCSS slows down significantly. This point often looks like an “elbow” in the plot.</p> <p>The “elbow” indicates a good balance between having too few clusters (high WCSS) and too many clusters (overfitting and less interpretability). It’s a pragmatic approach, though not always perfectly clear-cut.</p> <h3 id="strengths-of-k-means">Strengths of K-Means</h3> <ul> <li> <strong>Simplicity and Interpretability:</strong> It’s straightforward to understand and implement. The clusters are defined by their centroids, which are easy to interpret.</li> <li> <strong>Computational Efficiency:</strong> For datasets with a large number of observations, K-Means is generally quite fast, especially compared to more complex clustering algorithms. Its time complexity is approximately $O(n \cdot K \cdot D \cdot I)$, where $n$ is data points, $K$ is clusters, $D$ is dimensions, and $I$ is iterations.</li> <li> <strong>Versatility:</strong> It’s widely used across various domains for tasks like customer segmentation, document analysis, image compression, and anomaly detection.</li> </ul> <h3 id="limitations-and-considerations">Limitations and Considerations</h3> <p>No algorithm is perfect, and K-Means has its quirks:</p> <ul> <li> <strong>Sensitive to Initialization:</strong> Because centroids are randomly initialized, different runs of K-Means can yield different clusterings, especially with suboptimal starting points. This is often mitigated by running the algorithm multiple times with different initializations and choosing the result with the lowest WCSS.</li> <li> <strong>Requires Specifying <code class="language-plaintext highlighter-rouge">K</code>:</strong> As discussed, deciding <code class="language-plaintext highlighter-rouge">K</code> beforehand can be challenging without prior domain knowledge.</li> <li> <strong>Assumes Spherical Clusters:</strong> K-Means implicitly assumes that clusters are roughly spherical and of similar size and density. It struggles with clusters of irregular shapes (e.g., crescent moons, intertwined spirals) or varying densities.</li> <li> <strong>Sensitive to Outliers:</strong> Outliers can drastically pull centroids towards them, distorting cluster boundaries. Preprocessing steps like outlier detection or using more robust clustering methods might be necessary.</li> <li> <strong>Numerical Data Only:</strong> K-Means works with numerical data. Categorical features often require encoding before being used with K-Means.</li> </ul> <h3 id="real-world-applications">Real-World Applications</h3> <p>Beyond the LEGO bricks, K-Means is a workhorse in the real world:</p> <ul> <li> <strong>Customer Segmentation:</strong> Grouping customers based on purchasing behavior or demographics to tailor marketing strategies.</li> <li> <strong>Image Compression:</strong> Reducing the number of colors in an image by clustering similar colors together, representing them with their centroid color.</li> <li> <strong>Document Clustering:</strong> Organizing large collections of text documents into topics for easier navigation and analysis.</li> <li> <strong>Geospatial Analysis:</strong> Identifying areas with similar characteristics, such as crime hotspots or regions with similar ecological features.</li> <li> <strong>Anomaly Detection:</strong> Data points that are far from any cluster centroid could be identified as anomalies or outliers.</li> </ul> <h3 id="wrapping-up">Wrapping Up</h3> <p>K-Means Clustering, despite its simplicity, is a powerful and versatile tool in the unsupervised learning arsenal. It’s a fantastic entry point for understanding how machines can find structure in data without explicit guidance. While it has its limitations, knowing when and how to apply it can unlock valuable insights from your datasets.</p> <p>So, the next time you look at a scattered pile of data, remember the magic of K-Means, ready to bring order and reveal the hidden groupings within. Go forth and cluster!</p> <hr> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>