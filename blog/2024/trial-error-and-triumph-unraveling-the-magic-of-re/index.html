<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Trial, Error, and Triumph: Unraveling the Magic of Reinforcement Learning | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/trial-error-and-triumph-unraveling-the-magic-of-re/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Trial, Error, and Triumph: Unraveling the Magic of Reinforcement Learning</h1> <p class="post-meta"> Created on May 03, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/reinforcement-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Reinforcement Learning</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/agent"> <i class="fa-solid fa-hashtag fa-sm"></i> Agent</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>Have you ever wondered how a baby learns to walk? Or how you got good at your favorite video game? You probably didn’t read a manual covering every single scenario. Instead, you tried things, stumbled, got back up, and slowly, surely, you figured it out. You learned by <em>doing</em>.</p> <p>This fundamental human (and animal) way of learning—through interaction, feedback, and experience—is precisely what Reinforcement Learning (RL) tries to emulate in machines. It’s a field that has completely captivated me, and today, I want to take you on a journey into its core principles. Get ready to explore how we’re teaching machines to become self-improving agents!</p> <h3 id="beyond-supervised-the-learning-by-doing-paradigm">Beyond Supervised: The “Learning by Doing” Paradigm</h3> <p>You’re probably familiar with Machine Learning’s two most common flavors:</p> <ol> <li> <strong>Supervised Learning:</strong> Learning from labeled examples (e.g., “this is a cat,” “that’s a dog”). We provide the answers, and the model learns to predict them.</li> <li> <strong>Unsupervised Learning:</strong> Finding patterns in unlabeled data (e.g., grouping similar customers). We let the model discover structure on its own.</li> </ol> <p>Reinforcement Learning is different. It doesn’t rely on a dataset of correct answers, nor does it just find hidden structures. Instead, an RL agent learns to <em>make decisions</em> in an <em>environment</em> to achieve a <em>goal</em>. It’s like training a pet: you don’t tell it <em>exactly</em> what to do at every step; you reward desired behaviors and discourage undesirable ones. Over time, the pet figures out the optimal strategy.</p> <h3 id="the-rl-sandbox-agents-environments-and-rewards">The RL Sandbox: Agents, Environments, and Rewards</h3> <p>Let’s break down the key players in the RL game:</p> <ol> <li> <strong>The Agent:</strong> This is our learner, the decision-maker. Think of it as the robot, the AI character in a game, or the algorithm trying to manage a data center’s energy consumption.</li> <li> <strong>The Environment:</strong> This is everything the agent interacts with. It could be a physical maze, a chessboard, a video game world, or a complex simulation of a financial market.</li> <li> <strong>State ($S_t$):</strong> At any given moment, the environment is in a particular <em>state</em>. For a robot in a maze, the state might be its current coordinates. For a game AI, it could be the entire game board configuration.</li> <li> <strong>Action ($A_t$):</strong> Based on its current state, the agent chooses an <em>action</em> to take. The robot moves left, the AI moves a chess piece, the energy manager adjusts server power.</li> <li> <strong>Reward ($R_{t+1}$):</strong> After taking an action, the environment provides immediate feedback in the form of a scalar <em>reward</em>. This is the agent’s guiding signal. A positive reward means “good job!”, a negative reward means “not so good.” In our maze, reaching the exit might be +100, hitting a wall -1, and each step taken -0.1 (to encourage efficiency).</li> </ol> <p>This interaction forms a loop:</p> <p>$S_t \xrightarrow{A_t} R_{t+1}, S_{t+1} \xrightarrow{A_{t+1}} R_{t+2}, S_{t+2} \dots$</p> <p>The agent observes the environment’s state, takes an action, receives a reward, and transitions to a new state. This cycle continues, forming an “episode” of interaction.</p> <h3 id="the-agents-strategy-the-policy">The Agent’s Strategy: The Policy</h3> <p>How does an agent decide which action to take? It follows a <em>policy</em>, denoted by $\pi$. A policy is essentially the agent’s strategy or rulebook. It maps states to actions.</p> <table> <tbody> <tr> <td>Formally, a policy $\pi(a</td> <td>s)$ gives the probability of taking action $a$ when in state $s$. The ultimate goal of RL is to find an <em>optimal policy</em>, $\pi^*$, that maximizes the total expected reward over the long run.</td> </tr> </tbody> </table> <h3 id="thinking-long-term-value-functions-and-the-discount-factor">Thinking Long-Term: Value Functions and the Discount Factor</h3> <p>If an agent only cared about immediate rewards, it might make short-sighted decisions. Imagine you’re in a maze, and there’s a small reward for turning right immediately, but a much larger reward for going left and navigating a longer path to the exit. A short-sighted agent would always turn right.</p> <p>This is why RL agents need to think long-term. They need to estimate the <em>value</em> of being in a certain state, or taking a certain action from a state, considering all future rewards. This is where <strong>Value Functions</strong> come in.</p> <p>We use a <strong>discount factor</strong> ($\gamma$, a number between 0 and 1) to weigh immediate rewards more heavily than future rewards. A reward received now is worth more than the same reward received later.</p> <p>The total <em>return</em> from time $t$ onwards is: $G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$</p> <p>Now, we can define our value functions:</p> <ol> <li> <p><strong>State-Value Function ($V^\pi(s)$):</strong> This tells us <em>how good it is for the agent to be in a particular state s</em> if it follows policy $\pi$. It’s the expected return (sum of future discounted rewards) starting from state $s$ and then consistently following policy $\pi$. $V^\pi(s) = E_\pi [G_t | S_t = s]$</p> </li> <li> <p><strong>Action-Value Function ($Q^\pi(s,a)$):</strong> This tells us <em>how good it is to take a specific action a from a particular state s</em> if it then follows policy $\pi$. This is often more useful because if we know the $Q$-values for all possible actions from a state, we can simply pick the action with the highest $Q$-value! $Q^\pi(s,a) = E_\pi [G_t | S_t = s, A_t = a]$</p> </li> </ol> <p>The optimal policy $\pi^<em>$ is the one that achieves the highest possible value function ($V^</em>(s)$ or $Q^<em>(s,a)$). Once we have $Q^</em>(s,a)$, finding the optimal action is as simple as: $\pi^<em>(s) = \arg\max_{a} Q^</em>(s,a)$</p> <h3 id="the-big-challenge-exploration-vs-exploitation">The Big Challenge: Exploration vs. Exploitation</h3> <p>A crucial dilemma in RL is the <strong>exploration-exploitation trade-off</strong>.</p> <ul> <li> <strong>Exploitation:</strong> The agent uses its current knowledge to choose the action it believes will yield the most reward. It exploits what it already knows.</li> <li> <strong>Exploration:</strong> The agent tries new, potentially suboptimal actions to discover more about the environment and potentially find even better rewards.</li> </ul> <p>If an agent only exploits, it might get stuck in a locally optimal solution, never finding the true best path. If it only explores, it wastes time taking random actions. A common strategy to balance these is the $\epsilon$-greedy approach: with a small probability $\epsilon$, the agent chooses a random action (explores); otherwise, it chooses the action with the highest estimated $Q$-value (exploits).</p> <h3 id="learning-algorithms-how-agents-get-smart">Learning Algorithms: How Agents Get Smart</h3> <p>So, how do agents actually learn $V^\pi(s)$ or $Q^\pi(s,a)$ without knowing the entire environment upfront?</p> <h4 id="1-monte-carlo-mc-learning">1. Monte Carlo (MC) Learning</h4> <p>Imagine playing an entire game of chess. At the end, you know if you won or lost. Monte Carlo methods learn by completing full “episodes” (e.g., an entire game), calculating the total return for each state-action pair encountered, and then averaging these returns over many episodes. It’s like playing a game hundreds of times and then averaging the scores for each move you made.</p> <h4 id="2-temporal-difference-td-learning">2. Temporal Difference (TD) Learning</h4> <p>While Monte Carlo waits until the <em>end</em> of an episode, TD learning is more impatient. It learns <em>mid-episode</em>, updating its estimates based on the immediate reward and its estimate of the <em>next</em> state’s value. This is called <strong>bootstrapping</strong>.</p> <p>The core idea is that if your current estimate for $V(S_t)$ is good, then $R_{t+1} + \gamma V(S_{t+1})$ (the immediate reward plus the discounted value of the next state) should also be a good estimate for $V(S_t)$. The difference between these two is the <strong>TD Error</strong>:</p> <p>$TD_Error = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)$</p> <p>We then use this error to update our value function estimate:</p> <p>$V(S_t) \leftarrow V(S_t) + \alpha [R_{t+1} + \gamma V(S_{t+1}) - V(S_t)]$</p> <p>Here, $\alpha$ is the learning rate, controlling how big a step we take in updating our estimate.</p> <p>One of the most famous TD algorithms is <strong>Q-Learning</strong>. It’s an <em>off-policy</em> algorithm, meaning it learns the optimal Q-function ($Q^*(s,a)$) even while following an exploratory policy (like $\epsilon$-greedy). The Q-learning update rule is:</p> <p>$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma \max_{a’} Q(S_{t+1}, a’) - Q(S_t, A_t)]$</p> <p>This means we update our estimate for $Q(S_t, A_t)$ towards the immediate reward plus the discounted <em>maximum</em> Q-value of the next state ($S_{t+1}$), which represents the best possible future.</p> <h3 id="deep-reinforcement-learning-when-things-get-complex">Deep Reinforcement Learning: When Things Get Complex</h3> <p>What if our states are images, or there are millions of possible states? Traditional tabular methods (where we store Q-values in a table) become impossible. This is where <strong>Deep Reinforcement Learning (DRL)</strong> comes in.</p> <table> <tbody> <tr> <td>DRL combines the power of deep neural networks with RL algorithms. Instead of a table, a neural network acts as a <em>function approximator</em> for our value function ($Q(s,a)$) or our policy ($\pi(a</td> <td>s)$).</td> </tr> </tbody> </table> <ul> <li> <strong>Deep Q-Networks (DQN):</strong> Pioneered by DeepMind, DQN used a neural network to estimate the Q-values. It revolutionized RL by defeating human experts in various Atari games. Key innovations included <em>experience replay</em> (storing past experiences and replaying them to the network, breaking correlations) and <em>target networks</em> (a separate, slowly updated network for generating target Q-values, improving stability).</li> <li> <strong>Policy Gradient Methods:</strong> Instead of learning value functions, these methods directly learn a policy that maps states to actions. They essentially optimize the parameters of the policy network to maximize the expected return. Algorithms like <strong>REINFORCE</strong> and <strong>Actor-Critic</strong> (where an “actor” learns the policy and a “critic” learns the value function to guide the actor) fall into this category.</li> </ul> <h3 id="real-world-magic-where-rl-shines">Real-World Magic: Where RL Shines</h3> <p>RL is no longer just a theoretical concept; it’s powering incredible advancements:</p> <ul> <li> <strong>Game Playing:</strong> From AlphaGo conquering the ancient game of Go to OpenAI Five dominating Dota 2, RL agents have achieved superhuman performance in complex games.</li> <li> <strong>Robotics:</strong> Teaching robots delicate manipulation tasks, walking, and navigating complex environments.</li> <li> <strong>Self-Driving Cars:</strong> Training autonomous vehicles in simulations to handle diverse driving scenarios and make optimal decisions.</li> <li> <strong>Resource Management:</strong> Optimizing energy consumption in data centers (Google’s DeepMind used RL to cut cooling costs by 40%!).</li> <li> <strong>Personalized Recommendations:</strong> Fine-tuning recommendation engines for a more engaging user experience.</li> <li> <strong>Drug Discovery &amp; Materials Science:</strong> Exploring vast chemical spaces to find new molecules with desired properties.</li> </ul> <h3 id="the-road-ahead-challenges-and-opportunities">The Road Ahead: Challenges and Opportunities</h3> <p>Despite its triumphs, RL is still an active research area. Some key challenges include:</p> <ul> <li> <strong>Sample Efficiency:</strong> RL often requires a massive amount of interaction (data) with the environment to learn effectively, which can be expensive or time-consuming in the real world.</li> <li> <strong>Exploration in Complex Environments:</strong> Ensuring the agent explores effectively without getting stuck in local optima.</li> <li> <strong>Safety and Interpretability:</strong> How can we guarantee RL agents behave safely, and can we understand <em>why</em> they make certain decisions?</li> <li> <strong>Real-world Deployment:</strong> Bridging the gap between simulation-trained agents and robust performance in the physical world.</li> </ul> <p>The future of Reinforcement Learning is incredibly exciting. Imagine agents that can learn continuously in dynamic environments, collaborate with humans, and adapt to unforeseen circumstances. We’re moving towards a future where machines don’t just follow instructions but truly learn to understand and interact with the world around them.</p> <h3 id="my-take">My Take</h3> <p>As a data scientist and aspiring MLE, diving into RL has been incredibly rewarding. It pushes the boundaries of what machine intelligence can achieve, moving us from pattern recognition to genuine decision-making. The blend of mathematical rigor with intuitive concepts of trial and error makes it a fascinating domain. If you’re looking for a field that feels like you’re teaching machines to dream, explore, and conquer, then Reinforcement Learning is definitely for you.</p> <p>What do you think? Are you ready to dive into the world of agents, rewards, and optimal policies? Let me know your thoughts!</p> <hr> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>