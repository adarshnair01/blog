<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> From Yes/No to the World: Unpacking the Elegance of Logistic Regression | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/from-yesno-to-the-world-unpacking-the-elegance-of/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">From Yes/No to the World: Unpacking the Elegance of Logistic Regression</h1> <p class="post-meta"> Created on April 02, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/logistic-regression"> <i class="fa-solid fa-hashtag fa-sm"></i> Logistic Regression</a>   <a href="/blog/blog/tag/classification"> <i class="fa-solid fa-hashtag fa-sm"></i> Classification</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/statistics"> <i class="fa-solid fa-hashtag fa-sm"></i> Statistics</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hello fellow data adventurers!</p> <p>Today, I want to share a journey into one of the most fundamental yet powerful algorithms in a data scientist’s toolkit: <strong>Logistic Regression</strong>. If you’ve ever pondered how systems decide whether an email is spam, if a customer will churn, or if a medical test result indicates a disease, you’ve likely encountered the magic of logistic regression. It’s a cornerstone of classification, and understanding it deeply is like getting a secret key to unlocking a vast kingdom of machine learning problems.</p> <p>So, grab your imaginary explorer’s hat, and let’s demystify this beautiful algorithm together!</p> <h3 id="the-yes-or-no-conundrum-why-linear-regression-falls-short">The “Yes” or “No” Conundrum: Why Linear Regression Falls Short</h3> <p>My first encounter with classification problems often led me to a simple question: “Why can’t we just use linear regression?” After all, linear regression is fantastic at predicting continuous values, like house prices or temperatures. If I want to predict whether a student <em>passes</em> (1) or <em>fails</em> (0) based on study hours, why not just fit a line?</p> <p>Let’s imagine we plot study hours against pass/fail (0 or 1):</p> <ul> <li> <strong>Linear Regression’s Approach:</strong> It would try to draw a straight line that best fits these points.</li> <li> <strong>The Problem:</strong> <ol> <li> <strong>Out-of-Bounds Predictions:</strong> A straight line can easily output values less than 0 or greater than 1. What does a probability of -0.5 or 1.2 mean? It doesn’t make sense for a binary outcome. Probabilities <em>must</em> be between 0 and 1.</li> <li> <strong>Thresholding Arbitrariness:</strong> Even if we decide to round values (e.g., &gt;0.5 means pass), the line’s steepness and position can be heavily swayed by outliers, making our decision boundary unstable and hard to interpret.</li> <li> <strong>Non-Linear Relationship:</strong> The relationship between study hours and passing is likely not perfectly linear. There’s usually a point where a little more study makes a big difference, and then it plateaus.</li> </ol> </li> </ul> <p>This is where the limitations become glaring. We need a function that naturally constrains its output to be within the (0, 1) range, effectively squishing any input into a probability.</p> <h3 id="enter-the-sigmoid-our-s-shaped-hero">Enter the Sigmoid: Our S-Shaped Hero</h3> <p>This is where Logistic Regression truly earns its stripes, thanks to a special function called the <strong>Sigmoid function</strong>, also known as the <strong>logistic function</strong>. Think of it as a gatekeeper that takes any real-numbered input and transforms it into a value between 0 and 1.</p> <p>The sigmoid function looks like this:</p> <p>$ \sigma(z) = \frac{1}{1 + e^{-z}} $</p> <p>Where:</p> <ul> <li>$ \sigma(z) $ (pronounced “sigma of z”) is the output probability.</li> <li>$ e $ is Euler’s number (approximately 2.718).</li> <li>$ z $ is the input to the function.</li> </ul> <p>Let’s unpack what $z$ is. In linear regression, we had $ y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \dots $. For logistic regression, our input $z$ is essentially that same linear combination of our features and their corresponding weights (or coefficients):</p> <p>$ z = \beta_0 + \beta_1x_1 + \beta_2x_2 + \dots + \beta_nx_n $</p> <p>Or, more compactly using vector notation:</p> <p>$ z = \mathbf{w}^T\mathbf{x} + b $</p> <p>Here:</p> <ul> <li>$ \mathbf{w} $ is a vector of weights (coefficients).</li> <li>$ \mathbf{x} $ is a vector of input features.</li> <li>$ b $ is the bias term (intercept).</li> </ul> <p><strong>Why is the Sigmoid function so perfect for this?</strong></p> <ol> <li> <strong>Output Range:</strong> No matter how large or small $z$ is, $ \sigma(z) $ will always be between 0 and 1. <ul> <li>As $ z $ approaches positive infinity, $ e^{-z} $ approaches 0, so $ \sigma(z) $ approaches $ \frac{1}{1+0} = 1 $.</li> <li>As $ z $ approaches negative infinity, $ e^{-z} $ approaches positive infinity, so $ \sigma(z) $ approaches $ \frac{1}{1+\infty} = 0 $.</li> <li>When $ z = 0 $, $ \sigma(z) = \frac{1}{1+e^0} = \frac{1}{1+1} = 0.5 $.</li> </ul> </li> <li> <strong>S-Shape:</strong> This characteristic S-shape is ideal for modeling probabilities. It means that small changes in $z$ around the midpoint ($z=0$, where probability is 0.5) lead to significant changes in probability, while changes far from the midpoint (very positive or very negative $z$) lead to smaller changes. This mimics real-world phenomena where there’s often a tipping point.</li> </ol> <h3 id="the-logistic-regression-model-predicting-probabilities">The Logistic Regression Model: Predicting Probabilities</h3> <table> <tbody> <tr> <td>So, putting it all together, the Logistic Regression model predicts the <em>probability</em> that an outcome $Y$ belongs to a certain class (let’s say, class 1). We denote this as $ P(Y=1</td> <td>X) $:</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$ P(Y=1</td> <td>X) = h_{\mathbf{w},b}(\mathbf{x}) = \frac{1}{1 + e^{-(\mathbf{w}^T\mathbf{x} + b)}} $</td> </tr> </tbody> </table> <p>Where $ h_{\mathbf{w},b}(\mathbf{x}) $ is our hypothesis function.</p> <p>This output $ h_{\mathbf{w},b}(\mathbf{x}) $ is interpreted directly as the probability of the positive class (e.g., the probability of being spam).</p> <p><strong>How do we make a “Yes/No” decision then?</strong></p> <p>We set a <strong>decision threshold</strong>. The most common threshold is 0.5.</p> <ul> <li>If $ h_{\mathbf{w},b}(\mathbf{x}) \geq 0.5 $, we predict Class 1 (e.g., spam).</li> <li>If $ h_{\mathbf{w},b}(\mathbf{x}) &lt; 0.5 $, we predict Class 0 (e.g., not spam).</li> </ul> <p>Notice that when $ h_{\mathbf{w},b}(\mathbf{x}) = 0.5 $, it means $ \mathbf{w}^T\mathbf{x} + b = 0 $. This equation defines our <strong>decision boundary</strong> – the line (or hyperplane in higher dimensions) that separates the two classes. It’s the point where the model is equally unsure, predicting 50/50 probability.</p> <h3 id="finding-the-best-fit-the-cost-of-being-wrong-log-loss">Finding the Best Fit: The Cost of Being Wrong (Log Loss)</h3> <p>With linear regression, we used Mean Squared Error (MSE) to measure how well our line fit the data. Can we do the same for Logistic Regression? Unfortunately, no. If we used MSE with the sigmoid function, our cost function would be <strong>non-convex</strong>, meaning it would have many local minima. Gradient Descent (our optimization friend) would easily get stuck in one of these local minima and fail to find the globally optimal weights.</p> <p>This is where the <strong>Log Loss</strong> function, also known as <strong>Binary Cross-Entropy</strong>, comes to our rescue. This function is specifically designed for classification tasks and ensures our cost function is convex, allowing Gradient Descent to find the optimal global minimum.</p> <table> <tbody> <tr> <td>For a single training example $ (x, y) $, where $ y $ is the true label (0 or 1) and $ h(x) $ is our predicted probability ($ P(Y=1</td> <td>X) $), the log loss is:</td> </tr> </tbody> </table> <p>$ L(h(x), y) = -[y\log(h(x)) + (1-y)\log(1-h(x))] $</p> <p>Let’s break it down:</p> <ul> <li> <strong>If the true label $ y = 1 $:</strong> The term $ (1-y)\log(1-h(x)) $ becomes zero. The loss simplifies to $ -\log(h(x)) $. <ul> <li>If $ h(x) $ is close to 1 (correct prediction), $ \log(h(x)) $ is close to 0, so loss is small.</li> <li>If $ h(x) $ is close to 0 (wrong prediction with high confidence), $ \log(h(x)) $ becomes a large negative number, making the loss a large positive number. This heavily penalizes confident wrong predictions.</li> </ul> </li> <li> <strong>If the true label $ y = 0 $:</strong> The term $ y\log(h(x)) $ becomes zero. The loss simplifies to $ -\log(1-h(x)) $. <ul> <li>If $ h(x) $ is close to 0 (correct prediction), $ (1-h(x)) $ is close to 1, so $ \log(1-h(x)) $ is close to 0, and loss is small.</li> <li>If $ h(x) $ is close to 1 (wrong prediction with high confidence), $ (1-h(x)) $ is close to 0, making $ \log(1-h(x)) $ a large negative number, and the loss a large positive number. Again, confident wrong predictions are heavily penalized.</li> </ul> </li> </ul> <p>The total cost function $ J(\mathbf{w}, b) $ for all $ m $ training examples is the average log loss:</p> <p>$ J(\mathbf{w}, b) = -\frac{1}{m} \sum<em>{i=1}^m [y^{(i)}\log(h</em>{\mathbf{w},b}(\mathbf{x}^{(i)})) + (1-y^{(i)})\log(1-h_{\mathbf{w},b}(\mathbf{x}^{(i)}))] $</p> <p>Our goal is to find the values of $ \mathbf{w} $ and $ b $ that minimize this cost function. This is typically done using an optimization algorithm like <strong>Gradient Descent</strong>. Gradient Descent iteratively adjusts $ \mathbf{w} $ and $ b $ in the direction that reduces the cost, slowly “descending” towards the minimum of the cost function.</p> <h3 id="regularization-keeping-our-model-in-check">Regularization: Keeping Our Model in Check</h3> <p>Just like in linear regression, logistic regression can suffer from overfitting – where the model learns the training data too well, including its noise, and performs poorly on new, unseen data. To combat this, we often add <strong>regularization terms</strong> to our cost function.</p> <p>The two most common types are L1 (Lasso) and L2 (Ridge) regularization:</p> <ul> <li> <table> <tbody> <tr> <td> <strong>L1 Regularization:</strong> Adds $ \lambda \sum_{j=1}^n</td> <td>\beta_j</td> <td>$ to the cost function. It can drive some coefficients to exactly zero, effectively performing feature selection.</td> </tr> </tbody> </table> </li> <li> <strong>L2 Regularization:</strong> Adds $ \frac{\lambda}{2} \sum_{j=1}^n \beta_j^2 $ to the cost function. It shrinks coefficients towards zero, preventing any single feature from dominating the prediction.</li> </ul> <p>Here, $ \lambda $ is a hyperparameter that controls the strength of regularization. It’s a crucial tool for balancing bias and variance.</p> <h3 id="strengths-and-limitations-knowing-when-to-use-it">Strengths and Limitations: Knowing When to Use It</h3> <p>Every tool has its best use case, and Logistic Regression is no exception.</p> <p><strong>Strengths:</strong></p> <ul> <li> <strong>Simplicity and Interpretability:</strong> It’s relatively easy to understand how features contribute to the prediction. The coefficients ($ \beta_j $) can be interpreted in terms of log-odds, telling us how much the log-odds of the positive outcome change for a one-unit increase in a feature.</li> <li> <strong>Probabilistic Outputs:</strong> It provides well-calibrated probabilities, which are incredibly useful beyond just making a binary decision. Knowing a customer is 95% likely to churn is more actionable than just “will churn.”</li> <li> <strong>Efficiency:</strong> It’s computationally efficient, making it a great baseline model, especially for large datasets.</li> <li> <strong>Robustness:</strong> Less prone to overfitting than more complex models if regularization is applied.</li> </ul> <p><strong>Limitations:</strong></p> <ul> <li> <strong>Assumes Linear Separability:</strong> It works best when the classes are (or are nearly) linearly separable. If the relationship between features and the target is highly non-linear, Logistic Regression might struggle unless you perform sophisticated feature engineering (e.g., creating polynomial features).</li> <li> <strong>Independence of Observations:</strong> Assumes that observations are independent.</li> <li> <strong>Sensitive to Outliers:</strong> Like linear regression, it can be sensitive to outliers, especially without proper regularization.</li> <li> <strong>Multicollinearity:</strong> If features are highly correlated (multicollinearity), the interpretability of individual coefficients can be compromised.</li> </ul> <h3 id="real-world-applications">Real-World Applications</h3> <p>You’ll find Logistic Regression quietly working behind the scenes in countless applications:</p> <ul> <li> <strong>Spam Detection:</strong> Is this email spam or not?</li> <li> <strong>Medical Diagnosis:</strong> Is a tumor malignant or benign? Does a patient have a certain disease?</li> <li> <strong>Credit Scoring:</strong> Will a loan applicant default on their loan?</li> <li> <strong>Customer Churn Prediction:</strong> Will a customer cancel their subscription?</li> <li> <strong>Marketing:</strong> Will a customer click on an ad?</li> </ul> <h3 id="wrapping-up">Wrapping Up</h3> <p>Logistic Regression, with its clever use of the sigmoid function and the robust log loss, stands as a testament to elegant problem-solving in machine learning. It bridges the gap between our desire for “yes/no” answers and the probabilistic nature of the world. It might not be the flashiest algorithm compared to deep neural networks or complex ensembles, but its simplicity, interpretability, and widespread applicability make it an indispensable tool for any aspiring (or seasoned!) data scientist.</p> <p>It’s a fantastic starting point for any classification task, providing a solid baseline against which more complex models can be measured. So, next time you see a binary decision being made by a computer, give a little nod to the sigmoid and the mighty Logistic Regression!</p> <p>Keep exploring, and happy modeling!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>