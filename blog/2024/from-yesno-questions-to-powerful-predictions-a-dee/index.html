<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> From Yes/No Questions to Powerful Predictions: A Deep Dive into Decision Trees | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/from-yesno-questions-to-powerful-predictions-a-dee/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">From Yes/No Questions to Powerful Predictions: A Deep Dive into Decision Trees</h1> <p class="post-meta"> Created on November 09, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/decision-trees"> <i class="fa-solid fa-hashtag fa-sm"></i> Decision Trees</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/algorithms"> <i class="fa-solid fa-hashtag fa-sm"></i> Algorithms</a>   <a href="/blog/blog/tag/supervised-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Supervised Learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>Have you ever found yourself weighing a decision? Maybe it’s as simple as “Should I wear a jacket today?” or “What should I eat for dinner?” We instinctively break down these big questions into smaller, more manageable ones:</p> <ul> <li>“Is it cold outside?” (If yes, wear a jacket.)</li> <li>“Is it raining?” (If yes, definitely a jacket, maybe an umbrella!)</li> <li>“Am I hungry for something savory or sweet?” (If savory, “Do I want pasta or a sandwich?”)</li> </ul> <p>This step-by-step, question-and-answer process is incredibly intuitive to us humans. What if I told you that one of the most fundamental and powerful algorithms in Machine Learning mimics exactly this thought process? Welcome to the wonderful world of <strong>Decision Trees</strong>!</p> <h3 id="whats-a-decision-tree-anyway">What’s a Decision Tree Anyway?</h3> <p>At its core, a Decision Tree is like a flowchart. You start at the very top (the <strong>root node</strong>) with your entire dataset. Then, based on a specific question about your data, you branch off to different paths. Each path leads to another question (an <strong>internal node</strong>) or, eventually, to a final answer (a <strong>leaf node</strong>).</p> <p>Let’s visualize it:</p> <ul> <li> <strong>Root Node:</strong> Represents the entire dataset, the very first decision point.</li> <li> <strong>Internal Node:</strong> Represents a test on an attribute (a feature of your data). Think of it as a question like “Is the temperature &gt; 25°C?”.</li> <li> <strong>Branch (Edge):</strong> The outcome of a test. If the answer to the temperature question is “Yes” or “No”, you follow the corresponding branch.</li> <li> <strong>Leaf Node (Terminal Node):</strong> Represents the final decision or prediction. This is where the buck stops!</li> </ul> <p>Imagine we’re trying to decide if we should play outside based on the weather. Here’s how a simple decision tree might look:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                  [Is the Weather Sunny?]
                     /         \
                 Yes            No
                /                 \
        [Is Humidity High?]     [Is it Raining?]
           /     \                 /      \
       Yes        No          Yes        No
      /            \           /          \
  Don't Play    Play        Don't Play    Play
</code></pre></div></div> <p>In this example, “Weather,” “Humidity,” and “Raining” are our <em>features</em> or <em>attributes</em>, and “Play” or “Don’t Play” are our <em>target classes</em> or <em>labels</em>. Pretty straightforward, right?</p> <h3 id="the-why-how-do-decision-trees-learn">The “Why”: How Do Decision Trees Learn?</h3> <p>This is where the magic (and a bit of math) happens. When we build a decision tree from data, how does the algorithm know which question to ask first? How does it decide “Is the Weather Sunny?” is a better starting point than “Is Humidity High?”?</p> <p>The goal is to ask questions that split our data in the “best” way possible. What does “best” mean? It means creating branches where the data points within each branch are as <em>pure</em> as possible. In simple terms, we want each leaf node to contain data points that mostly belong to <em>one</em> single class.</p> <p>To quantify “purity” (or its opposite, “impurity”), Decision Trees use statistical measures. The two most common ones are <strong>Gini Impurity</strong> and <strong>Entropy</strong>.</p> <h4 id="1-gini-impurity">1. Gini Impurity</h4> <p>Gini Impurity measures the probability of misclassifying a randomly chosen element in the dataset if it were randomly labeled according to the distribution of labels in the subset. A Gini Impurity of 0 means perfect purity (all elements belong to the same class), while a Gini Impurity of 1 (or close to 1) means maximum impurity (elements are equally distributed across classes).</p> <p>The formula for Gini Impurity is:</p> <p>$G = 1 - \sum_{i=1}^{C} (p_i)^2$</p> <p>Where:</p> <ul> <li>$C$ is the number of classes.</li> <li>$p_i$ is the proportion (or probability) of observations belonging to class $i$ in the node.</li> </ul> <p>Let’s take an example: Suppose a node has 10 data points: 6 “Play” and 4 “Don’t Play”.</p> <ul> <li>$C=2$ (Play, Don’t Play)</li> <li>$p_{Play} = 6/10 = 0.6$</li> <li>$p_{Don’t Play} = 4/10 = 0.4$</li> </ul> <p>$G = 1 - ((0.6)^2 + (0.4)^2)$ $G = 1 - (0.36 + 0.16)$ $G = 1 - 0.52$ $G = 0.48$</p> <p>Now, imagine a node that’s perfectly pure: 10 “Play” and 0 “Don’t Play”.</p> <ul> <li>$p_{Play} = 10/10 = 1$</li> <li>$p_{Don’t Play} = 0/10 = 0$</li> </ul> <p>$G = 1 - ((1)^2 + (0)^2)$ $G = 1 - (1 + 0)$ $G = 0$</p> <p>See? A Gini of 0 means perfect purity!</p> <h4 id="2-entropy">2. Entropy</h4> <p>Entropy, originating from information theory, measures the randomness or disorder within a set of data. If a node is perfectly pure (all data points belong to the same class), its entropy is 0. If a node is perfectly mixed (e.g., an equal number of “Play” and “Don’t Play”), its entropy is maximal.</p> <p>The formula for Entropy is:</p> <p>$H = - \sum_{i=1}^{C} p_i \log_2 (p_i)$</p> <p>Where:</p> <ul> <li>$C$ is the number of classes.</li> <li>$p_i$ is the proportion of observations belonging to class $i$ in the node.</li> <li>The $\log_2$ (logarithm base 2) is used because we’re thinking in terms of “bits” of information.</li> </ul> <p>Using our previous example: 6 “Play” and 4 “Don’t Play”.</p> <ul> <li>$p_{Play} = 0.6$</li> <li>$p_{Don’t Play} = 0.4$</li> </ul> <p>$H = - (0.6 \log_2(0.6) + 0.4 \log_2(0.4))$ $H = - (0.6 \times -0.737 + 0.4 \times -1.322)$ $H = - (-0.4422 - 0.5288)$ $H = - (-0.971)$ $H \approx 0.971$</p> <p>For a perfectly pure node (10 “Play”, 0 “Don’t Play”):</p> <ul> <li>$p_{Play} = 1$</li> <li>$p_{Don’t Play} = 0$</li> </ul> <p>$H = - (1 \log_2(1) + 0 \log_2(0))$ Note: $\log_2(1) = 0$ and $0 \log_2(0)$ is typically treated as 0 in this context. $H = - (1 \times 0 + 0)$ $H = 0$</p> <p>Again, 0 entropy means perfect purity!</p> <h4 id="3-information-gain-ig">3. Information Gain (IG)</h4> <p>Now that we can measure impurity, how do we choose the best split? We use <strong>Information Gain (IG)</strong> (often used with Entropy) or <strong>Gini Gain</strong> (used with Gini Impurity). These measures quantify how much the impurity <em>decreases</em> after we split a node based on a particular feature.</p> <p>The idea is simple: we want to pick the feature that gives us the <em>most</em> reduction in impurity – the highest Information Gain.</p> <p>For Entropy, Information Gain is calculated as:</p> <table> <tbody> <tr> <td>$IG(S, A) = H(S) - \sum_{v \in Values(A)} \frac{</td> <td>S_v</td> <td>}{</td> <td>S</td> <td>} H(S_v)$</td> </tr> </tbody> </table> <p>Where:</p> <ul> <li>$S$ is the entropy of the parent node before the split.</li> <li>$A$ is the attribute (feature) we are considering for the split.</li> <li>$Values(A)$ are the possible values of attribute $A$.</li> <li>$S_v$ is the subset of $S$ for which attribute $A$ has value $v$.</li> <li> <table> <tbody> <tr> <td>$</td> <td>S_v</td> <td>$ is the number of elements in $S_v$.</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>$</td> <td>S</td> <td>$ is the total number of elements in the parent node.</td> </tr> </tbody> </table> </li> </ul> <p>Essentially, it’s: <code class="language-plaintext highlighter-rouge">(Entropy of Parent) - (Weighted Average of Entropy of Children)</code></p> <p>The algorithm will calculate the Information Gain for <em>every possible feature split</em> at each node and choose the one that maximizes this gain. This greedy approach ensures we’re making the “best” local decision at each step.</p> <h3 id="building-the-tree-the-id3c45cart-algorithms">Building the Tree: The ID3/C4.5/CART Algorithms</h3> <p>The process of building a Decision Tree is iterative and recursive:</p> <ol> <li> <strong>Start:</strong> Begin with the entire dataset at the root node.</li> <li> <strong>Calculate Impurity:</strong> Calculate the Gini Impurity or Entropy of the current node.</li> <li> <strong>Find Best Split:</strong> For every feature, and for every possible split point within that feature, calculate the Information Gain (or Gini Gain) if we were to split on it.</li> <li> <strong>Split Node:</strong> Choose the feature and split point that yields the highest Information Gain. This becomes the next internal node.</li> <li> <strong>Create Branches:</strong> Divide the dataset into subsets based on the chosen split.</li> <li> <strong>Recurse:</strong> Apply steps 2-5 to each new child node.</li> <li> <strong>Stop:</strong> The recursion stops when: <ul> <li>A node becomes pure (all data points belong to the same class). This becomes a leaf node.</li> <li>There are no more features to split on.</li> <li>A pre-defined stopping criterion is met (e.g., maximum tree depth reached, minimum number of samples required to make a split, minimum samples in a leaf node). This helps prevent overfitting!</li> </ul> </li> </ol> <h3 id="the-upsides-why-i-love-decision-trees">The Upsides: Why I Love Decision Trees</h3> <ul> <li> <strong>Interpretability &amp; Explainability (White-Box Model):</strong> This is perhaps their biggest strength! You can literally visualize the decision-making process. For complex models, knowing <em>why</em> a prediction was made is crucial.</li> <li> <strong>Easy to Understand:</strong> Even without a deep dive into the math, the concept of a flowchart is intuitive.</li> <li> <strong>No Data Preprocessing Required:</strong> Unlike many other algorithms, Decision Trees don’t require feature scaling (like standardization or normalization). They handle both numerical and categorical features naturally.</li> <li> <strong>Handle Non-Linear Relationships:</strong> They can model complex, non-linear relationships between features and the target variable.</li> <li> <strong>Robust to Outliers:</strong> They tend to be less affected by outliers compared to models that rely on distances or means.</li> </ul> <h3 id="the-downsides-where-they-can-be-tricky">The Downsides: Where They Can Be Tricky</h3> <ul> <li> <strong>Overfitting:</strong> This is the biggest Achilles’ heel. A very deep tree can learn the training data too well, memorizing noise rather than the underlying patterns. This leads to poor performance on new, unseen data.</li> <li> <strong>Instability:</strong> Small changes in the training data can lead to a completely different tree structure.</li> <li> <strong>Bias Towards Features with More Levels:</strong> Features with many unique values (e.g., an ID number) can appear to offer higher information gain, leading the tree to prioritize them even if they aren’t truly predictive.</li> <li> <strong>Local Optima:</strong> The greedy approach (making the best split at each step) doesn’t guarantee finding the globally optimal tree. Finding a truly optimal tree is an NP-complete problem, meaning it’s computationally very expensive.</li> </ul> <h3 id="beyond-single-trees-ensemble-power">Beyond Single Trees: Ensemble Power!</h3> <p>The limitations of single Decision Trees, especially overfitting, led to the development of powerful <strong>ensemble methods</strong>. These methods combine multiple Decision Trees to create even more robust and accurate models:</p> <ul> <li> <strong>Random Forests:</strong> Builds many Decision Trees, each on a random subset of the data and a random subset of features. The final prediction is an aggregation (voting for classification, averaging for regression) of all individual trees. This significantly reduces overfitting and improves stability.</li> <li> <strong>Gradient Boosting (e.g., XGBoost, LightGBM, CatBoost):</strong> Builds trees sequentially, where each new tree tries to correct the errors made by the previous ones. This focuses on challenging data points and can achieve state-of-the-art performance.</li> </ul> <h3 id="conclusion-your-foundational-friend">Conclusion: Your Foundational Friend</h3> <p>Decision Trees are more than just a simple algorithm; they’re a foundational concept in machine learning. They offer an intuitive gateway into understanding how algorithms can learn from data by breaking down complex problems into a series of logical steps. While a single tree might have its weaknesses, it’s the building block for some of the most powerful and widely used machine learning techniques today.</p> <p>So, the next time you’re faced with a big decision, remember the humble Decision Tree – a simple yet profoundly intelligent way to navigate the forest of data! Keep exploring, keep questioning, and keep learning!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>