<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Backpropagation: Unraveling the Brain's Secret to Learning from Mistakes | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/backpropagation-unraveling-the-brains-secret-to-le/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Backpropagation: Unraveling the Brain's Secret to Learning from Mistakes</h1> <p class="post-meta"> Created on September 30, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/neural-networks"> <i class="fa-solid fa-hashtag fa-sm"></i> Neural Networks</a>   <a href="/blog/blog/tag/backpropagation"> <i class="fa-solid fa-hashtag fa-sm"></i> Backpropagation</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Imagine you’re learning to ride a bike. You push off, wobble, and then… <em>thud</em>. You’ve made a mistake. What do you do next? You don’t just randomly flail around; you subtly adjust your balance, maybe lean a little less, push a little harder with one foot, or steer slightly differently based on <em>how</em> you fell. You learn from your error by figuring out which actions contributed to the fall and how to correct them.</p> <p>This intuitive process of learning from mistakes, adjusting actions based on outcomes, is fundamental to intelligence – both biological and artificial. In the world of Artificial Intelligence, specifically with Neural Networks, this intricate dance of learning is orchestrated by a powerful, yet often misunderstood, algorithm: <strong>Backpropagation</strong>.</p> <p>Today, we’re going on a journey, almost like opening a personal journal, to explore this masterpiece. We’ll demystify backpropagation, understand its intuition, and even peek behind the curtain at the beautiful math that makes deep learning possible.</p> <h3 id="the-neurons-tale-a-quick-refresher">The Neuron’s Tale: A Quick Refresher</h3> <p>Before we dive into how neural networks learn, let’s quickly recap what they <em>are</em>. At its heart, a neural network is a collection of interconnected “neurons” organized into layers.</p> <p>Each neuron takes inputs, multiplies them by a set of <strong>weights</strong> ($w$), adds a <strong>bias</strong> ($b$), and then passes the result through an non-linear <strong>activation function</strong> ($\sigma$) to produce an output.</p> <p>Here’s a single neuron’s calculation: $z = \sum (x_i w_i) + b$ $a = \sigma(z)$</p> <p>Where:</p> <ul> <li>$x_i$ are the inputs</li> <li>$w_i$ are the weights</li> <li>$b$ is the bias</li> <li>$z$ is the weighted sum (pre-activation)</li> <li>$\sigma$ is the activation function (e.g., ReLU, Sigmoid)</li> <li>$a$ is the output (activation) of the neuron</li> </ul> <p>These neurons are stacked in layers: an <strong>input layer</strong>, one or more <strong>hidden layers</strong>, and an <strong>output layer</strong>. Information flows forward through the network – this is called the <strong>forward pass</strong>. When we feed an image of a cat into a network, it performs a forward pass to predict whether it’s a cat or a dog.</p> <h3 id="the-moment-of-truth-quantifying-wrong">The Moment of Truth: Quantifying “Wrong”</h3> <p>After the forward pass, our neural network spits out a prediction ($\hat{y}$). But how good is this prediction? We compare it to the actual correct answer ($y$) using a <strong>loss function</strong> (or cost function). The loss function gives us a numerical value indicating “how wrong” our network’s prediction was.</p> <p>A common loss function for regression tasks is the Mean Squared Error (MSE): $L = (y - \hat{y})^2$</p> <p>For classification, we often use Cross-Entropy Loss. The goal of training a neural network is simple: <strong>minimize this loss function.</strong> We want our network to make predictions that are as close to the truth as possible, resulting in the smallest possible loss.</p> <h3 id="the-million-dollar-question-how-do-we-adjust">The Million-Dollar Question: How Do We Adjust?</h3> <p>Now comes the tricky part. We know our network made a mistake (high loss). We want to adjust the weights and biases to reduce this loss in the future. But which weights? And by how much?</p> <p>Imagine a vast, complex landscape. The “height” of this landscape at any point represents the loss, and our current position is determined by the current values of all our weights and biases. Our goal is to find the lowest point in this landscape (minimum loss).</p> <p>The most common strategy to navigate this landscape is <strong>Gradient Descent</strong>. The gradient tells us the direction of the steepest ascent. To minimize the loss, we want to move in the opposite direction – the direction of steepest descent.</p> <p>So, for each weight ($w$) and bias ($b$), we need to calculate its <strong>gradient</strong> with respect to the loss function. This gradient, $\frac{\partial L}{\partial w}$ (pronounced “partial L by partial w”), tells us how much a tiny change in that specific weight affects the total loss.</p> <p>Once we have these gradients, we update our weights and biases: $w_{new} = w_{old} - \alpha \frac{\partial L}{\partial w_{old}}$ $b_{new} = b_{old} - \alpha \frac{\partial L}{\partial b_{old}}$</p> <p>Here, $\alpha$ is the <strong>learning rate</strong>, a small positive number that determines the size of the step we take in the direction of steepest descent.</p> <p>The problem, however, is that a neural network can have millions, even billions, of weights and biases. Manually calculating all these gradients for every single parameter would be computationally impossible. This is where Backpropagation elegantly steps in.</p> <h3 id="backpropagation-the-blame-game-backwards">Backpropagation: The Blame Game, Backwards</h3> <p>The intuition behind backpropagation is surprisingly simple: it’s a systematic way of propagating the error backward through the network, assigning “blame” to each weight and bias based on its contribution to the overall error. It’s like tracing back the dominoes to find out which one was the first to fall.</p> <p>Let’s consider a simple scenario. Our network makes a prediction, and it’s wrong. The output layer is directly responsible for this error. But that output layer’s activations were determined by the weights and biases connecting it to the previous hidden layer. And those hidden layer activations were, in turn, determined by <em>their</em> weights and biases, and so on.</p> <p>Backpropagation uses the <strong>chain rule</strong> from calculus to efficiently compute these gradients. The chain rule states that if $C$ depends on $y$, and $y$ depends on $x$, then the rate of change of $C$ with respect to $x$ is: $\frac{dC}{dx} = \frac{dC}{dy} \cdot \frac{dy}{dx}$</p> <p>This rule is precisely what allows us to “chain” the derivatives together from the output layer all the way back to the input layer.</p> <h3 id="the-math-behind-the-magic-a-step-by-step-walkthrough">The Math Behind the Magic: A Step-by-Step Walkthrough</h3> <p>Let’s consider a very simple three-layer network: an input layer, one hidden layer, and an output layer. We want to find $\frac{\partial L}{\partial w}$ for all weights.</p> <p>For any neuron, its output $a$ is $\sigma(z)$, where $z = \sum (x_i w_i) + b$.</p> <h4 id="1-calculating-error-at-the-output-layer">1. Calculating Error at the Output Layer</h4> <p>This is where the error originates. For a single output neuron $k$, the loss $L$ depends directly on its activation $\hat{y}_k$. We start by calculating the gradient of the loss with respect to the output neuron’s activation: $\frac{\partial L}{\partial \hat{y}_k}$</p> <p>Next, we need the gradient of the loss with respect to the output neuron’s pre-activation ($z_k$). Using the chain rule: $\frac{\partial L}{\partial z_k} = \frac{\partial L}{\partial \hat{y}_k} \cdot \frac{\partial \hat{y}_k}{\partial z_k}$</p> <p>Since $\hat{y}_k = \sigma(z_k)$, then $\frac{\partial \hat{y}_k}{\partial z_k} = \sigma’(z_k)$ (the derivative of the activation function). So, for the output layer, our “error signal” is $\delta_k = \frac{\partial L}{\partial z_k} = (y_k - \hat{y}_k) \cdot \sigma’(z_k)$ (for MSE with linear output). More generally, $\delta_k = \text{error} \times \text{local gradient}$.</p> <p>Now that we have $\frac{\partial L}{\partial z_k}$, we can find the gradients for the weights ($w_{jk}$) connecting the hidden layer neuron $j$ to the output neuron $k$, and the bias ($b_k$) of the output neuron: $\frac{\partial L}{\partial w_{jk}} = \frac{\partial L}{\partial z_k} \cdot \frac{\partial z_k}{\partial w_{jk}}$ Since $z_k = \sum_j (a_j w_{jk}) + b_k$, then $\frac{\partial z_k}{\partial w_{jk}} = a_j$ (where $a_j$ is the activation of hidden neuron $j$). So, $\frac{\partial L}{\partial w_{jk}} = \delta_k \cdot a_j$</p> <p>And for the bias: $\frac{\partial L}{\partial b_k} = \frac{\partial L}{\partial z_k} \cdot \frac{\partial z_k}{\partial b_k}$ Since $\frac{\partial z_k}{\partial b_k} = 1$, then $\frac{\partial L}{\partial b_k} = \delta_k \cdot 1 = \delta_k$</p> <p>These are the gradients for the weights and biases of the <em>output layer</em>.</p> <h4 id="2-propagating-error-to-the-hidden-layer">2. Propagating Error to the Hidden Layer</h4> <p>Here’s where the “back” in backpropagation truly shines. The hidden layer neurons don’t directly contribute to the loss. Their influence is indirect, through how they affect the output layer neurons.</p> <p>To find the gradient for a weight ($w_{ij}$) connecting input neuron $i$ to hidden neuron $j$, we first need the “error signal” for the hidden neuron $j$, which is $\frac{\partial L}{\partial z_j}$.</p> <p>How does the loss $L$ depend on $z_j$? It depends on $z_j$ because $z_j$ influences $a_j$, which then influences <em>all</em> the $z_k$ values in the output layer, which then influence $L$. So we need to sum up the influence from all output neurons $k$: $\frac{\partial L}{\partial z_j} = \sum_k \left( \frac{\partial L}{\partial z_k} \cdot \frac{\partial z_k}{\partial a_j} \cdot \frac{\partial a_j}{\partial z_j} \right)$</p> <p>Let’s break this down:</p> <ul> <li>$\frac{\partial L}{\partial z_k}$ is the error signal ($\delta_k$) we just calculated for output neuron $k$.</li> <li>$\frac{\partial z_k}{\partial a_j}$ tells us how much hidden neuron $j$’s activation affects output neuron $k$’s pre-activation. From $z_k = \sum_j (a_j w_{jk}) + b_k$, we see $\frac{\partial z_k}{\partial a_j} = w_{jk}$.</li> <li>$\frac{\partial a_j}{\partial z_j}$ is simply the derivative of the hidden layer’s activation function: $\sigma’(z_j)$.</li> </ul> <p>So, the error signal for hidden neuron $j$ is: $\delta_j = \frac{\partial L}{\partial z_j} = \left( \sum_k \delta_k w_{jk} \right) \cdot \sigma’(z_j)$</p> <p>Notice the sum: each output neuron $k$ “sends back” its error signal ($\delta_k$) weighted by the strength of the connection ($w_{jk}$) it has with the hidden neuron $j$. This aggregated error is then multiplied by the hidden neuron’s local gradient ($\sigma’(z_j)$).</p> <p>Once we have $\delta_j = \frac{\partial L}{\partial z_j}$, we can calculate the gradients for weights ($w_{ij}$) connecting input neuron $i$ to hidden neuron $j$, and the bias ($b_j$) of the hidden neuron: $\frac{\partial L}{\partial w_{ij}} = \frac{\partial L}{\partial z_j} \cdot \frac{\partial z_j}{\partial w_{ij}} = \delta_j \cdot x_i$ (where $x_i$ is the input from neuron $i$) $\frac{\partial L}{\partial b_j} = \frac{\partial L}{\partial z_j} \cdot \frac{\partial z_j}{\partial b_j} = \delta_j \cdot 1 = \delta_j$</p> <p>And there you have it! We’ve calculated gradients for all weights and biases in the network by systematically working backward from the output layer.</p> <h3 id="the-backpropagation-algorithm-summarized">The Backpropagation Algorithm Summarized</h3> <ol> <li> <strong>Forward Pass</strong>: Feed input data through the network, layer by layer, calculating activations for each neuron until the output prediction ($\hat{y}$) is obtained. Store all intermediate pre-activations ($z$) and activations ($a$).</li> <li> <strong>Calculate Loss</strong>: Compare $\hat{y}$ with the true label ($y$) using the chosen loss function $L$.</li> <li> <strong>Backward Pass (Backpropagation)</strong>: <ul> <li> <strong>Output Layer</strong>: Calculate the error signal $\delta$ for the output layer neurons. Compute gradients for the output layer’s weights and biases using this $\delta$.</li> <li> <strong>Hidden Layers (iterating backward)</strong>: For each hidden layer, calculate its $\delta$ by “propagating” the error signals from the next (already processed) layer backward. Then, use this new $\delta$ to compute the gradients for that hidden layer’s weights and biases.</li> </ul> </li> <li> <strong>Update Weights</strong>: Using the calculated gradients, update all weights and biases in the network using the gradient descent rule ($w_{new} = w_{old} - \alpha \frac{\partial L}{\partial w}$).</li> </ol> <p>Repeat this entire process for many iterations (epochs) and mini-batches of data until the network’s performance converges.</p> <h3 id="why-is-backpropagation-so-powerful">Why is Backpropagation so Powerful?</h3> <p>Backpropagation is not just an algorithm; it’s a computational revolution.</p> <ul> <li> <strong>Efficiency</strong>: Without backpropagation, we would have to calculate each weight’s gradient by slightly perturbing it and re-running the forward pass. This would be incredibly slow, especially for deep networks. Backprop calculates all gradients in a single backward pass, making it vastly more efficient.</li> <li> <strong>Scalability</strong>: This efficiency allows us to train neural networks with millions or even billions of parameters, which are the backbone of modern AI systems like image recognition, natural language processing, and autonomous driving.</li> <li> <strong>Foundation of Deep Learning</strong>: It’s the engine that powers every modern deep learning framework, from TensorFlow to PyTorch. Understanding backpropagation is understanding the core of how these powerful models learn.</li> </ul> <h3 id="reflecting-on-learning">Reflecting on Learning</h3> <p>Just like our bike rider, the neural network learns by analyzing its mistakes. Backpropagation is the sophisticated “internal coach” that dissects every misstep, identifies which part of the “muscle memory” (weights and biases) needs adjustment, and guides the network toward better performance.</p> <p>It’s an elegant demonstration of how local computations (derivatives at each neuron) can be combined to solve a global optimization problem (minimizing the overall loss). When you next see an AI generate stunning art, translate languages seamlessly, or beat a grandmaster at chess, remember the silent, diligent work of backpropagation, the unsung hero that taught it all.</p> <p>This journey into backpropagation isn’t just about understanding an algorithm; it’s about appreciating the ingenuity that underpins our modern AI landscape. Keep exploring, keep questioning, and keep learning!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>