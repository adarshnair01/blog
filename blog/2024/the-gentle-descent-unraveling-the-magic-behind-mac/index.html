<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Gentle Descent: Unraveling the Magic Behind Machine Learning's Core | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/the-gentle-descent-unraveling-the-magic-behind-mac/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Gentle Descent: Unraveling the Magic Behind Machine Learning's Core</h1> <p class="post-meta"> Created on September 07, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/gradient-descent"> <i class="fa-solid fa-hashtag fa-sm"></i> Gradient Descent</a>   <a href="/blog/blog/tag/optimization"> <i class="fa-solid fa-hashtag fa-sm"></i> Optimization</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/algorithms"> <i class="fa-solid fa-hashtag fa-sm"></i> Algorithms</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Imagine you’re standing on a mountain, blindfolded, and your goal is to reach the lowest point in the valley. You can’t see, but you can feel the slope beneath your feet. How would you proceed? You’d probably take a step in the direction that feels most steeply downhill. You’d repeat this process, one careful step after another, until you eventually felt no more slope – meaning you’ve reached the bottom.</p> <p>This, my friends, is the intuitive heart of <strong>Gradient Descent</strong>, one of the most fundamental and powerful optimization algorithms in the entire field of machine learning. It’s the silent workhorse behind countless models, enabling them to learn, adapt, and make increasingly accurate predictions. Today, I want to take you on a journey to understand this elegant concept, from its core idea to its practical applications.</p> <h3 id="the-problem-minimizing-error">The Problem: Minimizing Error</h3> <p>At its essence, machine learning is often about building a model that can make predictions or classifications. But how do we know if our model is any good? We define a <strong>cost function</strong> (sometimes called a loss function or error function). This function quantifies “how wrong” our model is for a given set of parameters.</p> <p>Think of our mountain analogy again: the height you are above sea level is your “cost.” Your goal is to minimize this height. In machine learning, our goal is to minimize the output of the cost function.</p> <p>Let’s say we’re trying to predict house prices based on their size. A very simple model might look like this:</p> \[\hat{y} = mx + b\] <p>Where:</p> <ul> <li>$ \hat{y} $ is the predicted house price.</li> <li>$ x $ is the house size.</li> <li>$ m $ is the slope (how much price changes per unit of size).</li> <li>$ b $ is the y-intercept (base price).</li> </ul> <p>Our model’s parameters are $m$ and $b$. We want to find the <em>best</em> values for $m$ and $b$ that make our predictions $ \hat{y} $ as close as possible to the actual house prices $y$.</p> <p>A common cost function for this type of problem is the <strong>Mean Squared Error (MSE)</strong>:</p> \[J(m, b) = \frac{1}{2n} \sum_{i=1}^n (\hat{y}_i - y_i)^2 = \frac{1}{2n} \sum_{i=1}^n (mx_i + b - y_i)^2\] <p>Here, $J(m, b)$ represents our cost, dependent on the parameters $m$ and $b$. The $ \frac{1}{2} $ is just for mathematical convenience (it simplifies the derivative), and $n$ is the number of data points.</p> <p>If we plot this cost function with $m$ and $b$ on the x-y axes and $J(m, b)$ on the z-axis, it often looks like a bowl or a valley. Our blindfolded mountain climber is now standing somewhere on this bowl, and they need to find the very bottom – the point where $J(m, b)$ is minimized.</p> <h3 id="the-gradient-feeling-the-slope">The “Gradient”: Feeling the Slope</h3> <p>This is where calculus, specifically derivatives, comes to our rescue. The <strong>gradient</strong> of a function is a vector that points in the direction of the <em>steepest ascent</em> (the direction of the greatest increase) of the function.</p> <p>If you’re trying to minimize a function, you need to go in the <em>opposite</em> direction of its gradient. It’s like feeling the slope: if the ground slopes upwards to your right, you move to your left to go downhill.</p> <p>For our cost function $J(m, b)$, we need to find the partial derivatives with respect to each parameter ($m$ and $b$):</p> <ul> <li>$ \frac{\partial J}{\partial m} $: How much the cost changes if we slightly change $m$.</li> <li>$ \frac{\partial J}{\partial b} $: How much the cost changes if we slightly change $b$.</li> </ul> <p>Let’s calculate these for our MSE cost function:</p> <p>\(\frac{\partial J}{\partial m} = \frac{1}{n} \sum_{i=1}^n (mx_i + b - y_i)x_i\) \(\frac{\partial J}{\partial b} = \frac{1}{n} \sum_{i=1}^n (mx_i + b - y_i)\)</p> <p>These derivatives tell us the slope of the cost function with respect to $m$ and $b$ at our current point. The vector formed by these partial derivatives, $ \nabla J(m,b) = \left[ \frac{\partial J}{\partial m}, \frac{\partial J}{\partial b} \right] $, is our gradient.</p> <h3 id="the-descent-taking-the-steps">The “Descent”: Taking the Steps</h3> <p>Now that we know which way is “most uphill,” we simply take a step in the opposite direction. This is an iterative process. We start with some initial, often random, values for $m$ and $b$, and then we repeatedly update them.</p> <p>The update rule for each parameter looks like this:</p> \[\theta_{new} = \theta_{old} - \alpha \nabla J(\theta_{old})\] <p>Where:</p> <ul> <li>$ \theta $ represents our parameters (so, for our example, it could be $m$ or $b$).</li> <li>$ \theta_{old} $ is the current value of the parameter.</li> <li>$ \theta_{new} $ is the updated value of the parameter.</li> <li>$ \nabla J(\theta_{old}) $ is the gradient of the cost function with respect to $ \theta $ at the current parameter values.</li> <li>$ \alpha $ (alpha) is a crucial term called the <strong>learning rate</strong>.</li> </ul> <p>Applying this to our $m$ and $b$ parameters:</p> <p>\(m_{new} = m_{old} - \alpha \frac{\partial J}{\partial m}\) \(b_{new} = b_{old} - \alpha \frac{\partial J}{\partial b}\)</p> <p>We repeat these updates over many <strong>iterations</strong> (often called epochs), gradually moving down the cost function’s surface until we ideally reach the minimum.</p> <h3 id="the-learning-rate--alpha--the-goldilocks-number">The Learning Rate ($ \alpha $): The Goldilocks Number</h3> <p>The learning rate $ \alpha $ is perhaps the most critical hyperparameter in Gradient Descent. It dictates the size of the steps we take down the slope.</p> <ul> <li> <strong>If $ \alpha $ is too small:</strong> We’ll take tiny, hesitant steps. Convergence will be very slow, potentially taking a long time to reach the minimum. It’s like painstakingly inching down the mountain.</li> <li> <strong>If $ \alpha $ is too large:</strong> We might take giant leaps, overshooting the minimum. We could bounce around erratically, fail to converge, or even diverge entirely and shoot off to infinity! Imagine trying to navigate a narrow valley by taking mile-long strides.</li> </ul> <p>The ideal $ \alpha $ is “just right”—large enough to converge efficiently but small enough not to overshoot. Finding this sweet spot often involves experimentation and techniques like learning rate schedules (where $ \alpha $ decreases over time) or more advanced optimizers.</p> <h3 id="types-of-gradient-descent">Types of Gradient Descent</h3> <p>While the core idea remains the same, how we calculate the gradient (and thus, how often we update the parameters) leads to different flavors of Gradient Descent:</p> <ol> <li> <strong>Batch Gradient Descent (BGD):</strong> <ul> <li> <strong>How it works:</strong> In each iteration, we calculate the gradient using <em>all</em> the training examples in our dataset.</li> <li> <strong>Pros:</strong> Produces a very smooth convergence path. For convex cost functions (like our MSE example), it’s guaranteed to find the global minimum.</li> <li> <strong>Cons:</strong> Can be very slow and computationally expensive for large datasets because it processes the entire dataset for <em>each single update</em>. Imagine calculating the slope of the <em>entire mountain</em> before taking one step.</li> </ul> </li> <li> <strong>Stochastic Gradient Descent (SGD):</strong> <ul> <li> <strong>How it works:</strong> Instead of using all data, we randomly pick <em>one single training example</em> at each iteration to calculate the gradient and update the parameters.</li> <li> <strong>Pros:</strong> Much, much faster, especially for large datasets, as it makes many updates per “epoch” (one pass through the entire dataset). The noisy updates can also help escape shallow local minima in complex cost landscapes.</li> <li> <strong>Cons:</strong> The updates are noisy and erratic because each step is based on only one example. The cost function might not smoothly decrease but rather jump around. It might struggle to settle precisely at the minimum.</li> </ul> </li> <li> <strong>Mini-Batch Gradient Descent:</strong> <ul> <li> <strong>How it works:</strong> This is the most common and practical approach. We use a small, randomly selected “batch” of training examples (e.g., 32, 64, 128 samples) to compute the gradient and update parameters.</li> <li> <strong>Pros:</strong> Strikes a balance between BGD and SGD. It’s faster than BGD (fewer gradient calculations per update) and provides more stable updates than SGD (less noisy because it’s averaging over a small batch). It leverages highly optimized matrix operations in modern hardware.</li> <li> <strong>Cons:</strong> Still has hyperparameters to tune (batch size).</li> </ul> </li> </ol> <h3 id="challenges-and-considerations">Challenges and Considerations</h3> <p>Gradient Descent, while powerful, isn’t without its quirks:</p> <ul> <li> <strong>Local Minima &amp; Saddle Points:</strong> For non-convex cost functions (common in deep learning), there might be multiple “valleys” (local minima) instead of one global minimum. BGD might get stuck in a local minimum. SGD and mini-batch, with their noisy updates, can sometimes “kick” the optimization out of a local minimum and towards a better one. Saddle points are another tricky spot where the gradient is zero, but it’s not a minimum (it’s a minimum in one direction, a maximum in another).</li> <li> <strong>Vanishing/Exploding Gradients:</strong> In deep neural networks, gradients can become extremely small (vanishing) or extremely large (exploding) as they propagate back through layers. This can make learning very slow or unstable.</li> <li> <strong>Feature Scaling:</strong> If your features (like house size and number of bedrooms) have vastly different scales, the cost function surface can become very elongated, making Gradient Descent take a zig-zag path and converge slowly. Scaling features (e.g., normalization) can make the surface more spherical, leading to faster convergence.</li> </ul> <h3 id="beyond-the-basics-adaptive-optimizers">Beyond the Basics: Adaptive Optimizers</h3> <p>While basic Gradient Descent is foundational, modern deep learning often employs more sophisticated optimizers that build upon its principles. Algorithms like <strong>Momentum</strong>, <strong>AdaGrad</strong>, <strong>RMSprop</strong>, and especially <strong>Adam</strong> (Adaptive Moment Estimation) dynamically adjust the learning rate for each parameter, incorporate past gradient information, and generally lead to faster and more stable training. They’re essentially smarter ways to take those steps down the mountain.</p> <h3 id="conclusion-the-elegant-engine">Conclusion: The Elegant Engine</h3> <p>Gradient Descent is a marvel of mathematical elegance and practical utility. It’s the engine that powers the “learning” in machine learning, allowing models to iteratively refine their understanding of data by minimizing their errors. From predicting housing prices to recognizing faces in images and translating languages, this seemingly simple algorithm is at the core of making intelligent machines a reality.</p> <p>Understanding Gradient Descent isn’t just about passing a test; it’s about grasping the fundamental mechanism by which modern AI systems learn and evolve. So, next time you see a machine learning model performing its magic, remember the blindfolded climber carefully feeling their way down the slope – remember the gentle, yet powerful, descent of the gradient. It’s a journey of continuous improvement, much like our own learning process in the vast landscape of data science.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>