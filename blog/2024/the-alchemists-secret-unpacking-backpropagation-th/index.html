<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Alchemist's Secret: Unpacking Backpropagation, the Engine of AI Learning | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/2024/the-alchemists-secret-unpacking-backpropagation-th/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Alchemist's Secret: Unpacking Backpropagation, the Engine of AI Learning</h1> <p class="post-meta"> Created on December 10, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/neural-networks"> <i class="fa-solid fa-hashtag fa-sm"></i> Neural Networks</a>   <a href="/blog/blog/tag/backpropagation"> <i class="fa-solid fa-hashtag fa-sm"></i> Backpropagation</a>   <a href="/blog/blog/tag/gradient-descent"> <i class="fa-solid fa-hashtag fa-sm"></i> Gradient Descent</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>As a budding data scientist or machine learning engineer, you’ve probably heard the buzzwords: neural networks, deep learning, AI. You’ve seen models classify images, translate languages, and even generate art. But peel back the layers of these impressive feats, and you’ll find a foundational algorithm working tirelessly behind the scenes, an elegant piece of calculus that makes it all possible: <strong>Backpropagation</strong>.</p> <p>It’s one of those topics that can feel intimidating at first, shrouded in complex Greek letters and partial derivatives. But trust me, once you grasp its core intuition, it feels less like a mystical incantation and more like a clever, systematic way to learn from mistakes. Think of this as your personal journal entry into understanding the “how” behind AI’s learning process.</p> <h3 id="the-problem-a-brain-that-doesnt-know-what-its-doing-yet">The Problem: A Brain That Doesn’t Know What It’s Doing (Yet!)</h3> <p>Imagine you’re trying to teach a baby how to distinguish between a cat and a dog. Initially, the baby has no idea. You show them a picture of a cat and say “cat.” Then a dog and say “dog.” They make mistakes, and you correct them. Over time, they start to get it right.</p> <p>A neural network is much like that baby. At its core, it’s a collection of interconnected “neurons” organized into layers. Each connection between neurons has a numerical value called a <strong>weight</strong>, and each neuron has a <strong>bias</strong>. These weights and biases are the network’s “knowledge” or parameters. When you first create a neural network, these weights and biases are initialized randomly – meaning the network literally knows nothing. Its initial predictions are pure guesswork.</p> <p>Here’s a super simplified view of what a single neuron does:</p> <p>It takes inputs ($x_1, x_2, \dots, x_n$), multiplies them by their respective weights ($w_1, w_2, \dots, w_n$), adds a bias ($b$), sums them up ($z$), and then passes this sum through an activation function ($\sigma$) to produce an output ($a$).</p> <p>$z = \sum_{i} w_i x_i + b$ $a = \sigma(z)$</p> <p>When you string many of these neurons together into layers, you get a neural network. Information flows from the input layer, through “hidden” layers, to the output layer – this is called the <strong>forward pass</strong>.</p> <h3 id="measuring-mistakes-the-loss-function">Measuring Mistakes: The Loss Function</h3> <p>After the network makes a prediction during the forward pass, we need a way to tell <em>how wrong</em> it was. This is where the <strong>loss function</strong> (or cost function) comes in. It’s a mathematical function that quantifies the difference between the network’s prediction ($\hat{y}$) and the actual correct answer ($y$).</p> <p>A common loss function for regression tasks is the Mean Squared Error (MSE):</p> <p>$L = \frac{1}{2}(y - \hat{y})^2$</p> <p>The <code class="language-plaintext highlighter-rouge">1/2</code> is just for convenience when differentiating later. The larger the difference between $y$ and $\hat{y}$, the larger the loss. Our ultimate goal? To <strong>minimize this loss</strong>. We want to tweak our weights and biases so that the network’s predictions get as close as possible to the true values, making the loss as small as possible.</p> <h3 id="finding-the-right-path-gradient-descent">Finding the Right Path: Gradient Descent</h3> <p>How do we minimize the loss? Imagine you’re standing on a mountain (the “loss landscape”), and you want to reach the lowest point (the minimum loss). If you can only see your immediate surroundings, what do you do? You look around and take a small step in the direction that goes downhill most steeply.</p> <p>This “direction of steepest descent” is precisely what the <strong>gradient</strong> tells us. In multivariable calculus, the gradient is a vector that points in the direction of the greatest increase of a function. If we want to <em>decrease</em> the function, we move in the opposite direction of the gradient.</p> <p>So, for each weight ($w$) and bias ($b$) in our network, we want to calculate how much a tiny change in that weight or bias would affect the total loss. Mathematically, we want to find the partial derivatives: $\frac{\partial L}{\partial w}$ and $\frac{\partial L}{\partial b}$. These are our gradients.</p> <p>Once we have these gradients, we update our weights and biases using the following rule:</p> <p>$w_{new} = w_{old} - \alpha \frac{\partial L}{\partial w}$ $b_{new} = b_{old} - \alpha \frac{\partial L}{\partial b}$</p> <p>Here, $\alpha$ is the <strong>learning rate</strong>, a small positive number that controls the size of our steps down the mountain. If $\alpha$ is too large, we might overshoot the minimum; if it’s too small, learning will be very slow. This iterative process of calculating gradients and updating parameters is called <strong>Gradient Descent</strong>.</p> <h3 id="the-challenge-gradients-in-deep-networks">The Challenge: Gradients in Deep Networks</h3> <p>Calculating $\frac{\partial L}{\partial w}$ for a single weight in a simple network seems manageable. But what about a deep neural network with millions of weights, spanning many layers? How does a weight in the <em>first</em> layer affect the final loss? Its impact is indirect, filtered through multiple layers of neurons and activation functions.</p> <p>Manually calculating each $\frac{\partial L}{\partial w}$ would be incredibly inefficient, like trying to figure out the effect of one tiny screw on a car’s top speed by rebuilding the entire car for every screw adjustment.</p> <p>This is where <strong>Backpropagation</strong> swoops in as our hero.</p> <h3 id="backpropagation-the-chain-rules-grand-tour">Backpropagation: The Chain Rule’s Grand Tour</h3> <p>Backpropagation is an ingenious algorithm that efficiently calculates all the gradients $\frac{\partial L}{\partial w}$ and $\frac{\partial L}{\partial b}$ for <em>every</em> weight and bias in the network. Its brilliance lies in reusing calculations and propagating the “error signal” backward through the network, leveraging the <strong>chain rule</strong> of calculus.</p> <p>Let’s break down the intuition:</p> <ol> <li> <p><strong>Forward Pass:</strong> We first perform a forward pass, feeding input data through the network, layer by layer, calculating the activations of all neurons, until we get the final output prediction $\hat{y}$. At this point, we also calculate the overall loss $L$.</p> </li> <li> <p><strong>Calculate Output Layer Error:</strong> The journey backward begins at the very end. We know how much the final prediction $\hat{y}$ deviates from the true label $y$, and therefore, we can calculate how much the loss function $L$ changes with respect to the output neuron’s activation. This is our initial “error signal” or “sensitivity.”</p> <p>More precisely, we calculate the gradient of the loss with respect to the weighted input ($z$) of the output layer. Let’s call this $\delta^{(L)}$ (delta for the Last layer):</p> <p>$\delta^{(L)} = \frac{\partial L}{\partial a^{(L)}} \cdot \sigma’(z^{(L)})$</p> <p>Where $\frac{\partial L}{\partial a^{(L)}}$ is how sensitive the loss is to the output activation, and $\sigma’(z^{(L)})$ is the derivative of the activation function, telling us how sensitive the activation is to its weighted input.</p> </li> <li> <p><strong>Propagate Error Backward:</strong> Now, the magic happens. We have the error signal for the <em>output layer</em>. How does this error relate to the weights and biases in the <em>previous</em> hidden layer? And then the layer before that?</p> <p>This is where the chain rule comes into play. The change in loss due to a weight in an earlier layer depends on:</p> <ul> <li>How much that weight affects its neuron’s output.</li> <li>How much that neuron’s output affects the next layer’s neurons.</li> <li>…and so on, all the way to how the final output neuron affects the loss.</li> </ul> <p>Instead of recalculating everything, backpropagation cleverly reuses the error signals. For each layer $l$, the error signal $\delta^{(l)}$ can be calculated from the error signal of the <em>next</em> layer, $\delta^{(l+1)}$:</p> <p>$\delta^{(l)} = ((W^{(l+1)})^T \delta^{(l+1)}) \cdot \sigma’(z^{(l)})$</p> <ul> <li>$(W^{(l+1)})^T$: This transposes the weights of the <em>next</em> layer, effectively “routing” the error signal backward through the same connections (but in reverse).</li> <li>$\delta^{(l+1)}$: The error from the layer ahead.</li> <li>$\sigma’(z^{(l)})$ : The derivative of the activation function at the current layer, telling us how sensitive the current neuron’s activation is to its input.</li> </ul> <p>Think of it like tracing a defect in a manufacturing line: if the final product is flawed, you can trace the blame backward. The earlier a machine is in the line, the more its blame is “filtered” by subsequent machines.</p> </li> <li> <p><strong>Calculate Gradients for Weights and Biases:</strong> Once we have these $\delta$ (error signals) for each layer, calculating the individual gradients for weights and biases becomes straightforward:</p> <p>For a weight $w^{(l)}<em>{jk}$ (connecting neuron $k$ in layer $l-1$ to neuron $j$ in layer $l$): $\frac{\partial L}{\partial w^{(l)}</em>{jk}} = a_k^{(l-1)} \delta_j^{(l)}$</p> <p>For a bias $b^{(l)}<em>{j}$ (for neuron $j$ in layer $l$): $\frac{\partial L}{\partial b^{(l)}</em>{j}} = \delta_j^{(l)}$</p> <p>Essentially, the gradient for a weight is the activation of the neuron it comes <em>from</em> multiplied by the error signal of the neuron it goes <em>to</em>. The gradient for a bias is simply the error signal of its neuron.</p> </li> </ol> <h3 id="why-is-backpropagation-so-powerful">Why is Backpropagation So Powerful?</h3> <p>The true genius of backpropagation is its <strong>efficiency</strong>. Instead of calculating each gradient independently (which would involve running a separate forward pass for each parameter in a finite difference approximation, for instance), it calculates <em>all</em> gradients for <em>all</em> parameters in essentially two passes: one forward, one backward.</p> <p>This efficiency is what allowed deep neural networks to become computationally feasible. Without backpropagation, training networks with millions or billions of parameters would be impossible within a reasonable timeframe. It’s the engine that powers modern AI, enabling systems to learn from vast amounts of data.</p> <h3 id="the-learning-loop-putting-it-all-together">The Learning Loop: Putting It All Together</h3> <p>So, a typical training step for a neural network looks like this:</p> <ol> <li> <strong>Initialize Weights/Biases:</strong> Start with random values.</li> <li> <strong>Loop for many “epochs” (training iterations):</strong> a. <strong>Forward Pass:</strong> Feed input data through the network, calculate activations for all neurons, and get the final prediction $\hat{y}$. b. <strong>Calculate Loss:</strong> Compare $\hat{y}$ with the true label $y$ using the loss function $L$. c. <strong>Backward Pass (Backpropagation):</strong> Starting from the output layer, calculate the error signals ($\delta$) for each layer backward. Use these error signals to compute $\frac{\partial L}{\partial w}$ and $\frac{\partial L}{\partial b}$ for all weights and biases. d. <strong>Update Parameters:</strong> Adjust all weights and biases using gradient descent: $w \leftarrow w - \alpha \frac{\partial L}{\partial w}$ and $b \leftarrow b - \alpha \frac{\partial L}{\partial b}$.</li> <li> <strong>Repeat:</strong> Go back to step 2a with the updated parameters, continuously refining the network’s knowledge until the loss is minimized (or stops improving significantly).</li> </ol> <h3 id="backpropagation-the-unsung-hero">Backpropagation: The Unsung Hero</h3> <p>Backpropagation, discovered and popularized by Werbos, Rumelhart, Hinton, and Williams in the 1970s and 80s, is not just an algorithm; it’s a cornerstone of modern artificial intelligence. It transformed neural networks from theoretical curiosities into powerful learning machines.</p> <p>Understanding backpropagation isn’t just about memorizing formulas; it’s about grasping the elegance of how a complex system can systematically learn from its errors, layer by layer, connection by connection. It’s the core mechanism that allows AI to “understand” patterns, make decisions, and push the boundaries of what machines can achieve.</p> <p>So, the next time you marvel at an AI’s capabilities, remember the quiet, mathematical magic of backpropagation, diligently working behind the scenes, turning mistakes into knowledge. It’s truly the alchemist’s secret to turning raw data into intelligence.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>