<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Data Scientist's Litmus Test: Unmasking Truth with Hypothesis Testing | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/2024/the-data-scientists-litmus-test-unmasking-truth-wi/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Data Scientist's Litmus Test: Unmasking Truth with Hypothesis Testing</h1> <p class="post-meta"> Created on July 12, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/hypothesis-testing"> <i class="fa-solid fa-hashtag fa-sm"></i> Hypothesis Testing</a>   <a href="/blog/blog/tag/statistics"> <i class="fa-solid fa-hashtag fa-sm"></i> Statistics</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/a-b-testing"> <i class="fa-solid fa-hashtag fa-sm"></i> A/B Testing</a>   <a href="/blog/blog/tag/inferential-statistics"> <i class="fa-solid fa-hashtag fa-sm"></i> Inferential Statistics</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hello, fellow data explorer!</p> <p>Have you ever found yourself staring at a dataset, a hunch brewing in your mind, but no solid way to prove or disprove it? Maybe you just launched a new feature on your website, and your analytics dashboard shows a slight increase in user engagement. Is it real? Is it significant? Or just a random fluctuation, a trick of the light in the vast, noisy ocean of data?</p> <p>This is where one of the most powerful tools in the data scientist’s arsenal comes into play: <strong>Hypothesis Testing</strong>. For me, understanding hypothesis testing was like gaining a superpower – suddenly, I could move beyond just <em>describing</em> data to <em>making inferences</em> about the world it represents. It’s the bridge from raw observations to confident, data-driven decisions.</p> <h3 id="the-great-game-of-doubt-and-evidence">The Great Game of Doubt and Evidence</h3> <p>Imagine you’re a detective. Someone makes a claim: “The average processing time for our new algorithm is faster than the old one.” As a good detective (or data scientist!), your first instinct isn’t to believe it outright. Your default position is doubt. You assume the <em>status quo</em> is true – that there’s <em>no difference</em> in processing times. This is your starting point, your <strong>null hypothesis</strong>.</p> <p>Then, you go looking for evidence. You gather data (run the new algorithm multiple times, measure processing times). If the evidence you collect is so overwhelmingly different from what you’d expect under the <em>status quo</em>, then you might conclude that your initial assumption (the null hypothesis) was probably wrong. You’d then accept the alternative – that the new algorithm <em>is</em> faster.</p> <p>This little mental model – the courtroom drama of doubt and evidence – is the heart of hypothesis testing.</p> <h3 id="setting-the-stage-null-and-alternative-hypotheses">Setting the Stage: Null and Alternative Hypotheses</h3> <p>Let’s formalize our detective work. Every hypothesis test starts with two opposing statements:</p> <ol> <li> <strong>The Null Hypothesis ($H_0$)</strong>: This is the statement of “no effect,” “no difference,” or “no change.” It’s your default assumption, the position you are trying to find evidence <em>against</em>. <ul> <li> <em>Example</em>: $H_0$: The average processing time of the new algorithm is <em>equal to</em> the old algorithm ($\mu_{new} = \mu_{old}$).</li> <li> <em>Example</em>: $H_0$: The new marketing campaign has <em>no effect</em> on sales (sales remain the same).</li> <li>Think of it as the “innocent until proven guilty” statement.</li> </ul> </li> <li> <strong>The Alternative Hypothesis ($H_1$ or $H_A$)</strong>: This is the statement you are trying to prove. It’s the claim that an effect, difference, or change <em>does</em> exist. It’s often what you, the researcher, suspect to be true. <ul> <li> <em>Example</em>: $H_1$: The average processing time of the new algorithm is <em>faster than</em> the old algorithm ($\mu_{new} &lt; \mu_{old}$).</li> <li> <em>Example</em>: $H_1$: The new marketing campaign <em>increased</em> sales (sales are higher).</li> <li>This is the “guilty” statement, which requires significant evidence to accept.</li> </ul> </li> </ol> <p>It’s crucial that these two hypotheses are mutually exclusive and collectively exhaustive. They cover all possibilities.</p> <h3 id="the-perilous-path-type-i-and-type-ii-errors">The Perilous Path: Type I and Type II Errors</h3> <p>No detective work is flawless, and neither is statistical inference. We can make mistakes. There are two types of errors we worry about:</p> <ul> <li> <strong>Type I Error ($\alpha$)</strong>: This is like convicting an innocent person. You <strong>reject the null hypothesis ($H_0$) when it is actually true.</strong> <ul> <li> <em>In our algorithm example</em>: You conclude the new algorithm is faster, but in reality, it’s not (any observed difference was just random chance).</li> <li>This is often called a “false positive.”</li> <li>The probability of making a Type I error is denoted by $\alpha$ (alpha), also known as the <strong>significance level</strong>. We typically set $\alpha$ to values like 0.05 (5%) or 0.01 (1%). This means we are willing to accept a 5% (or 1%) chance of making a Type I error.</li> </ul> </li> <li> <strong>Type II Error ($\beta$)</strong>: This is like letting a guilty person go free. You <strong>fail to reject the null hypothesis ($H_0$) when it is actually false.</strong> <ul> <li> <em>In our algorithm example</em>: You conclude the new algorithm is <em>not</em> faster (or there’s no sufficient evidence to say it is), but in reality, it <em>is</em> faster. You missed out on an improvement!</li> <li>This is often called a “false negative.”</li> <li>The probability of making a Type II error is denoted by $\beta$ (beta). The “power” of a test is $1 - \beta$, representing the probability of correctly rejecting a false null hypothesis.</li> </ul> </li> </ul> <p>There’s a trade-off here: reducing the chance of a Type I error (e.g., setting a very low $\alpha$) often increases the chance of a Type II error, and vice versa. As data scientists, we need to carefully consider the consequences of each type of error in our specific context. For instance, in drug testing, a Type I error (approving a drug that doesn’t work) might be worse than a Type II error (missing a drug that does work).</p> <h3 id="the-evidence-collector-test-statistics-and-the-p-value">The Evidence Collector: Test Statistics and the P-value</h3> <p>Once we have our hypotheses and a chosen significance level ($\alpha$), we gather data and calculate a <strong>test statistic</strong>. This statistic is a single number that summarizes your sample data in a way that allows you to compare it to what you would expect if the null hypothesis were true.</p> <p>The specific formula for the test statistic depends on the type of data, the hypotheses, and the underlying distribution. Common test statistics include:</p> <ul> <li> <strong>Z-score</strong>: For large samples or when the population standard deviation is known.</li> <li> <strong>T-score</strong>: For smaller samples or when the population standard deviation is unknown (which is most common!).</li> <li> <strong>Chi-squared statistic</strong>: For categorical data.</li> <li> <strong>F-statistic</strong>: For comparing variances or multiple group means (ANOVA).</li> </ul> <p>For our example, let’s say we’re comparing a sample mean to a known population mean (or a hypothesized mean). A common test statistic is the t-statistic:</p> <p>$t = \frac{\bar{x} - \mu_0}{s/\sqrt{n}}$</p> <p>Where:</p> <ul> <li>$\bar{x}$ is our sample mean.</li> <li>$\mu_0$ is the hypothesized population mean from our null hypothesis.</li> <li>$s$ is the sample standard deviation.</li> <li>$n$ is the sample size.</li> </ul> <p>This formula essentially tells us <em>how many standard errors</em> our sample mean is away from the hypothesized population mean. A larger absolute value of $t$ indicates that our sample mean is further away from what $H_0$ predicts.</p> <p>Once we have our test statistic, we calculate the <strong>p-value</strong>. This is perhaps the most famous (and often misunderstood) number in hypothesis testing.</p> <p><strong>The p-value is the probability of observing a test statistic as extreme as, or more extreme than, the one calculated from your sample data, <em>assuming that the null hypothesis ($H_0$) is true</em>.</strong></p> <p>Let’s break that down:</p> <ul> <li>“As extreme as, or more extreme than”: If you got a t-score of 2.5, you’re asking, “What’s the probability of getting a t-score of 2.5 or higher (or -2.5 or lower, if it’s a two-tailed test) <em>just by random chance</em> if $H_0$ were true?”</li> <li>“Assuming that the null hypothesis is true”: This is critical! The p-value <em>does not</em> tell you the probability that the null hypothesis is true. It tells you how likely your data is <em>if the null hypothesis is true</em>.</li> </ul> <p>A small p-value means your observed data would be very unlikely if $H_0$ were true. This makes you suspicious of $H_0$.</p> <h3 id="the-verdict-making-your-decision">The Verdict: Making Your Decision</h3> <p>The final step is to compare your p-value to your chosen significance level ($\alpha$).</p> <ul> <li> <strong>If p-value $\leq \alpha$</strong>: This means our observed data is unlikely to have occurred if $H_0$ were true. We have strong enough evidence to <strong>reject the null hypothesis ($H_0$)</strong>. We then conclude that there is statistically significant evidence to support the alternative hypothesis ($H_1$).</li> <li> <strong>If p-value $&gt; \alpha$</strong>: This means our observed data is reasonably likely to occur even if $H_0$ were true. We <strong>fail to reject the null hypothesis ($H_0$)</strong>. This <em>does not</em> mean we accept $H_0$ as true; it simply means we don’t have enough evidence to confidently say it’s false.</li> </ul> <p>It’s like the detective saying, “I don’t have enough evidence to convict, so I’m letting the suspect go.” It doesn’t mean the suspect is innocent, just that the case isn’t strong enough.</p> <h3 id="a-walkthrough-example-the-average-app-engagement">A Walkthrough Example: The Average App Engagement</h3> <p>Let’s imagine you’re a data scientist at a company that just released a new version of its mobile app. Historically, the average daily engagement time for users of the old app was 30 minutes ($\mu_0 = 30$). You want to know if the new app has genuinely increased engagement.</p> <ol> <li> <strong>Formulate Hypotheses:</strong> <ul> <li>$H_0$: The average daily engagement time for the new app is still 30 minutes ($\mu = 30$). (No change)</li> <li>$H_1$: The average daily engagement time for the new app is greater than 30 minutes ($\mu &gt; 30$). (We suspect an increase)</li> <li>This is a <strong>one-tailed test</strong> because we’re only interested if the engagement <em>increased</em>. If we wanted to know if it <em>changed</em> (either increased or decreased), it would be a two-tailed test ($\mu \neq 30$).</li> </ul> </li> <li> <strong>Set the Significance Level ($\alpha$):</strong> <ul> <li>Let’s choose $\alpha = 0.05$. This means we’re willing to accept a 5% chance of falsely concluding that engagement increased when it actually didn’t.</li> </ul> </li> <li> <strong>Collect Data and Calculate Test Statistic:</strong> <ul> <li>You collect a random sample of $n=100$ users of the new app.</li> <li>You find their average daily engagement time is $\bar{x} = 32.5$ minutes.</li> <li>The sample standard deviation is $s = 10$ minutes.</li> </ul> <p>Now, calculate the t-statistic: $t = \frac{\bar{x} - \mu_0}{s/\sqrt{n}}$ $t = \frac{32.5 - 30}{10/\sqrt{100}}$ $t = \frac{2.5}{10/10}$ $t = \frac{2.5}{1}$ $t = 2.5$</p> <p>Our calculated t-statistic is 2.5. This tells us our sample mean (32.5) is 2.5 standard errors above the hypothesized population mean (30).</p> </li> <li> <strong>Determine the P-value:</strong> <ul> <li>To find the p-value, we need to consult a t-distribution table or use statistical software (like Python’s <code class="language-plaintext highlighter-rouge">scipy.stats</code>). For a one-tailed test with $n-1 = 99$ degrees of freedom and a t-statistic of 2.5, the p-value is approximately 0.007.</li> <li>This means there’s a 0.7% chance of observing an average engagement time of 32.5 minutes or more, <em>if the true average engagement time was still 30 minutes</em>.</li> </ul> </li> <li> <strong>Make a Decision:</strong> <ul> <li>Compare the p-value to $\alpha$: $0.007 \leq 0.05$.</li> <li>Since the p-value (0.007) is less than our significance level (0.05), we <strong>reject the null hypothesis ($H_0$)</strong>.</li> </ul> </li> <li> <strong>Interpret the Results:</strong> <ul> <li>We conclude that there is statistically significant evidence, at the 0.05 level, to suggest that the new app <em>has</em> increased the average daily user engagement time beyond 30 minutes.</li> <li>This result can confidently inform product decisions, marketing strategies, and resource allocation. It’s not just a hunch; it’s data-backed!</li> </ul> </li> </ol> <h3 id="beyond-the-basics-what-else-to-consider">Beyond the Basics: What Else to Consider</h3> <ul> <li> <p><strong>Confidence Intervals</strong>: Often, hypothesis testing goes hand-in-hand with confidence intervals. A 95% confidence interval for our new app’s average engagement might be, for example, [30.5, 34.5] minutes. If this interval does <em>not</em> contain the null hypothesis value (30 minutes), it aligns with rejecting the null hypothesis. Confidence intervals give you a range of plausible values for the true population parameter, not just a yes/no answer.</p> </li> <li> <p><strong>Assumptions</strong>: Every statistical test comes with assumptions (e.g., normality of data, independence of observations, equal variances). Violating these assumptions can invalidate your results. A good data scientist always checks these before trusting their p-values.</p> </li> <li> <p><strong>Effect Size</strong>: While a p-value tells you if an effect is statistically significant, it doesn’t tell you if it’s <em>practically</em> significant. A very small effect could be statistically significant with a large enough sample size. Effect size measures (like Cohen’s d) quantify the magnitude of the observed effect, giving you a fuller picture.</p> </li> </ul> <h3 id="wrapping-up">Wrapping Up</h3> <p>Hypothesis testing is more than just a statistical procedure; it’s a structured way of thinking critically about data. It empowers us to move beyond gut feelings and anecdotal evidence, enabling us to make informed, defensible decisions in the often-ambiguous world of data science and machine learning. From A/B testing new website designs to evaluating the performance of a new machine learning model, the principles of hypothesis testing are fundamental.</p> <p>So, the next time you encounter a claim or a potential pattern in your data, put on your detective hat. Formulate your hypotheses, gather your evidence, weigh the probabilities, and let the data speak for itself. Your statistical superpower awaits!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>