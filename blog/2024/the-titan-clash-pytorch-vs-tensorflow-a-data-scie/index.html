<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Titan Clash: PyTorch vs. TensorFlow – A Data Scientist's Deep Dive | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/the-titan-clash-pytorch-vs-tensorflow-a-data-scie/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Titan Clash: PyTorch vs. TensorFlow – A Data Scientist's Deep Dive</h1> <p class="post-meta"> Created on November 04, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/pytorch"> <i class="fa-solid fa-hashtag fa-sm"></i> PyTorch</a>   <a href="/blog/blog/tag/tensorflow"> <i class="fa-solid fa-hashtag fa-sm"></i> TensorFlow</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/ai-frameworks"> <i class="fa-solid fa-hashtag fa-sm"></i> AI Frameworks</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hello, fellow explorers of the digital frontier!</p> <p>If you’re anything like me, when you first dipped your toes into the exhilarating ocean of Deep Learning, you probably encountered two colossal names: <strong>PyTorch</strong> and <strong>TensorFlow</strong>. It felt a bit like choosing between two superpowers, each promising to unlock incredible potential. For a long time, the debate was fierce, almost tribal. But as with all things in tech, the landscape evolves, and today, the story is far more nuanced, more fascinating.</p> <p>This isn’t just a dry comparison; it’s a personal journey through understanding these frameworks, what makes them tick, and how they empower us to build truly intelligent systems. Whether you’re a high school student just getting started or a seasoned data scientist, I hope this deep dive clarifies the unique magic of each and helps you decide which tool fits your next grand project.</p> <h3 id="the-genesis-a-tale-of-two-philosophies">The Genesis: A Tale of Two Philosophies</h3> <p>Before we pit them against each other, let’s understand their origins.</p> <p><strong>TensorFlow</strong>, born out of Google Brain in 2015, was designed with a grand vision: to be a robust, scalable, and production-ready framework for machine intelligence. It aimed to support research <em>and</em> deployment from the get-go, powering everything from Google Search to self-driving cars. Its initial versions (TensorFlow 1.x) were known for their power but also for a steeper learning curve, often feeling less “Pythonic.”</p> <p><strong>PyTorch</strong>, developed by Facebook’s AI Research lab (FAIR) and open-sourced in 2016, emerged with a different philosophy. It prioritized flexibility, ease of use, and a more intuitive, Python-integrated approach. It quickly became the darling of the research community, especially for rapid prototyping and exploring novel architectures.</p> <p>At their core, both frameworks provide the fundamental building blocks for deep learning: <strong>Tensors</strong> and <strong>Automatic Differentiation</strong>.</p> <h4 id="tensors-the-universal-language-of-deep-learning">Tensors: The Universal Language of Deep Learning</h4> <p>Think of Tensors as the fundamental data structure in deep learning. Just like a number is a 0-dimensional array, a list of numbers is a 1-dimensional array (vector), and a matrix is a 2-dimensional array, Tensors are simply <strong>$n$-dimensional arrays</strong>. They are the numerical containers for all data in your neural network – images, text, weights, biases, activations, you name it. Both PyTorch and TensorFlow rely heavily on them, providing optimized operations for Tensor manipulation, often leveraging GPU acceleration.</p> <h4 id="automatic-differentiation-autograd-the-magic-behind-learning">Automatic Differentiation (Autograd): The Magic Behind Learning</h4> <p>How do neural networks learn? Through a process called <strong>backpropagation</strong>, which relies on calculating <strong>gradients</strong>. A gradient, in simple terms, tells us the direction and magnitude of change in our model’s error (loss) with respect to its parameters (weights and biases). To minimize this error, we use optimization algorithms (like <strong>Gradient Descent</strong>), which nudge the parameters in the direction opposite to the gradient.</p> <p>This is where automatic differentiation comes in. It’s an incredibly clever technique that computes these gradients efficiently. Both frameworks have sophisticated <code class="language-plaintext highlighter-rouge">autograd</code> engines. When you perform an operation on a tensor, the framework automatically builds a computational graph in the background, keeping track of how outputs were derived from inputs. When it’s time to backpropagate, it traverses this graph backward, applying the chain rule of calculus to compute all the necessary gradients.</p> <p>A simplified gradient descent update rule looks something like this: $ \theta_{new} = \theta_{old} - \alpha \nabla J(\theta) $ where $ \theta $ represents our model parameters, $ \alpha $ is the learning rate (how big a step we take), and $ \nabla J(\theta) $ is the gradient of our loss function $ J $ with respect to $ \theta $.</p> <h3 id="the-heart-of-the-matter-static-vs-dynamic-graphs-and-their-evolution">The Heart of the Matter: Static vs. Dynamic Graphs (and their Evolution)</h3> <p>This is perhaps the most significant historical differentiator between PyTorch and TensorFlow 1.x.</p> <p><strong>TensorFlow 1.x (Static Graphs - “Define-and-Run”)</strong> Imagine you’re an architect. With TF 1.x, you first had to draw the entire blueprint of your house (the computational graph). You’d define all the operations – additions, multiplications, convolutions – but no actual computation would happen yet. Only after the entire blueprint was complete would you “run” it by feeding data into a <code class="language-plaintext highlighter-rouge">tf.Session</code>.</p> <ul> <li> <strong>Pros:</strong> Highly optimized for deployment, potential for global optimizations, easier to export to other platforms once compiled.</li> <li> <strong>Cons:</strong> Very difficult to debug (standard Python debuggers couldn’t inspect intermediate values easily), less flexible for models with dynamic structures (e.g., RNNs where sequence lengths vary).</li> </ul> <p><strong>PyTorch (Dynamic Graphs - “Define-by-Run”)</strong> Now, imagine you’re building with LEGOs. With PyTorch, you add one block, then another, and as you add each block, you can immediately see the result. The computational graph is built on the fly, as your code executes. If you have a conditional statement or a loop, the graph adapts during execution.</p> <ul> <li> <strong>Pros:</strong> Incredibly flexible, much easier to debug (because it behaves like standard Python code), intuitive, great for research and rapid prototyping.</li> <li> <strong>Cons:</strong> Historically, deployment was perceived as less straightforward than TF 1.x.</li> </ul> <p><strong>TensorFlow 2.x (Eager Execution - “Define-by-Run” by Default)</strong> This is where the story gets really interesting! Google learned from PyTorch’s success. With TensorFlow 2.x, released in 2019, they fundamentally changed their approach. <strong>Eager execution</strong> became the default, meaning operations are executed immediately and return their values, just like in PyTorch. The <code class="language-plaintext highlighter-rouge">tf.keras</code> API also became the central, high-level way to build models, offering a much more user-friendly experience.</p> <p>This shift brought TensorFlow much closer to PyTorch’s development experience, significantly bridging the gap that once defined their rivalry.</p> <h3 id="beyond-graphs-a-feature-by-feature-showdown">Beyond Graphs: A Feature-by-Feature Showdown</h3> <p>With TF 2.x blurring the lines on dynamic graphs, let’s look at other key comparison points.</p> <ol> <li> <strong>Ease of Use &amp; Learning Curve:</strong> <ul> <li> <strong>PyTorch:</strong> Often cited as more “Pythonic” and intuitive, especially for those familiar with Python. Its dynamic graph makes debugging a breeze, feeling very similar to debugging any other Python script. I remember picking it up and feeling immediately productive.</li> <li> <strong>TensorFlow:</strong> TF 1.x had a notoriously steep learning curve. However, TF 2.x, with Keras as its high-level API and eager execution, has dramatically improved. It’s now very user-friendly and comparable to PyTorch in terms of initial learning.</li> </ul> </li> <li> <strong>Debugging:</strong> <ul> <li> <strong>PyTorch:</strong> Its define-by-run nature means you can use standard Python debuggers (like <code class="language-plaintext highlighter-rouge">pdb</code> or integrated IDE debuggers) to step through your code, inspect tensors at any point, and understand exactly what’s happening. This is a huge win for troubleshooting.</li> <li> <strong>TensorFlow:</strong> TF 1.x debugging was notoriously difficult. TF 2.x’s eager execution makes debugging significantly better, bringing it much closer to PyTorch’s experience.</li> </ul> </li> <li> <strong>Community &amp; Ecosystem:</strong> <ul> <li> <strong>TensorFlow:</strong> Being older and backed by Google, it boasts a massive, mature community and a vast ecosystem. This includes: <ul> <li> <strong>TensorBoard:</strong> A powerful visualization tool.</li> <li> <strong>TensorFlow Lite:</strong> For mobile and embedded devices.</li> <li> <strong>TensorFlow.js:</strong> For running models in the browser.</li> <li> <strong>TF Serving:</strong> For production deployment.</li> <li> <strong>TPU support:</strong> Excellent integration with Google’s custom hardware.</li> </ul> </li> <li> <strong>PyTorch:</strong> While younger, its community is incredibly vibrant and rapidly growing, especially in academic research. It has a rich set of official libraries like <code class="language-plaintext highlighter-rouge">torchvision</code>, <code class="language-plaintext highlighter-rouge">torchaudio</code>, and <code class="language-plaintext highlighter-rouge">torchtext</code> for specific domains. It’s also gaining traction in industry, and its research momentum often leads to new state-of-the-art models being released first in PyTorch.</li> </ul> </li> <li> <strong>Deployment &amp; Production:</strong> <ul> <li> <strong>TensorFlow:</strong> Historically, this was TensorFlow’s undisputed strong suit. Tools like TF Serving, TF Lite, and TF.js made deploying models at scale, on various platforms, relatively seamless. Its static graph compilation in TF 1.x was also an advantage for optimization.</li> <li> <strong>PyTorch:</strong> While initially less focused on production, PyTorch has made massive strides with <strong>TorchScript</strong> and <strong>LibTorch</strong>. TorchScript allows you to serialize PyTorch models into a static graph format that can be run independently of Python (e.g., in C++ applications), making deployment much more robust. <strong>ONNX</strong> (Open Neural Network Exchange) also provides an interoperability standard that both frameworks support, allowing for model conversion between them.</li> </ul> </li> <li> <strong>Model Building APIs:</strong> <ul> <li> <strong>PyTorch:</strong> Typically involves building models using <code class="language-plaintext highlighter-rouge">torch.nn.Module</code> classes. You define your layers in the <code class="language-plaintext highlighter-rouge">__init__</code> method and specify the forward pass in the <code class="language-plaintext highlighter-rouge">forward</code> method. It offers a lot of flexibility.</li> <li> <strong>TensorFlow:</strong> <code class="language-plaintext highlighter-rouge">tf.keras</code> is the default and recommended high-level API. Keras offers a very user-friendly way to build models (Sequential API, Functional API) and is incredibly powerful. For more custom needs, you can still subclass <code class="language-plaintext highlighter-rouge">tf.keras.Model</code> or <code class="language-plaintext highlighter-rouge">tf.keras.layers.Layer</code>, giving you fine-grained control.</li> </ul> </li> <li> <strong>Data Pipelining:</strong> <ul> <li> <strong>TensorFlow:</strong> <code class="language-plaintext highlighter-rouge">tf.data</code> is an incredibly powerful and efficient API for building complex and performant input pipelines, especially for large datasets. It handles operations like loading, preprocessing, batching, and shuffling very well.</li> <li> <strong>PyTorch:</strong> While it doesn’t have a single, unified data API as comprehensive as <code class="language-plaintext highlighter-rouge">tf.data</code>, it provides <code class="language-plaintext highlighter-rouge">torch.utils.data.Dataset</code> and <code class="language-plaintext highlighter-rouge">torch.utils.data.DataLoader</code>. You create custom <code class="language-plaintext highlighter-rouge">Dataset</code> classes to load individual data samples and use <code class="language-plaintext highlighter-rouge">DataLoader</code> to handle batching, shuffling, and multi-process loading. It’s highly flexible but often requires a bit more manual setup for complex pipelines.</li> </ul> </li> </ol> <h3 id="when-to-choose-which">When to Choose Which?</h3> <p>The “right” choice often depends on your specific needs, team background, and project goals.</p> <p><strong>Choose PyTorch if:</strong></p> <ul> <li> <strong>You’re in research or rapid prototyping:</strong> Its flexibility and ease of debugging make iterating on new ideas incredibly fast.</li> <li> <strong>You prefer a more “Pythonic” and imperative style:</strong> If you love the feel of standard Python, PyTorch will likely resonate more with you.</li> <li> <strong>You’re dealing with dynamic graphs:</strong> Models like certain RNNs, Transformers, or architectures with varying input sizes or computational paths are often more natural to implement in PyTorch.</li> <li> <strong>You’re just starting out:</strong> Many beginners find PyTorch’s API more approachable initially.</li> <li><strong>Your team has a strong Python background and prioritizes quick experimentation.</strong></li> </ul> <p><strong>Choose TensorFlow if:</strong></p> <ul> <li> <strong>You’re focused on production deployment:</strong> TF’s robust ecosystem for deployment (TF Serving, TF Lite, TF.js) still gives it an edge for industrial-scale applications.</li> <li> <strong>You need scalability and performance for extremely large datasets:</strong> <code class="language-plaintext highlighter-rouge">tf.data</code> and <code class="language-plaintext highlighter-rouge">tf.distribute.Strategy</code> (for distributed training) are highly optimized.</li> <li> <strong>You’re working with specific hardware like Google TPUs:</strong> TensorFlow provides excellent native support.</li> <li> <strong>Your team already uses it extensively:</strong> Ecosystem lock-in is real, and leveraging existing expertise can be crucial.</li> <li><strong>You prefer a high-level API like Keras for most of your model building.</strong></li> </ul> <h3 id="the-exciting-convergence">The Exciting Convergence</h3> <p>What I find most exciting is the convergence. TensorFlow 2.x’s adoption of eager execution and Keras by default has made it significantly more user-friendly and research-friendly, blurring the lines that once sharply divided the two. PyTorch, in turn, has invested heavily in production features with TorchScript, making it a much stronger contender for deployment.</p> <p>This isn’t a winner-take-all scenario. Both frameworks are thriving, pushing the boundaries of what’s possible in AI. They are learning from each other, borrowing best practices, and ultimately giving us, the developers, more powerful and flexible tools.</p> <h3 id="my-two-cents">My Two Cents</h3> <p>In my own journey, I started with TensorFlow 1.x (and the frustration that came with it!), then found immense joy and productivity in PyTorch, especially for research projects. Now, with TensorFlow 2.x, I find myself equally comfortable in both, choosing based on the specific project’s requirements rather than an inherent bias.</p> <p>Ultimately, mastering one framework well is more valuable than superficially knowing both. The underlying concepts of deep learning – tensors, computational graphs, backpropagation, model architectures – are universal. Once you understand these, switching between frameworks becomes much easier.</p> <p>So, don’t agonize too much over the choice. Pick one, dive deep, build something incredible, and then maybe, just maybe, explore the other. The world of AI is vast, and these two titans are here to help us navigate it. Happy coding!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>