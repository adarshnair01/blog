<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> My Journey into the Digital Brain: Demystifying Neural Networks | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/my-journey-into-the-digital-brain-demystifying-neu/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">My Journey into the Digital Brain: Demystifying Neural Networks</h1> <p class="post-meta"> Created on December 28, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/neural-networks"> <i class="fa-solid fa-hashtag fa-sm"></i> Neural Networks</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey there, fellow curious minds!</p> <p>I still remember the first time I truly “got” the concept of a neural network. It wasn’t through a textbook or a dense academic paper, but through a simple, visual explanation that connected the dots between biology and bytes. It blew my mind to think that something so complex, so seemingly intelligent, could be built from such fundamental pieces. Today, I want to take you on that same journey, exploring these incredible systems that are reshaping our world, from recommending your next favorite song to powering self-driving cars.</p> <p>Let’s dive in!</p> <h3 id="the-spark-of-inspiration-our-own-brains">The Spark of Inspiration: Our Own Brains</h3> <p>Before we talk about silicon, let’s talk about goo. Yes, our brains! They are arguably the most complex and powerful “processors” known. At their core, brains are made of billions of tiny, interconnected units called <strong>neurons</strong>.</p> <p>Think of a biological neuron like this:</p> <ul> <li>It receives electrical signals (information) from other neurons through its <strong>dendrites</strong>.</li> <li>It processes these signals in its central body, the <strong>soma</strong>.</li> <li>If the sum of these signals is strong enough (exceeds a certain threshold), it “fires” and sends its own signal down its <strong>axon</strong> to other neurons.</li> </ul> <p>This elegant system of receiving, processing, and transmitting information is what allows us to think, learn, feel, and perceive the world. What if we could build something similar, but in a computer? That’s precisely where the idea of an <strong>Artificial Neural Network (ANN)</strong> began.</p> <h3 id="the-artificial-neuron-the-perceptron">The Artificial Neuron: The Perceptron</h3> <p>Our digital journey starts with the simplest building block: the <strong>artificial neuron</strong>, often called a <strong>perceptron</strong>. It’s a mathematical model inspired by its biological counterpart.</p> <p>Imagine our perceptron is trying to decide if an email is spam or not. It receives several pieces of information (inputs) about the email:</p> <ol> <li>Does it contain suspicious keywords (e.g., “free money,” “urgent”)?</li> <li>Is the sender unknown?</li> <li>Are there many typos?</li> </ol> <p>Each of these inputs ($x_1, x_2, …, x_n$) is fed into our artificial neuron. But not all inputs are equally important. Some cues might be stronger indicators of spam than others. This is where <strong>weights</strong> come in. Each input $x_i$ is multiplied by a corresponding weight $w_i$, which essentially represents how important that input is for the neuron’s decision.</p> <p>So, we sum up these weighted inputs. We also add a <strong>bias</strong> term ($b$), which you can think of as an adjustable threshold that makes it easier or harder for the neuron to “fire,” regardless of the inputs.</p> <p>Mathematically, the sum of weighted inputs plus the bias is often called the <strong>net input</strong> or <strong>pre-activation</strong>:</p> <p>$Z = (w_1 x_1 + w_2 x_2 + … + w_n x_n) + b$</p> <p>This can be written more compactly using summation notation:</p> <p>$Z = \sum_{i=1}^{n} (w_i x_i) + b$</p> <p>After calculating $Z$, the neuron applies an <strong>activation function</strong> $f$. This function decides whether the neuron “fires” and what its output will be. It introduces non-linearity, which is absolutely crucial for neural networks to learn complex patterns. Without it, stacking multiple layers would just be equivalent to a single layer, limiting their power!</p> <p>The final output ($A$) of our artificial neuron is:</p> <p>$A = f(Z)$</p> <p>Common activation functions include:</p> <ul> <li> <strong>Sigmoid:</strong> Squashes the output between 0 and 1, useful for probabilities.</li> <li> <strong>ReLU (Rectified Linear Unit):</strong> Outputs the input if positive, 0 otherwise. $f(x) = \max(0, x)$. It’s very popular for its simplicity and effectiveness.</li> </ul> <p>So, for our spam detector, if $A$ is greater than, say, 0.5, the email might be flagged as spam.</p> <h3 id="building-a-network-layers-of-understanding">Building a Network: Layers of Understanding</h3> <p>A single perceptron is quite limited. It can only solve linearly separable problems (think of drawing a single straight line to separate two groups of data points). But the real magic happens when we connect many of these artificial neurons together, forming layers, much like the intricate networks in our brains. This is what we call a <strong>Neural Network</strong>.</p> <p>A typical neural network has at least three types of layers:</p> <ol> <li> <strong>Input Layer:</strong> This layer simply receives the raw data (our spam indicators, pixels of an image, words in a sentence). There’s one neuron per input feature.</li> <li> <strong>Hidden Layers:</strong> These are the “brains” of the operation. Neurons in hidden layers process the information from the previous layer, extracting increasingly complex features and patterns. A network with many hidden layers is what we call a <strong>Deep Neural Network</strong> – hence the term “Deep Learning”! Each neuron in a hidden layer is connected to every neuron in the <em>previous</em> layer and every neuron in the <em>next</em> layer (in a “fully connected” network).</li> <li> <strong>Output Layer:</strong> This layer produces the final result. For our spam detector, it might be a single neuron outputting a probability of spam. For an image classifier identifying animals, it might have one neuron for “cat,” one for “dog,” one for “bird,” etc.</li> </ol> <p>Information flows forward through the network, from the input layer, through the hidden layers, to the output layer. This is called the <strong>forward pass</strong>.</p> <h3 id="the-art-of-learning-how-neural-networks-get-smart">The Art of Learning: How Neural Networks Get Smart</h3> <p>Now, here’s the billion-dollar question: How do these networks actually <em>learn</em>? How do they know what weights and biases to use to make accurate predictions?</p> <p>It’s an iterative process of trial and error, guided by a brilliant algorithm called <strong>Backpropagation</strong>, coupled with <strong>Gradient Descent</strong>.</p> <ol> <li> <p><strong>Start with a Guess:</strong> When we first build a neural network, all the weights and biases are initialized randomly. Naturally, its initial predictions will be terrible, like a baby just learning to walk.</p> </li> <li> <p><strong>Forward Pass (Make a Prediction):</strong> We feed our training data (e.g., millions of emails, some labeled spam, some not) through the network. It makes a prediction based on its current, random weights and biases.</p> </li> <li> <p><strong>Calculate the Error (Loss):</strong> We compare the network’s prediction ($\hat{y}$) with the actual correct answer ($y$) from our training data. The difference between these two is the <strong>error</strong> or <strong>loss</strong>. A common way to quantify this is using the Mean Squared Error:</p> <p>$L = (\hat{y} - y)^2$</p> <p>Our goal is to minimize this loss. Think of it like trying to navigate a vast, hilly landscape. The height of the land represents the loss, and our position represents the current set of weights and biases. We want to find the lowest point in this landscape.</p> </li> <li> <p><strong>Backward Pass (Backpropagation):</strong> This is where the magic happens. We need to figure out how much each individual weight and bias contributed to the overall error. Backpropagation efficiently calculates the <strong>gradient</strong> of the loss function with respect to every single weight and bias in the network.</p> <p>The gradient tells us the “slope” of our error landscape at our current position. It indicates which direction we should move (change the weights) to decrease the loss most effectively. Imagine you’re blindfolded on a mountain and want to get to the valley floor as fast as possible. You’d feel the slope under your feet and take a step in the steepest downward direction. That’s essentially what the gradient does.</p> <p>Mathematically, we’re calculating $\frac{\partial L}{\partial w_i}$ for each weight $w_i$, which tells us how much the loss changes when $w_i$ changes.</p> </li> <li> <p><strong>Adjust Weights (Gradient Descent):</strong> Once we have the gradients, we update our weights and biases to reduce the loss. We nudge each weight a small amount in the direction opposite to its gradient (because we want to go <em>down</em> the slope). The size of this nudge is controlled by a parameter called the <strong>learning rate</strong> ($\alpha$). A small learning rate means tiny steps, a large one means big jumps.</p> <p>The update rule for a weight $w_i$ is:</p> <p>$w_i \leftarrow w_i - \alpha \frac{\partial L}{\partial w_i}$</p> <p>We do the same for biases.</p> </li> <li> <p><strong>Repeat!</strong> We repeat steps 2-5 thousands, millions, or even billions of times, feeding the network different batches of training data. With each iteration, the weights and biases are fine-tuned, and the network gradually gets better and better at making accurate predictions. It learns to recognize complex patterns and features that even a human might struggle to articulate.</p> </li> </ol> <h3 id="beyond-the-basics-specialized-architectures">Beyond the Basics: Specialized Architectures</h3> <p>The fully connected neural network we’ve discussed is a powerful general-purpose tool. However, for specific types of data, specialized architectures have emerged that are even more efficient and effective:</p> <ul> <li> <p><strong>Convolutional Neural Networks (CNNs):</strong> These are the rockstars of image and video processing. Instead of connecting every neuron, CNNs use “convolutional” layers that scan images for local patterns (edges, textures, shapes), much like how our visual cortex processes parts of an image. This makes them incredibly good at tasks like image recognition, object detection, and even generating realistic images.</p> </li> <li> <p><strong>Recurrent Neural Networks (RNNs):</strong> When dealing with sequential data like text, speech, or time series, RNNs shine. They have “memory” because they can pass information from one step in a sequence to the next. This allows them to understand context and temporal dependencies, making them ideal for natural language processing (translation, text generation) and speech recognition. While vanilla RNNs face challenges with long sequences, more advanced variants like LSTMs (Long Short-Term Memory) and GRUs (Gated Recurrent Units) address these issues.</p> </li> </ul> <h3 id="the-power-and-the-promise-and-a-little-caution">The Power and the Promise (and a Little Caution)</h3> <p>Neural networks are behind many of the AI breakthroughs we see today:</p> <ul> <li> <strong>Image Recognition:</strong> Identifying faces in photos, diagnosing diseases from medical scans.</li> <li> <strong>Natural Language Processing:</strong> Powering virtual assistants, translating languages, writing coherent articles.</li> <li> <strong>Recommendation Systems:</strong> Suggesting products on Amazon, movies on Netflix.</li> <li> <strong>Autonomous Systems:</strong> Enabling self-driving cars, controlling robots.</li> </ul> <p>They are incredibly powerful, but they are not magic. They require vast amounts of data, significant computational resources, and careful engineering. They can also be “black boxes,” meaning it’s sometimes hard to understand <em>why</em> they made a particular decision, which is an active area of research. And, just like humans, they can be susceptible to bias if trained on biased data.</p> <h3 id="my-takeaway--your-next-step">My Takeaway &amp; Your Next Step</h3> <p>My journey into neural networks has been one of constant fascination. From the simple elegance of a single perceptron to the intricate dance of backpropagation across millions of parameters in a deep network, it’s a testament to how combining simple rules can lead to emergent intelligence. It’s a field that’s moving at lightning speed, constantly evolving with new architectures and training techniques.</p> <p>If you’re reading this, you’ve already taken the first step: curiosity. My advice? Don’t stop here. Try building a simple perceptron in Python, experiment with different activation functions, or delve deeper into how backpropagation is derived. The best way to truly understand these concepts is to get your hands dirty!</p> <p>The digital brain is an open book, waiting for your next chapter. What will you discover?</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>