<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Unpacking the Black Box: My Journey into Explainable AI (XAI) | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/unpacking-the-black-box-my-journey-into-explainabl/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Unpacking the Black Box: My Journey into Explainable AI (XAI)</h1> <p class="post-meta"> Created on July 19, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/explainable-ai"> <i class="fa-solid fa-hashtag fa-sm"></i> Explainable AI</a>   <a href="/blog/blog/tag/xai"> <i class="fa-solid fa-hashtag fa-sm"></i> XAI</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/ai-ethics"> <i class="fa-solid fa-hashtag fa-sm"></i> AI Ethics</a>   <a href="/blog/blog/tag/interpretability"> <i class="fa-solid fa-hashtag fa-sm"></i> Interpretability</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>My journey into data science, much like many of yours, started with a rush of excitement. The sheer power of machine learning models to identify patterns, make predictions, and even generate new content felt like magic. I remember the thrill of building my first convolutional neural network that could classify images with incredible accuracy, or a recurrent network that could predict stock prices (even if not perfectly!).</p> <p>But soon, a nagging question began to emerge from the shadows of my glowing accuracy scores and impressive F1-scores: “How did it <em>know</em> that?”</p> <p>It’s a question that plagues even the most seasoned AI practitioners, because as models become more complex – deep neural networks, ensemble methods, sophisticated transformers – their internal workings often become as opaque as a sealed black box. We feed them data, they spit out answers, but the reasoning process remains a mystery. And frankly, that’s a problem.</p> <p>This is where my fascination with <strong>Explainable AI (XAI)</strong> truly began.</p> <h3 id="the-why-of-xai-more-than-just-a-good-grade">The “Why” of XAI: More Than Just a Good Grade</h3> <p>Imagine you’re a doctor, and an AI recommends a specific, invasive treatment for a patient. Or you’re an applicant, and an AI denies you a crucial loan. Perhaps you’re an autonomous vehicle engineer, and your self-driving car suddenly swerves unexpectedly. In all these scenarios, simply knowing <em>what</em> the AI decided isn’t enough. You need to understand <em>why</em>.</p> <p>My “a-ha!” moment wasn’t a single event, but a growing unease. I realized that merely achieving high accuracy wasn’t the end goal; it was just the beginning. Without understanding, we face significant risks:</p> <ol> <li> <strong>Lack of Trust:</strong> How can we trust a system we don’t understand? If an AI makes a critical decision, humans need to audit it, confirm it, and feel confident in its reasoning.</li> <li> <strong>Debugging &amp; Improvement:</strong> If a model makes a mistake, how do we fix it if we don’t know <em>why</em> it failed? XAI helps us pinpoint errors, identify biases in data, or even discover flaws in our model architecture.</li> <li> <strong>Fairness &amp; Ethics:</strong> Opaque models can perpetuate and amplify societal biases present in training data, leading to discriminatory outcomes. XAI allows us to audit for fairness and ensure our models are making equitable decisions.</li> <li> <strong>Compliance &amp; Regulations:</strong> In regulated industries like finance, healthcare, or legal, “black box” decisions are often unacceptable. Regulations like GDPR’s “right to explanation” are pushing for greater transparency.</li> <li> <strong>Scientific Discovery:</strong> Sometimes, the patterns an AI discovers can reveal new insights about the underlying problem itself, leading to scientific breakthroughs. But only if we can interpret those patterns.</li> </ol> <p>This isn’t just a technical challenge; it’s a societal one. XAI is about empowering us – the data scientists, the domain experts, and the end-users – to look inside the black box and demand accountability.</p> <h3 id="lifting-the-lid-types-of-interpretability">Lifting the Lid: Types of Interpretability</h3> <p>Before diving into specific techniques, it’s helpful to categorize how we approach interpretability.</p> <p><strong>1. Intrinsic Interpretability (Glass Box Models):</strong> These are models that are inherently simple and whose internal workings are easy for humans to understand, often by design. Think of them as “glass boxes” where you can see all the gears turning.</p> <ul> <li> <p><strong>Linear Regression:</strong> Perhaps the simplest. For a model with one output and multiple features $x_i$, the prediction is: \(y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_n x_n\) Here, $\beta_i$ directly tells us how much the output $y$ changes for a one-unit change in $x_i$, holding other features constant. It’s wonderfully straightforward.</p> </li> <li> <p><strong>Decision Trees:</strong> These are like flowcharts. Each split represents a simple rule based on a feature (e.g., “Is age &gt; 30?”). You can trace any decision path from the root to a leaf node to understand how a prediction was made.</p> </li> </ul> <p>While intrinsically interpretable models are great, they often lack the predictive power of their more complex counterparts for many real-world problems. This leads us to the second type.</p> <p><strong>2. Post-hoc Explainability (Shining a Light into the Black Box):</strong> This is where the bulk of XAI research lies. These techniques are applied <em>after</em> a complex “black box” model has been trained. They don’t change the model itself but provide insights into its behavior. We can further break this down:</p> <ul> <li> <strong>Local Explanations:</strong> Explaining <em>why</em> a specific prediction was made for a <em>single instance</em>. For example, “Why was <em>this specific loan application</em> denied?”</li> <li> <strong>Global Explanations:</strong> Explaining the overall behavior of the model. For example, “What are the most important features <em>in general</em> for predicting loan approval?”</li> </ul> <h3 id="my-favorite-xai-tools-peeking-inside">My Favorite XAI Tools: Peeking Inside</h3> <p>Let’s explore some powerful post-hoc techniques that have truly transformed how I interact with my models.</p> <h4 id="1-lime-local-interpretable-model-agnostic-explanations">1. LIME: Local Interpretable Model-agnostic Explanations</h4> <p>LIME (Ribas et al., 2016) was one of the first techniques that really clicked for me. Its core idea is simple yet elegant: Even if a complex model behaves non-linearly globally, it might behave linearly around a specific data point.</p> <p><strong>The Analogy:</strong> Imagine a highly detailed, complex map of a mountainous region. If you zoom in really close to a specific point, the small area around that point looks relatively flat, or at least can be approximated by a simple slope. LIME does exactly that for model predictions.</p> <p><strong>How it Works (Simplified):</strong> For a single prediction you want to explain:</p> <ol> <li> <strong>Perturb the Input:</strong> Create many slightly modified versions (perturbations) of your original input data point. For an image, this might mean blurring parts of it; for text, removing some words; for tabular data, slightly changing feature values.</li> <li> <strong>Get Predictions:</strong> Feed these perturbed samples into your original “black box” model and get its predictions.</li> <li> <strong>Weight by Proximity:</strong> Assign weights to these perturbed samples based on how close they are to the original input (the closer, the higher the weight).</li> <li> <strong>Train an Interpretable Model:</strong> Train a simple, interpretable model (like a linear regression or a simple decision tree) on these weighted, perturbed samples and their black-box predictions.</li> <li> <strong>Explain!</strong> The interpretable model then provides a local explanation for the original prediction.</li> </ol> <p><strong>Example Output:</strong> For an image classification, LIME might highlight specific pixels or segments that contributed most to the model classifying an image as, say, “cat.” For a loan application, it might show that “credit score” and “debt-to-income ratio” were the most influential features for <em>this particular applicant’s</em> denial.</p> <p>LIME is <em>model-agnostic</em>, meaning it can be applied to any black-box model, which makes it incredibly versatile.</p> <h4 id="2-shap-shapley-additive-explanations">2. SHAP: SHapley Additive exPlanations</h4> <p>If LIME gave me a peek, SHAP (Lundberg &amp; Lee, 2017) provided a more rigorous, theoretically grounded framework for understanding feature contributions. SHAP values are based on <strong>Shapley values</strong> from cooperative game theory.</p> <p><strong>The Analogy:</strong> Imagine a team project where several students contributed. How do you fairly distribute the credit for the final grade among them? Shapley values provide a fair way to assign “credit” (or blame) to each feature for a model’s prediction. Each feature’s contribution is its average marginal contribution across all possible coalitions (combinations) of features.</p> <p><strong>The Math (Simplified):</strong> For a model $f$ and an input $x$, the SHAP value $\phi_i(f, x)$ for feature $i$ is calculated as: \(\phi_i = \sum_{S \subseteq N \setminus \{i\}} \frac{|S|!(|N|-|S|-1)!}{|N|!} (f_x(S \cup \{i\}) - f_x(S))\) Let’s break this down:</p> <ul> <li>$N$: The set of all features.</li> <li>$S$: A subset of features that does <em>not</em> include feature $i$.</li> <li> <table> <tbody> <tr> <td>$</td> <td>S</td> <td>$: The number of features in subset $S$.</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>$</td> <td>N</td> <td>$: The total number of features.</td> </tr> </tbody> </table> </li> <li>$f_x(S \cup {i})$: The model’s prediction when features in $S$ <em>and</em> feature $i$ are present.</li> <li>$f_x(S)$: The model’s prediction when only features in $S$ are present (feature $i$ is “removed” or “masked”).</li> </ul> <table> <tbody> <tr> <td>The formula essentially calculates the change in prediction when feature $i$ is added to every possible subset $S$ of other features, and then averages these changes. The factor $\frac{</td> <td>S</td> <td>!(</td> <td>N</td> <td>-</td> <td>S</td> <td>-1)!}{</td> <td>N</td> <td>!}$ accounts for the permutations, ensuring fairness across all possible “entry orders” of features.</td> </tr> </tbody> </table> <p><strong>Why it’s powerful:</strong> SHAP guarantees consistency (if a model changes such that a feature has a larger impact, its SHAP value won’t decrease) and local accuracy (the sum of SHAP values for all features equals the difference between the prediction and the baseline/average prediction).</p> <p><strong>Example Output:</strong> SHAP provides a direct numerical value for how much each feature pushes the prediction away from the average prediction. Positive SHAP values indicate features pushing the prediction higher, and negative values indicate features pushing it lower. This can be visualized to show which features are driving a particular prediction, both locally for a single instance and globally across many instances.</p> <h4 id="3-permutation-feature-importance">3. Permutation Feature Importance</h4> <p>While LIME and SHAP excel at local explanations, sometimes we need a simpler, global view of feature importance. Permutation Feature Importance is a robust and model-agnostic way to do this.</p> <p><strong>The Concept:</strong> How much does shuffling the values of a single feature impact the model’s performance? If shuffling a feature significantly degrades the model’s performance, that feature is important. If it has little effect, the feature isn’t very important.</p> <p><strong>How it Works (Simplified):</strong></p> <ol> <li> <strong>Train your model</strong> and evaluate its performance on a held-out validation set (e.g., calculate accuracy, F1-score, MSE). This is your baseline.</li> <li> <strong>For each feature:</strong> <ul> <li>Randomly shuffle the values of <em>only that one feature</em> in the validation set.</li> <li>Make predictions on this new, shuffled dataset.</li> <li>Evaluate the model’s performance again.</li> </ul> </li> <li> <strong>Calculate Importance:</strong> The drop in performance (baseline performance - shuffled performance) indicates the importance of that feature. A large drop means the feature was crucial.</li> </ol> <p>Permutation importance is intuitive, easy to implement, and provides a clear global ranking of features, helping us understand which inputs generally drive the model’s overall behavior.</p> <h3 id="the-road-ahead-challenges-and-future-directions">The Road Ahead: Challenges and Future Directions</h3> <p>While XAI has made incredible strides, it’s a rapidly evolving field with its own set of challenges:</p> <ol> <li> <strong>The Interpretability-Accuracy Trade-off:</strong> Often, the most accurate models are the least interpretable, and vice-versa. Finding the right balance for a given application is a continuous challenge.</li> <li> <strong>Human Factors:</strong> An explanation is only as good as a human’s ability to understand it. XAI isn’t just about generating numbers; it’s about effective communication. How do we present explanations in a way that is intuitive, actionable, and doesn’t overwhelm the user?</li> <li> <strong>Adversarial Explanations:</strong> Can explanations themselves be manipulated to hide biases or malicious intent? This is a growing concern.</li> <li> <strong>Context Matters:</strong> What constitutes a “good” explanation varies widely depending on the user (e.g., a data scientist, a domain expert, a layperson) and the specific task.</li> <li> <strong>Computational Cost:</strong> Some XAI methods, especially SHAP, can be computationally intensive, particularly for large datasets and complex models.</li> </ol> <p>The future of XAI is exciting. We’re seeing research into counterfactual explanations (what would need to change for a different prediction?), causal explanations (identifying actual cause-and-effect relationships), and the integration of XAI directly into model design (inherently interpretable neural networks).</p> <h3 id="my-takeaway-beyond-the-black-box">My Takeaway: Beyond the Black Box</h3> <p>My journey into Explainable AI has profoundly changed how I approach building and deploying machine learning models. I no longer chase accuracy scores blindly. Instead, I ask: “Can I explain this? Can I trust it? Is it fair?”</p> <p>For those of you just starting out, or even those deep into your data science careers, I urge you to embrace XAI. It’s not just a niche area; it’s becoming fundamental to responsible AI development. We are building the future, and that future must be transparent, accountable, and understandable.</p> <p>The black box era is slowly giving way to an era of clarity. And that, to me, is truly magical.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>