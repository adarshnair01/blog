<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Peeling Back the Layers: Understanding Neural Networks, One Neuron at a Time | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/peeling-back-the-layers-understanding-neural-netwo/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Peeling Back the Layers: Understanding Neural Networks, One Neuron at a Time</h1> <p class="post-meta"> Created on June 23, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/neural-networks"> <i class="fa-solid fa-hashtag fa-sm"></i> Neural Networks</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/artificial-intelligence"> <i class="fa-solid fa-hashtag fa-sm"></i> Artificial Intelligence</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>From deciphering handwritten digits to powering recommendation engines and even driving autonomous cars, Artificial Intelligence (AI) has become an inseparable part of our modern world. And at the heart of many of these breathtaking advancements lies a concept inspired by the most complex machine we know: the human brain. Today, I want to take you on a journey to demystify one of AI’s most powerful tools: Neural Networks.</p> <p>When I first encountered Neural Networks, the name itself sounded intimidating. “Neural!” “Network!” It conjured images of intricate brain scans and complex mathematical equations. But as I delved deeper, I realized that while the underlying mechanisms can get complex, the core ideas are surprisingly elegant and intuitive. Think of this as my personal journal entry, sharing the “aha!” moments and simplifying the concepts that once seemed daunting.</p> <h3 id="whats-the-big-idea-a-brainy-analogy">What’s the Big Idea? A Brainy Analogy</h3> <p>Imagine your own brain. It’s a vast network of tiny processing units called neurons, constantly sending signals to each other. When you learn something new – say, how to ride a bike – your brain isn’t just storing a static piece of information. It’s strengthening certain connections between neurons and weakening others, creating a complex pathway that represents that skill.</p> <p>Artificial Neural Networks (ANNs), often just called Neural Networks (NNs), draw inspiration from this biological marvel. Instead of biological neurons, we have <em>artificial neurons</em> (or nodes). Instead of biological connections, we have <em>weighted connections</em>. And instead of electrical impulses, we pass <em>numerical data</em>. The goal? To create a system that can “learn” from data by adjusting these weighted connections, much like our brains learn through experience.</p> <h3 id="the-single-neuron-a-simple-decision-maker">The Single Neuron: A Simple Decision-Maker</h3> <p>Let’s start with the smallest unit: a single artificial neuron, also known as a <strong>perceptron</strong>. It’s remarkably simple, yet it forms the foundation of all complex networks.</p> <p>Imagine you’re trying to decide if you should go out for a walk. You might consider a few factors:</p> <ol> <li>Is the sun shining? (Yes/No)</li> <li>Is it too cold? (Yes/No)</li> <li>Do I have free time? (Yes/No)</li> </ol> <p>Each of these factors is an <strong>input</strong> to your “decision neuron.” Not all factors are equally important, right? Maybe sunshine is a big motivator for you, while temperature is only a minor concern. This “importance” is captured by <strong>weights</strong>. A higher weight means that input has a stronger influence on the neuron’s decision.</p> <p>So, for our artificial neuron:</p> <ul> <li>It receives several <strong>inputs</strong> ($x_1, x_2, \ldots, x_n$).</li> <li>Each input is multiplied by a corresponding <strong>weight</strong> ($w_1, w_2, \ldots, w_n$).</li> <li>These weighted inputs are summed up: $\sum_{i=1}^{n} w_i x_i$.</li> <li>A <strong>bias</strong> term ($b$) is added to this sum. Think of the bias as an extra push or pull, making the neuron more or less likely to “fire” regardless of the inputs.</li> <li>Finally, this sum ($z = \sum w_i x_i + b$) is passed through an <strong>activation function</strong> ($f$), which decides the neuron’s final <strong>output</strong>.</li> </ul> <p>Mathematically, the output of a single neuron can be expressed as: \(\text{Output} = f\left(\sum_{i=1}^{n} w_i x_i + b\right)\)</p> <p>This equation, simple as it looks, is the beating heart of every neural network.</p> <h3 id="the-spark-of-life-activation-functions">The Spark of Life: Activation Functions</h3> <p>Why do we need an activation function ($f$)? Why can’t we just output the sum?</p> <p>If neurons simply outputted the sum of their weighted inputs, a neural network would just be a series of linear equations. This means that no matter how many layers you stacked, the network could only learn linear relationships – which are quite limited! The real world is full of complex, non-linear patterns (think image recognition, natural language understanding).</p> <p>Activation functions introduce <strong>non-linearity</strong> into the network, allowing it to learn and model these complex relationships. They essentially decide whether a neuron “fires” or not, and to what extent.</p> <p>Let’s look at a few popular ones:</p> <ol> <li> <strong>Sigmoid Function:</strong> <ul> <li>Formula: $\sigma(z) = \frac{1}{1 + e^{-z}}$</li> <li>Output range: (0, 1)</li> <li>Intuition: Squashes any input value into a range between 0 and 1. Historically popular for output layers in binary classification, where 0.5 can be a decision boundary. Imagine it like a dimmer switch, smoothly transitioning from “off” to “on.”</li> </ul> </li> <li> <strong>Rectified Linear Unit (ReLU):</strong> <ul> <li>Formula: $ReLU(z) = \max(0, z)$</li> <li>Output range: $[0, \infty)$</li> <li>Intuition: This one is super simple and widely used in hidden layers today. If the input is positive, it outputs the input as is. If the input is negative, it outputs zero. It’s like a gate: if the signal is strong enough (positive), it passes it through; otherwise, it stops it. ReLU helps solve a problem called “vanishing gradients” that plagued older activation functions.</li> </ul> </li> <li> <strong>Hyperbolic Tangent (Tanh):</strong> <ul> <li>Formula: $\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$</li> <li>Output range: (-1, 1)</li> <li>Intuition: Similar to sigmoid, but outputs values between -1 and 1. This can sometimes lead to faster training in certain network configurations compared to sigmoid.</li> </ul> </li> </ol> <p>Choosing the right activation function can significantly impact a network’s performance and training speed.</p> <h3 id="from-single-neuron-to-network-building-layers">From Single Neuron to Network: Building Layers</h3> <p>Now, let’s connect multiple neurons. A Neural Network isn’t just one neuron; it’s many neurons organized into <strong>layers</strong>.</p> <ul> <li> <strong>Input Layer:</strong> This layer receives the raw data. If you’re classifying images of handwritten digits, the input layer might have 784 neurons (for a 28x28 pixel image), one for each pixel’s intensity value. These neurons don’t perform any computation, they just pass the data along.</li> <li> <strong>Hidden Layers:</strong> These are the “thinking” layers. Each neuron in a hidden layer takes inputs from the previous layer, applies weights, adds a bias, and passes the result through an activation function. The “deep” in “Deep Learning” refers to networks with many hidden layers. Each successive hidden layer learns more complex features from the data. For example, the first hidden layer might detect edges in an image, the next might combine edges to find shapes, and a later layer might combine shapes to recognize objects.</li> <li> <strong>Output Layer:</strong> This layer provides the network’s final answer. The number of neurons here depends on the problem. For binary classification (e.g., “cat” or “dog”), it might be one neuron with a sigmoid activation. For multi-class classification (e.g., “cat”, “dog”, “bird”), it might have one neuron per class, often with a softmax activation function which turns raw scores into probabilities that sum to 1.</li> </ul> <p>When all neurons in one layer are connected to every neuron in the next layer, we call it a <strong>fully connected</strong> or <strong>dense</strong> layer. This is the most common type for simpler networks.</p> <h3 id="the-magic-of-learning-how-neural-networks-get-smart">The Magic of Learning: How Neural Networks Get Smart</h3> <p>This is where things get really fascinating. How does a network “learn” to adjust those weights and biases to make correct predictions? It’s a three-step dance:</p> <h4 id="1-forward-propagation-making-a-guess">1. Forward Propagation: Making a Guess</h4> <p>Imagine you give the network an image of a handwritten ‘7’.</p> <ul> <li>The pixel values enter the input layer.</li> <li>These values travel through the first hidden layer, multiplied by their weights, summed, activated, and passed to the next layer.</li> <li>This process continues layer by layer until the data reaches the output layer, which then spits out its “guess” – perhaps saying “I think this is a ‘1’ with 80% probability, a ‘7’ with 10% probability, and so on.”</li> </ul> <p>This journey of data from input to output is called <strong>forward propagation</strong>.</p> <h4 id="2-the-loss-function-measuring-how-wrong">2. The Loss Function: Measuring “How Wrong”</h4> <p>After the network makes a guess, we need to know how good or bad that guess was. This is where the <strong>loss function</strong> (or cost function) comes in. It calculates the difference between the network’s prediction ($\hat{y}$) and the actual correct answer ($y$). A larger loss means a worse prediction.</p> <p>For example, in a regression problem (predicting a continuous value like house prices), a common loss function is the <strong>Mean Squared Error (MSE)</strong>: \(L = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2\) Here, $N$ is the number of data points, $y_i$ is the actual value, and $\hat{y}_i$ is the predicted value. Squaring the difference ensures positive values and penalizes larger errors more heavily.</p> <p>For classification problems, a popular choice is <strong>Cross-Entropy Loss</strong>. The goal during training is always to minimize this loss.</p> <h4 id="3-backpropagation-adjusting-the-knobs">3. Backpropagation: Adjusting the “Knobs”</h4> <p>Now for the real genius! We know <em>how wrong</em> the network was, but how do we know <em>which</em> weights and biases to change, and by <em>how much</em>, to make it less wrong next time? This is the role of <strong>backpropagation</strong>.</p> <p>Imagine you’re tuning a complex radio with hundreds of tiny knobs (weights and biases). You turn one knob slightly and see if the signal (loss) gets better or worse. You want to turn all the knobs in the direction that makes the signal clearest (minimizes loss).</p> <p>Backpropagation uses <strong>calculus</strong>, specifically the <strong>chain rule</strong> for derivatives, to figure this out. It calculates the <strong>gradient</strong> of the loss function with respect to each weight and bias in the network. The gradient essentially tells us the direction of the steepest increase in loss. To minimize loss, we want to move in the <em>opposite</em> direction of the gradient.</p> <p>This process is called <strong>Gradient Descent</strong>. For each weight ($w$) and bias ($b$), we update them using this rule: \(w_{new} = w_{old} - \alpha \frac{\partial L}{\partial w}\) \(b_{new} = b_{old} - \alpha \frac{\partial L}{\partial b}\) Here, $\frac{\partial L}{\partial w}$ is the partial derivative of the loss function with respect to weight $w$, telling us how much the loss changes when $w$ changes. The $\alpha$ (alpha) term is the <strong>learning rate</strong>. It’s a crucial <strong>hyperparameter</strong> that determines how big of a step we take in the direction opposite to the gradient. A small learning rate means slow but steady learning; a large one might make the network overshoot the optimal weights or even diverge.</p> <p>This process of forward propagation, calculating loss, and then backpropagating to update weights and biases is repeated many times, often for thousands or millions of data samples. Each full pass through the entire training dataset is called an <strong>epoch</strong>. Over many epochs, the network gradually learns to identify patterns, and its predictions become more and more accurate.</p> <h3 id="stepping-up-beyond-the-basics">Stepping Up: Beyond the Basics</h3> <p>What we’ve discussed so far forms the basis of a <strong>Feedforward Neural Network</strong>. But the world of Neural Networks is vast!</p> <ul> <li> <strong>Convolutional Neural Networks (CNNs):</strong> Revolutionized image recognition. They use specialized layers called “convolutional layers” to automatically detect features like edges, textures, and shapes, making them incredibly effective for tasks like image classification and object detection.</li> <li> <strong>Recurrent Neural Networks (RNNs):</strong> Designed for sequential data, like text or time series. They have “memory” – they can pass information from one step in a sequence to the next, which is vital for understanding context in sentences or predicting future stock prices.</li> <li> <strong>Transformers:</strong> The current state-of-the-art for many natural language processing (NLP) tasks, powering models like ChatGPT. They utilize a mechanism called “attention” to weigh the importance of different parts of the input sequence, allowing them to handle very long-range dependencies more effectively than RNNs.</li> </ul> <h3 id="my-takeaway-and-your-next-steps">My Takeaway and Your Next Steps</h3> <p>Neural Networks, at their core, are powerful pattern recognition machines. They might seem complex with all the math and jargon, but remember: they’re just fancy function approximators, trying to map inputs to outputs by adjusting millions of tiny internal “knobs.”</p> <p>My journey into understanding them has been incredibly rewarding, opening up a world of possibilities for solving real-world problems. They’re not magic, but the results they achieve often feel magical!</p> <p>If you’re a high school student or an aspiring data scientist, my advice is to:</p> <ol> <li> <strong>Don’t be afraid of the math:</strong> Focus on the intuition first. The equations are tools to precisely describe the intuition.</li> <li> <strong>Play with code:</strong> Tools like TensorFlow and PyTorch make it easy to build and experiment with networks without getting bogged down in low-level implementation.</li> <li> <strong>Start simple:</strong> Build a basic perceptron, then a small feedforward network, and watch it learn. The “aha!” moments are truly satisfying.</li> </ol> <p>The field of AI and Neural Networks is still evolving at an astonishing pace. There are ethical considerations, new architectures being invented, and countless unsolved problems waiting for curious minds like yours. So, dive in, experiment, and who knows what incredible things you’ll build or discover!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>