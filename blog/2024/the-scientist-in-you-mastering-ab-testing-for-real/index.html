<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Scientist in You: Mastering A/B Testing for Real-World Impact | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/the-scientist-in-you-mastering-ab-testing-for-real/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Scientist in You: Mastering A/B Testing for Real-World Impact</h1> <p class="post-meta"> Created on September 26, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/a-b-testing"> <i class="fa-solid fa-hashtag fa-sm"></i> A/B Testing</a>   <a href="/blog/blog/tag/statistics"> <i class="fa-solid fa-hashtag fa-sm"></i> Statistics</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/experiment-design"> <i class="fa-solid fa-hashtag fa-sm"></i> Experiment Design</a>   <a href="/blog/blog/tag/hypothesis-testing"> <i class="fa-solid fa-hashtag fa-sm"></i> Hypothesis Testing</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey there, future data wizards and curious minds!</p> <p>Remember that time you couldn’t decide between two options? Maybe it was which outfit to wear, which video game to play, or even which route to take to school. You probably weighed the pros and cons, maybe even tried one for a bit, then switched. Well, in the world of products, websites, and marketing, making such decisions based on “gut feeling” or “what sounds good” is a recipe for disaster. That’s where A/B testing swoops in – it’s our scientific superpower to move from opinions to quantifiable, data-driven decisions.</p> <p>Think of yourself as a detective, but instead of solving crimes, you’re solving puzzles about human behavior. Which headline makes people click more? Does a green button truly outperform a red one? Will this new feature actually make users happier or just confuse them? A/B testing gives us the framework to answer these questions with confidence, transforming guesswork into informed action.</p> <h3 id="what-exactly-is-ab-testing">What <em>Exactly</em> is A/B Testing?</h3> <p>At its core, A/B testing is a controlled experiment. Imagine you have two versions of something – let’s call them “A” and “B”.</p> <ul> <li> <strong>Version A (Control):</strong> This is your existing or current version. It’s the baseline we compare against.</li> <li> <strong>Version B (Treatment):</strong> This is your new version, with a <em>single, specific change</em> you want to test.</li> </ul> <p>We then show Version A to one group of users and Version B to another, equally representative group of users, at the <em>same time</em>. We measure how each group interacts with their respective version based on a predefined metric (like clicks, purchases, sign-ups). By comparing these metrics, we can statistically determine if Version B is truly better, worse, or no different from Version A.</p> <p>It’s like a scientific experiment you might have done in a lab, but instead of testing fertilizers on plants, we’re testing interface changes on users! The goal is to isolate the effect of <em>one</em> change.</p> <h3 id="the-anatomy-of-an-ab-test-a-step-by-step-journey">The Anatomy of an A/B Test: A Step-by-Step Journey</h3> <p>Ready to run your first experiment? Here’s the typical roadmap we follow:</p> <h4 id="1-formulate-a-hypothesis">1. Formulate a Hypothesis</h4> <p>Every good experiment starts with a clear question and a testable hypothesis. We usually set up two hypotheses:</p> <ul> <li> <strong>Null Hypothesis ($H_0$):</strong> This assumes there is <em>no significant difference</em> between Version A and Version B in terms of the metric we care about. For example, “$H_0$: The conversion rate for Version A is the same as for Version B.”</li> <li> <strong>Alternative Hypothesis ($H_1$):</strong> This is what we hope to prove – that there <em>is</em> a significant difference. It could be one-sided (e.g., “$H_1$: The conversion rate for Version B is <em>greater than</em> Version A”) or two-sided (e.g., “$H_1$: The conversion rate for Version B is <em>different from</em> Version A”).</li> </ul> <p>Having these clearly defined helps us frame our analysis later.</p> <h4 id="2-define-your-metrics">2. Define Your Metrics</h4> <p>What are you trying to improve? This needs to be quantifiable.</p> <ul> <li> <strong>Primary Metric:</strong> This is the <em>single most important</em> metric you’re trying to move. Examples include: Click-Through Rate (CTR), Conversion Rate, Revenue Per User, Time Spent on Page. Focusing on one primary metric helps avoid “p-hacking” and ensures a clear success criterion.</li> <li> <strong>Secondary Metrics:</strong> These are other metrics you’ll monitor to ensure your change doesn’t negatively impact other aspects. For instance, if you optimize for CTR, you might also watch bounce rate to make sure users aren’t just clicking and leaving immediately.</li> </ul> <p>Let’s say we’re testing a new signup button color. Our primary metric might be “Signup Conversion Rate.”</p> <h4 id="3-prepare-your-variations">3. Prepare Your Variations</h4> <p>This is where you get creative!</p> <ul> <li> <strong>Control (A):</strong> The current, established version.</li> <li> <strong>Treatment (B):</strong> Your proposed change. Remember the golden rule: <strong>change only one thing at a time!</strong> If you change the button color <em>and</em> the text on the button, and you see an improvement, you won’t know if it was the color, the text, or a combination. This makes it impossible to learn effectively.</li> </ul> <h4 id="4-randomly-split-your-traffic">4. Randomly Split Your Traffic</h4> <p>This is perhaps the most crucial step for ensuring your results are valid. You need to randomly assign users to either see Version A or Version B.</p> <p>Why random? Because we want to ensure that the two groups are as similar as possible in every way <em>except</em> for the change we’re testing. If one group accidentally gets all the “power users” and the other gets “new users,” your results will be biased. Randomization helps balance out these natural variations, allowing us to confidently attribute any observed differences to our change.</p> <p>Typically, traffic is split 50/50, but you might use other ratios if, for example, the new version is risky and you want to expose fewer users to it initially.</p> <h4 id="5-run-the-experiment">5. Run the Experiment</h4> <p>Once everything is set up, you let the experiment run. This isn’t a race! You need to run the test for long enough to:</p> <ul> <li> <strong>Collect sufficient data:</strong> More on this when we talk statistics!</li> <li> <strong>Account for cyclical variations:</strong> User behavior can change throughout the week or month. Running for at least one full week (or multiples of a week) helps capture these patterns.</li> <li> <strong>Avoid “novelty effect”:</strong> Sometimes, new things get a temporary boost just because they’re new. Running the test for longer helps distinguish between genuine improvement and fleeting novelty.</li> </ul> <p>Crucially, <strong>do not “peek” at your results and stop the test early</strong> just because you see an early positive trend. This can lead to false positives and incorrect conclusions. You pre-determine the duration or required sample size and stick to it.</p> <h4 id="6-analyze-the-results">6. Analyze the Results</h4> <p>This is where the magic of statistics comes in! After collecting enough data, we compare the performance of Group A and Group B using statistical methods to determine if the observed difference is “statistically significant” or just due to random chance.</p> <h3 id="the-statistical-heartbeat-hypothesis-testing-demystified">The Statistical Heartbeat: Hypothesis Testing Demystified</h3> <p>We’re not just looking for <em>any</em> difference; we’re looking for a <em>reliable</em> difference. Since we’re only testing a sample of our total users, we need to infer what would happen with the entire population.</p> <p>Let’s stick with our signup button example.</p> <ul> <li> <strong>Group A:</strong> $N_A$ users saw the old button. $X_A$ of them signed up. Our observed conversion rate is $\hat{p}_A = X_A/N_A$.</li> <li> <strong>Group B:</strong> $N_B$ users saw the new button. $X_B$ of them signed up. Our observed conversion rate is $\hat{p}_B = X_B/N_B$.</li> </ul> <p>We want to know: Is $\hat{p}_B$ truly better than $\hat{p}_A$, or did Group B just get lucky?</p> <p>Here’s how we typically approach it:</p> <ol> <li> <strong>Calculate the Difference:</strong> We look at the raw difference: $\hat{p}_B - \hat{p}_A$.</li> <li> <p><strong>Standard Error of the Difference:</strong> We need to know how much we expect this difference to vary purely by chance. This is captured by the standard error (SE). For comparing two proportions, a common approximation for the standard error of the difference is: \(\text{SE} = \sqrt{\frac{\hat{p}\_A(1-\hat{p}\_A)}{N_A} + \frac{\hat{p}\_B(1-\hat{p}\_B)}{N_B}}\) This formula tells us, on average, how much the observed difference between the two conversion rates might fluctuate if we were to repeat the experiment many times.</p> </li> <li> <p><strong>Z-score (Test Statistic):</strong> We then calculate a Z-score, which tells us how many standard errors our observed difference is away from the difference hypothesized under the null hypothesis (which is usually 0): \(Z = \frac{(\hat{p}\_B - \hat{p}\_A) - 0}{\text{SE}} = \frac{\hat{p}\_B - \hat{p}\_A}{\text{SE}}\) A larger absolute Z-score indicates a greater difference relative to the expected variability.</p> </li> <li> <strong>P-value:</strong> This is the superstar of hypothesis testing. The p-value is the probability of observing a difference as extreme as, or more extreme than, what we actually saw, <em>assuming the null hypothesis is true</em> (i.e., assuming there’s <em>no real difference</em> between A and B). <ul> <li>If your p-value is low (typically below a predetermined <strong>significance level</strong>, denoted by $\alpha$, often 0.05 or 5%), it means that such an extreme result would be very unlikely to occur by random chance if $H_0$ were true. Therefore, we <strong>reject the null hypothesis</strong> in favor of the alternative hypothesis. We declare the result “statistically significant.”</li> <li>If your p-value is high (e.g., $p &gt; 0.05$), it means the observed difference could easily have happened by chance, even if $H_0$ were true. In this case, we <strong>fail to reject the null hypothesis</strong>. This <em>doesn’t</em> mean $H_0$ is true; it just means we don’t have enough evidence to say it’s false.</li> </ul> </li> <li> <strong>Confidence Intervals:</strong> Another powerful concept! A confidence interval provides a range of values within which we are confident the true population parameter (e.g., the true difference in conversion rates) lies. For example, a 95% confidence interval for the difference in conversion rates means that if we were to repeat this experiment many times, 95% of the calculated intervals would contain the true difference. If this interval <em>does not</em> include zero, it further supports the idea that there’s a statistically significant difference.</li> </ol> <h4 id="minimum-detectable-effect-mde-and-sample-size">Minimum Detectable Effect (MDE) and Sample Size</h4> <p>Before you even start an A/B test, you need to decide two things:</p> <ul> <li> <strong>Minimum Detectable Effect (MDE):</strong> What’s the smallest percentage improvement you’d consider valuable enough to implement? An A/B test is designed to tell you if the difference is statistically significant, but it also matters if that difference is <em>practically significant</em>. A 0.001% increase in conversion might be statistically significant with enough data, but probably not worth the effort.</li> <li> <strong>Sample Size:</strong> How many users do you need in each group ($N_A$ and $N_B$)? This depends on your baseline metric, your desired MDE, your chosen significance level ($\alpha$), and your desired <strong>statistical power</strong> (the probability of correctly detecting a real effect if one exists, usually 0.8 or 80%). There are calculators (often called “A/B test sample size calculators”) that take these inputs and tell you how many users you need to enroll in your experiment. Running with too few users is a common mistake that leads to inconclusive results.</li> </ul> <h3 id="when-to-use-ab-testing-and-when-not-to">When to Use A/B Testing (and When Not To)</h3> <p>A/B testing is incredibly powerful, but it’s not a silver bullet.</p> <p><strong>Use A/B Testing When:</strong></p> <ul> <li>You want to make incremental, data-driven improvements to existing features or designs.</li> <li>You have a clear, measurable metric you want to optimize.</li> <li>You have enough traffic or users to achieve statistical significance within a reasonable timeframe.</li> <li>You want to understand the impact of a specific, isolated change.</li> </ul> <p><strong>Don’t Use A/B Testing When:</strong></p> <ul> <li>You’re launching a completely new product or feature that has no baseline to compare against (you might need multivariate testing or broader qualitative research).</li> <li>The changes are so massive that you can’t isolate variables easily (consider A/B/C/D… or a full redesign with user testing).</li> <li>Your traffic is very low, making it difficult to reach statistical significance.</li> <li>The change has significant ethical implications that need careful consideration beyond just metrics.</li> </ul> <h3 id="common-pitfalls-and-best-practices">Common Pitfalls and Best Practices</h3> <p>As with any powerful tool, there are ways to misuse A/B testing.</p> <p><strong>Common Pitfalls:</strong></p> <ol> <li> <strong>Stopping tests early (“peeking”):</strong> This is a huge no-no! It drastically increases the chance of false positives.</li> <li> <strong>Not randomizing properly:</strong> Leading to biased groups and invalid results.</li> <li> <strong>Testing too many variables at once:</strong> Makes it impossible to attribute success or failure to a specific change.</li> <li> <strong>Ignoring sample size calculations:</strong> Running tests with insufficient power means you might miss real improvements.</li> <li> <strong>Running tests for too short a period:</strong> Missing cyclical trends or confusing novelty effects with true long-term gains.</li> <li> <strong>Not defining primary and secondary metrics clearly:</strong> Leading to ambiguous interpretation of results.</li> </ol> <p><strong>Best Practices:</strong></p> <ol> <li> <strong>Pre-determine everything:</strong> Hypothesis, metrics, sample size, duration, and significance level should all be decided <em>before</em> the test begins.</li> <li> <strong>Focus on one primary metric:</strong> Keep it simple and clear what “success” looks like.</li> <li> <strong>Ensure true randomization:</strong> Use reliable A/B testing platforms that handle traffic splitting correctly.</li> <li> <strong>Be patient:</strong> Let the experiment run its course as planned.</li> <li> <strong>Document your experiments:</strong> What was tested, why, results, and what was learned. This builds institutional knowledge.</li> <li> <strong>Learn from every test:</strong> Even if your treatment doesn’t “win,” you’ve gained valuable insight into user behavior.</li> </ol> <h3 id="conclusion">Conclusion</h3> <p>A/B testing is more than just a technique; it’s a mindset. It embodies the scientific method, allowing us to systematically experiment, gather evidence, and make informed decisions that drive real impact. Whether you’re optimizing a website, refining a product, or crafting compelling marketing messages, the ability to design, execute, and interpret A/B tests is an indispensable skill for anyone working with data.</p> <p>So, unleash your inner scientist! Start questioning assumptions, formulate hypotheses, and let the data guide your path. The world of A/B testing is waiting for you to discover powerful insights and build truly user-centric experiences. Happy experimenting!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>