<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Deep Learning Arena: My Journey Through PyTorch vs TensorFlow | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/the-deep-learning-arena-my-journey-through-pytorch/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Deep Learning Arena: My Journey Through PyTorch vs TensorFlow</h1> <p class="post-meta"> Created on March 06, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/pytorch"> <i class="fa-solid fa-hashtag fa-sm"></i> PyTorch</a>   <a href="/blog/blog/tag/tensorflow"> <i class="fa-solid fa-hashtag fa-sm"></i> TensorFlow</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Ah, the exhilarating, sometimes bewildering, world of deep learning! I remember when I first dipped my toes into this fascinating domain. It was a mix of awe and a healthy dose of confusion. Neural networks, backpropagation, optimizers – it felt like a secret language. But almost immediately, a question began to surface, echoing through forums, tutorials, and academic papers: “PyTorch or TensorFlow?”</p> <p>It’s a debate that, for a time, felt as intense as choosing between Marvel and DC. Both are incredible, powerful, and have passionate communities. As a budding data scientist, I knew I couldn’t just pick one at random; I needed to understand their souls, their philosophies, and where they truly excelled. So, I embarked on a journey to understand these two giants, and I’d love to share what I’ve learned along the way.</p> <h3 id="the-bedrock-tensors-and-automatic-gradients">The Bedrock: Tensors and Automatic Gradients</h3> <p>Before we dive into the specifics of PyTorch and TensorFlow, let’s briefly touch upon the fundamental concepts they both share. Think of these as the universal tools in our deep learning toolbox.</p> <p>At the very core, deep learning frameworks operate on <strong>tensors</strong>. What’s a tensor? It’s simply a multi-dimensional array. If you’ve ever worked with NumPy, you’re already familiar with the concept. A single number is a 0-dimensional tensor (a scalar). A list of numbers is a 1-dimensional tensor (a vector), like $ \mathbf{x} \in \mathbb{R}^n $. A table of numbers is a 2-dimensional tensor (a matrix), like $ \mathbf{M} \in \mathbb{R}^{m \times n} $. And images, with their height, width, and color channels, often live as 3- or 4-dimensional tensors. These tensors are the universal language for data in neural networks.</p> <p>The second, arguably more magical, shared concept is <strong>automatic differentiation</strong>, often called “autograd.” Training a neural network involves finding the right set of weights and biases that minimize an error (loss) function. This optimization process relies heavily on calculus, specifically calculating the <em>gradients</em> of the loss with respect to these weights. Calculating these gradients by hand for complex networks would be a nightmare. Both PyTorch and TensorFlow automatically keep track of all the operations performed on tensors and can compute these gradients for us using the chain rule. This means if we have a loss $L$ that depends on some prediction $y$, which in turn depends on weights $w$, they can calculate $ \frac{\partial L}{\partial w} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial w} $ with ease. This “autograd” feature is truly the engine that powers deep learning.</p> <h3 id="tensorflow-the-google-giants-blueprint">TensorFlow: The Google Giant’s Blueprint</h3> <p>My first encounter with TensorFlow was through its sheer reputation. Developed by Google, it felt like the enterprise-grade solution, built for scale and production-readiness.</p> <p><strong>A Static Graph Philosophy (Pre-TF 2.0)</strong></p> <p>The defining characteristic of early TensorFlow was its <strong>static computational graph</strong>. Imagine you’re an architect. With TensorFlow, you’d meticulously design the <em>entire</em> blueprint of your neural network <em>first</em>. Every operation, every connection, would be laid out in this graph. Only after the blueprint was complete would you “run” data through it.</p> <p>This static graph approach had some significant advantages:</p> <ul> <li> <strong>Optimization</strong>: Because the entire graph was known beforehand, TensorFlow could perform incredible optimizations – pruning unused nodes, fusing operations, and even compiling parts of the graph for faster execution.</li> <li> <strong>Deployment</strong>: This rigid blueprint made it incredibly easy to deploy models across various platforms (mobile, web, specialized hardware) without needing the original Python environment. Think TensorFlow Lite for mobile or TensorFlow.js for the browser.</li> <li> <strong>Distributed Training</strong>: Training massive models across many machines was streamlined because the graph could be efficiently partitioned and distributed.</li> </ul> <p><strong>The Keras Revolution</strong></p> <p>However, early TensorFlow could be notoriously verbose and complex, especially for beginners. Enter <strong>Keras</strong>. This high-level API, eventually integrated directly into TensorFlow (tf.keras), was a game-changer. It allowed users to build and train neural networks with just a few lines of code, abstracting away much of the underlying complexity. It made TensorFlow approachable, like getting a powerful LEGO set with clear instructions instead of just a pile of individual bricks.</p> <p><strong>TensorBoard and the Ecosystem</strong></p> <p>TensorFlow also came with a formidable ecosystem. <strong>TensorBoard</strong> quickly became indispensable for visualizing training progress, model architectures, and debugging. Beyond that, tools like TensorFlow Extended (TFX) offered comprehensive pipelines for production ML, and TensorFlow Serving made model deployment a breeze.</p> <p><strong>The Evolution: TensorFlow 2.0 and Eager Execution</strong></p> <p>The biggest shift in TensorFlow’s history was arguably TensorFlow 2.0. Google listened to the community’s feedback, especially the desire for a more interactive and Pythonic experience. TF 2.0 embraced <strong>eager execution</strong>, which is TensorFlow’s version of a dynamic graph. This meant operations could be run immediately and interactively, just like standard Python. While static graphs are still available (via <code class="language-plaintext highlighter-rouge">tf.function</code>), eager execution became the default, making debugging easier and the development flow much more intuitive. It felt like TensorFlow was learning to dance!</p> <h3 id="pytorch-the-research-darlings-flexibility">PyTorch: The Research Darling’s Flexibility</h3> <p>My journey then led me to PyTorch, and I immediately felt a different vibe. Developed by Facebook (now Meta AI), PyTorch originated from a research-first philosophy, prioritizing flexibility, ease of use, and a deeply Pythonic feel.</p> <p><strong>The Dynamic Graph Philosophy (Define-by-Run)</strong></p> <p>PyTorch’s core strength lies in its <strong>dynamic computational graph</strong>, often called “define-by-run.” Instead of building a fixed blueprint, PyTorch constructs the computational graph <em>as operations are executed</em>. Imagine you’re building a LEGO model, but instead of a manual, you’re just picking up bricks and connecting them one by one, making decisions as you go.</p> <p>This dynamic nature offers tremendous advantages, especially for researchers and experimenters:</p> <ul> <li> <strong>Intuitive Debugging</strong>: Because the graph is built on the fly, you can use standard Python debugging tools (like <code class="language-plaintext highlighter-rouge">pdb</code>) to inspect tensors and understand the flow at any point. No more opaque graph sessions!</li> <li> <strong>Flexibility</strong>: This is crucial for models with variable input sizes, conditional logic, or complex control flow (e.g., recurrent neural networks where the sequence length can vary). The graph can adapt and change with each forward pass.</li> <li> <strong>Pythonic</strong>: PyTorch feels incredibly natural for anyone familiar with Python and NumPy. Its API is clean, straightforward, and generally mirrors Python’s idioms. This makes the learning curve remarkably gentle for many.</li> </ul> <p><strong>A Vibrant Research Community</strong></p> <p>PyTorch quickly became the darling of the academic and research communities. Its flexibility allowed researchers to rapidly prototype novel architectures and experiment with cutting-edge ideas. Many groundbreaking papers you read today are often implemented and released in PyTorch.</p> <p><strong>Addressing Production: TorchScript and JIT</strong></p> <p>While originally perceived as less production-ready than TensorFlow, PyTorch has made significant strides. <strong>TorchScript</strong> and the <strong>JIT (Just-In-Time) compiler</strong> allow you to “trace” or “script” your dynamic Python model into a static graph representation. This compiled version can then be optimized, serialized, and deployed to production environments without needing Python, effectively bridging the gap between research flexibility and deployment efficiency. Libraries like PyTorch Lightning also simplify the training boilerplate, making research more organized and reproducible.</p> <h3 id="my-personal-arena-choosing-your-champion">My Personal Arena: Choosing Your Champion</h3> <p>So, after exploring both, which one did I choose? The truth is, it’s not a simple either/or. My journey has led me to appreciate both for different reasons, and more often than not, the “best” choice depends on the specific project and context.</p> <ul> <li> <strong>When I lean towards PyTorch:</strong> For rapid prototyping, research projects, or when I need maximum flexibility and easy debugging. If I’m trying out a new paper’s architecture or building something with complex, conditional logic, PyTorch’s define-by-run nature is a lifesaver. Its Pythonic feel just clicks with my development style.</li> <li> <strong>When I consider TensorFlow:</strong> For robust, large-scale production deployments, especially if I need to deploy across a variety of platforms (edge devices, web browsers) or leverage Google’s extensive ML ecosystem. If I’m working in a team already standardized on TensorFlow, or if a project demands the raw optimization capabilities of a compiled static graph (even with eager execution as default), TensorFlow is a solid contender.</li> </ul> <p><strong>The Closing Gap</strong></p> <p>What’s truly exciting is how much these two frameworks have learned from each other. TensorFlow’s adoption of eager execution and Keras vastly improved its developer experience, making it more PyTorch-like. PyTorch’s advancements in TorchScript and deployment tools have bolstered its production capabilities, making it more TensorFlow-like. The line between them continues to blur, offering us the best of both worlds.</p> <p>Let’s illustrate the fundamental difference in gradient computation conceptually: Imagine we have a simple function $ f(x) = x^2 $ and we want to find its derivative $ \frac{df}{dx} $ at $ x=2 $. We know the analytical solution is $ 2x $, so at $ x=2 $, the gradient should be $ 4 $.</p> <p>In a dynamic graph (PyTorch, or TensorFlow Eager), it might look conceptually like this:</p> <ol> <li>Initialize <code class="language-plaintext highlighter-rouge">x = 2.0</code> and tell the system to track gradients for <code class="language-plaintext highlighter-rouge">x</code>.</li> <li>Compute <code class="language-plaintext highlighter-rouge">y = x * x</code> (or <code class="language-plaintext highlighter-rouge">x**2</code>). The system immediately knows <code class="language-plaintext highlighter-rouge">y</code> depends on <code class="language-plaintext highlighter-rouge">x</code>.</li> <li>Call <code class="language-plaintext highlighter-rouge">y.backward()</code> (or <code class="language-plaintext highlighter-rouge">tf.GradientTape().gradient(y, x)</code>). The system traces back from <code class="language-plaintext highlighter-rouge">y</code> to <code class="language-plaintext highlighter-rouge">x</code> and calculates $ \frac{dy}{dx} $ on the spot, storing it with <code class="language-plaintext highlighter-rouge">x</code>.</li> <li>Retrieve <code class="language-plaintext highlighter-rouge">x.grad</code>, which would be $ 4.0 $.</li> </ol> <p>In a purely static graph (older TensorFlow), you’d first define the entire graph symbolically:</p> <ol> <li>Define a <code class="language-plaintext highlighter-rouge">placeholder</code> for <code class="language-plaintext highlighter-rouge">x</code>.</li> <li>Define <code class="language-plaintext highlighter-rouge">y = x * x</code>.</li> <li>Define the <code class="language-plaintext highlighter-rouge">gradient_op = tf.gradients(y, x)</code>.</li> <li>Only <em>then</em>, in a session, would you feed the value <code class="language-plaintext highlighter-rouge">x=2.0</code> into the <code class="language-plaintext highlighter-rouge">placeholder</code> and execute <code class="language-plaintext highlighter-rouge">gradient_op</code> to get <code class="language-plaintext highlighter-rouge">4.0</code>.</li> </ol> <p>The difference is subtle but profound in terms of interactive development and debugging.</p> <h3 id="conclusion-embrace-the-power-choose-your-adventure">Conclusion: Embrace the Power, Choose Your Adventure</h3> <p>Ultimately, the “PyTorch vs TensorFlow” debate isn’t about finding a single winner. Both are incredibly mature, powerful, and essential tools in the deep learning landscape. They represent different philosophies that have converged over time, offering robust solutions for almost any deep learning task.</p> <p>My advice? Don’t get bogged down in the holy war. Understand their core strengths, try them both out, and see which one resonates more with your personal workflow and the demands of your project. The world of AI is vast and exciting, and mastering either (or both!) of these frameworks will empower you to build amazing things, from intelligent chatbots to self-driving cars. The real battle isn’t between the frameworks; it’s against the unsolved problems, and with these powerful tools at our disposal, we’re well-equipped for the fight!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>