<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Time-Traveling Neurons: Unlocking Memory in Recurrent Neural Networks | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/the-time-traveling-neurons-unlocking-memory-in-rec/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Time-Traveling Neurons: Unlocking Memory in Recurrent Neural Networks</h1> <p class="post-meta"> Created on July 28, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/recurrent-neural-networks"> <i class="fa-solid fa-hashtag fa-sm"></i> Recurrent Neural Networks</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> NLP</a>   <a href="/blog/blog/tag/sequential-data"> <i class="fa-solid fa-hashtag fa-sm"></i> Sequential Data</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="the-time-traveling-neurons-unlocking-memory-in-recurrent-neural-networks">The Time-Traveling Neurons: Unlocking Memory in Recurrent Neural Networks</h2> <p>Imagine trying to understand a story by reading each word completely in isolation, forgetting everything you just read. Sounds impossible, right? Yet, for the longest time, that’s exactly how our early neural networks operated. They were brilliant at identifying patterns in independent data points – “Is this a cat or a dog?” “Is this digit a 3 or an 8?” – but they struggled when the <em>order</em> of information mattered. This is where Recurrent Neural Networks (RNNs) step onto the stage, giving machines the ability to remember, to learn from sequences, and to process information with a crucial sense of context.</p> <h3 id="the-problem-when-order-matters">The Problem: When Order Matters</h3> <p>Let’s consider a simple sentence: “I saw a saw.” A traditional feedforward neural network would process “I,” then “saw,” then “a,” then “saw.” Each “saw” would be treated as an entirely new, independent input. It wouldn’t inherently know that the first “saw” is a verb (the act of seeing) and the second “saw” is a noun (a tool). The meaning hinges entirely on the preceding words.</p> <p>This limitation plagues many real-world problems:</p> <ul> <li> <strong>Natural Language Processing (NLP):</strong> Understanding sentences, translating languages, predicting the next word.</li> <li> <strong>Time Series Data:</strong> Forecasting stock prices, weather patterns, or sensor readings.</li> <li> <strong>Speech Recognition:</strong> Transcribing spoken words into text.</li> <li> <strong>Music Generation:</strong> Creating a melody that flows coherently.</li> </ul> <p>In all these scenarios, the data isn’t a collection of isolated points; it’s a sequence where each element’s meaning is influenced by what came before it. We needed a neural network that had a memory, a way to carry information forward in time.</p> <h3 id="enter-recurrent-neural-networks-giving-machines-a-short-term-memory">Enter Recurrent Neural Networks: Giving Machines a Short-Term Memory</h3> <p>RNNs are specially designed to handle sequential data. Their core innovation lies in a loop, allowing information to persist. Think of it like a sticky note that a neuron can write on after processing one piece of information, and then read from when processing the next. This “sticky note” is called the <strong>hidden state</strong>.</p> <p>Let’s break down the magic.</p> <h4 id="the-core-idea-recurrence">The Core Idea: Recurrence</h4> <p>Unlike feedforward networks, where information flows strictly in one direction (input -&gt; hidden layers -&gt; output), RNNs introduce a feedback loop. At each time step ($t$), the RNN doesn’t just take the current input ($x_t$); it also takes the hidden state from the previous time step ($h_{t-1}$). This hidden state $h_{t-1}$ is essentially the “memory” of the network regarding what it has seen so far in the sequence.</p> <p>Here’s a simplified view of the operations at a single time step:</p> <ol> <li> <strong>Current Input:</strong> The network receives the current input $x_t$ (e.g., the current word in a sentence).</li> <li> <strong>Previous Hidden State:</strong> It also receives the hidden state $h_{t-1}$ from the previous time step.</li> <li> <strong>New Hidden State Calculation:</strong> These two pieces of information ($x_t$ and $h_{t-1}$) are combined, typically multiplied by weight matrices, added to a bias, and then passed through an activation function (like tanh or ReLU) to produce a new hidden state $h_t$. This $h_t$ now encapsulates information from <em>both</em> the current input and the “memory” of past inputs.</li> <li> <strong>Output Calculation:</strong> Finally, $h_t$ can be used to generate an output $y_t$ (e.g., predicting the next word, classifying the sentence sentiment, etc.).</li> </ol> <p>Crucially, the <em>same</em> set of weights ($W_{xh}, W_{hh}, W_{hy}$) is used at <em>every</em> time step. This sharing of weights is what allows the network to learn patterns that are consistent across different positions in a sequence. It’s like having one set of rules that apply regardless of whether you’re processing the first word or the fifth.</p> <h4 id="the-math-behind-the-memory">The Math Behind the Memory</h4> <p>Let’s look at the basic equations for a simple RNN layer:</p> <ol> <li> <p><strong>Hidden State Calculation:</strong> $h_t = \tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)$</p> <ul> <li>$h_t$: The new hidden state at time step $t$.</li> <li>$h_{t-1}$: The hidden state from the previous time step $t-1$. This is the “memory.”</li> <li>$x_t$: The input at time step $t$.</li> <li>$W_{hh}$: Weight matrix for the recurrent connection (how much the previous hidden state influences the current one).</li> <li>$W_{xh}$: Weight matrix for the input (how much the current input influences the current hidden state).</li> <li>$b_h$: Bias vector for the hidden state.</li> <li>$\tanh$: An activation function (often used to squash values between -1 and 1).</li> </ul> </li> <li> <p><strong>Output Calculation:</strong> $y_t = W_{hy} h_t + b_y$</p> <ul> <li>$y_t$: The output at time step $t$.</li> <li>$W_{hy}$: Weight matrix connecting the hidden state to the output.</li> <li>$b_y$: Bias vector for the output.</li> <li>(An activation function like softmax for classification might be applied to $y_t$ later).</li> </ul> </li> </ol> <p>You can see how $h_t$ directly depends on $h_{t-1}$. This chain is what gives the RNN its “memory.”</p> <h4 id="unrolling-the-rnn-through-time">Unrolling the RNN Through Time</h4> <p>To understand how an RNN processes an entire sequence, it’s often helpful to “unroll” it. Imagine our sentence: “The cat sat on the mat.”</p> <ul> <li> <strong>Time Step 1 ($t=0$):</strong> Input $x_0$ (“The”). An initial hidden state $h_{-1}$ (often initialized to zeros) is combined with $x_0$ to produce $h_0$. An output $y_0$ might be generated.</li> <li> <strong>Time Step 2 ($t=1$):</strong> Input $x_1$ (“cat”). The network now takes $x_1$ <em>and</em> the previously computed $h_0$ to produce $h_1$. Output $y_1$ is generated.</li> <li> <strong>Time Step 3 ($t=2$):</strong> Input $x_2$ (“sat”). It uses $x_2$ and $h_1$ to produce $h_2$.</li> <li>…and so on, until the end of the sequence.</li> </ul> <p>When unrolled, an RNN looks like a very deep feedforward network, but with a critical difference: the same weights ($W_{hh}, W_{xh}, W_{hy}$) are used at every layer (time step). This means training involves <strong>Backpropagation Through Time (BPTT)</strong>, which is essentially standard backpropagation applied over the unrolled network.</p> <h3 id="what-rnns-are-good-for-a-world-of-sequential-data">What RNNs Are Good For: A World of Sequential Data</h3> <p>RNNs truly shine in tasks where the sequence matters:</p> <ul> <li> <strong>Machine Translation:</strong> “Translate English to French.” An RNN processes the English sentence word by word and generates the French translation.</li> <li> <strong>Text Generation:</strong> Given a starting phrase, an RNN can predict the next word, then the next, generating coherent text.</li> <li> <strong>Sentiment Analysis:</strong> “This movie was absolutely fantastic!” An RNN can process the words and determine the overall positive sentiment.</li> <li> <strong>Speech Recognition:</strong> Converting audio signals (a sequence of sound waves) into a sequence of text.</li> <li> <strong>Time Series Forecasting:</strong> Predicting the next value in a sequence, like stock prices or temperature readings.</li> </ul> <h3 id="the-achilles-heel-vanishing-and-exploding-gradients">The Achilles’ Heel: Vanishing and Exploding Gradients</h3> <p>While revolutionary, basic RNNs come with a significant challenge, especially when dealing with very long sequences: <strong>the vanishing and exploding gradient problem.</strong></p> <p>During BPTT, gradients (which tell the network how to adjust its weights during training) are computed by repeatedly multiplying matrices.</p> <ul> <li> <p><strong>Vanishing Gradients:</strong> If these matrices contain values less than 1 (common with activation functions like tanh and sigmoid), repeatedly multiplying them causes the gradients to shrink exponentially as they propagate backward through time. This means that errors at the end of a long sequence have almost no impact on the weights associated with early time steps. The network “forgets” information from earlier parts of the sequence, making it hard to learn long-term dependencies. Imagine trying to hear a whisper across a very long, noisy corridor.</p> </li> <li> <p><strong>Exploding Gradients:</strong> Conversely, if the matrices contain values greater than 1, gradients can grow exponentially, leading to extremely large weight updates. This causes instability during training, often resulting in “NaN” (Not a Number) errors. It’s like a snowball rolling downhill, picking up speed and size until it becomes an uncontrollable avalanche.</p> </li> </ul> <p>These problems severely limited the ability of simple RNNs to learn dependencies that spanned many time steps (e.g., in a long paragraph, where the meaning of a pronoun might depend on a noun mentioned 50 words ago).</p> <h3 id="the-evolution-lstms-and-grus-to-the-rescue">The Evolution: LSTMs and GRUs to the Rescue</h3> <p>To combat the vanishing gradient problem, researchers developed more sophisticated RNN architectures, most notably <strong>Long Short-Term Memory (LSTMs)</strong> and <strong>Gated Recurrent Units (GRUs)</strong>.</p> <p>These networks introduce “gates” – special mechanisms that regulate the flow of information into and out of the hidden state (and in the case of LSTMs, a separate “cell state”). Think of these gates as intelligent traffic controllers for information:</p> <ul> <li> <strong>Forget Gate:</strong> Decides what information from the previous cell state should be thrown away.</li> <li> <strong>Input Gate:</strong> Decides what new information from the current input should be stored in the cell state.</li> <li> <strong>Output Gate:</strong> Decides what part of the cell state should be outputted as the hidden state.</li> </ul> <p>By selectively remembering and forgetting, LSTMs and GRUs can learn much longer-term dependencies than vanilla RNNs. They effectively provide a more controlled “memory management system,” preventing gradients from vanishing too quickly (or exploding, thanks to techniques like gradient clipping). GRUs are a slightly simplified version of LSTMs, often offering similar performance with fewer parameters, making them faster to train.</p> <h3 id="beyond-the-basics-practical-considerations-and-the-road-ahead">Beyond the Basics: Practical Considerations and the Road Ahead</h3> <p>RNNs, especially their gated variants (LSTMs and GRUs), have been foundational in many AI breakthroughs. However, the field continues to evolve:</p> <ul> <li> <strong>Bidirectional RNNs (Bi-RNNs):</strong> Sometimes, context from the future is also important. Bi-RNNs process the sequence in both forward and backward directions, combining their hidden states to get a richer representation.</li> <li> <strong>Stacked RNNs:</strong> For more complex tasks, multiple RNN layers can be stacked on top of each other, allowing for hierarchical feature extraction.</li> <li> <strong>Attention Mechanisms &amp; Transformers:</strong> While RNNs are powerful, they still struggle with extremely long sequences due to their sequential nature. The introduction of “attention mechanisms” and the rise of the <strong>Transformer architecture</strong> have revolutionized NLP and other sequence tasks. Transformers can process all parts of a sequence simultaneously, making them much faster and better at capturing long-range dependencies. However, the core lessons learned from RNNs about processing sequential data and managing memory were crucial stepping stones to these advancements.</li> </ul> <h3 id="conclusion-giving-machines-a-sense-of-time">Conclusion: Giving Machines a Sense of Time</h3> <p>Recurrent Neural Networks represent a monumental leap in our ability to teach machines about the real world, where data often unfolds over time. By introducing the concept of a “memory” through their recurrent connections, RNNs empower AI to understand context, generate creative sequences, and make predictions based on the narrative of data.</p> <p>While newer architectures like Transformers have taken the spotlight for many state-of-the-art applications, understanding RNNs is fundamental. They laid the groundwork, taught us invaluable lessons about sequential data, and showed us that with a bit of “time-traveling” memory, our artificial neurons can truly begin to comprehend the stories hidden within our data. It’s like giving our machines a sense of time, enabling them to understand the narrative of data.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>