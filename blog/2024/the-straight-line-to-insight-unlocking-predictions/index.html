<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Straight Line to Insight: Unlocking Predictions with Linear Regression | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/the-straight-line-to-insight-unlocking-predictions/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Straight Line to Insight: Unlocking Predictions with Linear Regression</h1> <p class="post-meta"> Created on December 09, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/linear-regression"> <i class="fa-solid fa-hashtag fa-sm"></i> Linear Regression</a>   <a href="/blog/blog/tag/statistics"> <i class="fa-solid fa-hashtag fa-sm"></i> Statistics</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/predictive-modeling"> <i class="fa-solid fa-hashtag fa-sm"></i> Predictive Modeling</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hello fellow data explorers!</p> <p>Today, I want to talk about an algorithm that feels like magic but is rooted in beautiful simplicity. It’s often the first tool in a data scientist’s toolkit, a foundational concept that, once understood, unlocks a whole world of predictive modeling. I’m talking about <strong>Linear Regression</strong>.</p> <p>My first encounter with linear regression felt a bit like finding a secret superpower. I was trying to predict exam scores based on study hours, and I quickly realized that simply averaging past scores wasn’t going to cut it. I needed something that could find a <em>relationship</em> between my study time and my grades. That’s when I stumbled upon this gem, and honestly, it changed how I looked at data forever.</p> <h3 id="what-is-linear-regression-really">What is Linear Regression, Really?</h3> <p>At its heart, Linear Regression is about finding the “best fitting” straight line through a set of data points. Imagine you have a scatter plot of data – let’s say, a graph where each point represents a student’s study hours on the X-axis and their exam score on the Y-axis. You can probably see a general trend, right? More study hours usually mean higher scores. Linear Regression’s job is to mathematically draw the line that best captures this trend.</p> <p>Why a straight line? Because it’s the simplest way to model a relationship. And sometimes, the simplest explanation is the most powerful. It allows us to predict a <strong>continuous target variable</strong> (like price, score, temperature, etc.) based on one or more <strong>predictor variables</strong> (like size, study hours, humidity, etc.).</p> <h3 id="the-intuition-drawing-the-best-line">The Intuition: Drawing the “Best” Line</h3> <p>When you look at a scatter plot, you might instinctively try to draw a line that goes through the “middle” of the points. But what does “middle” really mean? How do we ensure our line isn’t too high, too low, or tilted incorrectly?</p> <p>That’s where the math comes in, and it’s surprisingly intuitive. Our goal is to find a line such that the total “distance” from all the data points to the line is minimized. We want our predicted values to be as close to the actual values as possible.</p> <h3 id="the-math-behind-the-magic-from-intuition-to-equation">The Math Behind the Magic: From Intuition to Equation</h3> <p>Let’s start with the simplest form: <strong>Simple Linear Regression</strong>. This is when we have just one predictor variable.</p> <p>The equation for a straight line that you probably remember from algebra class is:</p> <p>$y = mx + b$</p> <p>In the world of statistics and machine learning, we often use slightly different notation, but it means exactly the same thing:</p> <p>$y = \beta_0 + \beta_1 x$</p> <p>Let’s break down this equation:</p> <ul> <li>$y$: This is our <strong>dependent variable</strong> or <strong>target variable</strong>. It’s what we’re trying to predict (e.g., exam score).</li> <li>$x$: This is our <strong>independent variable</strong> or <strong>predictor variable</strong>. It’s the feature we’re using to make the prediction (e.g., study hours).</li> <li>$\beta_0$ (beta-nought): This is the <strong>y-intercept</strong>. It’s the value of $y$ when $x$ is 0. In our example, it would be the predicted exam score for someone who studied 0 hours.</li> <li>$\beta_1$ (beta-one): This is the <strong>slope</strong> of the line. It tells us how much $y$ is expected to change for every one-unit increase in $x$. So, if $\beta_1 = 5$, it means for every extra hour studied, the exam score is predicted to increase by 5 points.</li> </ul> <p>Our job in Linear Regression is to find the values of $\beta_0$ and $\beta_1$ that define the “best fit” line.</p> <h4 id="how-do-we-find-the-best-fit-the-cost-function">How Do We Find the “Best Fit”? The Cost Function!</h4> <p>So, how do we quantify “best fit”? We need a way to measure how good (or bad) our line is. This is where the concept of a <strong>cost function</strong> (also known as a loss function) comes into play.</p> <p>For each data point, there’s an actual $y_i$ value (the real exam score) and a predicted $\hat{y}_i$ value (the score our line predicts for the given study hours $x_i$). The difference between the actual and predicted value, $(y_i - \hat{y}_i)$, is called the <strong>residual</strong> or <strong>error</strong>.</p> <p>If we simply summed up all these errors, positive errors (where our prediction was too low) and negative errors (where our prediction was too high) would cancel each other out. This wouldn’t give us a true sense of the overall error.</p> <p>To avoid this cancellation, we do something clever: we <strong>square</strong> each error.</p> <p>$(y_i - \hat{y}_i)^2$</p> <p>Squaring ensures that all errors are positive, and it also penalizes larger errors more heavily than smaller ones (e.g., an error of 2 becomes 4, but an error of 10 becomes 100).</p> <p>The most common cost function for Linear Regression is the <strong>Mean Squared Error (MSE)</strong>. It’s simply the average of all these squared errors:</p> <p>$MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$</p> <p>Here:</p> <ul> <li>$n$: The number of data points.</li> <li>$\sum_{i=1}^{n}$: Summing up all the squared errors from the first to the $n$-th data point.</li> <li>$\hat{y}_i = \beta_0 + \beta_1 x_i$: This is our prediction for the $i$-th data point using our current $\beta_0$ and $\beta_1$.</li> </ul> <p><strong>Our ultimate goal is to find the values of $\beta_0$ and $\beta_1$ that MINIMIZE this MSE.</strong></p> <h4 id="minimization-finding-the-optimal-beta_0-and-beta_1">Minimization: Finding the Optimal $\beta_0$ and $\beta_1$</h4> <p>How do we minimize the MSE? For simple linear regression, there’s a closed-form solution using calculus. By taking the partial derivatives of the MSE with respect to $\beta_0$ and $\beta_1$ and setting them to zero, we can directly solve for the optimal values.</p> <p>For those curious, the formulas derived are known as the <strong>Ordinary Least Squares (OLS)</strong> estimates:</p> <p>$\beta_1 = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i - \bar{x})^2}$</p> <p>$\beta_0 = \bar{y} - \beta_1 \bar{x}$</p> <p>Where $\bar{x}$ and $\bar{y}$ are the means of the $x$ and $y$ values, respectively. You don’t need to memorize these, but it’s cool to know they exist and are derived from minimizing the sum of squared errors!</p> <p>For more complex scenarios (especially with many features), a more iterative approach called <strong>Gradient Descent</strong> is often used. It’s like finding the bottom of a valley by taking small steps downhill. But that’s a story for another blog post!</p> <h3 id="extending-the-idea-multiple-linear-regression">Extending the Idea: Multiple Linear Regression</h3> <p>What if we want to predict exam scores not just based on study hours, but also on prior knowledge, attendance, and caffeine intake? That’s where <strong>Multiple Linear Regression</strong> comes in.</p> <p>Instead of just one $x$ variable, we now have multiple $x$ variables ($x_1, x_2, x_3, \dots, x_p$). The equation simply expands:</p> <p>$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p$</p> <p>Each $\beta$ coefficient now represents the change in $y$ for a one-unit change in its corresponding $x$ variable, <em>holding all other $x$ variables constant</em>. This is incredibly powerful because it allows us to understand the individual impact of different factors on our target variable.</p> <p>For those who enjoy a more compact notation, this can also be expressed using vectors:</p> <p>$y = X\beta$</p> <p>Where $y$ is a vector of target values, $X$ is a matrix of predictor variables (with an added column of ones for the intercept), and $\beta$ is a vector of coefficients.</p> <h3 id="the-assumptions-of-linear-regression">The Assumptions of Linear Regression</h3> <p>While powerful, Linear Regression comes with a few important assumptions. Ignoring them can lead to misleading results!</p> <ol> <li> <strong>Linearity:</strong> There must be a linear relationship between the predictor variables and the target variable. If the relationship is curved, a straight line won’t fit well.</li> <li> <strong>Independence of Errors:</strong> The residuals (errors) should be independent of each other. This means one error shouldn’t influence the next.</li> <li> <strong>Homoscedasticity:</strong> The variance of the errors should be constant across all levels of the predictor variables. In simpler terms, the spread of the residuals should be roughly the same across the predicted values.</li> <li> <strong>Normality of Errors:</strong> The errors should be normally distributed. This is important for calculating confidence intervals and hypothesis tests.</li> <li> <strong>No Multicollinearity:</strong> For multiple linear regression, the predictor variables should not be highly correlated with each other. If they are, it becomes difficult to determine the individual effect of each predictor.</li> </ol> <p>Data scientists often check these assumptions using residual plots and statistical tests after training a model.</p> <h3 id="when-to-use-it-and-when-not-to">When to Use It (and When Not To)</h3> <p><strong>Use Cases:</strong></p> <ul> <li> <strong>Predicting house prices:</strong> Based on size, number of bedrooms, location, etc.</li> <li> <strong>Sales forecasting:</strong> Based on advertising spend, seasonality, past sales.</li> <li> <strong>Medical research:</strong> Predicting blood pressure based on age, weight, and diet.</li> <li> <strong>Economic modeling:</strong> Predicting GDP growth based on interest rates, inflation.</li> </ul> <p><strong>Limitations:</strong></p> <ul> <li> <strong>Assumes linearity:</strong> Cannot capture complex, non-linear relationships directly. For those, you might need polynomial regression, decision trees, or neural networks.</li> <li> <strong>Sensitive to outliers:</strong> Extreme data points can heavily influence the line and skew the coefficients.</li> <li> <strong>Doesn’t work well for categorical targets:</strong> If you want to predict ‘yes’ or ‘no’, or ‘cat’ vs ‘dog’, you’ll need classification algorithms like Logistic Regression.</li> </ul> <h3 id="a-peek-into-practice-with-python">A Peek into Practice (with Python!)</h3> <p>Implementing Linear Regression is incredibly straightforward with libraries like Scikit-learn in Python.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># (Conceptual Python code)
</span><span class="kn">from</span> <span class="n">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Sample Data: Study hours and Exam Scores
</span><span class="n">study_hours</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">]).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># X
</span><span class="n">exam_scores</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">55</span><span class="p">,</span> <span class="mi">60</span><span class="p">,</span> <span class="mi">65</span><span class="p">,</span> <span class="mi">70</span><span class="p">,</span> <span class="mi">75</span><span class="p">,</span> <span class="mi">80</span><span class="p">,</span> <span class="mi">85</span><span class="p">,</span> <span class="mi">90</span><span class="p">,</span> <span class="mi">95</span><span class="p">,</span> <span class="mi">100</span><span class="p">])</span>      <span class="c1"># y
</span>
<span class="c1"># Create a Linear Regression model
</span><span class="n">model</span> <span class="o">=</span> <span class="nc">LinearRegression</span><span class="p">()</span>

<span class="c1"># Train the model (find the best beta0 and beta1)
</span><span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">study_hours</span><span class="p">,</span> <span class="n">exam_scores</span><span class="p">)</span>

<span class="c1"># Make a prediction
</span><span class="n">new_study_hours</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mf">7.5</span><span class="p">]])</span>
<span class="n">predicted_score</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">new_study_hours</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Predicted score for 7.5 hours of study: </span><span class="si">{</span><span class="n">predicted_score</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Intercept (beta0): </span><span class="si">{</span><span class="n">model</span><span class="p">.</span><span class="n">intercept_</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Coefficient (beta1): </span><span class="si">{</span><span class="n">model</span><span class="p">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>In this example, <code class="language-plaintext highlighter-rouge">model.fit()</code> is where all the magic happens – it calculates the $\beta_0$ and $\beta_1$ values that minimize the MSE for our data. <code class="language-plaintext highlighter-rouge">model.intercept_</code> gives us $\beta_0$, and <code class="language-plaintext highlighter-rouge">model.coef_</code> gives us $\beta_1$.</p> <h3 id="conclusion-the-unsung-hero-of-prediction">Conclusion: The Unsung Hero of Prediction</h3> <p>Linear Regression might seem simple, even basic, compared to the flashy neural networks and complex ensemble models dominating headlines today. But don’t let its simplicity fool you. It is often the first model to try, a powerful baseline, and a fantastic tool for understanding the direct, linear relationships within your data.</p> <p>It’s a testament to how even a “straight line” can reveal profound insights and make surprisingly accurate predictions. Understanding Linear Regression is not just about learning an algorithm; it’s about building a fundamental intuition for how machines learn from data.</p> <p>So, next time you see a scatter plot, try to visualize that “best fit” line. You’re already thinking like a data scientist! Go forth and explore, the world of data is waiting for your insights.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>