<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Unmasking the Shadows: Decoding Bias in Machine Learning | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/2024/unmasking-the-shadows-decoding-bias-in-machine-lea/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Unmasking the Shadows: Decoding Bias in Machine Learning</h1> <p class="post-meta"> Created on October 12, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/ai-ethics"> <i class="fa-solid fa-hashtag fa-sm"></i> AI Ethics</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/bias"> <i class="fa-solid fa-hashtag fa-sm"></i> Bias</a>   <a href="/blog/blog/tag/fairness"> <i class="fa-solid fa-hashtag fa-sm"></i> Fairness</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Ever thought about how a computer decides whether you get a loan, a job interview, or even what news article pops up on your feed? It’s easy to imagine these systems as purely objective, spitting out decisions based on cold, hard logic. But what if I told you that the very machines we build to be fair can sometimes inherit our deepest, most unconscious prejudices?</p> <p>As a data scientist, I’ve spent countless hours staring at datasets, building models, and pushing the boundaries of what machines can learn. And the more I build, the more profoundly I realize something critical: <strong>Machine Learning models are not born neutral.</strong> They are reflections of the data they consume and the humans who design them. This realization led me down a rabbit hole, exploring one of the most pressing challenges in AI today: <strong>Bias in Machine Learning.</strong></p> <h3 id="what-exactly-is-bias-in-machine-learning">What Exactly <em>Is</em> Bias in Machine Learning?</h3> <p>When we talk about “bias” in everyday language, we often mean prejudice or favoritism towards a person or group. In Machine Learning, it’s remarkably similar. Fundamentally, <strong>ML bias is a systematic and unfair discrimination by an AI system against certain individuals or groups of individuals, leading to skewed, inaccurate, or unjust outcomes.</strong></p> <p>This isn’t just a philosophical debate; it has tangible, real-world consequences. Imagine a medical diagnostic tool that performs worse for certain demographics, or a hiring algorithm that consistently overlooks qualified candidates from underrepresented groups. These aren’t hypothetical scenarios; they’ve happened. And understanding <em>why</em> they happen is our first step towards preventing them.</p> <h3 id="the-many-faces-of-bias-where-does-it-come-from">The Many Faces of Bias: Where Does It Come From?</h3> <p>Bias doesn’t just appear out of thin air. It’s often baked into the ingredients we use to cook up our AI models: the data, the algorithms, and even how people interact with the system.</p> <h4 id="1-data-bias-the-skewed-mirror-of-reality">1. Data Bias: The Skewed Mirror of Reality</h4> <p>Think of your dataset as a mirror reflecting the world. If that mirror is cracked, dusty, or positioned incorrectly, the reflection will be distorted. Data bias is probably the most common and insidious form of bias.</p> <ul> <li> <strong>Historical Bias:</strong> This is perhaps the most challenging to tackle because it comes directly from past and present societal inequalities. If a historical dataset shows, for example, that men were predominantly hired for leadership roles, an ML model trained on this data might learn to associate “male” with “leader,” even if that’s not true today. It’s not the model <em>inventing</em> prejudice; it’s <em>learning</em> it from historical trends.</li> <li> <strong>Selection Bias:</strong> This occurs when the data used to train the model isn’t representative of the real-world population the model will be deployed on. <ul> <li> <em>Example:</em> Training a facial recognition system primarily on images of light-skinned individuals. When deployed, it will perform significantly worse on people with darker skin tones because it simply hasn’t “seen” enough examples to learn their unique features effectively.</li> </ul> </li> <li> <strong>Measurement Bias:</strong> This happens when there are systematic errors in how data is collected or measured. <ul> <li> <em>Example:</em> If sensors designed to detect skin conditions perform differently based on skin pigmentation, the data collected will be inherently biased, leading to an ML model that propagates these measurement inaccuracies.</li> </ul> </li> <li> <strong>Reporting Bias:</strong> People are more likely to report certain types of information than others. This can lead to datasets that overemphasize some aspects while ignoring others. <ul> <li> <em>Example:</em> News articles might focus more on negative events, leading an NLP model to associate certain entities with negativity.</li> </ul> </li> </ul> <h4 id="2-algorithmic-bias-the-recipes-secret-ingredient">2. Algorithmic Bias: The Recipe’s Secret Ingredient</h4> <p>Even if your data seems perfect (which it rarely is), bias can still creep in through the algorithms themselves or how they’re designed.</p> <ul> <li> <strong>Feature Selection Bias:</strong> Sometimes, engineers unknowingly choose features that are highly correlated with sensitive attributes (like race or gender) but don’t explicitly represent them. <ul> <li> <em>Example:</em> Using ZIP codes in a credit risk model. While not explicitly race-based, ZIP codes can often correlate strongly with racial demographics and socio-economic status due to historical segregation, indirectly introducing bias.</li> </ul> </li> <li> <strong>Inductive Bias (Model Bias):</strong> This refers to the assumptions a learning algorithm makes to generalize from training data to unseen data. Different algorithms have different inductive biases. <ul> <li> <em>Example:</em> A linear model has an inductive bias that assumes relationships are linear. If the underlying data for certain groups exhibits complex, non-linear patterns, a linear model might underperform for those groups, effectively showing bias.</li> </ul> </li> </ul> <h4 id="3-interaction-bias-the-feedback-loop">3. Interaction Bias: The Feedback Loop</h4> <p>This form of bias emerges when users interact with a biased system, which then reinforces and amplifies the existing bias.</p> <ul> <li> <em>Example:</em> A search engine that returns sexist stereotypes for certain job queries. If users click on these results more often (perhaps out of curiosity or because they appear higher), the algorithm might learn that these results are “relevant” and continue to promote them, creating a vicious cycle.</li> </ul> <h3 id="real-world-consequences-when-algorithms-go-wrong">Real-World Consequences: When Algorithms Go Wrong</h3> <p>These aren’t just theoretical discussions. Here are a few prominent examples that highlight the urgent need to address ML bias:</p> <ul> <li> <strong>Criminal Justice:</strong> The COMPAS algorithm, used in U.S. courts to predict recidivism (likelihood of re-offending), was found to disproportionately label Black defendants as high-risk, even when they didn’t re-offend, and white defendants as low-risk, even when they did. This directly impacted parole and sentencing decisions.</li> <li> <strong>Hiring:</strong> Amazon famously scrapped an AI recruiting tool because it showed bias against women. Trained on resumes submitted over a decade, which predominantly came from men in the tech industry, the system learned to penalize resumes containing words like “women’s” and even downgrade candidates who attended women’s colleges.</li> <li> <strong>Healthcare:</strong> Several risk-prediction algorithms used in healthcare systems have shown bias, underestimating the health needs of Black patients compared to white patients, potentially leading to less access to care.</li> <li> <strong>Facial Recognition:</strong> Studies have repeatedly shown that many commercial facial recognition systems have significantly higher error rates for women and people of color, especially darker-skinned women. This has serious implications for surveillance and law enforcement.</li> </ul> <p>These cases make it clear: the stakes are incredibly high. Our algorithms aren’t just reflecting reality; they’re actively shaping it, and if we’re not careful, they can perpetuate and even amplify societal injustices.</p> <h3 id="the-challenge-why-isnt-it-easy-to-fix">The Challenge: Why Isn’t It Easy to Fix?</h3> <p>You might be thinking, “Well, just train it on good data, right?” If only it were that simple!</p> <ol> <li> <strong>Data Reflects Reality:</strong> Our historical data is inherently biased because our societies have been biased. “Neutral” historical data simply doesn’t exist for many domains.</li> <li> <strong>Defining “Fairness” is Hard:</strong> What does “fair” actually mean? Is it equal outcomes for all groups? Equal opportunities? Equal error rates? Different definitions of fairness can often conflict, and prioritizing one might mean compromising on another. For example, ensuring equal error rates for all groups might lead to different decision thresholds, which some might argue is <em>un</em>fair.</li> <li> <strong>Black Box Models:</strong> Many powerful ML models, like deep neural networks, are notoriously complex. Understanding <em>why</em> they make a specific decision can be incredibly difficult, making it hard to pinpoint the source of bias.</li> <li> <strong>Trade-offs:</strong> Sometimes, achieving fairness can come at the cost of a slight decrease in overall model accuracy. Deciding where to draw that line is an ethical and business challenge.</li> </ol> <h3 id="fighting-the-shadows-strategies-for-debiasing-ml">Fighting the Shadows: Strategies for Debiasing ML</h3> <p>While complex, the battle against ML bias is not unwinnable. As data scientists and ML engineers, we have a crucial role to play. Here are some strategies we employ:</p> <h4 id="1-pre-processing-intervening-before-training-data-level">1. Pre-processing: Intervening Before Training (Data-level)</h4> <p>This is about cleaning and preparing your data <em>before</em> it ever sees a model.</p> <ul> <li> <strong>Conscious Data Collection:</strong> Actively seeking out diverse and representative datasets. This might involve collecting more data for underrepresented groups or using synthetic data generation techniques carefully.</li> <li> <strong>Bias Detection Tools:</strong> Using statistical analyses, visualizations, and specific bias detection libraries to identify biases within the dataset itself. For example, comparing feature distributions across different sensitive groups (e.g., age, gender, race) to spot discrepancies.</li> <li> <strong>Data Re-sampling and Augmentation:</strong> If a group is underrepresented, we might oversample their data or generate synthetic examples. Conversely, we might undersample overrepresented groups.</li> </ul> <h4 id="2-in-processing-intervening-during-training-algorithm-level">2. In-processing: Intervening During Training (Algorithm-level)</h4> <p>These techniques modify the learning algorithm itself or its objective function.</p> <ul> <li> <strong>Fairness-Aware Algorithms:</strong> Some algorithms are designed with fairness constraints built directly into their optimization process. They might try to minimize prediction error <em>while also</em> ensuring parity across groups.</li> <li> <strong>Adversarial Debiasing:</strong> This involves training an additional neural network (an “adversary”) alongside your main model. The adversary tries to predict the sensitive attribute (e.g., race) from the model’s learned representations. The main model is then trained to make predictions <em>without</em> allowing the adversary to predict the sensitive attribute, thus removing the sensitive information from the learned features.</li> <li> <strong>Careful Feature Engineering:</strong> Thoughtfully selecting and transforming features, questioning whether any chosen feature could indirectly encode sensitive information.</li> </ul> <h4 id="3-post-processing-intervening-after-training-model-output-level">3. Post-processing: Intervening After Training (Model Output-level)</h4> <p>These methods adjust the model’s predictions <em>after</em> it has been trained.</p> <ul> <li> <strong>Threshold Adjustment:</strong> If a model outputs probabilities, we can adjust the decision threshold ($&gt;$0.5 for a positive prediction) differently for various groups to achieve a fairer outcome.</li> <li> <strong>Fairness Metrics:</strong> We use specific metrics to quantify bias and evaluate the fairness of our models. Some common ones include: <ul> <li> <strong>Demographic Parity (Statistical Parity):</strong> Requires that the proportion of individuals receiving a positive outcome is the same across different groups. $P(\hat{Y}=1 | A=a) = P(\hat{Y}=1 | A=b)$ where $\hat{Y}=1$ means a positive prediction (e.g., getting a loan), $A$ is the sensitive attribute (e.g., race), and $a, b$ are different groups within $A$. This means the acceptance rate should be the same for all groups.</li> <li> <strong>Equal Opportunity:</strong> Requires that individuals who <em>truly</em> belong to the positive class have an equal chance of being classified as such, regardless of their group. In other words, the true positive rate (recall) should be equal across groups. $P(\hat{Y}=1 | Y=1, A=a) = P(\hat{Y}=1 | Y=1, A=b)$ where $Y=1$ means the true outcome is positive.</li> <li> <strong>Equalized Odds:</strong> A stronger condition, requiring that both the true positive rate and the true negative rate are equal across groups. $P(\hat{Y}=1 | Y=y, A=a) = P(\hat{Y}=1 | Y=y, A=b)$ for $y \in {0, 1}$ This implies the model makes errors equally for both positive and negative actual outcomes, across all groups.</li> </ul> </li> <li> <strong>Explainable AI (XAI):</strong> Tools and techniques that help us understand <em>why</em> an AI system made a particular decision. This transparency is crucial for auditing models and identifying biased decision paths.</li> </ul> <h4 id="4-human-oversight--ethical-guidelines">4. Human Oversight &amp; Ethical Guidelines</h4> <p>Ultimately, technology is a tool. We need diverse teams building AI, clear ethical guidelines, continuous monitoring of deployed models, and robust feedback mechanisms to catch emerging biases. A purely technical solution isn’t enough; it requires a human-centric approach.</p> <h3 id="conclusion-our-role-in-a-fairer-future">Conclusion: Our Role in a Fairer Future</h3> <p>The journey to unbiased AI is long and complex. It’s not about achieving perfect neutrality – that might be an impossible dream given the world we live in – but about diligently working to minimize harm and promote equitable outcomes.</p> <p>As future data scientists, ML engineers, or simply informed citizens, we have a profound responsibility. We must be critical of our data sources, thoughtful in our model design, and proactive in evaluating and mitigating bias. Every line of code, every dataset chosen, every model deployed carries ethical weight.</p> <p>Let’s strive to build intelligent systems that not only push the boundaries of technology but also uphold the principles of fairness and justice, ensuring that the future we’re creating is one where technology empowers everyone, not just a privileged few. It’s a challenge, but one that is absolutely worth taking on.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>