<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Unveiling Hidden Patterns: A Deep Dive into K-Means Clustering | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/unveiling-hidden-patterns-a-deep-dive-into-k-means/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Unveiling Hidden Patterns: A Deep Dive into K-Means Clustering</h1> <p class="post-meta"> Created on June 17, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a> Â  Â· Â  <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a> Â  <a href="/blog/blog/tag/k-means"> <i class="fa-solid fa-hashtag fa-sm"></i> K-Means</a> Â  <a href="/blog/blog/tag/clustering"> <i class="fa-solid fa-hashtag fa-sm"></i> Clustering</a> Â  <a href="/blog/blog/tag/unsupervised-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Unsupervised Learning</a> Â  <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone! ğŸ‘‹</p> <p>I remember the first time I stumbled upon the concept of â€œunsupervised learning.â€ It felt a bit like magic. Weâ€™re used to teaching computers by showing them examples with answers (thatâ€™s supervised learning: â€œthis is a cat,â€ â€œthis is not a catâ€). But what if you just throw a mountain of data at a machine and ask, â€œHey, can you find any interesting groups here?â€ No labels, no pre-defined categories â€“ just raw data and a desire for insight.</p> <p>This is where the fascinating world of clustering comes in, and today, weâ€™re going to pull back the curtain on one of its most iconic stars: <strong>K-Means Clustering</strong>. Itâ€™s a foundational algorithm in data science, elegant in its simplicity yet incredibly powerful in its applications. If youâ€™ve ever felt overwhelmed by raw data and wished it would just sort itself out, youâ€™re in the right place!</p> <h2 id="what-in-the-world-is-clustering">What in the World is Clustering?</h2> <p>Before we jump into K-Means, letâ€™s briefly define what clustering is. Imagine you have a giant box of LEGO bricks. You havenâ€™t been given any instructions, but you instinctively start grouping them by color, size, or shape. Thatâ€™s essentially what clustering algorithms do with data.</p> <p>In the realm of machine learning, <strong>clustering</strong> is an unsupervised learning task where the goal is to group a set of objects in such a way that objects in the same group (called a <em>cluster</em>) are more similar to each other than to those in other groups. The key here is â€œunsupervisedâ€ â€“ we donâ€™t have pre-existing labels telling us which group each data point belongs to. Weâ€™re letting the algorithm discover these hidden structures itself.</p> <p>Think about it:</p> <ul> <li> <strong>Classification:</strong> â€œIs this email spam or not spam?â€ (You know the categories beforehand).</li> <li> <strong>Clustering:</strong> â€œCan you group these emails into naturally occurring topics?â€ (You donâ€™t know the topics beforehand).</li> </ul> <h2 id="the-k-means-idea-a-visual-intuition">The K-Means Idea: A Visual Intuition</h2> <p>At its heart, K-Means is beautifully intuitive. Letâ€™s imagine you have a scatter plot of data points, and you want to group them into, say, three distinct clusters.</p> <ol> <li> <strong>You randomly pick <code class="language-plaintext highlighter-rouge">K</code> points</strong> (in our case, <code class="language-plaintext highlighter-rouge">K=3</code>) to be the initial â€œcentersâ€ of your clusters. We call these <strong>centroids</strong>. Donâ€™t worry if theyâ€™re in bad spots initially; the algorithm will fix it!</li> <li> <strong>Every other data point then rushes to join its nearest centroid.</strong> Itâ€™s like a magnet pulling metal shavings.</li> <li>Once all points have found a â€œhome,â€ <strong>each centroid looks at all the points that joined it and says, â€œOkay, I should probably move to the very center of <em>my</em> group.â€</strong> So, each centroid recalculates its position to be the average (mean) of all the points currently assigned to it.</li> <li>Now that the centroids have moved, <strong>all the data points check again: â€œAm I still closest to my current centroid, or is there a <em>new</em> centroid closer to me?â€</strong> Some points might switch allegiance!</li> <li> <strong>Steps 2, 3, and 4 repeat.</strong> The centroids move, points re-assign, centroids move againâ€¦ This dance continues until the centroids stop moving significantly, or the points stop switching groups. At this point, we say the algorithm has <strong>converged</strong>.</li> </ol> <p>And voilÃ ! Youâ€™re left with <code class="language-plaintext highlighter-rouge">K</code> distinct groups, each with a centroid snugly nestled in its center. Pretty neat, right?</p> <h2 id="diving-deeper-the-k-means-algorithm-step-by-step">Diving Deeper: The K-Means Algorithm Step-by-Step</h2> <p>Letâ€™s put on our more formal data science hats and walk through the algorithm with a bit more precision.</p> <p>We start with a dataset of <code class="language-plaintext highlighter-rouge">n</code> data points, say $\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_n$, where each $\mathbf{x}_j$ is a vector in some D-dimensional space (e.g., if you have height and weight, D=2). Our goal is to partition these <code class="language-plaintext highlighter-rouge">n</code> points into <code class="language-plaintext highlighter-rouge">K</code> clusters, $C_1, C_2, \ldots, C_K$.</p> <h3 id="step-1-initialization">Step 1: Initialization</h3> <p>First, we need to choose the number of clusters, <code class="language-plaintext highlighter-rouge">K</code>. This is often the trickiest part (more on this later!). Once <code class="language-plaintext highlighter-rouge">K</code> is chosen, we initialize <code class="language-plaintext highlighter-rouge">K</code> centroids. The most common way is to randomly select <code class="language-plaintext highlighter-rouge">K</code> data points from our dataset to serve as the initial centroids, $\mathbf{\mu}_1, \mathbf{\mu}_2, \ldots, \mathbf{\mu}_K$.</p> <p><em>Self-correction:</em> Random initialization can sometimes lead to suboptimal clustering. A popular improvement is <strong>K-Means++</strong>, which strategically chooses initial centroids to be far apart, often leading to better results and faster convergence. For simplicity, weâ€™ll stick to random for now.</p> <h3 id="step-2-assignment-step-the-expectation-step">Step 2: Assignment Step (The â€œExpectationâ€ Step)</h3> <p>This is where each data point finds its home. For every data point $\mathbf{x}<em>j$, we calculate its distance to _every</em> centroid $\mathbf{\mu}_k$. The most common distance metric used is <strong>Euclidean distance</strong>, which you might remember from geometry:</p> <p>$d(\mathbf{x}, \mathbf{c}) = \sqrt{\sum_{i=1}^{D} (x_i - c_i)^2}$</p> <p>Where $\mathbf{x} = (x_1, \ldots, x_D)$ is a data point and $\mathbf{c} = (c_1, \ldots, c_D)$ is a centroid.</p> <p>Once weâ€™ve calculated all the distances, each data point $\mathbf{x}_j$ is assigned to the cluster $C_k$ whose centroid $\mathbf{\mu}_k$ is closest.</p> <p>Formally, for an iteration $t$: $C_k^{(t)} = { \mathbf{x}<em>j : ||\mathbf{x}_j - \mathbf{\mu}_k^{(t)}|| \le ||\mathbf{x}_j - \mathbf{\mu}</em>{kâ€™}^{(t)}|| \text{ for all } kâ€™ \ne k }$</p> <p>This means cluster $C_k$ at iteration $t$ contains all data points $\mathbf{x}<em>j$ that are closer to centroid $\mathbf{\mu}_k^{(t)}$ than to any other centroid $\mathbf{\mu}</em>{kâ€™}^{(t)}$.</p> <h3 id="step-3-update-step-the-maximization-step">Step 3: Update Step (The â€œMaximizationâ€ Step)</h3> <p>After all points have been assigned, the centroids need to move to the â€œcenter of gravityâ€ of their newly formed clusters. Each centroid $\mathbf{\mu}_k$ is recalculated as the mean of all data points currently assigned to its cluster $C_k$.</p> <table> <tbody> <tr> <td>$\mathbf{\mu}_k^{(t+1)} = \frac{1}{</td> <td>C_k^{(t)}</td> <td>} \sum_{\mathbf{x}_j \in C_k^{(t)}} \mathbf{x}_j$</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>Here, $</td> <td>C_k^{(t)}</td> <td>$ is the number of data points in cluster $C_k$ at iteration $t$. This update rule minimizes the <strong>within-cluster sum of squares (WCSS)</strong>, also known as <strong>inertia</strong>, for the given cluster assignments. The overall objective function K-Means aims to minimize is:</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>$J = \sum_{k=1}^K \sum_{\mathbf{x} \in C_k}</td> <td>Â </td> <td>\mathbf{x} - \mathbf{\mu}_k</td> <td>Â </td> <td>^2$</td> </tr> </tbody> </table> <p>This function measures the sum of squared distances between each point and its assigned centroid across all clusters. The smaller this value, the more compact and â€œtightâ€ the clusters are.</p> <h3 id="step-4-convergence">Step 4: Convergence</h3> <p>Steps 2 and 3 are repeated iteratively. The algorithm stops when:</p> <ul> <li>The centroids no longer move significantly between iterations.</li> <li>The cluster assignments of the data points no longer change.</li> <li>A maximum number of iterations has been reached (to prevent infinite loops in rare cases).</li> </ul> <p>At this point, we have our final <code class="language-plaintext highlighter-rouge">K</code> clusters and their respective centroids.</p> <h2 id="choosing-the-right-k-the-elbow-method">Choosing the Right â€˜Kâ€™: The Elbow Method</h2> <p>As I hinted earlier, one of the biggest questions with K-Means is: â€œHow do I choose the optimal <code class="language-plaintext highlighter-rouge">K</code>?â€ Itâ€™s not always obvious how many natural groups exist in your data. Enter the <strong>Elbow Method</strong>!</p> <p>The Elbow Method uses the concept of the <strong>Within-Cluster Sum of Squares (WCSS)</strong>, which is exactly the objective function $J$ we talked about: the sum of the squared distances between each point and its assigned centroid.</p> <p>Hereâ€™s how it works:</p> <ol> <li>Run the K-Means algorithm for a range of <code class="language-plaintext highlighter-rouge">K</code> values (e.g., from 1 to 10 or 15).</li> <li>For each <code class="language-plaintext highlighter-rouge">K</code>, calculate the WCSS.</li> <li>Plot the WCSS values against the corresponding <code class="language-plaintext highlighter-rouge">K</code> values.</li> </ol> <p>What youâ€™ll typically see is that as <code class="language-plaintext highlighter-rouge">K</code> increases, the WCSS value decreases. This makes sense: the more clusters you have, the closer the centroids will be to their respective data points, and thus the lower the sum of squared distances.</p> <p>However, at some point, adding more clusters provides diminishing returns. The graph will look like an arm bending, and the â€œelbowâ€ of that arm signifies the optimal <code class="language-plaintext highlighter-rouge">K</code>. Itâ€™s the point where the decrease in WCSS starts to slow down significantly.</p> <p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Kmeans-elbow-plot.svg/600px-Kmeans-elbow-plot.svg.png" alt="Elbow Method Plot Example"> <em>(Imagine a plot where the x-axis is â€˜Number of Clusters (K)â€™ and the y-axis is â€˜WCSSâ€™. It starts high and drops sharply, then flattens out. The â€œelbowâ€ is the point where the steep drop lessens.)</em></p> <p>While the Elbow Method is a popular heuristic, itâ€™s not always definitive. Sometimes the â€œelbowâ€ isnâ€™t clear, and domain knowledge or other metrics (like silhouette score) might be needed to make an informed decision about <code class="language-plaintext highlighter-rouge">K</code>.</p> <h2 id="strengths-and-weaknesses-of-k-means">Strengths and Weaknesses of K-Means</h2> <p>Every powerful tool has its quirks. K-Means is no exception.</p> <h3 id="strengths">Strengths:</h3> <ul> <li> <strong>Simplicity and Interpretability:</strong> Itâ€™s easy to understand and implement, making it a great starting point for clustering tasks.</li> <li> <strong>Efficiency:</strong> Itâ€™s computationally very fast, especially for large datasets with many features, making it scalable.</li> <li> <strong>Versatility:</strong> Itâ€™s widely applicable across many domains and types of data.</li> </ul> <h3 id="weaknesses">Weaknesses:</h3> <ul> <li> <strong>Requires specifying <code class="language-plaintext highlighter-rouge">K</code>:</strong> As we saw, choosing <code class="language-plaintext highlighter-rouge">K</code> can be subjective and tricky.</li> <li> <strong>Sensitive to Initial Centroids:</strong> Random initialization can lead to different results each time, potentially converging to a local optimum rather than the global optimum. (K-Means++ helps mitigate this).</li> <li> <strong>Assumes Spherical Clusters:</strong> K-Means works best when clusters are roughly spherical, similarly sized, and have similar densities. It struggles with irregularly shaped clusters or clusters of very different sizes.</li> <li> <strong>Sensitive to Outliers:</strong> Outliers can significantly pull a centroid towards them, distorting the cluster shape.</li> <li> <strong>Requires Numerical Data:</strong> It typically works with numerical data and struggles with categorical features without proper encoding.</li> <li> <strong>Sensitive to Feature Scaling:</strong> If features have very different scales (e.g., height in meters vs. weight in grams), features with larger scales might dominate the distance calculation. Itâ€™s often crucial to scale your features (e.g., using StandardScaler) before applying K-Means.</li> </ul> <h2 id="real-world-applications">Real-World Applications</h2> <p>Despite its limitations, K-Means is a workhorse in the data science world. Here are a few examples:</p> <ul> <li> <strong>Customer Segmentation:</strong> Grouping customers based on purchase history, demographics, or browsing behavior to tailor marketing strategies.</li> <li> <strong>Document Clustering:</strong> Organizing large collections of text documents by topic, helping with information retrieval.</li> <li> <strong>Image Compression (Color Quantization):</strong> Reducing the number of distinct colors in an image while maintaining visual quality, by grouping similar colors.</li> <li> <strong>Anomaly Detection:</strong> Identifying unusual patterns or outliers in data, which might indicate fraud, network intrusion, or manufacturing defects.</li> <li> <strong>Genetics:</strong> Grouping gene expressions with similar patterns to understand biological processes.</li> </ul> <h2 id="conclusion">Conclusion</h2> <p>So there you have it â€“ K-Means Clustering! Itâ€™s a fantastic example of how a relatively simple iterative process can uncover profound insights from raw, unlabeled data. Weâ€™ve journeyed from a high-level intuition to the mathematical heart of its operations, explored how to pick the mysterious â€˜Kâ€™, and weighed its pros and cons.</p> <p>As you continue your data science journey, youâ€™ll find K-Means to be a fundamental building block. It might not be the fanciest algorithm, but its elegance and effectiveness make it an indispensable tool for anyone looking to make sense of the vast, unstructured datasets that surround us. Go forth and cluster!</p> <p>Happy clustering! âœ¨</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> Â© Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>