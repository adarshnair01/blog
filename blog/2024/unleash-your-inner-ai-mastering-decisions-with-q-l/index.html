<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Unleash Your Inner AI: Mastering Decisions with Q-Learning! | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/unleash-your-inner-ai-mastering-decisions-with-q-l/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Unleash Your Inner AI: Mastering Decisions with Q-Learning!</h1> <p class="post-meta"> Created on May 27, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/reinforcement-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Reinforcement Learning</a>   <a href="/blog/blog/tag/q-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Q-Learning</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/artificial-intelligence"> <i class="fa-solid fa-hashtag fa-sm"></i> Artificial Intelligence</a>   <a href="/blog/blog/tag/decision-making"> <i class="fa-solid fa-hashtag fa-sm"></i> Decision Making</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>Have you ever found yourself fascinated by how AI agents in video games seem to learn complex strategies, or how a robot can figure out the best path through a maze all by itself? It’s not magic, it’s often the result of some incredibly clever algorithms from the world of Reinforcement Learning (RL). And today, I want to pull back the curtain on one of the most foundational and intuitive of these: <strong>Q-Learning</strong>.</p> <p>Think of Q-Learning as an AI’s personal guide to making the best possible decisions in any situation, purely by learning from experience, just like how we learn to ride a bike or play a new game. It’s an algorithm that’s surprisingly simple at its core, yet incredibly powerful in its applications. Let’s dive in!</p> <h3 id="the-world-of-reinforcement-learning-a-quick-detour">The World of Reinforcement Learning: A Quick Detour</h3> <p>Before we get to Q-Learning, let’s briefly set the stage with Reinforcement Learning. Imagine you have an <strong>agent</strong> (that’s our AI, robot, or game character) existing in an <strong>environment</strong> (the maze, the game world, the real world).</p> <p>The agent’s goal is to maximize a cumulative <strong>reward</strong>. For example:</p> <ul> <li>In a maze, finding cheese might give a +100 reward, hitting a wall -1, and taking a step -0.1.</li> <li>In a game, winning a match might be +1000, losing -1000.</li> </ul> <p>At any given moment, the agent is in a certain <strong>state</strong> ($s$). From this state, it can perform an <strong>action</strong> ($a$). Performing an action moves the agent to a new state ($s’$) and earns it a <strong>reward</strong> ($R$). This continuous loop of (state -&gt; action -&gt; reward -&gt; new state) is how the agent interacts with its environment.</p> <p>Our ultimate goal in RL is to find the best <strong>policy</strong> – a strategy that tells our agent what action to take in every possible state to maximize its total future reward.</p> <h3 id="enter-q-learning-the-cheat-sheet-for-smart-decisions">Enter Q-Learning: The “Cheat Sheet” for Smart Decisions</h3> <p>Q-Learning is a <strong>model-free, value-based</strong> reinforcement learning algorithm. “Model-free” means our agent doesn’t need to understand the underlying physics or rules of the environment; it just needs to interact with it. “Value-based” means it tries to learn the “value” of taking certain actions in certain states.</p> <p>The “Q” in Q-Learning stands for “Quality.” Specifically, it represents the <strong>Quality of an action in a given state</strong>. We denote this as $Q(s, a)$.</p> <p>Imagine our agent building a giant “cheat sheet” or a lookup table, which we call the <strong>Q-table</strong>. This table has rows for every possible state and columns for every possible action. Each cell $Q(s, a)$ in this table will eventually store the maximum expected future reward an agent can get if it takes action $a$ in state $s$ and then acts optimally thereafter.</p> <table> <thead> <tr> <th style="text-align: left">State \ Action</th> <th style="text-align: left">Move Up</th> <th style="text-align: left">Move Down</th> <th style="text-align: left">Move Left</th> <th style="text-align: left">Move Right</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">State 1</td> <td style="text-align: left">0</td> <td style="text-align: left">0</td> <td style="text-align: left">0</td> <td style="text-align: left">0</td> </tr> <tr> <td style="text-align: left">State 2</td> <td style="text-align: left">0</td> <td style="text-align: left">0</td> <td style="text-align: left">0</td> <td style="text-align: left">0</td> </tr> <tr> <td style="text-align: left">…</td> <td style="text-align: left">…</td> <td style="text-align: left">…</td> <td style="text-align: left">…</td> <td style="text-align: left">…</td> </tr> <tr> <td style="text-align: left">State N</td> <td style="text-align: left">0</td> <td style="text-align: left">0</td> <td style="text-align: left">0</td> <td style="text-align: left">0</td> </tr> </tbody> </table> <p>Initially, all Q-values are typically zero. Our agent starts clueless, but with each interaction with the environment, it updates these values, slowly building its cheat sheet to perfection.</p> <h3 id="how-our-agent-learns-the-dance-of-exploration-and-exploitation">How Our Agent Learns: The Dance of Exploration and Exploitation</h3> <p>The core idea of Q-Learning is that the agent continuously updates its Q-table based on its experiences. But how does it choose which action to take? This brings us to a crucial concept: <strong>exploration vs. exploitation</strong>.</p> <ol> <li> <strong>Exploration</strong>: Trying new things. Our agent might take a random action, even if it doesn’t seem optimal, just to see what happens. This is how it discovers new paths and potentially higher rewards it didn’t know existed. Think of it like trying a new restaurant you’ve never been to.</li> <li> <strong>Exploitation</strong>: Sticking with what works best. Our agent chooses the action that has the highest Q-value for its current state, based on what it has learned so far. This is like going to your favorite restaurant because you know you’ll have a good meal.</li> </ol> <p>To balance these, we often use an <strong>$\epsilon$-greedy policy</strong> (epsilon-greedy).</p> <ul> <li>With a probability of $\epsilon$ (epsilon), the agent chooses a random action (explores).</li> <li>With a probability of $1 - \epsilon$, the agent chooses the action with the highest Q-value for its current state (exploits).</li> </ul> <p>Typically, $\epsilon$ starts high (agent explores a lot) and gradually decreases over time (agent exploits more as it learns).</p> <h3 id="the-magic-formula-the-q-learning-update-rule">The Magic Formula: The Q-Learning Update Rule</h3> <p>This is where the mathematical beauty of Q-Learning comes in. Every time our agent takes an action, observes a reward, and lands in a new state, it updates the Q-value for the (state, action) pair it just experienced.</p> <p>The update rule is based on the <strong>Bellman equation</strong>, a cornerstone of dynamic programming and optimal control. Intuitively, it says that the value of being in a certain state and taking a certain action should be the immediate reward received, plus the discounted maximum future reward from the next state.</p> <p>Here’s the Q-Learning update rule:</p> <p>$Q(s, a) \leftarrow Q(s, a) + \alpha [R + \gamma \max_{a’} Q(s’, a’) - Q(s, a)]$</p> <p>Let’s break down each component:</p> <ul> <li>$Q(s, a)$: This is the current Q-value for taking action $a$ in state $s$. It’s what we want to update.</li> <li>$\alpha$ (alpha): The <strong>learning rate</strong>. This value (between 0 and 1) determines how much new information overrides old information. <ul> <li>If $\alpha = 0$, the agent learns nothing.</li> <li>If $\alpha = 1$, the agent only considers the newest information.</li> <li>A common value is 0.1, meaning 10% of the update comes from the new experience.</li> </ul> </li> <li>$R$: The <strong>immediate reward</strong> received after taking action $a$ in state $s$.</li> <li>$\gamma$ (gamma): The <strong>discount factor</strong>. This value (between 0 and 1) determines the importance of future rewards. <ul> <li>If $\gamma = 0$, the agent is “myopic” and only considers immediate rewards.</li> <li>If $\gamma = 1$, the agent values future rewards just as much as immediate ones.</li> <li>A common value is 0.9, meaning future rewards are 90% as valuable as immediate ones. This helps the agent prioritize achieving goals faster.</li> </ul> </li> <li>$\max_{a’} Q(s’, a’)$: This is the <strong>maximum possible Q-value for the next state ($s’$) across all possible actions ($a’$)</strong>. This is the core of “acting optimally thereafter.” It represents the best future reward the agent <em>could</em> get from the new state $s’$.</li> <li>$[R + \gamma \max_{a’} Q(s’, a’) - Q(s, a)]$: This entire term is the <strong>temporal difference (TD) error</strong>. It’s the difference between the <em>newly estimated value</em> (the value of taking action $a$ in state $s$, considering the immediate reward and the best possible future) and the <em>current Q-value</em> for that state-action pair.</li> </ul> <p>So, in plain English, the update rule says: “Adjust your current understanding of how good it is to take action $a$ in state $s$ by a small amount (determined by $\alpha$) in the direction of what you <em>just experienced</em> (immediate reward $R$) plus the <em>best possible future reward</em> you can expect from where you landed ($\gamma \max_{a’} Q(s’, a’)$).”</p> <h3 id="hyperparameters-the-q-learning-control-panel">Hyperparameters: The Q-Learning Control Panel</h3> <p>We’ve already met them, but let’s give our key hyperparameters their proper introduction:</p> <ul> <li> <strong>Learning Rate ($\alpha$)</strong>: Controls how quickly the agent adapts to new information. Too high, and it might forget old valuable lessons; too low, and it learns painfully slowly.</li> <li> <strong>Discount Factor ($\gamma$)</strong>: Shapes the agent’s time horizon. Higher values make the agent think long-term; lower values make it focus on immediate gratification.</li> <li> <strong>Exploration Rate ($\epsilon$)</strong>: Manages the balance between exploring new possibilities and exploiting known good actions. It’s often annealed (reduced) over time, starting high and gradually decreasing, to allow initial broad exploration and later refined optimization.</li> </ul> <p>Tuning these values is a crucial part of making Q-Learning work effectively for a specific problem.</p> <h3 id="a-simple-example-the-hungry-robot">A Simple Example: The Hungry Robot</h3> <p>Imagine a tiny robot in a 5x5 grid. Its goal: find the cheese! Some squares have walls (bad!), some are empty, and one has cheese (good!).</p> <ol> <li> <strong>States</strong>: Each square in the grid is a state (25 states).</li> <li> <strong>Actions</strong>: Move Up, Down, Left, Right (4 actions).</li> <li> <strong>Rewards</strong>: <ul> <li>Finding cheese: +100</li> <li>Hitting a wall: -10</li> <li>Moving to an empty square: -1 (small cost to encourage efficiency)</li> </ul> </li> </ol> <p><strong>The Learning Process:</strong></p> <ul> <li> <strong>Initialize</strong>: The robot creates a 25x4 Q-table, all values are 0.</li> <li> <strong>Episode 1</strong>: The robot starts at a random square. Since all Q-values are 0, it picks a random action (due to high $\epsilon$). It moves, gets a reward, and updates the Q-value for the (previous state, action) using our formula. It keeps moving randomly, sometimes hitting walls, sometimes moving to empty squares, until it hits the cheese (or a max number of steps). It makes mistakes, but each mistake, each success, updates its Q-table.</li> <li> <strong>Episode 2… 1000… 10000</strong>: Over many episodes, the robot repeatedly explores and exploits. The Q-values for paths leading to the cheese start to grow. Negative rewards for hitting walls make those actions less appealing. As $\epsilon$ decreases, the robot starts to follow the paths with higher Q-values more consistently.</li> <li> <strong>Converged Q-Table</strong>: Eventually, the Q-table stabilizes. For any state, the robot can now look up the Q-values and instantly know the “best” action (the one with the highest Q-value) to take to reach the cheese optimally. It has effectively built a map of optimal decisions!</li> </ul> <h3 id="the-power-and-limitations-of-q-learning">The Power and Limitations of Q-Learning</h3> <p><strong>Strengths:</strong></p> <ul> <li> <strong>Model-Free</strong>: It doesn’t need a mathematical model of the environment. Just interacting is enough. This makes it very flexible.</li> <li> <strong>Simplicity</strong>: Conceptually, it’s quite straightforward, making it a great entry point into RL.</li> <li> <strong>Guaranteed Convergence</strong>: Under certain conditions (finite states/actions, sufficient exploration), Q-Learning is guaranteed to find the optimal policy.</li> </ul> <p><strong>Limitations:</strong></p> <ul> <li> <strong>Curse of Dimensionality</strong>: The Q-table grows linearly with the number of states and actions. For complex environments with millions or infinite states (like a self-driving car seeing continuous road conditions, or a game with high-resolution pixel inputs), storing a Q-table becomes impossible. This is where <strong>Deep Q-Networks (DQNs)</strong> come in, replacing the table with a neural network to <em>approximate</em> Q-values.</li> <li> <strong>Slow Convergence</strong>: For large environments, even with a feasible Q-table, it can take a very long time to explore sufficiently and converge to optimal Q-values.</li> </ul> <h3 id="where-q-learning-shines">Where Q-Learning Shines</h3> <p>Despite its limitations, Q-Learning is foundational and incredibly useful:</p> <ul> <li> <strong>Game AI</strong>: From simple maze games to more complex strategic games, it can teach agents to play optimally.</li> <li> <strong>Robotics</strong>: Simple navigation tasks, learning optimal gripping strategies.</li> <li> <strong>Resource Management</strong>: Optimizing energy consumption, traffic flow.</li> <li> <strong>Personalized Recommendations</strong>: Helping systems learn user preferences.</li> </ul> <h3 id="conclusion-your-ai-journey-starts-here">Conclusion: Your AI Journey Starts Here!</h3> <p>Q-Learning is more than just an algorithm; it’s a testament to how complex intelligence can emerge from simple rules and persistent trial and error. It shows us that learning from experience, making mistakes, and constantly refining our understanding of the world is a powerful strategy, whether you’re a human, a robot, or a line of code.</p> <p>So, the next time you see an AI agent making smart choices, remember the humble Q-table and the journey of exploration and exploitation that led to its intelligence. This is just one step into the fascinating world of Reinforcement Learning, and I hope it sparks your curiosity to explore even deeper!</p> <p>Happy learning!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>