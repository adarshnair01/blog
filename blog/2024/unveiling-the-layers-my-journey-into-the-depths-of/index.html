<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Unveiling the Layers: My Journey into the Depths of Deep Learning | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/unveiling-the-layers-my-journey-into-the-depths-of/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Unveiling the Layers: My Journey into the Depths of Deep Learning</h1> <p class="post-meta"> Created on November 16, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/neural-networks"> <i class="fa-solid fa-hashtag fa-sm"></i> Neural Networks</a>   <a href="/blog/blog/tag/artificial-intelligence"> <i class="fa-solid fa-hashtag fa-sm"></i> Artificial Intelligence</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>As a kid, I was always fascinated by the idea of machines that could think. I mean, who wasn’t captivated by HAL 9000 or R2-D2? The notion of artificial intelligence felt like pure science fiction, a distant dream confined to the silver screen. Fast forward to today, and that dream is rapidly becoming a reality, largely thanks to a groundbreaking field called <strong>Deep Learning</strong>.</p> <p>I remember my first encounter with the term “deep learning” – it sounded mysterious, almost like a secret society of algorithms. But as I delved deeper (pun intended!), I discovered a world of incredible ingenuity, drawing inspiration from the very organ that makes us human: the brain. In this post, I want to take you on my journey through understanding deep learning, breaking down its core concepts in an accessible yet comprehensive way.</p> <h3 id="what-exactly-is-deep-learning">What Exactly <em>Is</em> Deep Learning?</h3> <p>At its heart, Deep Learning is a specialized subfield of <strong>Machine Learning</strong>, which itself is a branch of <strong>Artificial Intelligence</strong>. Think of it like a set of Russian nesting dolls: AI is the largest, Machine Learning fits inside, and Deep Learning is nestled within that.</p> <p>What makes Deep Learning “deep”? It’s all about the architecture. Unlike traditional machine learning algorithms that often rely on a human expert to <em>hand-engineer</em> features (e.g., “tell the computer to look for edges in an image”), deep learning models can <em>automatically learn</em> these intricate features from raw data. They do this by stacking many layers of artificial “neurons” – hence the “deep” part.</p> <p>Imagine you’re trying to teach a child to recognize a cat. You don’t give them a list of rules like “if it has pointy ears AND whiskers AND meows, then it’s a cat.” Instead, you show them many pictures of cats, and over time, they learn to identify the underlying patterns that define “cat-ness.” Deep learning works in a remarkably similar fashion, but on a colossal scale.</p> <h3 id="the-neuron-the-brains-basic-building-block-reimagined">The Neuron: The Brain’s Basic Building Block, Reimagined</h3> <p>The fundamental unit of a deep learning model is the <strong>artificial neuron</strong>, often called a <strong>perceptron</strong>. This concept, first proposed in the 1950s, is a simplified mathematical model inspired by biological neurons.</p> <p>So, how does it work? A biological neuron receives electrical signals through its dendrites, processes them in the cell body, and then, if the signal is strong enough, fires an output signal through its axon.</p> <p>An artificial neuron mirrors this:</p> <ol> <li> <strong>Inputs ($x_i$)</strong>: It receives multiple input signals.</li> <li> <strong>Weights ($w_i$)</strong>: Each input is multiplied by a “weight,” which represents the importance or strength of that input.</li> <li> <strong>Summation</strong>: All these weighted inputs are summed up.</li> <li> <strong>Bias ($b$)</strong>: A ‘bias’ term is added to this sum. Think of it as an adjustable threshold – it allows the neuron to activate even if all inputs are zero, or conversely, makes it harder to activate.</li> <li> <strong>Activation Function ($f$)</strong>: Finally, this sum passes through an “activation function” which decides whether the neuron should “fire” or not.</li> </ol> <p>Mathematically, the output ($y$) of a single neuron can be expressed as:</p> <p>$y = f(\sum_{i=1}^n w_i x_i + b)$</p> <p>Where:</p> <ul> <li>$x_i$ are the inputs.</li> <li>$w_i$ are their corresponding weights.</li> <li>$b$ is the bias.</li> <li>$f$ is the activation function.</li> </ul> <p>Why an activation function? Imagine if neurons just outputted a simple sum. Stacking them would just create one big, complex linear equation. Activation functions (like <strong>ReLU</strong> for Rectified Linear Unit, or <strong>Sigmoid</strong>) introduce non-linearity, allowing the network to learn much more complex and non-linear relationships in the data. Without them, deep learning wouldn’t be able to solve the intricate problems it tackles today.</p> <h3 id="from-single-neurons-to-networks-the-architecture-unveiled">From Single Neurons to Networks: The Architecture Unveiled</h3> <p>One neuron isn’t very smart. But connect thousands, or even millions, of them in layers, and you get something incredibly powerful: a <strong>Neural Network</strong>.</p> <p>A typical deep neural network consists of:</p> <ol> <li> <strong>Input Layer</strong>: This is where your raw data (e.g., pixel values of an image, words in a sentence) enters the network.</li> <li> <strong>Hidden Layers</strong>: These are the “deep” part. Data from the input layer passes through one or more hidden layers. Each neuron in a hidden layer takes inputs from all neurons in the previous layer, processes them, and passes its output to the next layer. These layers are where the magic happens – where the network learns to extract increasingly complex features from the data.</li> <li> <strong>Output Layer</strong>: This layer provides the final prediction or classification (e.g., “cat” or “dog,” a numerical value for house price).</li> </ol> <p>The “depth” allows the network to learn hierarchical representations. For instance, in an image recognition task:</p> <ul> <li>The first hidden layer might learn to detect simple edges or corners.</li> <li>The next layer might combine these edges to recognize shapes like circles or squares.</li> <li>Subsequent layers might combine these shapes to identify parts of an object, like an eye or a wheel.</li> <li>Finally, the last hidden layer could assemble these parts to recognize a complete object, like a face or a car.</li> </ul> <p>This process, where data flows from the input layer through the hidden layers to the output layer, is called <strong>forward propagation</strong>. It’s how the network makes a prediction.</p> <h3 id="learning-is-iterative-how-networks-get-smart">Learning is Iterative: How Networks Get Smart</h3> <p>Making a prediction is one thing; making an <em>accurate</em> prediction is another. How does a neural network learn to adjust its weights and biases to become more accurate? This is the core of the training process, and it’s where the iterative nature of deep learning truly shines.</p> <p>It involves three key components:</p> <ol> <li> <p><strong>The Loss Function (Cost Function)</strong>: After the network makes a prediction, we need a way to measure how “wrong” it was. This is the job of the <strong>loss function</strong>. It quantifies the difference between the network’s predicted output ($\hat{y}$) and the actual correct output ($y$).</p> <p>For example, in a regression task (predicting a number), a common loss function is the <strong>Mean Squared Error (MSE)</strong>:</p> <p>$MSE = \frac{1}{N}\sum_{i=1}^N (y_i - \hat{y}_i)^2$</p> <p>Here, $N$ is the number of data points, $y_i$ is the actual value, and $\hat{y}<em>i$ is the network’s prediction. The goal during training is always to _minimize</em> this loss.</p> </li> <li> <p><strong>Optimization: Gradient Descent</strong>: Minimizing the loss function is like trying to find the lowest point in a complex, multi-dimensional valley. We want to adjust the weights and biases to reach that minimum. This is where <strong>gradient descent</strong> comes in.</p> <p>Imagine you’re blindfolded on a hillside and want to reach the bottom. What do you do? You feel the slope around you and take a small step downhill in the steepest direction. You repeat this process until you can’t go any further down.</p> <p>In mathematical terms, the “slope” is the <strong>gradient</strong> – the partial derivative of the loss function with respect to each weight and bias. The gradient tells us the direction of the steepest <em>ascent</em>. To minimize loss, we move in the <em>opposite</em> direction.</p> <p>The update rule for a weight ($w$) looks like this:</p> <p>$w_{new} = w_{old} - \alpha \frac{\partial L}{\partial w}$</p> <p>Here, $\frac{\partial L}{\partial w}$ is the gradient of the loss function ($L$) with respect to the weight ($w$). The $\alpha$ (alpha) is the <strong>learning rate</strong>, a crucial hyperparameter that controls the size of our “steps” down the hill. A small learning rate makes slow, cautious steps, while a large one takes big, potentially overshooting jumps.</p> </li> <li> <p><strong>Backpropagation: The Magic Behind the Learning</strong>: Calculating these gradients for millions of weights in a deep network might seem like an impossible task. This is where <strong>backpropagation</strong> (short for “backward propagation of errors”) shines. Developed in the 1970s and popularized in the 1980s, it’s an ingenious algorithm that efficiently computes the gradients for all weights and biases in the network.</p> <p>Think of it as the network’s feedback mechanism. After forward propagation makes a prediction and the loss is calculated, backpropagation essentially sends the “error signal” backward through the network, layer by layer. It determines how much each weight and bias contributed to the final error and how much they need to be adjusted. It’s like a coach telling each player exactly how they need to change their technique based on the team’s overall performance.</p> </li> </ol> <p>This cycle – forward propagation, calculate loss, backpropagation, update weights – is repeated thousands or millions of times over many “epochs” (passes over the entire dataset) until the network’s predictions are acceptably accurate.</p> <h3 id="why-now-the-pillars-of-deep-learnings-resurgence">Why Now? The Pillars of Deep Learning’s Resurgence</h3> <p>While the core ideas behind neural networks have been around for decades, deep learning’s explosion in recent years is due to three critical factors:</p> <ol> <li> <strong>Vast Amounts of Data</strong>: Deep learning models are data-hungry. The internet and digitalization have led to an unprecedented availability of data – images, text, audio, video – which is essential for training these complex models.</li> <li> <strong>Computational Power</strong>: Training deep networks requires immense computational resources. The advent of powerful <strong>GPUs (Graphics Processing Units)</strong>, originally designed for rendering graphics in video games, turned out to be perfectly suited for the parallel computations needed for neural networks.</li> <li> <strong>Algorithmic Advancements</strong>: Continuous research has led to more efficient network architectures (like Convolutional Neural Networks, Recurrent Neural Networks, and Transformers), better activation functions, and more sophisticated optimization techniques, making training faster and more stable.</li> </ol> <h3 id="beyond-the-basics-a-glimpse-into-specialized-architectures">Beyond the Basics: A Glimpse into Specialized Architectures</h3> <p>The foundational concepts we’ve discussed apply broadly, but the deep learning landscape is rich with specialized network architectures designed for different types of data and tasks:</p> <ul> <li> <strong>Convolutional Neural Networks (CNNs)</strong>: Revolutionized image recognition. They excel at automatically detecting patterns and hierarchies in visual data using “convolutional filters.”</li> <li> <strong>Recurrent Neural Networks (RNNs)</strong>: Designed to handle sequential data like text, speech, or time series. They have internal “memory” that allows them to process information based on previous inputs in a sequence.</li> <li> <strong>Transformers</strong>: The current state-of-the-art for natural language processing (NLP). They leverage an “attention mechanism” to weigh the importance of different parts of the input sequence, enabling powerful language translation, summarization, and generation.</li> </ul> <h3 id="challenges-and-the-road-ahead">Challenges and The Road Ahead</h3> <p>Despite its incredible power, deep learning isn’t without its challenges. Models can be <em>black boxes</em>, making it hard to understand <em>why</em> they make certain decisions, which is crucial in sensitive applications. They also require massive datasets and significant computational resources, raising concerns about accessibility and environmental impact. Ethical considerations surrounding bias in data and model decisions are also paramount.</p> <p>However, the field is evolving at a breathtaking pace. Researchers are working on making models more interpretable, efficient, and robust. We’re seeing deep learning integrate with other AI approaches, pushing the boundaries of what machines can achieve.</p> <h3 id="my-deep-learning-journey-continues">My Deep Learning Journey Continues…</h3> <p>My journey into deep learning has been nothing short of fascinating. It’s a field that constantly challenges your understanding and rewards curiosity. From simple perceptrons to complex transformers, the underlying principles are elegant, powerful, and deeply inspiring.</p> <p>Deep learning isn’t just about building smart algorithms; it’s about pushing the boundaries of what’s possible, automating complex tasks, and creating tools that can augment human intelligence in ways we’re only just beginning to imagine. If you’ve ever felt that spark of curiosity about how AI truly works, I hope this dive into deep learning has illuminated some of its magic and perhaps even inspired you to start your own exploration into this incredible field. The future, undoubtedly, will be deep.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>