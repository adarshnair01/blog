<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Descent into Learning: How Machines Find Their Way Down the Mountain of Error | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/the-descent-into-learning-how-machines-find-their/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Descent into Learning: How Machines Find Their Way Down the Mountain of Error</h1> <p class="post-meta"> Created on December 18, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/gradient-descent"> <i class="fa-solid fa-hashtag fa-sm"></i> Gradient Descent</a>   <a href="/blog/blog/tag/optimization"> <i class="fa-solid fa-hashtag fa-sm"></i> Optimization</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/algorithms"> <i class="fa-solid fa-hashtag fa-sm"></i> Algorithms</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>My journey into data science began much like many others: with a mix of excitement, curiosity, and a healthy dose of “wait, how does <em>that</em> work?!” One of the first profound ‘aha!’ moments I had wasn’t about a fancy neural network or a complex statistical model, but about something far more fundamental: <strong>Gradient Descent</strong>. It’s the engine behind so much of what we do in machine learning, and understanding it felt like unlocking a secret door to how machines truly learn.</p> <p>Imagine this with me: you’re standing on a vast, undulating landscape. It’s foggy, maybe even pitch black, so you can’t see anything beyond your feet. Your goal? To find the absolute lowest point in this landscape. How would you do it? You’d probably feel the slope beneath your feet and take a small step in the direction that feels steepest downwards, right? Then you’d repeat, step by small step, until you feel flat ground, signaling you’ve reached a valley floor.</p> <p>Congratulations! You’ve just intuitively performed Gradient Descent.</p> <h3 id="the-mountain-of-error-what-are-we-trying-to-minimize">The Mountain of Error: What Are We Trying to Minimize?</h3> <p>In machine learning, our “landscape” isn’t made of dirt and rocks; it’s a <strong>Cost Function</strong> (or Loss Function). This function measures how ‘wrong’ our model’s predictions are. Our goal is to find the set of model parameters (the numbers that define our model) that minimize this cost function – making our model as ‘right’ as possible.</p> <p>Let’s ground this with a simple example: <strong>Linear Regression</strong>. Suppose we want to predict a house price ($y$) based on its size ($x$). A simple linear model would look like this:</p> <p>$ \hat{y} = mx + b $</p> <p>Here, $\hat{y}$ is our predicted price, $x$ is the house size, $m$ is the slope (how much price increases per unit size), and $b$ is the y-intercept (the base price). Our model’s “parameters” are $m$ and $b$. Our goal is to find the <em>best</em> $m$ and $b$ that fit our data.</p> <p>To quantify “best,” we use a cost function. A common one for linear regression is the <strong>Mean Squared Error (MSE)</strong>:</p> <p>$ J(m, b) = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}<em>i)^2 = \frac{1}{N} \sum</em>{i=1}^{N} (y_i - (mx_i + b))^2 $</p> <p>Here, $N$ is the number of data points, $y_i$ is the actual price, and $\hat{y}_i$ is our model’s predicted price for the $i$-th house. We square the difference to ensure positive values and penalize larger errors more heavily. The division by $N$ gives us an average error.</p> <p>This $J(m, b)$ is our landscape. For linear regression, it often looks like a bowl or a valley (a convex function), meaning there’s only one lowest point – a global minimum. Finding that minimum is our quest!</p> <h3 id="the-gradient-feeling-the-slope">The “Gradient”: Feeling the Slope</h3> <p>Back to our blindfolded mountain climb. How do you know which way is steepest <em>down</em>? You feel the slope. In mathematics, the “slope” of a multi-variable function at a particular point is given by its <strong>gradient</strong>.</p> <p>The gradient is a vector of partial derivatives. A partial derivative tells us how much the cost function changes if we tweak just one parameter slightly, holding all others constant.</p> <p>For our MSE cost function $J(m, b)$, we need to calculate the partial derivatives with respect to $m$ and $b$:</p> <ol> <li> <p><strong>Partial Derivative with respect to $m$</strong>: $ \frac{\partial}{\partial m} J(m, b) = \frac{\partial}{\partial m} \left( \frac{1}{N} \sum_{i=1}^{N} (y_i - (mx_i + b))^2 \right) $ Using the chain rule, this becomes: $ \frac{\partial}{\partial m} J(m, b) = \frac{1}{N} \sum_{i=1}^{N} 2 (y_i - (mx_i + b)) (-x_i) $ $ \frac{\partial}{\partial m} J(m, b) = - \frac{2}{N} \sum_{i=1}^{N} x_i (y_i - \hat{y}_i) $</p> </li> <li> <p><strong>Partial Derivative with respect to $b$</strong>: $ \frac{\partial}{\partial b} J(m, b) = \frac{\partial}{\partial b} \left( \frac{1}{N} \sum_{i=1}^{N} (y_i - (mx_i + b))^2 \right) $ Again, using the chain rule: $ \frac{\partial}{\partial b} J(m, b) = \frac{1}{N} \sum_{i=1}^{N} 2 (y_i - (mx_i + b)) (-1) $ $ \frac{\partial}{\partial b} J(m, b) = - \frac{2}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i) $</p> </li> </ol> <p>These two partial derivatives together form the gradient $\nabla J(m, b)$. Critically, the gradient vector points in the direction of <em>steepest ascent</em> (uphill). Since we want to go <em>downhill</em> to minimize our error, we move in the <em>opposite</em> direction of the gradient.</p> <h3 id="the-descent-taking-a-step">The “Descent”: Taking a Step</h3> <p>Now that we know which way is down, how far do we step? This is where the <strong>learning rate</strong>, denoted by $\alpha$ (alpha), comes in. The learning rate is a crucial hyperparameter that determines the size of the steps we take down the cost function landscape.</p> <p>The core update rule for Gradient Descent is:</p> <p>$ \text{new_parameter} = \text{old_parameter} - \alpha \times \text{gradient_of_cost_w.r.t._parameter} $</p> <p>Applying this to our $m$ and $b$ parameters:</p> <p>$ m := m - \alpha \frac{\partial}{\partial m} J(m, b) $ $ b := b - \alpha \frac{\partial}{\partial b} J(m, b) $</p> <p>We repeat these steps iteratively:</p> <ol> <li>Start with some initial random values for $m$ and $b$.</li> <li>Calculate the gradient (partial derivatives) of the cost function at the current $(m, b)$ values using <em>all</em> our training data.</li> <li>Update $m$ and $b$ using the learning rate $\alpha$ and the calculated gradients.</li> <li>Repeat steps 2 and 3 for a fixed number of iterations (epochs) or until the changes in $m$ and $b$ become very small, indicating convergence.</li> </ol> <h4 id="the-goldilocks-zone-of-learning-rate-alpha">The Goldilocks Zone of Learning Rate ($\alpha$)</h4> <p>The learning rate is vital!</p> <ul> <li>If $\alpha$ is too small, we’ll take tiny steps and it will take an extremely long time to reach the minimum. Our model might “learn” agonizingly slowly.</li> <li>If $\alpha$ is too large, we might overshoot the minimum, bounce around erratically, or even diverge entirely, causing our error to explode.</li> <li>The goal is to find a “just right” $\alpha$ that allows for efficient, stable convergence. This often involves some experimentation or more advanced techniques like learning rate schedules.</li> </ul> <h3 id="variants-of-gradient-descent-different-ways-to-climb">Variants of Gradient Descent: Different Ways to Climb</h3> <p>The approach we’ve discussed so far, where we calculate the gradient using <em>all</em> our training examples at <em>each</em> step, is called <strong>Batch Gradient Descent</strong>. While it provides a precise estimate of the gradient and ensures stable convergence (assuming a proper learning rate), it can be computationally very expensive and slow for large datasets. Imagine having millions of house prices – calculating those sums for every single step would take ages!</p> <p>To address this, other variants emerged:</p> <ol> <li> <strong>Stochastic Gradient Descent (SGD)</strong>: Instead of using all data, SGD picks <em>one</em> random training example at a time to calculate the gradient and update the parameters. <ul> <li> <strong>Pros:</strong> Extremely fast updates. Can escape shallow local minima due to its noisy updates (like randomly bumping you off a small hill).</li> <li> <strong>Cons:</strong> The path to the minimum is much noisier and less stable. It might never perfectly converge to the exact minimum but rather oscillate around it.</li> </ul> <p>The update rule for SGD with a single example $(x_i, y_i)$: $ m := m - \alpha \frac{\partial}{\partial m} J(m, b) \text{ calculated using only } (x_i, y_i) $ $ b := b - \alpha \frac{\partial}{\partial b} J(m, b) \text{ calculated using only } (x_i, y_i) $</p> <p>Where $\frac{\partial}{\partial m} J(m, b)$ for a single example $(x_i, y_i)$ would be $-2x_i(y_i - (mx_i+b))$ and for $b$ would be $-2(y_i - (mx_i+b))$.</p> </li> <li> <strong>Mini-Batch Gradient Descent</strong>: This is arguably the most common and practical variant. It’s a hybrid approach where we use a small, randomly selected subset (a “mini-batch”) of training examples to calculate the gradient and update parameters. A typical mini-batch size might be 32, 64, 128, or 256. <ul> <li> <strong>Pros:</strong> Balances the computational efficiency of SGD with the stability of Batch Gradient Descent. It’s fast enough and provides a reasonable estimate of the gradient. Modern deep learning frameworks are highly optimized for mini-batch operations.</li> <li> <strong>Cons:</strong> Still has some noise compared to Batch GD, and the mini-batch size is another hyperparameter to tune.</li> </ul> </li> </ol> <h3 id="challenges-and-what-comes-next">Challenges and What Comes Next</h3> <p>While conceptually simple, Gradient Descent has its nuances:</p> <ul> <li> <strong>Local Minima:</strong> For complex cost functions (especially in deep neural networks), the landscape might have multiple valleys. Gradient Descent might get stuck in a “local minimum” (a valley that’s not the absolute lowest point) rather than finding the “global minimum.” SGD and mini-batch GD’s noisiness can sometimes help escape these.</li> <li> <strong>Saddle Points:</strong> These are points where the slope is zero in some directions but not a minimum (like a saddle on a horse). Gradient Descent can slow down significantly or get stuck here.</li> <li> <strong>Feature Scaling:</strong> If features have vastly different scales (e.g., house size in square feet vs. number of bedrooms), the cost function landscape can become very elongated and narrow, making it difficult for Gradient Descent to find its way efficiently. Scaling features to a similar range (e.g., 0-1 or mean 0, variance 1) can make the optimization process much faster.</li> </ul> <p>These challenges led to the development of more advanced “optimizers” built upon Gradient Descent, such as Momentum, AdaGrad, RMSprop, and the ever-popular Adam. These optimizers introduce clever mechanisms to adapt the learning rate during training, remember past gradients, or accelerate learning in relevant directions, further improving the efficiency and robustness of the descent.</p> <h3 id="why-is-gradient-descent-so-important">Why is Gradient Descent So Important?</h3> <p>Gradient Descent is the beating heart of much of modern machine learning, especially deep learning. Every time a neural network learns to recognize a cat in an image, translate a language, or generate text, it’s doing so by iteratively adjusting its millions of parameters using some form of Gradient Descent.</p> <p>Its elegance lies in its simplicity:</p> <ol> <li>Define a way to measure error (cost function).</li> <li>Calculate the direction of steepest error increase (gradient).</li> <li>Take a step in the opposite direction (descent) proportional to a learning rate.</li> <li>Repeat.</li> </ol> <p>This iterative process, much like our blindfolded mountain climber, allows complex models to navigate incredibly high-dimensional landscapes of parameters and find effective solutions.</p> <p>So, the next time you hear about a machine learning model achieving something incredible, take a moment to appreciate the humble yet mighty Gradient Descent working tirelessly behind the scenes, guiding the model down the mountain of error, one calculated step at a time. It’s a testament to how simple, iterative processes can lead to profound learning.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>