<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Secret Sauce of Learning: Unraveling Backpropagation's Magic | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/the-secret-sauce-of-learning-unraveling-backpropag/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Secret Sauce of Learning: Unraveling Backpropagation's Magic</h1> <p class="post-meta"> Created on October 13, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/neural-networks"> <i class="fa-solid fa-hashtag fa-sm"></i> Neural Networks</a>   <a href="/blog/blog/tag/backpropagation"> <i class="fa-solid fa-hashtag fa-sm"></i> Backpropagation</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/gradient-descent"> <i class="fa-solid fa-hashtag fa-sm"></i> Gradient Descent</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hello fellow data enthusiasts and curious minds!</p> <p>Today, I want to pull back the curtain on one of the most fundamental algorithms in the world of Artificial Intelligence: <strong>Backpropagation</strong>. If you’ve ever marveled at how a neural network can learn to identify objects in an image, understand your voice commands, or even generate human-like text, then you’ve witnessed the power of backpropagation in action. It’s the engine that drives deep learning, allowing these complex systems to refine their internal workings and continuously improve.</p> <p>When I first encountered the term, it sounded intimidating – “back… propagation??” – like some arcane ritual. But as I dove deeper, I realized it’s an elegant, almost beautiful, application of basic calculus principles. It’s not magic, but it certainly feels like it once you grasp its genius. Think of this post as a journey into the heart of how machines learn, laid out in a way that I hope is as clear and engaging as possible.</p> <h3 id="the-problem-how-do-machines-learn">The Problem: How Do Machines Learn?</h3> <p>Imagine you’re teaching a child to recognize a cat. You show them a picture and say, “That’s a cat.” They might point to a dog and say “cat,” and you’d correct them, “No, that’s a dog.” Over time, with enough examples and corrections, they learn to differentiate.</p> <p>Neural networks learn in a remarkably similar fashion. They start with a task, make a guess, evaluate how wrong their guess was, and then adjust their internal parameters (like a child updating their mental model) to make better guesses next time.</p> <p>But how does a computer “know” <em>how</em> to adjust? How does it figure out which internal “knobs and dials” (which we call <em>weights</em> and <em>biases</em>) contributed most to its mistake, and by how much? This is precisely the problem Backpropagation solves.</p> <h3 id="a-quick-peek-the-forward-pass">A Quick Peek: The Forward Pass</h3> <p>Before we go <em>backward</em>, let’s briefly understand the <em>forward</em> journey. A neural network is essentially a series of interconnected nodes (neurons) organized into layers.</p> <ol> <li> <strong>Input Layer:</strong> Your data (e.g., pixel values of an image, words in a sentence) enters here.</li> <li> <strong>Hidden Layers:</strong> Each neuron in a layer takes inputs from the previous layer, multiplies them by a <em>weight</em>, adds a <em>bias</em>, and then applies an <em>activation function</em>. This creates an output that feeds into the next layer. Mathematically, for a single neuron, this process looks something like this: \(z = \sum\_{i} (w_i x_i) + b\) \(a = \sigma(z)\) Where $x_i$ are the inputs, $w_i$ are the weights, $b$ is the bias, $\sigma$ is the activation function (like ReLU or Sigmoid), $z$ is the weighted sum, and $a$ is the activated output.</li> <li> <strong>Output Layer:</strong> The final layer produces the network’s prediction (e.g., “this is a cat,” “the stock price will go up”).</li> </ol> <p>This entire process, from input to output, is called the <strong>forward pass</strong>. At this point, the network has made its best guess with its current set of weights and biases.</p> <h3 id="quantifying-how-wrong-the-loss-function">Quantifying “How Wrong”: The Loss Function</h3> <p>Our network has made a prediction ($\hat{y}$), but we also know the <em>actual</em> correct answer ($y$). The next logical step is to measure the discrepancy between the two. This is where the <strong>loss function</strong> (or cost function) comes in. It’s a mathematical way to quantify “how wrong” our network’s prediction was.</p> <p>A common loss function for regression tasks is the Mean Squared Error (MSE): \(L = \frac{1}{N} \sum\_{i=1}^{N} (y_i - \hat{y}\_i)^2\) For classification, we often use Cross-Entropy Loss. The goal of training a neural network is to <strong>minimize this loss</strong>. We want to tweak our weights and biases so that the loss value gets as close to zero as possible.</p> <h3 id="the-path-to-improvement-gradient-descent">The Path to Improvement: Gradient Descent</h3> <p>Minimizing the loss function can be visualized as finding the bottom of a valley. Imagine you’re blindfolded on a mountain, and you want to reach the lowest point. How would you do it? You’d feel the slope around you and take a small step in the steepest <em>downhill</em> direction.</p> <p>In calculus, the “steepest downhill direction” is given by the <strong>negative gradient</strong>. The gradient is a vector of partial derivatives, indicating the rate of change of the loss function with respect to each weight and bias.</p> <p><strong>Gradient Descent</strong> is the optimization algorithm that uses these gradients. It tells us: \(w*{new} = w*{old} - \alpha \frac{\partial L}{\partial w}\) \(b*{new} = b*{old} - \alpha \frac{\partial L}{\partial b}\) Here, $\alpha$ is the <strong>learning rate</strong>, a small positive number that controls the size of our steps. A larger $\alpha$ means bigger steps, potentially reaching the minimum faster but risking overshooting it. A smaller $\alpha$ means slower, more cautious steps.</p> <p>The crucial question remains: How do we calculate $\frac{\partial L}{\partial w}$ and $\frac{\partial L}{\partial b}$ for <em>every single weight and bias</em> in a complex network with potentially millions of parameters? Doing this naively for each parameter would be computationally impossible.</p> <p><strong>Enter Backpropagation.</strong></p> <h3 id="the-core-idea-of-backpropagation-the-chain-rule-in-action">The Core Idea of Backpropagation: The Chain Rule in Action</h3> <p>Backpropagation is an ingenious and efficient way to calculate these gradients. It leverages the <strong>Chain Rule of calculus</strong>.</p> <p>Think of the network as a long chain of interconnected mathematical operations. The loss at the very end depends on the output of the last layer, which depends on the weights and biases of that layer, which in turn depend on the output of the previous layer, and so on, all the way back to the input.</p> <p>The Chain Rule states that if a variable $C$ depends on $B$, and $B$ depends on $A$, then the rate of change of $C$ with respect to $A$ is: \(\frac{\partial C}{\partial A} = \frac{\partial C}{\partial B} \cdot \frac{\partial B}{\partial A}\) Backpropagation applies this principle by calculating the gradients of the loss function with respect to the weights and biases starting from the <em>output layer</em> and moving <em>backward</em> through the network, layer by layer. It effectively “propagates” the error signal backward, determining how much each weight and bias contributed to the final error.</p> <p><strong>Analogy:</strong> Imagine a factory assembly line. If a defect is found in the final product (our “loss”), how do you figure out which specific machine or worker (our “weights” and “biases”) earlier in the line was responsible, and by how much? You’d trace the defect backward, stage by stage, attributing blame at each step based on its contribution to the overall error.</p> <h3 id="backpropagation-step-by-step-simplified">Backpropagation Step-by-Step (Simplified)</h3> <p>Let’s trace the “blame” backward. We want to find $\frac{\partial L}{\partial w}$ and $\frac{\partial L}{\partial b}$ for all layers.</p> <ol> <li> <p><strong>Start at the Output Layer:</strong> First, we calculate the gradient of the loss function with respect to the output of the <em>last</em> activated neuron, $\frac{\partial L}{\partial a^{(L)}}$. This tells us how much the loss changes if the final output activation changes. This is often straightforward, as we know the loss function.</p> </li> <li> <strong>Calculate Gradients for the Last Layer’s Weights and Biases:</strong> Now, let’s consider the weights ($w^{(L)}$) and biases ($b^{(L)}$) connecting the second-to-last layer to the output layer. <ul> <li>To find $\frac{\partial L}{\partial w^{(L)}}$, we use the chain rule: \(\frac{\partial L}{\partial w^{(L)}} = \frac{\partial L}{\partial a^{(L)}} \cdot \frac{\partial a^{(L)}}{\partial z^{(L)}} \cdot \frac{\partial z^{(L)}}{\partial w^{(L)}}\) <ul> <li>We already have $\frac{\partial L}{\partial a^{(L)}}$.</li> <li>$\frac{\partial a^{(L)}}{\partial z^{(L)}}$ is the derivative of the activation function at the output layer (e.g., derivative of sigmoid or softmax).</li> <li>$\frac{\partial z^{(L)}}{\partial w^{(L)}}$ is simply the input activation from the <em>previous</em> layer, $a^{(L-1)}$, because $z^{(L)} = w^{(L)} a^{(L-1)} + b^{(L)}$.</li> </ul> </li> <li>Similarly for the bias: \(\frac{\partial L}{\partial b^{(L)}} = \frac{\partial L}{\partial a^{(L)}} \cdot \frac{\partial a^{(L)}}{\partial z^{(L)}} \cdot \frac{\partial z^{(L)}}{\partial b^{(L)}}\) <ul> <li>Since $\frac{\partial z^{(L)}}{\partial b^{(L)}} = 1$, this simplifies to $\frac{\partial L}{\partial b^{(L)}} = \frac{\partial L}{\partial a^{(L)}} \cdot \frac{\partial a^{(L)}}{\partial z^{(L)}}$.</li> </ul> </li> </ul> <p>At this stage, we have the gradients for the weights and biases of the <em>last</em> layer.</p> </li> <li> <strong>Propagate the Error Signal to the Previous Layer:</strong> Before we can calculate the gradients for the <em>second-to-last</em> layer’s weights and biases, we need to know how much the loss is affected by the <em>activations</em> of that layer, i.e., $\frac{\partial L}{\partial a^{(L-1)}}$. This is the “error signal” that gets passed backward. \(\frac{\partial L}{\partial a^{(L-1)}} = \left( \frac{\partial L}{\partial a^{(L)}} \cdot \frac{\partial a^{(L)}}{\partial z^{(L)}} \right) \cdot \frac{\partial z^{(L)}}{\partial a^{(L-1)}}\) <ul> <li>The term in the parentheses is essentially the “error” at the output of the <em>current</em> layer before activation.</li> <li>$\frac{\partial z^{(L)}}{\partial a^{(L-1)}}$ is simply the weights $w^{(L)}$ connecting the previous layer’s activations to the current layer’s weighted sum. So, each previous layer’s activation receives a weighted sum of the error signals from the layer ahead of it.</li> </ul> </li> <li> <strong>Repeat for All Hidden Layers:</strong> Now that we have $\frac{\partial L}{\partial a^{(L-1)}}$, we treat it as our new “starting error” and repeat steps 2 and 3 for layer $L-1$, then $L-2$, and so on, until we reach the first hidden layer.</li> </ol> <p>This backward flow of calculating derivatives efficiently computes all the necessary gradients. It’s like a wave of blame propagating through the network, telling each parameter exactly how much it contributed to the final error.</p> <h3 id="the-learning-loop">The Learning Loop</h3> <p>Once Backpropagation has computed all the gradients ($\frac{\partial L}{\partial w}$ and $\frac{\partial L}{\partial b}$) for every weight and bias in the network, Gradient Descent takes over:</p> <ol> <li> <strong>Forward Pass:</strong> Input data flows through the network, generating an output prediction.</li> <li> <strong>Calculate Loss:</strong> The prediction is compared to the true value using the loss function.</li> <li> <strong>Backward Pass (Backpropagation):</strong> The error is propagated backward through the network to calculate the gradients for all weights and biases.</li> <li> <strong>Update Weights &amp; Biases (Gradient Descent):</strong> Each weight and bias is adjusted slightly in the direction that minimizes the loss, using the calculated gradients and the learning rate ($\alpha$).</li> <li> <strong>Repeat:</strong> This entire process (an “epoch”) is repeated thousands or millions of times, with different batches of data, allowing the network to continually learn and refine its parameters until the loss is minimized, and its predictions are accurate.</li> </ol> <h3 id="why-is-backpropagation-so-important">Why is Backpropagation so Important?</h3> <ul> <li> <strong>Efficiency:</strong> It’s an incredibly efficient algorithm. Instead of calculating each partial derivative independently, which would be prohibitively expensive for large networks, Backpropagation computes them all in a single backward pass. This efficiency is what made training deep neural networks feasible.</li> <li> <strong>Foundation of Deep Learning:</strong> Without backpropagation, deep learning as we know it simply wouldn’t exist. It’s the core algorithm that allows complex, multi-layered networks to learn intricate patterns from vast amounts of data.</li> <li> <strong>Generalizability:</strong> It’s a general algorithm that works for almost any differentiable activation function and network architecture.</li> </ul> <h3 id="conclusion-the-unsung-hero">Conclusion: The Unsung Hero</h3> <p>Backpropagation, at its heart, is an elegant application of the chain rule. It transforms the daunting task of optimizing millions of parameters into a manageable, iterative process. It’s the reason our AI models can learn, adapt, and perform incredible feats, from recognizing faces on your phone to powering self-driving cars.</p> <p>It might seem like a lot of moving parts when you first encounter it, but once you understand the “blame assignment” analogy and the power of the chain rule, its genius becomes clear. It’s a testament to how fundamental mathematical principles can unlock revolutionary technological advancements.</p> <p>So, the next time you marvel at an AI’s intelligence, remember the humble yet powerful algorithm working tirelessly behind the scenes: Backpropagation, the secret sauce that truly teaches machines to learn. Keep exploring, keep questioning, and keep building!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>