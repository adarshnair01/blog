<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Decoding the Crystal Ball: My Journey into Linear Regression's Simple Power | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/decoding-the-crystal-ball-my-journey-into-linear-r/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Decoding the Crystal Ball: My Journey into Linear Regression's Simple Power</h1> <p class="post-meta"> Created on September 14, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/linear-regression"> <i class="fa-solid fa-hashtag fa-sm"></i> Linear Regression</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/python"> <i class="fa-solid fa-hashtag fa-sm"></i> Python</a>   <a href="/blog/blog/tag/statistics"> <i class="fa-solid fa-hashtag fa-sm"></i> Statistics</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey there, fellow explorers of the data universe!</p> <p>You know that feeling when you look at a scatter plot, and your brain just <em>knows</em> there’s a line that could explain the relationship between the dots? That’s the intuitive spark that led me down the rabbit hole of one of the most foundational algorithms in machine learning: <strong>Linear Regression</strong>.</p> <p>It might sound fancy, but at its heart, Linear Regression is all about finding that “best-fit line” – a straight line that summarizes the trend in your data. It’s like having a simple crystal ball that, while not perfect, can give you pretty good guesses about what’s coming next, based on what’s happened before.</p> <p>I remember my first encounter with it. I had a dataset of house sizes and their corresponding prices. Plotted them, and <em>bam!</em> A clear upward trend. Bigger houses, higher prices. My goal? To predict the price of a house I hadn’t seen yet, based only on its size. This is where Linear Regression truly shines.</p> <h3 id="the-core-idea-drawing-the-best-line">The Core Idea: Drawing the “Best” Line</h3> <p>Imagine you have a bunch of points on a graph. Each point represents a house: its x-coordinate is the size (in square feet), and its y-coordinate is the price. What Linear Regression tries to do is draw a straight line through these points that best represents the overall relationship.</p> <p>Why a straight line? Because it’s simple, interpretable, and for many real-world scenarios, a linear relationship is a great starting point, even if it’s not perfectly accurate.</p> <p>The equation of a straight line, as you might recall from algebra, is:</p> <p>$y = mx + b$</p> <p>Where:</p> <ul> <li>$y$ is the value we want to predict (e.g., house price).</li> <li>$x$ is the input feature we’re using for prediction (e.g., house size).</li> <li>$m$ is the slope of the line (how much $y$ changes for every unit change in $x$).</li> <li>$b$ is the y-intercept (the value of $y$ when $x$ is 0).</li> </ul> <p>In the world of machine learning, we often use slightly different notation:</p> <p>$\hat{y} = \theta_0 + \theta_1 x_1$</p> <p>Here:</p> <ul> <li>$\hat{y}$ (pronounced “y-hat”) is our <em>predicted</em> value.</li> <li>$\theta_0$ (theta-naught) is the y-intercept.</li> <li>$\theta_1$ (theta-one) is the coefficient for our feature $x_1$.</li> </ul> <p>Our job, as data scientists, is to find the <em>best</em> values for $\theta_0$ and $\theta_1$. But what does “best” even mean?</p> <h3 id="beyond-one-feature-multiple-linear-regression">Beyond One Feature: Multiple Linear Regression</h3> <p>What if we want to predict a house price not just from its size, but also from the number of bedrooms, the age of the house, and its distance to the city center? This is where <strong>Multiple Linear Regression</strong> comes in. Instead of just one $x$, we have multiple $x$ variables (features).</p> <p>The equation expands beautifully:</p> <p>$\hat{y} = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + … + \theta_n x_n$</p> <p>Here, $x_1, x_2, …, x_n$ are our different features, and $\theta_1, \theta_2, …, \theta_n$ are their respective coefficients. $\theta_0$ is still our intercept.</p> <p>This can be written even more compactly using vectorized notation, which is super common in machine learning:</p> <p>$h_\theta(x) = \theta^T x$</p> <p>Where:</p> <ul> <li>$h_\theta(x)$ is our prediction function (often called hypothesis function).</li> <li>$\theta$ is a vector containing all our coefficients (including $\theta_0$, usually by adding a dummy $x_0=1$ to our feature vector).</li> <li>$x$ is a vector containing all our features for a single data point.</li> <li>$\theta^T x$ is the dot product of the two vectors.</li> </ul> <p>This elegant notation allows us to represent a potentially complex model in a very concise way.</p> <h3 id="the-best-line-defining-the-cost">The “Best” Line: Defining the Cost</h3> <p>Okay, so we’re looking for the “best” $\theta$ values. How do we quantify what “best” means?</p> <p>Think about it: a line is “best” if it’s as close as possible to <em>all</em> the data points. This means the difference between our predicted value ($\hat{y}$) and the actual value ($y$) should be as small as possible for every single data point. This difference is called the <strong>residual</strong> or <strong>error</strong>.</p> <p>A naive approach might be to just sum up all the errors: $\sum (y_i - \hat{y}_i)$. But positive errors (our line predicts too high) would cancel out negative errors (our line predicts too low), leading to a misleading sum.</p> <p>To avoid this, we square the errors! This not only makes all errors positive but also penalizes larger errors more heavily, pushing our line to be even closer to those far-off points.</p> <p>So, for a single data point $i$, the squared error is: $(y^{(i)} - \hat{y}^{(i)})^2$.</p> <p>To find the “best” line across <em>all</em> our data points, we sum up these squared errors for all $m$ data points and take their average. This gives us our <strong>Cost Function</strong>, often denoted $J(\theta)$ (pronounced “J of theta”):</p> <p>$J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)})^2$</p> <p>Let’s break this down:</p> <ul> <li>$m$: The total number of data points.</li> <li>$h_\theta(x^{(i)})$: Our predicted value for the $i$-th data point using our current $\theta$.</li> <li>$y^{(i)}$: The actual value for the $i$-th data point.</li> <li>$(h_\theta(x^{(i)}) - y^{(i)})^2$: The squared error for the $i$-th data point.</li> <li>$\sum_{i=1}^{m}$: Summing up all these squared errors.</li> <li>$\frac{1}{2m}$: We divide by $m$ to get the average (Mean Squared Error, or MSE), and the $\frac{1}{2}$ is a mathematical convenience that makes the calculus easier later on when we take derivatives (it cancels out a 2).</li> </ul> <p>Our ultimate goal is to find the values of $\theta$ that <strong>minimize</strong> this cost function $J(\theta)$. When $J(\theta)$ is at its minimum, our line is truly the “best fit.”</p> <h3 id="how-to-find-the-optimal-theta-gradient-descent">How to Find the Optimal $\theta$: Gradient Descent</h3> <p>Okay, we have a way to measure how good our line is (the cost function). Now, how do we actually <em>find</em> the $\theta$ values that minimize it?</p> <p>Imagine you’re standing on a mountain, blindfolded. You want to reach the lowest point (the bottom of the valley). What do you do? You feel around and take a small step in the direction of the steepest descent. You repeat this process, taking small steps downhill, until you can’t go any lower.</p> <p>This, in a nutshell, is <strong>Gradient Descent</strong>.</p> <p>In our case, the “mountain” is the graph of our cost function $J(\theta)$. Since $J(\theta)$ is a function of our parameters $\theta_0, \theta_1, …, \theta_n$, it’s a multi-dimensional surface. The “steepest descent” is given by the negative of the gradient (a vector of partial derivatives).</p> <p>The update rule for each parameter $\theta_j$ is:</p> <p>$\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta)$</p> <p>Let’s unpack that:</p> <ul> <li>$\theta_j$: Our parameter (e.g., $\theta_0$ or $\theta_1$) that we’re trying to optimize.</li> <li>$:= $: This means “update $\theta_j$ to be this new value.”</li> <li>$\alpha$ (alpha): This is the <strong>learning rate</strong>. It’s a small positive number that controls the size of each step we take down the mountain. If $\alpha$ is too small, it takes forever to reach the bottom. If it’s too large, we might overshoot the minimum and bounce around erratically, never settling.</li> <li>$\frac{\partial}{\partial \theta_j} J(\theta)$: This is the <strong>partial derivative</strong> of the cost function with respect to $\theta_j$. It tells us the slope of the cost function surface in the direction of $\theta_j$. Essentially, it points us in the direction of the steepest ascent, so we subtract it to go <em>down</em> the mountain.</li> </ul> <p>For our Linear Regression cost function, the partial derivative with respect to $\theta_j$ turns out to be:</p> <p>$\frac{\partial}{\partial \theta_j} J(\theta) = \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}$</p> <p>So, the Gradient Descent update rule becomes:</p> <p>$\theta_j := \theta_j - \alpha \frac{1}{m} \sum_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}$</p> <p>We repeat this update for <em>all</em> $\theta_j$ parameters simultaneously, for many iterations, until our parameters converge (meaning they stop changing significantly).</p> <h3 id="an-alternative-the-normal-equation-the-one-shot-solution">An Alternative: The Normal Equation (The “One-Shot” Solution)</h3> <p>While Gradient Descent is incredibly powerful and necessary for many complex models, for Linear Regression, there’s actually a direct, “one-shot” mathematical solution: the <strong>Normal Equation</strong>.</p> <p>Instead of iteratively stepping down, we can analytically find the $\theta$ that minimizes $J(\theta)$ by setting the derivative to zero and solving for $\theta$. The solution is:</p> <p>$\theta = (X^T X)^{-1} X^T y$</p> <p>Where:</p> <ul> <li>$\theta$: The vector of optimal parameters.</li> <li>$X$: The design matrix, where each row is a data point, and each column is a feature (with a column of ones added for the intercept $\theta_0$).</li> <li>$y$: The vector of actual target values.</li> <li>$X^T$: The transpose of matrix $X$.</li> <li>$(X^T X)^{-1}$: The inverse of the matrix $X^T X$.</li> </ul> <p><strong>Pros and Cons:</strong></p> <ul> <li> <strong>Normal Equation</strong>: No need to choose a learning rate $\alpha$, no iterations. It’s a direct solution. However, computing the inverse of a matrix can be computationally expensive (roughly $O(n^3)$ where $n$ is the number of features) for very large numbers of features. If $X^T X$ is not invertible (e.g., due to multicollinearity), it can also pose issues.</li> <li> <strong>Gradient Descent</strong>: Scales better to a very large number of features. It also forms the basis for optimizing far more complex models (like neural networks) where the Normal Equation isn’t feasible. But you <em>do</em> need to carefully choose your learning rate and number of iterations.</li> </ul> <h3 id="assumptions-of-linear-regression">Assumptions of Linear Regression</h3> <p>Linear Regression is powerful, but it comes with some assumptions about the data that, if violated, can make our model less reliable:</p> <ol> <li> <strong>Linearity</strong>: The relationship between features and the target variable should be linear. If it’s curved, a straight line won’t capture it well.</li> <li> <strong>Independence of Errors</strong>: The errors (residuals) should be independent of each other. This means one prediction being off shouldn’t influence another prediction being off.</li> <li> <strong>Homoscedasticity</strong>: The variance of the errors should be constant across all levels of the independent variables. In simpler terms, the spread of the residuals should be roughly the same across the entire range of predictions.</li> <li> <strong>Normality of Residuals</strong>: The errors should be normally distributed. This is particularly important for statistical inference (like confidence intervals), though less critical for just making predictions.</li> <li> <strong>No Multicollinearity</strong>: Independent variables should not be too highly correlated with each other. If they are, it can make it hard to determine the individual impact of each feature.</li> </ol> <p>When using Linear Regression, it’s good practice to check these assumptions. Visualizing residuals is a common way to do this.</p> <h3 id="evaluating-our-model-r-squared">Evaluating Our Model: R-squared</h3> <p>Once we’ve trained our model and found the “best” $\theta$ values, how do we know how good our predictions are? One common metric is <strong>R-squared ($R^2$)</strong>.</p> <p>$R^2 = 1 - \frac{SS_{res}}{SS_{tot}}$</p> <p>Where:</p> <ul> <li>$SS_{res}$ (Sum of Squares of Residuals): This is essentially our minimized cost function multiplied by $2m$ (i.e., $\sum (y^{(i)} - \hat{y}^{(i)})^2$). It measures how much variation in $y$ is <em>not</em> explained by our model.</li> <li>$SS_{tot}$ (Total Sum of Squares): This measures the total variation in $y$ (i.e., $\sum (y^{(i)} - \bar{y})^2$, where $\bar{y}$ is the mean of $y$).</li> </ul> <p>$R^2$ tells us the proportion of the variance in the dependent variable that is predictable from the independent variables. It ranges from 0 to 1.</p> <ul> <li>An $R^2$ of 1 means our model perfectly explains all the variance in $y$.</li> <li>An $R^2$ of 0 means our model explains none of the variance.</li> </ul> <h3 id="when-to-use-and-not-use-linear-regression">When to Use (and Not Use) Linear Regression</h3> <p><strong>Use it when:</strong></p> <ul> <li>You suspect a linear relationship between your features and target.</li> <li>You need a simple, interpretable model. The coefficients tell you the impact of each feature directly.</li> <li>Your dataset isn’t prohibitively large (for Normal Equation) or you can tune Gradient Descent effectively.</li> <li>You need a baseline model to compare more complex algorithms against.</li> </ul> <p><strong>Be cautious when:</strong></p> <ul> <li>The relationship is clearly non-linear (e.g., exponential growth). You might need to transform your data or use a different model.</li> <li>Your data violates the core assumptions (e.g., strong multicollinearity, non-normal residuals, heteroscedasticity).</li> <li>You have many outliers, as squared errors penalize them heavily, potentially skewing the line.</li> </ul> <h3 id="my-takeaway">My Takeaway</h3> <p>Linear Regression, despite its simplicity, is a cornerstone of predictive modeling. It’s often the first algorithm I reach for because it’s so intuitive and interpretable. It taught me the fundamental concepts of modeling, cost functions, and optimization – concepts that underpin almost every other machine learning algorithm.</p> <p>It’s not just a mathematical formula; it’s a way of thinking about relationships in data, a tool for distilling complex patterns into understandable lines. So, next time you see a scatter plot, remember the journey we took: from intuitive line-drawing to the mathematical precision of minimizing errors, all to predict the unseen.</p> <p>Keep exploring, keep learning, and happy modeling!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>