<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Unmasking Order: Diving Deep into the Magic of K-Means Clustering | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/unmasking-order-diving-deep-into-the-magic-of-k-me/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Unmasking Order: Diving Deep into the Magic of K-Means Clustering</h1> <p class="post-meta"> Created on December 15, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a> Â  Â· Â  <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a> Â  <a href="/blog/blog/tag/k-means"> <i class="fa-solid fa-hashtag fa-sm"></i> K-Means</a> Â  <a href="/blog/blog/tag/clustering"> <i class="fa-solid fa-hashtag fa-sm"></i> Clustering</a> Â  <a href="/blog/blog/tag/unsupervised-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Unsupervised Learning</a> Â  <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone! ğŸ‘‹</p> <p>Have you ever looked at a massive pile of something â€“ maybe a big box of LEGOs, a mixed bag of Halloween candy, or an overwhelming spreadsheet of customer data â€“ and wished it would just <em>organize itself</em>? You instinctively start grouping things: all the red LEGOs together, all the chocolate bars in one pile, all the customers who buy similar products in another.</p> <p>That innate human desire to find order, to discover patterns, and to categorize things is exactly what weâ€™re going to explore today with one of the simplest yet most powerful algorithms in a data scientistâ€™s toolkit: <strong>K-Means Clustering</strong>.</p> <h3 id="what-is-clustering-anyway-and-why-do-we-care">What is Clustering, Anyway? (And Why Do We Care?)</h3> <p>Before we dive into K-Means specifically, letâ€™s zoom out a bit. In the world of machine learning, we often talk about â€œsupervisedâ€ and â€œunsupervisedâ€ learning.</p> <ul> <li> <strong>Supervised Learning:</strong> This is like learning with a teacher. You have data points, and each data point comes with a â€œlabelâ€ â€“ the correct answer. Think spam detection (email is SPAM or NOT SPAM) or predicting house prices (price for <em>this</em> house is X). The algorithm learns from these labeled examples.</li> <li> <strong>Unsupervised Learning:</strong> This is like learning without a teacher. You just have a bunch of data, and no labels. Your goal isnâ€™t to predict a specific outcome, but to find inherent structures, patterns, or groupings within the data itself.</li> </ul> <p><strong>Clustering</strong> is a prime example of unsupervised learning. Its whole purpose is to group similar data points together. The â€œclustersâ€ are these groups, where points within a cluster are more similar to each other than they are to points in other clusters.</p> <p>Why do we care? Well, imagine trying to understand millions of customers. You canâ€™t analyze each one individually. But if you can group them into 3, 5, or 10 distinct â€œcustomer segmentsâ€ based on their purchasing habits, browsing history, or demographics, suddenly you can tailor marketing campaigns, develop targeted products, and make much smarter business decisions. This is just one of many applications!</p> <h3 id="meet-k-means-the-algorithm-that-finds-the-middle-ground">Meet K-Means: The Algorithm That Finds the Middle Ground</h3> <p>K-Means is a centroid-based clustering algorithm. â€œCentroidâ€ just means the center point of a cluster. Itâ€™s an iterative algorithm, meaning it repeats a few steps over and over again until it settles on a good solution.</p> <p>Letâ€™s break down the â€œKâ€ and the â€œMeansâ€ before we get into the steps:</p> <ul> <li> <strong>K:</strong> This is the number of clusters you want to find. Itâ€™s a hyperparameter, meaning you have to choose it <em>before</em> running the algorithm. If you want to group your customers into 3 segments, K would be 3. If you want 5, K would be 5.</li> <li> <strong>Means:</strong> This refers to how the cluster centers (centroids) are calculated. Each centroid is the <em>mean</em> (average) of all the data points assigned to that cluster.</li> </ul> <p>Now, letâ€™s walk through the algorithm step-by-step. Imagine you have a scatter plot of data points, and you want to group them into <code class="language-plaintext highlighter-rouge">K</code> clusters.</p> <h4 id="step-1-initialization---picking-your-starting-points">Step 1: Initialization - Picking Your Starting Points</h4> <p>The very first thing K-Means does is randomly pick <code class="language-plaintext highlighter-rouge">K</code> data points from your dataset to be the initial centroids. Think of these as your initial â€œguessâ€ for where the centers of your groups might be.</p> <p>Itâ€™s important to note that because these are chosen randomly, running K-Means multiple times might give you slightly different results. More on this later!</p> <h4 id="step-2-the-assignment-step-e-step---expectation">Step 2: The Assignment Step (E-step - Expectation)</h4> <p>Once you have your <code class="language-plaintext highlighter-rouge">K</code> centroids, the algorithm asks: â€œOkay, for every single data point, which of these <code class="language-plaintext highlighter-rouge">K</code> centroids is it closest to?â€</p> <p>Every data point gets assigned to the cluster whose centroid is nearest. How do we measure â€œnearestâ€? We typically use Euclidean distance. If you remember Pythagorasâ€™s theorem ($a^2 + b^2 = c^2$), youâ€™re already familiar with the core idea!</p> <p>For a data point $x$ and a centroid $c$, the Euclidean distance is calculated as:</p> <p>$d(x, c) = \sqrt{\sum_{i=1}^D (x_i - c_i)^2}$</p> <p>Where:</p> <ul> <li>$x_i$ and $c_i$ are the values of the $i$-th dimension (or feature) for the data point and the centroid, respectively.</li> <li>$D$ is the total number of dimensions (features) in your data.</li> </ul> <p>So, for each data point, we calculate its distance to <em>all</em> $K$ centroids and then assign it to the one with the smallest distance. After this step, all your data points are now â€œbelongingâ€ to one of the <code class="language-plaintext highlighter-rouge">K</code> initial clusters.</p> <h4 id="step-3-the-update-step-m-step---maximization">Step 3: The Update Step (M-step - Maximization)</h4> <p>Now that all data points have been assigned to a cluster, those initial, randomly placed centroids probably arenâ€™t in the <em>actual</em> center of their respective clusters anymore. Theyâ€™re like flags planted in the ground, but the group of people theyâ€™re supposed to represent has shifted.</p> <p>So, K-Means moves the centroids! For each cluster, the new centroid position is calculated as the <em>mean</em> (average) of all the data points currently assigned to that cluster.</p> <p>If $C_j$ represents the set of all data points assigned to cluster $j$, the new centroid $c_j$ for that cluster is:</p> <table> <tbody> <tr> <td>$c_j = \frac{1}{</td> <td>C_j</td> <td>} \sum_{x \in C_j} x$</td> </tr> </tbody> </table> <p>Where:</p> <ul> <li> <table> <tbody> <tr> <td>$</td> <td>C_j</td> <td>$ is the number of data points in cluster $j$.</td> </tr> </tbody> </table> </li> <li>$\sum_{x \in C_j} x$ is the sum of all data points (as vectors) in cluster $j$.</li> </ul> <p>This step literally pulls the centroids to the center of their assigned data points, making them a better representation of their clusters.</p> <h4 id="step-4-iteration---repeating-until-convergence">Step 4: Iteration - Repeating Until Convergence</h4> <p>The algorithm doesnâ€™t stop after one cycle! After updating the centroids, it goes back to <strong>Step 2 (Assignment Step)</strong>. Now that the centroids have moved, some data points might be closer to a <em>different</em> centroid than the one they were initially assigned to. So, they â€œswitch clusters.â€</p> <p>This process of assigning points to the closest centroid (E-step) and then moving the centroids to the mean of their new points (M-step) repeats.</p> <p>When does it stop? It stops when the centroids no longer move significantly between iterations, or when the assignments of data points to clusters no longer change. At this point, the algorithm has â€œconverged,â€ meaning it has found a stable set of clusters.</p> <h3 id="a-visual-metaphor">A Visual Metaphor</h3> <p>Imagine K-Means like a game of musical chairs with your data points!</p> <ol> <li> <strong>Random Centroids (K chairs):</strong> You randomly place <code class="language-plaintext highlighter-rouge">K</code> chairs on the dance floor.</li> <li> <strong>Assign to Closest (Find your chair):</strong> When the music stops, every person (data point) runs to the <em>closest</em> empty chair (centroid).</li> <li> <strong>Update Centroids (Move the chairs):</strong> Now, for each group of people around a chair, you calculate the <em>exact center</em> of that group. Thatâ€™s where you move the chair for the next round.</li> <li> <strong>Repeat:</strong> The music starts again, people move to the <em>new closest</em> chair, and the chairs keep adjusting until no one needs to move chairs anymore. Everyone is settled in their optimal spot.</li> </ol> <h3 id="key-considerations-and-gotchas-with-k-means">Key Considerations and â€œGotchasâ€ with K-Means</h3> <p>While K-Means is awesome, itâ€™s not a magic bullet. Here are a few important things to keep in mind:</p> <h4 id="1-the-challenge-of-choosing-k">1. The Challenge of Choosing K</h4> <p>Remember <code class="language-plaintext highlighter-rouge">K</code>, the number of clusters? You have to choose it upfront. But how do you know the â€œrightâ€ number of clusters for your data? This is often the trickiest part.</p> <p>One common technique is the <strong>Elbow Method</strong>:</p> <ul> <li>You run K-Means for a range of <code class="language-plaintext highlighter-rouge">K</code> values (e.g., K=1 to K=10).</li> <li>For each <code class="language-plaintext highlighter-rouge">K</code>, you calculate the <strong>Within-Cluster Sum of Squares (WCSS)</strong>, which is the sum of the squared distances between each point and its assigned centroid. A smaller WCSS means points are closer to their centroids, implying tighter clusters.</li> <li>You plot WCSS against <code class="language-plaintext highlighter-rouge">K</code>. As you increase <code class="language-plaintext highlighter-rouge">K</code>, WCSS will naturally decrease (because more clusters means points have less distance to travel to their centroid).</li> <li>The â€œelbowâ€ point on this plot is where the rate of decrease dramatically slows down. This point is often considered a good candidate for <code class="language-plaintext highlighter-rouge">K</code>, as adding more clusters beyond this point doesnâ€™t significantly improve the clustering quality.</li> </ul> <h4 id="2-local-optima-the-random-start-problem">2. Local Optima (The Random Start Problem)</h4> <p>Because K-Means starts with randomly placed centroids, it can sometimes get stuck in a â€œlocal optimum.â€ This means it finds a good set of clusters, but not necessarily the <em>best possible</em> set (the global optimum).</p> <p>Imagine trying to find the lowest point in a hilly landscape. If you start in a dip, you might think youâ€™ve found the lowest point, but there might be a much deeper valley elsewhere.</p> <p>To mitigate this, itâ€™s common practice to run K-Means multiple times with different random initializations and choose the clustering that results in the lowest WCSS. Many K-Means implementations (like scikit-learn in Python) do this by default with the <code class="language-plaintext highlighter-rouge">n_init</code> parameter.</p> <h4 id="3-sensitivity-to-outliers">3. Sensitivity to Outliers</h4> <p>Since centroids are calculated as the <em>mean</em> of data points, K-Means is sensitive to outliers (data points that are very far from the rest). A single extreme outlier can pull a centroid significantly, distorting the clusters. Pre-processing your data to handle outliers is often a good idea.</p> <h4 id="4-the-spherical-cluster-assumption">4. The Spherical Cluster Assumption</h4> <p>K-Means works best when clusters are roughly spherical (or blob-like) and of similar size and density. If your data has irregularly shaped clusters (like crescent moons, or intertwined spirals), K-Means will struggle because itâ€™s always trying to find a â€œcenterâ€ and assign points based on simple distance. Other algorithms, like DBSCAN, might be better suited for such cases.</p> <h4 id="5-scaling-matters">5. Scaling Matters!</h4> <p>If your features have very different scales (e.g., one feature ranges from 0-10, another from 0-1,000,000), the feature with the larger range will dominate the distance calculation. Itâ€™s crucial to <strong>normalize or standardize your data</strong> before running K-Means. This ensures all features contribute equally to the distance calculation.</p> <h3 id="real-world-applications">Real-World Applications</h3> <p>K-Means is incredibly versatile. Here are just a few ways itâ€™s used:</p> <ul> <li> <strong>Customer Segmentation:</strong> Grouping customers based on purchase history, browsing behavior, demographics, etc., for targeted marketing.</li> <li> <strong>Document Clustering:</strong> Grouping similar documents (news articles, research papers) together for easier navigation and discovery.</li> <li> <strong>Image Compression:</strong> Reducing the number of colors in an image (e.g., from millions to 256) by clustering similar colors. Each pixel is then represented by its cluster centroid color.</li> <li> <strong>Anomaly Detection:</strong> Identifying unusual data points that donâ€™t fit into any established cluster (e.g., fraudulent transactions).</li> <li> <strong>Geospatial Analysis:</strong> Grouping locations with similar characteristics (e.g., identifying distinct urban zones).</li> </ul> <h3 id="conclusion-a-simple-powerhouse">Conclusion: A Simple Powerhouse</h3> <p>K-Means Clustering, at its core, is a beautifully simple yet profoundly effective algorithm. It embodies the human quest for order, allowing us to take raw, unlabeled data and uncover the hidden structures within. While it has its quirks and assumptions, understanding these allows us to wield its power responsibly and effectively.</p> <p>From organizing your music library to helping businesses understand their customers, K-Means is a testament to how elegant mathematical ideas can unlock incredible insights in the messy, wonderful world of data. So, the next time you see a jumbled mess of information, remember K-Means â€“ it might just be the perfect tool to bring order to the chaos!</p> <p>Go forth and cluster! ğŸš€</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> Â© Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>