<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Secret Language of AI: My Journey into Natural Language Processing | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/the-secret-language-of-ai-my-journey-into-natural/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Secret Language of AI: My Journey into Natural Language Processing</h1> <p class="post-meta"> Created on March 24, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/natural-language-processing"> <i class="fa-solid fa-hashtag fa-sm"></i> Natural Language Processing</a>   <a href="/blog/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> NLP</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>My fascination with language began early. Not just the words themselves, but the nuances, the unspoken meanings, the way a slight change in tone or context could completely alter a message. It always struck me as a uniquely human superpower. Then, I encountered Artificial Intelligence, and a new question emerged: Could we ever teach machines to wield this superpower?</p> <p>That question led me down a thrilling path to <strong>Natural Language Processing (NLP)</strong>.</p> <h3 id="what-exactly-is-natural-language-processing">What Exactly <em>Is</em> Natural Language Processing?</h3> <p>At its core, NLP is a field of Artificial Intelligence that gives computers the ability to understand, interpret, and generate human language in a way that is both useful and meaningful. Think about it: our languages are messy, full of idioms, sarcasm, synonyms, and complex grammatical structures. A computer, on the other hand, understands only precise, logical commands. NLP is the bridge we build between these two worlds.</p> <p>My journey into NLP felt a bit like learning to translate for an alien species. First, I had to grasp how humans communicate, then figure out how to codify that into something a machine could process.</p> <h3 id="why-does-nlp-matter-so-much">Why Does NLP Matter So Much?</h3> <p>You interact with NLP every single day, often without even realizing it!</p> <ul> <li> <strong>Voice Assistants:</strong> Siri, Alexa, Google Assistant – they all use NLP to understand your spoken commands.</li> <li> <strong>Search Engines:</strong> Google doesn’t just match keywords; it tries to understand the <em>intent</em> behind your query.</li> <li> <strong>Machine Translation:</strong> Tools like Google Translate allow us to communicate across language barriers.</li> <li> <strong>Spam Filters:</strong> These guardians of your inbox analyze email content to identify and block unwanted messages.</li> <li> <strong>Sentiment Analysis:</strong> Businesses use NLP to gauge public opinion about their products from social media posts.</li> <li> <strong>Chatbots &amp; Customer Service:</strong> Many online support systems use NLP to understand your questions and provide automated responses.</li> </ul> <p>It’s clear that NLP isn’t just a niche area; it’s fundamental to how we interact with technology and information in the 21st century.</p> <h3 id="the-hurdle-why-is-human-language-so-hard-for-computers">The Hurdle: Why is Human Language So Hard for Computers?</h3> <p>Before diving into <em>how</em> we do NLP, it’s crucial to understand <em>why</em> it’s such a challenge. When I first started, I thought, “Just give the computer a dictionary!” Oh, how naive I was.</p> <p>Consider these simple sentences:</p> <ol> <li>“I saw a man with a telescope.”</li> <li>“Time flies like an arrow; fruit flies like a banana.”</li> </ol> <p>In sentence 1, did I see a man <em>who had</em> a telescope, or did I see a man <em>using</em> a telescope to look at something? The meaning changes depending on where the telescope is attached. This is <strong>ambiguity</strong>.</p> <p>In sentence 2, the word “flies” is used as a verb and then as a noun. “Like” is a preposition and then a verb. This illustrates <strong>polysemy</strong> (words with multiple meanings) and <strong>syntactic complexity</strong>. Humans effortlessly parse these; computers struggle.</p> <p>Add to this:</p> <ul> <li> <strong>Context:</strong> The meaning of a word often depends on the surrounding words.</li> <li> <strong>Idioms &amp; Slang:</strong> “It’s raining cats and dogs” makes no literal sense to a computer.</li> <li> <strong>Sarcasm &amp; Irony:</strong> Detecting these requires deep understanding of human emotion and social cues.</li> </ul> <p>These challenges are what make NLP both incredibly difficult and endlessly fascinating.</p> <h3 id="building-blocks-of-nlp-from-raw-text-to-numerical-insights">Building Blocks of NLP: From Raw Text to Numerical Insights</h3> <p>My first major realization was this: computers don’t understand words; they understand numbers. So, the initial, crucial step in NLP is transforming human language into a numerical format that algorithms can process. This process usually starts with several preprocessing steps.</p> <h4 id="1-text-preprocessing-cleaning-up-the-mess">1. Text Preprocessing: Cleaning Up the Mess</h4> <p>Imagine getting a transcript of spoken language – it’s full of filler words, mispronunciations, and incomplete sentences. Even written text has variations. Preprocessing is like tidying up before a big project.</p> <ul> <li> <strong>Tokenization:</strong> This is breaking down text into smaller units called “tokens.” Usually, these are words or punctuation marks. <ul> <li> <em>Example:</em> “Hello, world!” $\rightarrow$ [‘Hello’, ‘,’, ‘world’, ‘!’] It sounds simple, but even tokenization can be complex (e.g., handling contractions like “don’t”).</li> </ul> </li> <li> <strong>Lowercasing:</strong> Converting all text to lowercase to treat “The” and “the” as the same word.</li> <li> <strong>Stopword Removal:</strong> Eliminating common words like “a,” “an,” “the,” “is,” “and” that often carry little meaning for analysis but appear frequently. <ul> <li> <em>Example (after lowercasing and tokenization):</em> [‘the’, ‘quick’, ‘brown’, ‘fox’, ‘jumps’, ‘over’, ‘the’, ‘lazy’, ‘dog’] $\rightarrow$ [‘quick’, ‘brown’, ‘fox’, ‘jumps’, ‘lazy’, ‘dog’]</li> </ul> </li> <li> <strong>Stemming &amp; Lemmatization:</strong> Reducing words to their root form. <ul> <li> <strong>Stemming</strong> is a cruder process, often just chopping off suffixes. <code class="language-plaintext highlighter-rouge">running</code>, <code class="language-plaintext highlighter-rouge">runs</code>, <code class="language-plaintext highlighter-rouge">ran</code> might all become <code class="language-plaintext highlighter-rouge">run</code>. It might not result in a valid word.</li> <li> <strong>Lemmatization</strong> is more sophisticated, using a vocabulary and morphological analysis to get to the base or dictionary form (lemma). <code class="language-plaintext highlighter-rouge">better</code> $\rightarrow$ <code class="language-plaintext highlighter-rouge">good</code>, <code class="language-plaintext highlighter-rouge">am</code> $\rightarrow$ <code class="language-plaintext highlighter-rouge">be</code>.</li> </ul> </li> </ul> <p>These steps standardize the text, making it easier for machines to process.</p> <h4 id="2-representing-text-numerically-words-into-vectors">2. Representing Text Numerically: Words into Vectors</h4> <p>Now that our text is clean, how do we turn “quick brown fox” into numbers? This is where the magic (and some math) happens.</p> <h5 id="a-bag-of-words-bow">a. Bag-of-Words (BoW)</h5> <p>My first introduction to numerical representation was the <strong>Bag-of-Words (BoW)</strong> model. It’s wonderfully simple:</p> <ol> <li>Create a vocabulary of all unique words in your entire collection of documents (corpus).</li> <li>For each document, count the frequency of each word from the vocabulary.</li> <li>Represent each document as a vector where each dimension corresponds to a word in the vocabulary, and the value is its count.</li> </ol> <p><em>Example:</em></p> <ul> <li>Document 1: “The quick brown fox.”</li> <li>Document 2: “The quick red fox.”</li> <li>Vocabulary: [‘The’, ‘quick’, ‘brown’, ‘fox’, ‘red’]</li> </ul> <p>Vector for Document 1: $[1, 1, 1, 1, 0]$ (Counts of ‘The’, ‘quick’, ‘brown’, ‘fox’, ‘red’) Vector for Document 2: $[1, 1, 0, 1, 1]$</p> <p>The problem I quickly noticed with BoW is that common words like “the” (if not removed as stopwords) would have high counts across many documents, overshadowing more unique and potentially important words.</p> <h5 id="b-tf-idf-beyond-simple-counts">b. TF-IDF: Beyond Simple Counts</h5> <p>To address the BoW limitation, I learned about <strong>TF-IDF (Term Frequency-Inverse Document Frequency)</strong>. This technique assigns a weight to each word, reflecting its importance in a document relative to the entire corpus. It has two parts:</p> <ul> <li> <strong>Term Frequency (TF):</strong> How often a word appears in <em>a specific document</em>. \(TF(t, d) = \frac{\text{Number of times term t appears in document d}}{\text{Total number of terms in document d}}\)</li> <li> <p><strong>Inverse Document Frequency (IDF):</strong> How rare or common a word is across <em>all documents</em>. The rarer a word, the higher its IDF score. \(IDF(t, D) = \log \left( \frac{\text{Total number of documents D}}{\text{Number of documents with term t}} \right)\) (Here, $D$ represents the entire corpus of documents.)</p> </li> <li> <strong>TF-IDF Score:</strong> The product of TF and IDF. \(TFIDF(t, d, D) = TF(t, d) \times IDF(t, D)\) A high TF-IDF score means a word is frequent in a particular document but rare across other documents, making it a good indicator of that document’s content. This felt like a significant upgrade from simple BoW.</li> </ul> <h5 id="c-word-embeddings-capturing-meaning-and-context">c. Word Embeddings: Capturing Meaning and Context</h5> <p>While TF-IDF was powerful, it still treated words as individual, independent units. It didn’t capture semantic relationships (e.g., ‘king’ is related to ‘queen’ and ‘man’ is related to ‘woman’). This is where <strong>Word Embeddings</strong> came in.</p> <p>Imagine each word as a point in a high-dimensional space (e.g., 300 dimensions). Words with similar meanings or that appear in similar contexts are positioned closer to each other in this space. For instance, in a well-trained embedding model like Word2Vec or GloVe, the vector for “king” minus the vector for “man” plus the vector for “woman” would result in a vector very close to the vector for “queen”.</p> <p>This concept blew my mind! It meant we could not only represent words numerically but also capture their <em>meaning</em> and <em>relationships</em> in a continuous vector space. This was a massive leap for NLP, allowing models to understand context far better than before.</p> <h3 id="nlp-in-action-a-glimpse-at-applications">NLP in Action: A Glimpse at Applications</h3> <p>With numerical representations, we can apply various machine learning algorithms.</p> <ul> <li> <strong>Sentiment Analysis:</strong> By representing movie reviews using TF-IDF, we can train a classifier (e.g., a Support Vector Machine or Logistic Regression) to label reviews as “positive” or “negative” based on the patterns of words that appear in each category. If “amazing” and “hilarious” often appear in positive reviews, the model learns to associate those words with positive sentiment.</li> <li> <strong>Spam Detection:</strong> Similar to sentiment analysis, an email’s TF-IDF vector can be fed into a classifier to determine if it’s spam or not, learning from patterns of words common in known spam emails.</li> <li> <strong>Topic Modeling:</strong> Algorithms like Latent Dirichlet Allocation (LDA) can analyze a collection of documents and discover abstract “topics” that run through them, automatically grouping documents with similar themes.</li> </ul> <h3 id="the-deep-dive-deep-learnings-revolution-in-nlp">The Deep Dive: Deep Learning’s Revolution in NLP</h3> <p>As my journey continued, I encountered the transformative power of deep learning. While traditional NLP methods like BoW and TF-IDF are robust, they often struggle with long-range dependencies and truly understanding the sequence and context of words in complex sentences.</p> <ul> <li> <p><strong>Recurrent Neural Networks (RNNs) and LSTMs:</strong> These early deep learning models were designed to process sequential data like text. They had a “memory” of previous inputs, allowing them to consider words in context. While powerful, they had limitations, especially with very long sentences.</p> </li> <li> <p><strong>The Transformer Architecture:</strong> This is where modern NLP truly shines. Introduced in 2017, the Transformer model (and its revolutionary <strong>attention mechanism</strong>) changed everything. Instead of processing words sequentially, it processes all words in a sentence simultaneously, allowing it to weigh the importance of different words when encoding a particular word’s meaning.</p> <p>Imagine trying to understand the word “it” in the sentence: “The cat sat on the mat. It was soft.” To understand “it,” you need to know it refers to “mat.” The attention mechanism allows the model to “pay attention” to “mat” when processing “it,” even though they are separated. This parallel processing and powerful attention mechanism made Transformers incredibly efficient and effective.</p> <p>This led to the development of incredibly powerful models like:</p> <ul> <li> <strong>BERT (Bidirectional Encoder Representations from Transformers):</strong> Trained by Google, BERT can understand context from both left-to-right and right-to-left, making it exceptional for tasks like question answering and text classification.</li> <li> <strong>GPT (Generative Pre-trained Transformer) series:</strong> Developed by OpenAI, models like GPT-3 and GPT-4 are masters of text generation, capable of writing articles, poems, and even code that are remarkably human-like. They can <em>generate</em> new text based on a given prompt.</li> </ul> </li> </ul> <p>These deep learning models, especially Transformers, have pushed the boundaries of what’s possible in NLP, achieving performance levels that were once unimaginable.</p> <h3 id="challenges-and-the-exciting-road-ahead">Challenges and the Exciting Road Ahead</h3> <p>Despite these incredible advancements, NLP is far from “solved.” My journey continually reminds me of the remaining challenges:</p> <ul> <li> <strong>Nuance and Common Sense:</strong> While models can generate coherent text, do they truly <em>understand</em> common sense or the subtle nuances of human interaction? Often not.</li> <li> <strong>Bias:</strong> If a model is trained on biased data (e.g., text predominantly written by one demographic), it can perpetuate and even amplify those biases in its output. This is a critical ethical concern.</li> <li> <strong>Multilinguality:</strong> While progress has been made, truly robust NLP across hundreds of languages remains a complex task.</li> <li> <strong>The “Black Box” Problem:</strong> Deep learning models, especially large Transformers, can be incredibly complex, making it difficult to understand <em>why</em> they make certain decisions.</li> </ul> <p>The future of NLP is incredibly exciting. I see continued advancements in:</p> <ul> <li> <strong>More Human-like AI:</strong> Models that can engage in more natural, long-form conversations.</li> <li> <strong>Personalized Learning:</strong> AI tutors that adapt to individual student needs by understanding their learning patterns and questions.</li> <li> <strong>Accessibility:</strong> Tools that break down language barriers for people with disabilities or those who speak different languages.</li> </ul> <h3 id="my-continuing-journey">My Continuing Journey</h3> <p>Exploring Natural Language Processing has been one of the most rewarding parts of my data science journey. It’s a field that perfectly blends linguistics, computer science, and statistics, constantly pushing the boundaries of what machines can do. From teaching a computer to simply count words to enabling it to write poetry, the progress has been phenomenal.</p> <p>If you’re intrigued by how language works, how machines learn, and how we can build more intelligent systems, I highly encourage you to dive into NLP. It’s a field brimming with fascinating problems waiting to be solved, and the potential impact on humanity is immense. My own learning continues every day, and I’m excited to see where this “secret language” takes us next.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>