<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Don't Trust Your Model (Yet!): Why Cross-Validation is Your Best Friend | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/dont-trust-your-model-yet-why-cross-validation-is/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Don't Trust Your Model (Yet!): Why Cross-Validation is Your Best Friend</h1> <p class="post-meta"> Created on October 25, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/model-evaluation"> <i class="fa-solid fa-hashtag fa-sm"></i> Model Evaluation</a>   <a href="/blog/blog/tag/cross-validation"> <i class="fa-solid fa-hashtag fa-sm"></i> Cross-Validation</a>   <a href="/blog/blog/tag/overfitting"> <i class="fa-solid fa-hashtag fa-sm"></i> Overfitting</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey there, future data scientists and ML enthusiasts!</p> <p>Let me tell you a story. It’s a classic tale that many of us, including my younger self, have fallen for. You spend hours, maybe days, meticulously cleaning your data, engineering features, picking the perfect algorithm, and finally, you train your first machine learning model. You eagerly check the performance metrics on your <em>training data</em>, and boom! An astounding 99% accuracy. You feel like a genius, ready to conquer the world with your perfectly trained AI.</p> <p>But then, you try it on <em>new, unseen data</em> – the real world, if you will – and suddenly, that 99% plummets to a dismal 60%, or worse, it behaves completely randomly. What happened? Your model, my friend, became a “one-trick pony.” It memorized the training data like a script for a play, but couldn’t improvise or adapt when the scene changed. This, in the world of machine learning, is what we call <strong>overfitting</strong>.</p> <p>Overfitting is the bane of every machine learning practitioner’s existence. It happens when your model learns the training data too well, including its noise and idiosyncrasies, failing to generalize to new, unseen data. It’s like studying only the exact questions from last year’s exam, only to find this year’s exam has different, albeit related, questions. You <em>know</em> the old exam perfectly, but you don’t actually <em>understand</em> the subject matter.</p> <p>So, how do we prevent this heartbreak? How do we build models that are not just smart, but truly <em>wise</em> and adaptable? Enter <strong>Cross-Validation</strong>, a powerful technique that acts as your model’s ultimate reality check, ensuring it learns to generalize rather than just memorize.</p> <h3 id="the-problem-with-a-simple-traintest-split">The Problem with a Simple Train/Test Split</h3> <p>Before we dive into cross-validation, let’s quickly revisit the standard, simplest way to evaluate a model: the train/test split.</p> <p>Typically, you’d take your entire dataset and split it into two portions:</p> <ol> <li> <strong>Training Set:</strong> The larger chunk (e.g., 70-80%) used to teach your model.</li> <li> <strong>Test Set:</strong> The smaller, completely separate chunk (e.g., 20-30%) used to evaluate how well your trained model performs on data it has <em>never seen before</em>.</li> </ol> <p>This is a good start! It prevents your model from evaluating itself on the same data it learned from, which would always give deceptively high scores. However, a single train/test split has its limitations:</p> <ul> <li> <strong>Sensitivity to the Split:</strong> What if you get a “lucky” split, where the training set is particularly easy to learn from, or the test set is unusually simple? Your evaluation might be overly optimistic. Conversely, an “unlucky” split could make a good model look bad.</li> <li> <strong>Data Usage:</strong> You’re effectively holding back a significant portion of your data (the test set) from the training process. While essential for unbiased evaluation, if your dataset is small, this can mean your model doesn’t learn from as much data as it could.</li> <li> <strong>Variance in Performance:</strong> Different random splits could lead to different performance scores. How do you know which one is the “true” performance?</li> </ul> <p>This is where cross-validation comes in, offering a more robust and reliable way to assess your model’s real-world potential.</p> <h3 id="cross-validation-the-better-way-to-validate-your-model">Cross-Validation: The Better Way to Validate Your Model</h3> <p>The core idea behind cross-validation is simple yet profound: instead of splitting your data just <em>once</em>, you split it <em>multiple times</em>, training and testing your model on different subsets repeatedly. This process yields multiple performance scores, which you can then average to get a much more stable and reliable estimate of your model’s true generalization ability.</p> <p>Think of it like preparing for a big exam. Instead of just doing one practice test, you do several different practice tests, each covering a different mix of topics. This way, you don’t just memorize one set of answers; you build a more comprehensive understanding of the subject matter, and you get a better sense of your overall readiness.</p> <p>Let’s explore the most common and powerful type of cross-validation: <strong>K-Fold Cross-Validation</strong>.</p> <h4 id="k-fold-cross-validation-the-workhorse">K-Fold Cross-Validation: The Workhorse</h4> <p>K-Fold Cross-Validation is the gold standard for model evaluation. Here’s how it works:</p> <ol> <li> <strong>Divide into K Folds:</strong> You first shuffle your entire dataset randomly (this is important!) and then divide it into <code class="language-plaintext highlighter-rouge">K</code> equally sized segments or “folds.” A common choice for <code class="language-plaintext highlighter-rouge">K</code> is 5 or 10.</li> <li> <strong>Iterate K Times:</strong> You then perform <code class="language-plaintext highlighter-rouge">K</code> rounds of training and testing. In each round: <ul> <li> <strong>One Fold is the Test Set:</strong> One of the <code class="language-plaintext highlighter-rouge">K</code> folds is designated as the validation (or test) set.</li> <li> <strong>The Remaining K-1 Folds are the Training Set:</strong> The other <code class="language-plaintext highlighter-rouge">K-1</code> folds are combined to form the training set.</li> <li> <strong>Train and Evaluate:</strong> Your model is trained exclusively on the training set, and its performance is evaluated on the validation set. A score (e.g., accuracy, precision, F1-score) is recorded.</li> </ul> </li> <li> <strong>Average the Scores:</strong> After all <code class="language-plaintext highlighter-rouge">K</code> iterations are complete, you will have <code class="language-plaintext highlighter-rouge">K</code> different performance scores. You then average these <code class="language-plaintext highlighter-rouge">K</code> scores to get your model’s final, cross-validated performance estimate.</li> </ol> <p>Let’s visualize it for $K=5$:</p> <ul> <li> <table> <tbody> <tr> <td> <strong>Iteration 1:</strong> Folds 2, 3, 4, 5 (Train)</td> <td>Fold 1 (Test) -&gt; Score 1</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td> <strong>Iteration 2:</strong> Folds 1, 3, 4, 5 (Train)</td> <td>Fold 2 (Test) -&gt; Score 2</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td> <strong>Iteration 3:</strong> Folds 1, 2, 4, 5 (Train)</td> <td>Fold 3 (Test) -&gt; Score 3</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td> <strong>Iteration 4:</strong> Folds 1, 2, 3, 5 (Train)</td> <td>Fold 4 (Test) -&gt; Score 4</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td> <strong>Iteration 5:</strong> Folds 1, 2, 3, 4 (Train)</td> <td>Fold 5 (Test) -&gt; Score 5</td> </tr> </tbody> </table> </li> </ul> <p>The final cross-validated score is simply the average of these scores:</p> \[\text{CV Score} = \frac{1}{K} \sum_{i=1}^{K} \text{Score}_i\] <p><strong>Why is this so good?</strong></p> <ul> <li> <strong>All Data Used for Training and Testing:</strong> Every data point gets to be in the test set exactly once, and in the training set $K-1$ times. This maximizes the use of your valuable data.</li> <li> <strong>Reduced Variance:</strong> Averaging across multiple splits significantly reduces the impact of a single “lucky” or “unlucky” split, giving you a more stable and reliable performance estimate.</li> <li> <strong>Robustness:</strong> It provides a better indication of how your model will perform on unseen data in the real world.</li> </ul> <p><strong>Choosing K:</strong></p> <ul> <li> <strong>Small K (e.g., K=2 or 3):</strong> Fewer iterations, faster computation, but potentially higher bias (scores might be less reliable). Each test set is a larger proportion of the data, so the model trains on less unique data each time.</li> <li> <strong>Large K (e.g., K=10 or N, where N is total data points):</strong> More iterations, higher computation cost, but lower bias and more robust estimation. K=10 is a widely accepted sweet spot.</li> </ul> <h4 id="stratified-k-fold-cross-validation">Stratified K-Fold Cross-Validation</h4> <p>Imagine you’re trying to predict a rare event, like fraud detection, where fraudulent transactions might only be 1% of your dataset. If you use standard K-Fold, a fold might accidentally end up with <em>no</em> fraudulent transactions, or an unusually high number, leading to skewed evaluation.</p> <p><strong>Stratified K-Fold</strong> addresses this by ensuring that each fold maintains the same proportion of target classes as the original dataset. If fraud is 1% of your data, then each fold will also have approximately 1% fraudulent transactions. This is crucial for evaluating models on imbalanced datasets accurately.</p> <h4 id="leave-one-out-cross-validation-loocv">Leave-One-Out Cross-Validation (LOOCV)</h4> <p>This is an extreme case of K-Fold where $K$ is equal to the number of data points $N$ in your dataset.</p> <p>In LOOCV:</p> <ul> <li>You take one data point as the test set.</li> <li>The remaining $N-1$ data points form the training set.</li> <li>You repeat this process $N$ times, each time leaving out a different single data point for testing.</li> </ul> <p><strong>Pros:</strong> Provides a nearly unbiased estimate of generalization error because you’re training on almost all available data each time. <strong>Cons:</strong> Extremely computationally expensive for large datasets ($N$ iterations!). If you have 100,000 data points, you’re training and testing 100,000 models! It’s rarely used in practice unless your dataset is tiny.</p> <h4 id="time-series-cross-validation-walk-forward-validation">Time Series Cross-Validation (Walk-Forward Validation)</h4> <p>What if your data has a temporal component? For example, predicting stock prices or weather. You can’t just randomly shuffle and split time series data, because doing so would allow your model to “look into the future” (i.e., train on data points that occurred <em>after</em> the data points it’s trying to predict), leading to an unrealistic evaluation.</p> <p>For time series data, we use a technique like <strong>Walk-Forward Validation</strong>. Here’s the general idea:</p> <ul> <li> <strong>Initial Training Period:</strong> Start with an initial chunk of historical data for training.</li> <li> <strong>Validation Period:</strong> Test the model on the next immediate period of data.</li> <li> <strong>Walk Forward:</strong> Then, you ‘walk forward’ in time. You either expand your training data by adding the validation period to it, or you slide both your training and validation windows forward.</li> </ul> <p>This ensures that your model always trains on past data and predicts future data, mimicking real-world deployment.</p> <h3 id="when-to-use-cross-validation">When to Use Cross-Validation</h3> <p>Cross-validation isn’t just a fancy trick; it’s a fundamental part of the machine learning workflow. You should absolutely use it for:</p> <ol> <li> <strong>Model Selection:</strong> When comparing different algorithms (e.g., Logistic Regression vs. Random Forest vs. SVM), cross-validation gives you the most reliable way to determine which algorithm performs best on <em>your</em> data.</li> <li> <strong>Hyperparameter Tuning:</strong> When optimizing the settings of your chosen model (e.g., the number of trees in a Random Forest, the learning rate of a neural network), techniques like Grid Search and Random Search often use cross-validation internally to evaluate each combination of hyperparameters.</li> <li> <strong>Getting a Reliable Performance Estimate:</strong> Before deploying any model to production, you need a robust estimate of how it will perform in the wild. Cross-validation provides just that.</li> </ol> <h3 id="practical-considerations--tips">Practical Considerations &amp; Tips</h3> <ul> <li> <strong>Computational Cost:</strong> Be mindful that cross-validation is more computationally intensive than a single train/test split. For very large datasets, you might start with a smaller K or a simple train/test split for initial experimentation, then move to CV for final evaluation.</li> <li> <strong>Random Seeds:</strong> Always set a random seed for reproducibility when shuffling your data. This ensures that if you (or someone else) runs your code again, the folds will be split in the exact same way.</li> <li> <strong>Feature Scaling/Preprocessing:</strong> Any data preprocessing steps (like feature scaling with <code class="language-plaintext highlighter-rouge">StandardScaler</code> or imputation) should be applied <em>within</em> each cross-validation fold, using only the <em>training data</em> of that fold to fit the preprocessor. Applying it to the entire dataset beforehand can lead to <strong>data leakage</strong>, where information from the test set subtly influences the training process, resulting in overly optimistic scores.</li> <li> <strong>Pipelining:</strong> Libraries like scikit-learn offer <code class="language-plaintext highlighter-rouge">Pipeline</code> objects that neatly encapsulate these preprocessing steps and your model, ensuring proper application within cross-validation loops and preventing data leakage.</li> </ul> <h3 id="conclusion-trust-but-verify">Conclusion: Trust, but Verify</h3> <p>Cross-validation is not just a statistical technique; it’s a mindset. It’s about being rigorous, skeptical of initial successes, and striving for true generalizability in your machine learning models. It transforms your model from a memorizing student into a wise problem-solver, ready to tackle unseen challenges.</p> <p>As you embark on your data science journey, make cross-validation a cornerstone of your evaluation process. It’s a habit every great data scientist cultivates, and it will save you countless headaches, disappointments, and ultimately, build the trust necessary for deploying powerful and effective AI solutions.</p> <p>So, go forth, experiment, build, and most importantly: cross-validate! Your future robust models (and your stakeholders) will thank you.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>