<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Beyond the Predictions: Unveiling the 'Why' with Explainable AI (XAI) | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/beyond-the-predictions-unveiling-the-why-with-expl/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Beyond the Predictions: Unveiling the 'Why' with Explainable AI (XAI)</h1> <p class="post-meta"> Created on October 09, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/explainable-ai"> <i class="fa-solid fa-hashtag fa-sm"></i> Explainable AI</a>   <a href="/blog/blog/tag/xai"> <i class="fa-solid fa-hashtag fa-sm"></i> XAI</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/ai-ethics"> <i class="fa-solid fa-hashtag fa-sm"></i> AI Ethics</a>   <a href="/blog/blog/tag/interpretability"> <i class="fa-solid fa-hashtag fa-sm"></i> Interpretability</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Imagine you’re interacting with a brilliant, insightful colleague. This colleague consistently gives you perfect answers, solves complex problems, and even predicts future trends with uncanny accuracy. Sounds amazing, right? But there’s a catch: when you ask <em>how</em> they arrived at their conclusion, they simply shrug and say, “That’s just what my brain told me.” Frustrating, isn’t it?</p> <p>This, in essence, is the “black box” problem of modern Artificial Intelligence. As data scientists and machine learning engineers, we’ve achieved incredible feats with deep learning, gradient boosting, and other sophisticated models. These algorithms power everything from personalized recommendations and medical diagnoses to fraud detection and self-driving cars. They deliver astonishing performance, often surpassing human capabilities in specific tasks.</p> <p>However, the more powerful and complex these models become, the more opaque their decision-making processes often are. We feed them data, they churn away, and out comes a prediction or an action. But the journey from input to output remains largely a mystery. For simple models like linear regression, we can easily see the contribution of each feature. But try explaining how a neural network with millions of parameters decided that an image contains a cat, or why a credit application was denied by a complex ensemble model. It’s incredibly difficult.</p> <p>This is where <strong>Explainable AI (XAI)</strong> steps in. For me, XAI isn’t just a technical challenge; it’s a fundamental shift in how we build and interact with AI systems. It’s about bridging the gap between powerful algorithms and human understanding, ensuring that our AI companions aren’t just brilliant, but also transparent, accountable, and trustworthy.</p> <h3 id="what-exactly-is-explainable-ai-xai">What Exactly is Explainable AI (XAI)?</h3> <p>At its core, XAI is a set of techniques and methodologies aimed at making AI models more comprehensible to humans. It’s not just about improving accuracy – we’ve largely got that covered – but about increasing the <em>intelligibility</em> of our models.</p> <p>Think of it this way: a traditional machine learning pipeline often focuses on prediction accuracy (how well the model performs) and efficiency (how fast it runs). XAI adds a crucial third dimension: <strong>interpretability</strong> (how well a human can understand why the model made a certain decision).</p> <p>The primary goals of XAI include:</p> <ol> <li> <strong>Trust and Confidence:</strong> If users (doctors, judges, customers) understand <em>why</em> an AI made a decision, they are more likely to trust and adopt it.</li> <li> <strong>Debugging and Improvement:</strong> When a model makes a mistake, an explanation can help us understand <em>where</em> it went wrong, allowing us to fix biases or errors in the data or model architecture.</li> <li> <strong>Fairness and Ethics:</strong> XAI helps us detect and mitigate bias in AI systems, ensuring they don’t perpetuate or amplify societal discrimination.</li> <li> <strong>Regulatory Compliance:</strong> With laws like GDPR granting a “right to explanation,” XAI is becoming a legal necessity in many domains.</li> <li> <strong>Scientific Discovery:</strong> Sometimes, the AI can uncover novel patterns and relationships in data that even human experts hadn’t considered, leading to new insights.</li> </ol> <h3 id="why-do-we-need-xai-the-why-is-crucial">Why Do We Need XAI? The “Why” is Crucial</h3> <p>The necessity of XAI becomes glaringly obvious when we consider real-world applications:</p> <ul> <li> <strong>Healthcare:</strong> Imagine an AI system diagnosing cancer with 99% accuracy. Incredible! But if a doctor can’t understand <em>why</em> the AI flagged a specific region as cancerous – which features in the scan led to that decision – they might be hesitant to trust it, or might not be able to explain it to a patient. A false positive with no explanation could lead to unnecessary biopsies and immense stress.</li> <li> <strong>Finance and Lending:</strong> An AI model denies a loan application. The individual has a legal right to know <em>why</em>. Was it their credit score? Their employment history? A combination of factors? Without XAI, the “black box” simply says “no,” which is unacceptable.</li> <li> <strong>Autonomous Systems:</strong> A self-driving car makes an unexpected maneuver, or worse, causes an accident. Investigators need to understand the precise chain of decisions the AI made: what sensor data it processed, what objects it identified, and what path it chose, and why.</li> <li> <strong>Judicial Systems:</strong> AI being used for recidivism risk assessment. If a model predicts a higher risk for certain demographics due to historical biases in data, XAI can expose this bias, allowing for corrective action. Without it, we risk automating and amplifying injustice.</li> </ul> <p>In all these scenarios, simply knowing <em>what</em> the AI decided isn’t enough. We desperately need to know <em>why</em>.</p> <h3 id="a-peek-inside-the-black-box-techniques-and-approaches">A Peek Inside the Black Box: Techniques and Approaches</h3> <p>XAI methods broadly fall into two categories:</p> <h4 id="1-intrinsic-interpretability-white-box-models">1. Intrinsic Interpretability (White Box Models)</h4> <p>These are models that are inherently understandable due due to their simple structure. We can directly interpret how they make decisions.</p> <ul> <li> <p><strong>Linear Regression:</strong> One of the simplest and most interpretable models. The prediction for a linear model is given by: $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_n x_n$ Here, $\beta_i$ represents the weight or coefficient of feature $x_i$. A positive $\beta_i$ means that an increase in $x_i$ leads to an increase in $y$, and vice-versa. The magnitude of $\beta_i$ indicates the strength of this relationship. It’s straightforward to see the contribution of each feature.</p> </li> <li> <p><strong>Decision Trees:</strong> For small, simple trees, you can literally follow the branches to see the rules that lead to a prediction.</p> <ul> <li> <em>Example:</em> If (age &gt; 30) AND (income &gt; $50k) THEN loan_approved = True.</li> </ul> </li> </ul> <p>While highly interpretable, these models often lack the power and flexibility to capture complex, non-linear relationships present in high-dimensional real-world data. This is where the “black box” models excel, and where post-hoc explanations become crucial.</p> <h4 id="2-post-hoc-explanations-black-box-models">2. Post-Hoc Explanations (Black Box Models)</h4> <p>These techniques are applied <em>after</em> a complex, black-box model (like a deep neural network or a gradient boosting machine) has been trained. They don’t try to simplify the model itself, but rather to explain its behavior or specific predictions.</p> <p>Post-hoc methods can be further divided into:</p> <ul> <li> <strong>Local Explanations:</strong> Explaining a single, specific prediction.</li> <li> <strong>Global Explanations:</strong> Explaining the overall behavior of the model.</li> </ul> <p>Let’s dive into some popular techniques:</p> <h5 id="local-explanations-understanding-individual-decisions">Local Explanations: Understanding Individual Decisions</h5> <h6 id="lime-local-interpretable-model-agnostic-explanations"><strong>LIME (Local Interpretable Model-agnostic Explanations)</strong></h6> <p>LIME’s genius lies in its simplicity and model-agnostic nature. It doesn’t care if your black box is a neural network or an XGBoost model; it treats it as a function that takes an input and returns a prediction.</p> <p><strong>How it works:</strong></p> <ol> <li> <strong>Select an instance:</strong> Pick the specific data point you want to explain.</li> <li> <strong>Perturb the instance:</strong> Create many slightly modified versions of this instance by changing some of its features.</li> <li> <strong>Get predictions:</strong> Feed these perturbed instances to the black-box model and get its predictions.</li> <li> <strong>Weight by proximity:</strong> Assign higher weights to perturbed instances that are closer to the original instance.</li> <li> <strong>Train an interpretable model:</strong> Train a simple, interpretable model (like a linear regression or a shallow decision tree) on this new dataset of perturbed instances and their predictions, weighted by their proximity.</li> </ol> <p>This local, interpretable model then serves as an explanation for the original instance. It tells you which features were most influential in the black box’s decision <em>for that specific prediction</em>. It’s like asking your brilliant, silent colleague, “If I had done <em>this</em> slightly differently, would you still give me the same answer, and why?”</p> <h6 id="shap-shapley-additive-explanations"><strong>SHAP (SHapley Additive exPlanations)</strong></h6> <p>SHAP is arguably one of the most robust and theoretically sound XAI methods, rooted in cooperative game theory (specifically, Shapley values). The idea is to fairly distribute the “payout” (the model’s prediction) among the features, treating each feature as a player in a game.</p> <p><strong>The core idea:</strong> A SHAP value for a feature represents the average marginal contribution of that feature to the prediction, across all possible combinations (coalitions) of features.</p> <p>The SHAP explanation model is an additive feature attribution method: $g(z’) = \phi_0 + \sum_{i=1}^M \phi_i z’_i$ Where:</p> <ul> <li>$g(z’)$ is the explanation model (an interpretable model).</li> <li>$z’$ is a simplified input (e.g., binary representation indicating feature presence).</li> <li>$\phi_0$ is the expected output of the model when no features are present (the baseline).</li> <li>$\phi_i$ is the SHAP value for feature $i$, representing its contribution to the prediction.</li> </ul> <p><strong>Key advantages:</strong></p> <ul> <li> <strong>Consistency:</strong> If a feature genuinely contributes more to a model, its SHAP value will reflect that.</li> <li> <strong>Local Accuracy:</strong> The sum of SHAP values plus the baseline equals the model’s prediction for that instance.</li> <li> <strong>Global Insights:</strong> Individual SHAP values can be aggregated to understand overall feature importance and how features influence predictions across the entire dataset. This allows for creating “summary plots” showing which features are most important globally and how they push the prediction up or down.</li> </ul> <p>SHAP can explain virtually any model and offers a consistent way to quantify feature contributions, making it a powerful tool for both local understanding and global model interpretation.</p> <h5 id="global-explanations-understanding-overall-model-behavior">Global Explanations: Understanding Overall Model Behavior</h5> <p>While local explanations help us understand specific cases, we often need to grasp the general tendencies of our models.</p> <ul> <li> <p><strong>Permutation Feature Importance:</strong> This method measures how much the model’s prediction error increases when the values of a single feature are randomly shuffled (permuted). If shuffling a feature significantly increases the error, that feature is deemed important. It’s model-agnostic and provides a reliable measure of global importance.</p> </li> <li> <strong>Partial Dependence Plots (PDPs):</strong> PDPs show the average relationship between a feature (or two features) and the model’s predicted outcome, marginalizing over all other features. For example, a PDP could show how the probability of loan approval changes as income increases, assuming other features are held constant at their average (or specific) values. <ul> <li>Mathematically, for a prediction function $f(\mathbf{x})$, the partial dependence function for a feature $x_s$ is: $PD_s(x_s) = E_{x_C} [f(x_s, x_C)] = \int f(x_s, x_C) dP(x_C)$ where $x_C$ are all features other than $x_s$. In practice, this expectation is approximated by averaging over the values of $x_C$ in the training data.</li> </ul> </li> <li> <p><strong>Individual Conditional Expectation (ICE) Plots:</strong> ICE plots are similar to PDPs but show the relationship for <em>each individual instance</em> rather than an average. This can reveal heterogeneous effects that a PDP might obscure (e.g., how the effect of income on loan approval might differ for younger vs. older applicants).</p> </li> <li> <strong>Surrogate Models:</strong> Another approach is to train a simpler, interpretable model (like a decision tree or linear model) to mimic the predictions of the complex black-box model. If the surrogate model can achieve a high fidelity to the black-box model, its internal structure can then be used to explain the black-box’s behavior. This provides a “model of the model.”</li> </ul> <h3 id="challenges-and-future-directions">Challenges and Future Directions</h3> <p>Despite the rapid advancements, XAI is not without its challenges:</p> <ul> <li> <strong>The Fidelity-Interpretability Trade-off:</strong> There’s often a tension between a model’s complexity (and thus its accuracy/performance) and its interpretability. Simple models are easy to understand but might not capture all nuances; complex models excel in performance but are opaque. XAI aims to reduce this trade-off but rarely eliminates it entirely.</li> <li> <strong>Context Dependency:</strong> What constitutes a “good” explanation varies greatly depending on the audience. A data scientist might appreciate SHAP values, while a doctor might prefer visual overlays on an image, and a lawyer might need a clear, rule-based explanation.</li> <li> <strong>Computational Cost:</strong> Many XAI methods, especially those involving perturbations or sampling (like LIME and SHAP), can be computationally intensive, particularly for large datasets and complex models.</li> <li> <strong>Misinterpretations and Over-reliance:</strong> Explanations themselves can be misinterpreted or lead to a false sense of security if not used carefully. An explanation for one prediction might not generalize to others, and causality should not be directly inferred without careful consideration.</li> <li> <strong>Adversarial Explanations:</strong> Just as models can be attacked, explanations themselves can potentially be manipulated to mislead users.</li> </ul> <p>The field of XAI is still evolving rapidly. Future directions include:</p> <ul> <li> <strong>Human-Centric XAI:</strong> Developing explanations tailored to specific human needs and cognitive processes.</li> <li> <strong>Causal XAI:</strong> Moving beyond correlations to identify causal relationships.</li> <li> <strong>Explainable Reinforcement Learning:</strong> Applying XAI to agents that learn through interaction.</li> <li> <strong>Standardization and Benchmarking:</strong> Establishing common metrics and benchmarks for evaluating the quality of explanations.</li> <li> <strong>Integration into ML Workflows:</strong> Making XAI a standard, inherent part of the machine learning development lifecycle, not just an afterthought.</li> </ul> <h3 id="conclusion-building-trust-in-the-age-of-ai">Conclusion: Building Trust in the Age of AI</h3> <p>Explainable AI is no longer a niche research area; it’s a critical component for the responsible development and deployment of AI systems. As AI becomes more ubiquitous and impacts every facet of our lives, the ability to understand, trust, and control these powerful technologies is paramount.</p> <p>For me, working in data science and machine learning is about more than just building impressive predictive models. It’s about building <em>responsible</em> ones. XAI is the key to unlocking the true potential of AI, transforming it from a mysterious oracle into a trusted, transparent partner. It empowers us to debug, to ensure fairness, to comply with regulations, and ultimately, to learn from our own creations.</p> <p>The journey to fully transparent AI is long and complex, but with tools like LIME, SHAP, and PDPs, we’re making significant strides. Embracing XAI is not just about technical innovation; it’s about ethical imperative. It’s about ensuring that as AI advances, humanity remains in control, understanding the ‘why’ behind every ‘what’. And that, I believe, is a future worth building.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>