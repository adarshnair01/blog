<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Goldilocks Conundrum: Navigating Overfitting and Underfitting in Machine Learning | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/the-goldilocks-conundrum-navigating-overfitting-an/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Goldilocks Conundrum: Navigating Overfitting and Underfitting in Machine Learning</h1> <p class="post-meta"> Created on September 11, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/overfitting"> <i class="fa-solid fa-hashtag fa-sm"></i> Overfitting</a>   <a href="/blog/blog/tag/underfitting"> <i class="fa-solid fa-hashtag fa-sm"></i> Underfitting</a>   <a href="/blog/blog/tag/model-evaluation"> <i class="fa-solid fa-hashtag fa-sm"></i> Model Evaluation</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey fellow data explorers!</p> <p>It’s a beautiful day, and as I sit here sipping my coffee, reflecting on the countless models I’ve built (and broken!), one fundamental concept keeps resurfacing: the delicate dance between <strong>overfitting</strong> and <strong>underfitting</strong>. It’s not just a theoretical hurdle; it’s a real-world dilemma that every aspiring data scientist or MLE will face, repeatedly. Think of it as the Goldilocks problem of machine learning: finding the model that’s “just right.”</p> <p>I remember my early days, staring at perfect training accuracy, only to see my model crumble when presented with new data. Frustrating, right? Or the equally baffling scenario where my model couldn’t even learn the training data itself. Today, I want to unpack these two common pitfalls, explore why they happen, and arm you with strategies to navigate them like a pro.</p> <h3 id="the-machines-learning-journey-more-than-just-memorization">The Machine’s Learning Journey: More Than Just Memorization</h3> <p>At its heart, machine learning is about teaching computers to recognize patterns and make predictions or decisions based on data, without being explicitly programmed for every single scenario. We feed our models data, they “learn” from it, and then we expect them to perform well on <em>new, unseen data</em>. This last part, performing well on unseen data, is crucial. It’s called <strong>generalization</strong>.</p> <p>Imagine you’re studying for an exam. You want to understand the concepts, apply them to new problems, and get a good grade. You don’t just want to memorize the answers to the practice questions. If you only memorize, you’re likely to fail when the exam asks a slightly different question, even if it tests the same underlying concept. This analogy perfectly sets the stage for our two antagonists.</p> <hr> <h3 id="underfitting-the-lazy-student-high-bias">Underfitting: The Lazy Student (High Bias)</h3> <p>Let’s start with underfitting. This is when your model is like that lazy student who barely studies. It’s too simple, too rigid, or simply hasn’t learned enough from the training data to capture the underlying patterns.</p> <p><strong>What it looks like:</strong></p> <p>Imagine you have a dataset showing the relationship between hours studied and exam scores, and this relationship is actually quite curvy (e.g., initially slow improvement, then rapid, then diminishing returns). If you try to fit a simple straight line (a linear regression model) to this curved data, your line will never accurately represent the true pattern. It will consistently miss the mark.</p> <ul> <li> <strong>Visually:</strong> Think of a straight line trying to approximate a wave. It might get the general trend, but it will fail to capture the peaks and troughs.</li> <li> <strong>Performance:</strong> A tell-tale sign of an underfit model is that it performs poorly on <em>both</em> the training data and the test (unseen) data. Its accuracy will be low across the board. The model hasn’t even learned the examples it <em>has</em> seen, let alone new ones.</li> </ul> <p><strong>Why it happens:</strong></p> <ol> <li> <strong>Model is too simple:</strong> Using a linear model for inherently non-linear data.</li> <li> <strong>Insufficient features:</strong> Not providing enough relevant information to the model. If our “exam score” model only knew “hours studied” but missed “prior knowledge” or “sleep,” it’s missing key context.</li> <li> <strong>Too much regularization:</strong> We’ll talk about regularization later, but applying too much of it can constrain the model excessively, preventing it from learning.</li> <li> <strong>Insufficient training:</strong> Sometimes, the model just hasn’t had enough time or iterations to learn.</li> </ol> <p>In technical terms, an underfit model has <strong>high bias</strong>. Bias refers to the simplifying assumptions made by the model to make the target function easier to learn. A high-bias model makes too many assumptions or too strong assumptions, leading to a systematic error in prediction regardless of the specific data.</p> <p><strong>How to fix it:</strong></p> <ul> <li> <strong>Increase model complexity:</strong> Use a more sophisticated algorithm (e.g., switch from linear regression to polynomial regression, Decision Trees, or Neural Networks).</li> <li> <strong>Add more features:</strong> Provide more relevant input variables to your model. Feature engineering can be a game-changer here!</li> <li> <strong>Decrease regularization:</strong> If you’ve applied regularization, try reducing its strength.</li> <li> <strong>Train longer:</strong> For iterative models (like neural networks), sometimes a few more epochs can help.</li> </ul> <hr> <h3 id="overfitting-the-overzealous-memorizer-high-variance">Overfitting: The Overzealous Memorizer (High Variance)</h3> <p>Now, let’s consider the other extreme: overfitting. This is when your model is like that student who memorized every single practice question, including the typos, without understanding the underlying concepts. When the actual exam asks a question phrased even slightly differently, they’re lost.</p> <p><strong>What it looks like:</strong></p> <p>Imagine you have those same hours-studied vs. exam-score data points. An overfit model would draw an incredibly complex, squiggly line that perfectly passes through <em>every single data point</em> in your training set. It captures not just the true underlying pattern, but also the random noise, outliers, and peculiarities specific to <em>that particular training set</em>.</p> <ul> <li> <strong>Visually:</strong> A line that contorts itself to hit every single point, rather than finding a smooth, general trend. It looks “too good to be true” on the training data.</li> <li> <strong>Performance:</strong> The hallmark of an overfit model is excellent performance (very high accuracy, very low error) on the training data, but abysmal performance on new, unseen data. It has memorized the training set but failed to generalize.</li> </ul> <p><strong>Why it happens:</strong></p> <ol> <li> <strong>Model is too complex:</strong> Using a highly flexible model (e.g., a deep neural network with many layers, or a decision tree grown to its full depth) on a relatively small dataset.</li> <li> <strong>Too much noise in data:</strong> If your training data contains a lot of irrelevant information or errors, an overfit model will try to learn even that noise.</li> <li> <strong>Insufficient data:</strong> With too few examples, it’s easy for a complex model to just “memorize” them all.</li> <li> <strong>Too many features:</strong> Including too many features, especially irrelevant ones, can give the model too many degrees of freedom to fit to noise.</li> </ol> <p>Technically, an overfit model has <strong>high variance</strong>. Variance refers to the model’s sensitivity to small fluctuations in the training data. A high-variance model will learn a very specific (and often noisy) pattern from the training data, leading to large differences in predictions if it were trained on a slightly different dataset.</p> <p><strong>How to fix it:</strong></p> <p>This is where things get interesting, and we have a whole arsenal of techniques!</p> <ol> <li> <strong>More Data:</strong> The simplest (though not always easiest) solution. More diverse training data helps the model see the true patterns amidst the noise.</li> <li> <strong>Simplify the Model:</strong> <ul> <li>Reduce the number of features (feature selection).</li> <li>Use a simpler algorithm (e.g., reduce the number of layers in a neural network, prune a decision tree).</li> </ul> </li> <li> <strong>Regularization:</strong> This is a powerful technique that discourages learning overly complex models. It adds a penalty term to the loss function during training. <ul> <li> <strong>Lasso (L1 Regularization):</strong> Adds the absolute value of the magnitude of coefficients to the loss function. It can lead to sparse models, effectively performing feature selection by shrinking some coefficients to zero. $ \text{Loss} = \sum_{i=1}^N (y_i - \hat{y}<em>i)^2 + \lambda \sum</em>{j=1}^M |\beta_j| $ Where $\lambda$ is the regularization strength, and $\beta_j$ are the model coefficients.</li> <li> <strong>Ridge (L2 Regularization):</strong> Adds the squared magnitude of coefficients to the loss function. It shrinks coefficients towards zero but rarely makes them exactly zero. $ \text{Loss} = \sum_{i=1}^N (y_i - \hat{y}<em>i)^2 + \lambda \sum</em>{j=1}^M \beta_j^2 $ The $\lambda$ (lambda) parameter is crucial here; it controls how much we penalize complexity. A larger $\lambda$ means more penalty, leading to a simpler model.</li> </ul> </li> <li> <p><strong>Cross-Validation:</strong> Instead of a single train/test split, cross-validation (e.g., k-fold cross-validation) helps you get a more robust estimate of how well your model generalizes by training and testing on different subsets of your data multiple times. This helps detect overfitting by checking consistency across folds.</p> </li> <li> <p><strong>Early Stopping:</strong> For iterative models, you can monitor the model’s performance on a separate validation set during training. When the validation error starts to increase (even if training error is still decreasing), that’s often the sign of overfitting beginning, and you can stop training early.</p> </li> <li> <p><strong>Feature Engineering/Selection:</strong> Carefully selecting or creating features that are truly relevant to the problem can reduce noise and help the model focus on important patterns.</p> </li> <li> <strong>Ensemble Methods:</strong> Techniques like Bagging (e.g., Random Forests) or Boosting (e.g., Gradient Boosting Machines) combine multiple models to reduce variance and improve generalization.</li> </ol> <hr> <h3 id="the-goldilocks-zone-just-right-the-bias-variance-trade-off">The Goldilocks Zone: Just Right (The Bias-Variance Trade-off)</h3> <p>So, we have underfitting (too simple, high bias) and overfitting (too complex, high variance). Our goal is to find that “just right” spot in the middle, where the model is complex enough to capture the true underlying patterns but not so complex that it starts memorizing noise. This is famously known as the <strong>Bias-Variance Trade-off</strong>.</p> <p><strong>Total Error</strong> = <strong>Bias²</strong> + <strong>Variance</strong> + <strong>Irreducible Error</strong></p> <ul> <li> <strong>Bias:</strong> Error from erroneous assumptions in the learning algorithm. High bias leads to underfitting.</li> <li> <strong>Variance:</strong> Error from sensitivity to small fluctuations in the training set. High variance leads to overfitting.</li> <li> <strong>Irreducible Error:</strong> Error that cannot be reduced by any model. It’s inherent noise in the data itself (e.g., random measurement errors).</li> </ul> <p>Our mission, should we choose to accept it, is to minimize the sum of Bias² and Variance. As you increase model complexity, bias typically decreases (it can learn more), but variance increases (it becomes more sensitive to specific training data). Conversely, as you decrease complexity, bias increases, and variance decreases. We’re looking for the sweet spot where the combined error is at its minimum.</p> <h3 id="practical-implications-and-my-personal-approach">Practical Implications and My Personal Approach</h3> <p>When I’m building a model, this trade-off is always at the forefront of my mind.</p> <ol> <li> <strong>Data Splitting is Key:</strong> Always split your data into (at least) training, validation, and test sets. <ul> <li> <strong>Training Set:</strong> Used to train the model.</li> <li> <strong>Validation Set:</strong> Used to tune hyperparameters and check for overfitting during training (e.g., for early stopping). Never train on this!</li> <li> <strong>Test Set:</strong> A completely held-out set used <em>only once</em> at the very end to evaluate the final model’s true generalization performance.</li> </ul> </li> <li> <strong>Monitor Performance Curves:</strong> I always plot training loss/accuracy against validation loss/accuracy. <ul> <li>If both are high and flat: <strong>Underfitting</strong>.</li> <li>If training loss goes down but validation loss goes up: <strong>Overfitting</strong>.</li> <li>If both go down and converge to a low value: <strong>Just right!</strong> </li> </ul> </li> <li> <strong>Hyperparameter Tuning:</strong> This is where you adjust parameters like $\lambda$ in regularization, tree depth in a Decision Tree, or the number of layers in a Neural Network. Grid search, random search, or more advanced optimization techniques help find the best balance on the validation set.</li> </ol> <p>It’s an iterative process. You build a model, evaluate it, identify if it’s underfitting or overfitting, apply a suitable technique, and repeat. There’s no one-size-fits-all solution, but understanding the underlying problem empowers you to choose the right tool for the job.</p> <h3 id="wrapping-up">Wrapping Up</h3> <p>Navigating overfitting and underfitting is a core skill in machine learning. It’s about building models that are intelligent enough to learn true patterns but humble enough to admit when they don’t know something or when a particular detail is just noise. It’s about finding that delicate balance, that “Goldilocks Zone,” where our models can truly generalize and be valuable in the real world.</p> <p>So, the next time your model isn’t performing as expected, don’t despair! Take a deep breath, recall the lazy student and the overzealous memorizer, and systematically apply the techniques we’ve discussed. Your journey to becoming a skilled ML practitioner lies in mastering this fundamental challenge.</p> <p>Happy modeling!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>