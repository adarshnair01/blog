<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Art of Drawing the 'Best' Line: Unpacking Support Vector Machines | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/2024/the-art-of-drawing-the-best-line-unpacking-support/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Art of Drawing the 'Best' Line: Unpacking Support Vector Machines</h1> <p class="post-meta"> Created on December 17, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/classification"> <i class="fa-solid fa-hashtag fa-sm"></i> Classification</a>   <a href="/blog/blog/tag/support-vector-machines"> <i class="fa-solid fa-hashtag fa-sm"></i> Support Vector Machines</a>   <a href="/blog/blog/tag/svm"> <i class="fa-solid fa-hashtag fa-sm"></i> SVM</a>   <a href="/blog/blog/tag/supervised-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Supervised Learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hello fellow data adventurers!</p> <p>Have you ever looked at a messy scatter plot of data points, perhaps some representing ‘spam’ and others ‘not spam’, and tried to draw a line to separate them? It seems simple enough, right? Just draw a line between the two groups. But then you might ask yourself, “Which line is the <em>best</em> line?”</p> <p>This seemingly simple question is at the heart of many classification problems in machine learning. While simpler algorithms like Logistic Regression can draw a line, they don’t always pick the most <em>robust</em> one. This is where Support Vector Machines (SVMs) step onto the stage, not just drawing <em>any</em> line, but finding the <em>optimal</em> one.</p> <p>Let’s embark on a journey to demystify SVMs, starting from the simplest ideas and building up to their sophisticated capabilities.</p> <h3 id="the-core-idea-finding-the-best-separator">The Core Idea: Finding the “Best” Separator</h3> <p>Imagine you have a bunch of red dots and blue dots scattered on a piece of paper. Your task is to draw a straight line that separates the reds from the blues.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      . Blue
  .
.        Line A
.  .          \
----------------- Line B
          .  Red
            .
</code></pre></div></div> <p>You could probably draw many lines that perfectly separate them. But look closely: some lines might be very close to a few dots, while others might sit comfortably in the middle of the two groups.</p> <p>SVMs aim for that “comfortable middle ground.” They don’t just find <em>a</em> separating line; they find the separating line (or “hyperplane” in higher dimensions) that has the largest possible distance to the nearest training data points of any class. This distance is called the <strong>margin</strong>.</p> <p>Why maximize the margin? Think about it this way: a larger margin means that the decision boundary is further away from the data points it’s trying to classify. This makes the model more robust to new, unseen data. If a new data point is slightly perturbed, it’s less likely to cross the decision boundary and be misclassified if the margin is wide. It’s like building a wider road: less chance of veering off course!</p> <h3 id="the-mathematics-of-the-margin-the-hard-margin-svm">The Mathematics of the Margin: The Hard Margin SVM</h3> <p>Let’s get a little more formal. In a 2-dimensional space, our separating line can be represented by the equation $w_1 x_1 + w_2 x_2 + b = 0$. More generally, in an N-dimensional space, a hyperplane is defined by:</p> <p>$w \cdot x + b = 0$</p> <p>where $w$ is the normal vector to the hyperplane, $x$ is a data point, and $b$ is the bias.</p> <p>For any given data point $x_i$, its class $y_i$ will be either $+1$ (e.g., ‘blue’) or $-1$ (e.g., ‘red’). The SVM’s goal is to find $w$ and $b$ such that for all training points:</p> <ul> <li>If $y_i = +1$, then $w \cdot x_i + b \ge +1$</li> <li>If $y_i = -1$, then $w \cdot x_i + b \le -1$</li> </ul> <p>We can combine these into a single inequality:</p> <p>$y_i (w \cdot x_i + b) \ge 1$</p> <p>The points that lie exactly on $w \cdot x_i + b = +1$ and $w \cdot x_i + b = -1$ are called the <strong>Support Vectors</strong>. These are the crucial data points that “support” the hyperplane and define the margin. If you were to remove any other data point, the optimal hyperplane wouldn’t change. But remove a support vector, and the hyperplane would likely shift.</p> <table> <tbody> <tr> <td>The distance between these two hyperplanes ($w \cdot x + b = +1$ and $w \cdot x + b = -1$) is the width of the margin. This width can be mathematically shown to be $\frac{2}{</td> <td> </td> <td>w</td> <td> </td> <td>}$.</td> </tr> </tbody> </table> <table> <tbody> <tr> <td>To maximize this margin, we need to minimize $</td> <td> </td> <td>w</td> <td> </td> <td>$. For computational convenience, we usually minimize $\frac{1}{2}</td> <td> </td> <td>w</td> <td> </td> <td>^2$.</td> </tr> </tbody> </table> <p>So, the optimization problem for a <strong>Hard Margin SVM</strong> looks like this:</p> <p>Minimize $\frac{1}{2} ||w||^2$ Subject to $y_i (w \cdot x_i + b) \ge 1$ for all $i=1, \dots, N$</p> <p>This is a convex optimization problem, meaning there’s a unique global minimum, and it can be solved efficiently using techniques like quadratic programming, often involving Lagrange multipliers.</p> <h3 id="when-life-isnt-linearly-separable-the-soft-margin-svm">When Life Isn’t Linearly Separable: The Soft Margin SVM</h3> <p>The “Hard Margin” SVM works wonderfully if your data is perfectly linearly separable. But let’s be realistic: most real-world data is messy. You’ll often find overlapping points, outliers, or simply data that can’t be perfectly divided by a straight line.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      . Blue
  .  X
.  .          \
-----X---------- Line (still trying!)
          .  Red
            .
</code></pre></div></div> <p>(Where ‘X’ is a misplaced point)</p> <p>If we insist on a perfect separation, a Hard Margin SVM would fail to find a solution, or it would create a very complex, wiggly boundary that overfits the training data. This is where the <strong>Soft Margin SVM</strong> comes to the rescue.</p> <p>The idea is simple: allow some misclassifications or points to fall within the margin, but penalize them. We introduce <strong>slack variables</strong>, $\xi_i$ (Greek letter “xi”), for each data point $x_i$.</p> <ul> <li>If $\xi_i = 0$, the point is correctly classified and outside the margin.</li> <li>If $0 &lt; \xi_i &lt; 1$, the point is correctly classified but <em>inside</em> the margin.</li> <li>If $\xi_i \ge 1$, the point is misclassified.</li> </ul> <p>Our modified optimization problem becomes:</p> <p>Minimize $\frac{1}{2} ||w||^2 + C \sum_{i=1}^{N} \xi_i$ Subject to $y_i (w \cdot x_i + b) \ge 1 - \xi_i$ And $\xi_i \ge 0$ for all $i=1, \dots, N$</p> <p>Here, $C$ is a crucial hyperparameter, often called the <strong>regularization parameter</strong>.</p> <ul> <li> <strong>Small $C$</strong>: We allow more misclassifications (higher $\xi_i$ values). This leads to a wider margin, a simpler decision boundary, and might underfit if too small.</li> <li> <strong>Large $C$</strong>: We heavily penalize misclassifications. This pushes towards a hard margin, potentially a narrower margin, and a more complex boundary that might overfit if too large.</li> </ul> <p>The parameter $C$ allows us to control the trade-off between maximizing the margin (simplicity, generalization) and minimizing classification errors on the training data (accuracy on training). This is a classic example of the <strong>bias-variance trade-off</strong>.</p> <h3 id="beyond-lines-the-kernel-trick">Beyond Lines: The Kernel Trick!</h3> <p>Even with soft margins, we’re still stuck with <em>linear</em> decision boundaries. What if your data looks like concentric circles, or some other non-linear pattern? No straight line or hyperplane can separate these effectively in their original space.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>       . Red .
    . Red   Red .
  .  Blue     Blue  .
 . Blue         Blue .
  .  Blue     Blue  .
    . Red   Red .
       . Red .
</code></pre></div></div> <p>This is where SVMs unleash their most powerful weapon: the <strong>Kernel Trick</strong>.</p> <p>The core idea of the Kernel Trick is to transform our data into a higher-dimensional space where it <em>becomes</em> linearly separable.</p> <p>Imagine you have 2D data that forms two concentric circles (like the example above). No 2D line can separate them. But what if we could project this data into 3D? We might find that in this new 3D space, the inner circle’s points are on one “level” and the outer circle’s points are on another, allowing a flat plane (a hyperplane!) to separate them.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Original 2D space: x1, x2
Transformed 3D space: x1, x2, x1^2 + x2^2 (e.g.)
</code></pre></div></div> <p>The magic of the “kernel trick” is that we don’t actually need to compute the coordinates of the data points in this higher-dimensional space. Instead, we compute the <em>dot product</em> of the transformed vectors directly using a <strong>kernel function</strong>.</p> <p>The original optimization problem (when solved using its dual formulation) relies heavily on calculating dot products of data points: $\langle x_i, x_j \rangle$. The kernel trick replaces these dot products with a kernel function $K(x_i, x_j) = \phi(x_i) \cdot \phi(x_j)$, where $\phi$ is the mapping to the higher-dimensional space. We never explicitly compute $\phi(x_i)$! This saves immense computational cost.</p> <p>Some common kernel functions include:</p> <ol> <li> <strong>Linear Kernel</strong>: $K(x_i, x_j) = x_i^T x_j$ <ul> <li>This is equivalent to the linear SVM we discussed earlier.</li> </ul> </li> <li> <strong>Polynomial Kernel</strong>: $K(x_i, x_j) = (\gamma x_i^T x_j + r)^d$ <ul> <li> <code class="language-plaintext highlighter-rouge">d</code> is the degree of the polynomial. This creates polynomial decision boundaries.</li> </ul> </li> <li> <strong>Radial Basis Function (RBF) Kernel</strong> (also known as Gaussian Kernel): $K(x_i, x_j) = \exp(-\gamma ||x_i - x_j||^2)$ <ul> <li>This is arguably the most popular kernel. It essentially measures the similarity between two points based on their distance. <code class="language-plaintext highlighter-rouge">gamma</code> ($\gamma$) is another crucial hyperparameter here: <ul> <li> <strong>Large $\gamma$</strong>: A small radius of influence for each support vector. This leads to a very complex, “wiggly” decision boundary that tries to perfectly fit the training data. High chance of overfitting.</li> <li> <strong>Small $\gamma$</strong>: A large radius of influence. This results in a smoother, simpler decision boundary, potentially underfitting if too small.</li> </ul> </li> </ul> </li> </ol> <p>By combining the $C$ parameter (for soft margin) and $\gamma$ (for RBF kernel complexity), we gain immense flexibility and power to model a wide variety of non-linear relationships in data.</p> <h3 id="why-are-svms-still-relevant-and-their-gotchas">Why Are SVMs Still Relevant? (And Their Gotchas)</h3> <p>Despite the rise of deep learning, SVMs remain a powerful tool in a data scientist’s arsenal, especially for certain types of problems.</p> <p><strong>Pros:</strong></p> <ul> <li> <strong>Effective in high-dimensional spaces</strong>: They work well even when the number of features exceeds the number of samples.</li> <li> <strong>Memory efficient</strong>: Because they only use a subset of training points (the support vectors) in the decision function, they are very memory efficient.</li> <li> <strong>Versatile with kernels</strong>: Different kernel functions allow for great flexibility in modeling various decision boundaries.</li> <li> <strong>Robust to outliers</strong>: With a soft margin, they are less affected by individual noisy points.</li> </ul> <p><strong>Cons:</strong></p> <ul> <li> <strong>Computationally intensive for very large datasets</strong>: Training time can increase significantly with the number of samples, especially without specialized solvers or approximations.</li> <li> <strong>Sensitive to feature scaling</strong>: The kernel functions (especially RBF) are sensitive to the magnitude of the features. It’s crucial to normalize or standardize your data before using SVMs.</li> <li> <strong>Less interpretable</strong>: Understanding <em>why</em> an SVM made a particular decision can be harder than with, say, a decision tree. They don’t directly provide probability estimates easily.</li> <li> <strong>Parameter tuning</strong>: Choosing the right $C$ and kernel hyperparameters (like $\gamma$) can be challenging and often requires techniques like cross-validation and grid search.</li> </ul> <h3 id="a-glimpse-into-the-code-with-scikit-learn">A Glimpse into the Code (with Scikit-learn)</h3> <p>In Python, implementing an SVM is surprisingly straightforward thanks to libraries like Scikit-learn:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>
<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="n">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Generate some synthetic data for demonstration
</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="nf">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_redundant</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                           <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">n_clusters_per_class</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Split data into training and test sets
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Create an SVM classifier with an RBF kernel
# C=1.0 is a common starting point, gamma='scale' uses 1 / (n_features * X.var())
</span><span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="p">.</span><span class="nc">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="sh">'</span><span class="s">rbf</span><span class="sh">'</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="sh">'</span><span class="s">scale</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Train the model
</span><span class="n">clf</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Make predictions
</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Test Accuracy: </span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">y_pred</span> <span class="o">==</span> <span class="n">y_test</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">%</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># --- Visualizing the decision boundary (for 2D data) ---
# Create a mesh to plot the decision boundary
</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">].</span><span class="nf">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">].</span><span class="nf">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">].</span><span class="nf">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">].</span><span class="nf">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">),</span>
                     <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">))</span>

<span class="c1"># Predict the class for each point in the mesh
</span><span class="n">Z</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="p">.</span><span class="nf">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="p">.</span><span class="nf">ravel</span><span class="p">()])</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">xx</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">contourf</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="p">.</span><span class="n">cm</span><span class="p">.</span><span class="n">coolwarm</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="p">.</span><span class="n">cm</span><span class="p">.</span><span class="n">coolwarm</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">SVM Decision Boundary with RBF Kernel</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Feature 1</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Feature 2</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <p>This snippet shows you how simple it is to instantiate an <code class="language-plaintext highlighter-rouge">SVC</code> (Support Vector Classifier), choose your <code class="language-plaintext highlighter-rouge">kernel</code> (e.g., ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’), and set your <code class="language-plaintext highlighter-rouge">C</code> and <code class="language-plaintext highlighter-rouge">gamma</code> (if using RBF) parameters. The power is encapsulated within these few lines!</p> <h3 id="conclusion-the-elegant-separator">Conclusion: The Elegant Separator</h3> <p>From drawing a simple line to defining complex decision boundaries in high-dimensional spaces, Support Vector Machines offer an elegant and robust approach to classification. Their focus on maximizing the margin, coupled with the ingenious kernel trick, makes them incredibly versatile.</p> <p>While newer algorithms and deep learning models have gained popularity, SVMs remain a foundational concept and a powerful, interpretable choice for many supervised learning tasks, especially with structured, tabular data where feature engineering plays a significant role. Understanding how they work fundamentally deepens your grasp of machine learning principles like optimization, regularization, and feature transformation.</p> <p>So, the next time you see a classification problem, remember the “best line” and the clever machines that draw it – the Support Vector Machines! Keep exploring, keep learning, and keep asking “what’s the best way?”</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>