<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Dancing Through Noise: How Kalman Filters Make Sense of Our Messy World | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/dancing-through-noise-how-kalman-filters-make-sens/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Dancing Through Noise: How Kalman Filters Make Sense of Our Messy World</h1> <p class="post-meta"> Created on July 22, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/kalman-filter"> <i class="fa-solid fa-hashtag fa-sm"></i> Kalman Filter</a>   <a href="/blog/blog/tag/state-estimation"> <i class="fa-solid fa-hashtag fa-sm"></i> State Estimation</a>   <a href="/blog/blog/tag/sensor-fusion"> <i class="fa-solid fa-hashtag fa-sm"></i> Sensor Fusion</a>   <a href="/blog/blog/tag/time-series"> <i class="fa-solid fa-hashtag fa-sm"></i> Time Series</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>From the moment I first encountered the Kalman Filter, I was captivated. It felt like uncovering a secret superpower, an elegant mathematical dance that could cut through the cacophony of noisy data to reveal the underlying truth. As someone diving deep into data science and machine learning, this algorithm immediately struck me as a foundational piece of knowledge – a quiet hero working behind the scenes in countless applications we take for granted every day.</p> <p>In this post, I want to take you on my journey of understanding the Kalman Filter, breaking down its magic into digestible parts. Whether you’re a high school student curious about how your phone tracks your runs, or a fellow data science enthusiast looking to deepen your toolkit, my hope is that you’ll come away with a solid intuition for this remarkable piece of engineering.</p> <h3 id="the-ever-present-challenge-living-in-a-noisy-world">The Ever-Present Challenge: Living in a Noisy World</h3> <p>Imagine you’re trying to track something simple, like a ball rolling across a table. You have two sources of information:</p> <ol> <li> <strong>Your “Brain Model”:</strong> Based on physics, you can <em>predict</em> where the ball should be next, given its current position and velocity. (e.g., “It’s rolling at 1 meter/second, so in 1 second, it will be 1 meter further.”)</li> <li> <strong>Your “Eyes”:</strong> You can <em>observe</em> where the ball is with a camera or your own vision. (e.g., “My eyes tell me it’s at position X.”)</li> </ol> <p>Sounds straightforward, right? Not quite.</p> <p>The real world is messy.</p> <ul> <li> <strong>Your brain model isn’t perfect.</strong> The table might have tiny bumps, air resistance could be a factor, or your initial estimate of the ball’s speed might be slightly off. Your prediction has some <strong>process noise</strong> or uncertainty.</li> <li> <strong>Your eyes aren’t perfect either.</strong> The camera might have blurry pixels, lighting could be bad, or your own vision might be slightly inaccurate. Your measurement has some <strong>measurement noise</strong> or uncertainty.</li> </ul> <p>So, at any given moment, your prediction tells you one thing, and your measurement tells you another. Which one do you trust more? How do you combine them to get the <em>best possible estimate</em> of the ball’s true position? This is the fundamental problem the Kalman Filter was designed to solve.</p> <h3 id="the-core-idea-trust-and-blending">The Core Idea: Trust and Blending</h3> <p>At its heart, the Kalman Filter is an optimal estimator. “Optimal” here means it minimizes the mean squared error of the estimate, assuming certain conditions (which we’ll touch on later). Its brilliance lies in a continuous, iterative cycle of <strong>predicting</strong> where a system will be and then <strong>updating</strong> that prediction with actual measurements.</p> <p>Think of it like this:</p> <p>You have an idea of where something <em>should</em> be (your prediction). Then you get new information from a sensor (your measurement). You don’t just throw away your prediction and trust the sensor blindly, nor do you ignore the sensor and trust only your prediction. Instead, you <em>blend</em> them, with more weight given to the source you currently “trust” more. This “trust” is quantified by the uncertainty (or covariance) associated with each piece of information.</p> <h3 id="the-two-step-dance-predict-and-update">The Two-Step Dance: Predict and Update</h3> <p>The Kalman Filter operates in a continuous loop, always alternating between two phases:</p> <h4 id="phase-1-the-predict-step-time-update">Phase 1: The Predict Step (Time Update)</h4> <p>This is where the filter looks into the future (or rather, its internal model of the future). Based on the system’s previous estimated state and known inputs (like a robot motor command), it predicts the current state. Crucially, it also predicts how much more <em>uncertain</em> this prediction is compared to the previous state. Uncertainty always grows with prediction.</p> <p>Let’s introduce some basic notation:</p> <ul> <li>$\hat{x}_{k-1}$ is our best estimate of the system’s state at time $k-1$. This “state” could be anything we want to track: position, velocity, temperature, etc. It’s often a vector.</li> <li>$P_{k-1}$ is the covariance matrix representing the uncertainty of our state estimate $\hat{x}_{k-1}$. A smaller covariance means higher confidence.</li> </ul> <p><strong>Predicting the State:</strong> Our system has a model of how it evolves over time. For a linear system, this is represented by a state transition matrix $F_k$. We also might have external control inputs $u_k$ (like a motor command) that influence the system, handled by the control input matrix $B_k$.</p> <p>The predicted state $\hat{x}_k^-$ (the ‘minus’ superscript means it’s a <em>prior</em> estimate, before incorporating the current measurement) is calculated as:</p> <p>$\hat{x}<em>k^- = F_k \hat{x}</em>{k-1} + B_k u_k$</p> <ul> <li>$F_k$: The state transition model, applied to the previous state.</li> <li>$B_k$: The control input model, applied to the control vector $u_k$.</li> </ul> <p><strong>Predicting the Uncertainty (Covariance):</strong> As mentioned, uncertainty grows with prediction. We update the covariance matrix $P_k^-$ accordingly:</p> <p>$P_k^- = F_k P_{k-1} F_k^T + Q_k$</p> <ul> <li>$F_k P_{k-1} F_k^T$: This term propagates the <em>previous</em> state’s uncertainty forward through our model.</li> <li>$Q_k$: This is the process noise covariance matrix. It accounts for the uncertainty in our <em>model itself</em> (e.g., unmodeled disturbances, approximations). It adds uncertainty to our prediction.</li> </ul> <p>At the end of the predict step, we have a new predicted state ($\hat{x}_k^-$) and its associated uncertainty ($P_k^-$). This is our <em>prior</em> belief about the system’s state.</p> <h4 id="phase-2-the-update-step-measurement-update">Phase 2: The Update Step (Measurement Update)</h4> <p>Now, a new measurement $z_k$ arrives from our sensor. This is our <em>new piece of evidence</em>. The update step’s job is to refine our prior prediction $\hat{x}_k^-$ by incorporating this measurement.</p> <p><strong>1. Calculate the Innovation (Measurement Residual):</strong> The first thing we do is figure out how “surprised” we are by the new measurement. We compare the actual measurement $z_k$ with what our model <em>predicted</em> the measurement should be, based on our prior state estimate $\hat{x}_k^-$. The measurement matrix $H_k$ transforms the state space into the measurement space.</p> <p>$y_k = z_k - H_k \hat{x}_k^-$</p> <ul> <li>$y_k$: This is the innovation or measurement residual. It’s the difference between the actual measurement and the predicted measurement. If $y_k$ is small, our prediction was good. If it’s large, something is off.</li> </ul> <p><strong>2. Calculate the Innovation (or Residual) Covariance:</strong> We also need to know the uncertainty associated with this innovation. This combines the uncertainty from our predicted state with the uncertainty inherent in the measurement itself.</p> <p>$S_k = H_k P_k^- H_k^T + R_k$</p> <ul> <li>$H_k P_k^- H_k^T$: This propagates our predicted state uncertainty into the measurement space.</li> <li>$R_k$: This is the measurement noise covariance matrix, representing the inherent uncertainty or noise in our sensor readings.</li> </ul> <p><strong>3. Compute the Kalman Gain:</strong> This is where the real magic happens! The Kalman Gain, $K_k$, is the weighting factor. It determines how much we “trust” the new measurement versus our prediction. It’s calculated to minimize the posterior error covariance.</p> <p>$K_k = P_k^- H_k^T S_k^{-1}$</p> <ul> <li>Notice that $K_k$ is proportional to $P_k^-$ (our prediction uncertainty) and inversely proportional to $S_k$ (the innovation uncertainty). <ul> <li>If our prediction uncertainty ($P_k^-$) is high, and our measurement uncertainty ($R_k$, part of $S_k$) is low, then $K_k$ will be large. This means we lean heavily on the new measurement.</li> <li>If our prediction uncertainty is low, and our measurement uncertainty is high, then $K_k$ will be small. This means we stick closer to our prediction.</li> <li>The Kalman Gain dynamically adjusts based on the relative confidence in the prediction versus the measurement.</li> </ul> </li> </ul> <p><strong>4. Update the State Estimate:</strong> Now we combine our prior state estimate with the innovation, weighted by the Kalman Gain, to get our new, refined (posterior) state estimate $\hat{x}_k$.</p> <p>$\hat{x}_k = \hat{x}_k^- + K_k y_k$</p> <ul> <li>We take our prior belief and adjust it by a fraction ($K_k$) of the surprise ($y_k$).</li> </ul> <p><strong>5. Update the Covariance Estimate:</strong> Finally, we update our confidence in this new state estimate. The uncertainty of our estimate should <em>decrease</em> after incorporating a new measurement (unless the measurement itself is extremely noisy).</p> <p>$P_k = (I - K_k H_k) P_k^-$</p> <ul> <li>$I$ is the identity matrix. This equation shows how the Kalman Gain effectively reduces the uncertainty.</li> </ul> <p>And that’s it! With the updated state $\hat{x}_k$ and its covariance $P_k$, the filter is ready to loop back to the predict step for the next time instant.</p> <h3 id="the-beauty-of-the-kalman-gain-intuition">The Beauty of the Kalman Gain (Intuition)</h3> <p>To really grasp the Kalman Gain, imagine you’re trying to figure out your exact weight.</p> <ul> <li> <strong>Prediction:</strong> You weighed yourself yesterday, and you know you tend to fluctuate a bit. Your estimate is 70 kg, with a “trust radius” of $\pm$1 kg.</li> <li> <strong>Measurement:</strong> You step on a new scale. It reads 72 kg, but you know this scale isn’t super accurate; its “trust radius” is $\pm$2 kg.</li> </ul> <p>How do you combine 70 kg ($\pm$1 kg) and 72 kg ($\pm$2 kg)? You wouldn’t just average them (71 kg) because you trust your old estimate more than the new, shaky scale. You’d probably lean more towards 70 kg. The Kalman Gain is precisely what calculates this optimal “lean.” It ensures that if one source is very confident (small covariance), we give it more weight, and if another is very uncertain (large covariance), we give it less.</p> <h3 id="assumptions-and-limitations">Assumptions and Limitations</h3> <p>The standard Kalman Filter, as described here, relies on a few key assumptions:</p> <ol> <li> <strong>Linearity:</strong> The system’s dynamics and measurement models (the $F_k$ and $H_k$ matrices) must be linear.</li> <li> <strong>Gaussian Noise:</strong> The process noise ($w_k$) and measurement noise ($v_k$) are assumed to be zero-mean, white (uncorrelated over time), and follow a Gaussian (normal) distribution.</li> <li> <strong>Known Noise Covariances:</strong> The covariance matrices $Q_k$ and $R_k$ must be known and accurate. In practice, these often need to be tuned or estimated.</li> </ol> <p>What happens if these assumptions aren’t met?</p> <ul> <li>For <strong>non-linear systems</strong>, variations like the <strong>Extended Kalman Filter (EKF)</strong> linearize the system around the current state estimate using Jacobians.</li> <li>The <strong>Unscented Kalman Filter (UKF)</strong> uses a clever sampling technique (unscented transform) to approximate the distribution without explicit linearization, often performing better than EKF for highly non-linear systems.</li> <li>Other filters exist for non-Gaussian noise (e.g., particle filters).</li> </ul> <p>Despite these limitations, the linear Kalman Filter is incredibly robust and often performs well even with slight deviations from its assumptions.</p> <h3 id="why-this-matters-for-data-science-and-machine-learning">Why This Matters for Data Science and Machine Learning</h3> <p>The Kalman Filter might seem like an old algorithm (it was first published in 1960), but its principles are incredibly relevant and foundational in modern data science and machine learning:</p> <ul> <li> <strong>Time Series Analysis &amp; Forecasting:</strong> When dealing with noisy time series data (e.g., stock prices, sensor readings from an IoT device), Kalman filters can provide smoothed, more accurate estimates of the underlying trend or state, improving forecasts and anomaly detection.</li> <li> <strong>Sensor Fusion:</strong> In robotics, autonomous vehicles, and even your smartphone, multiple sensors (GPS, IMU, lidar, camera) provide different, often noisy, views of the world. Kalman filters are crucial for combining these diverse inputs into a single, highly accurate estimate of position, velocity, and orientation.</li> <li> <strong>Reinforcement Learning:</strong> In some reinforcement learning settings, the agent might not have direct access to the “true” state of the environment. Kalman filters (or their variants) can be used to estimate this hidden state, allowing the agent to make better decisions.</li> <li> <strong>Anomaly Detection:</strong> By constantly predicting a system’s state and comparing it to measurements, a large innovation ($y_k$) can signal an anomaly or a significant deviation from expected behavior.</li> <li> <strong>Model-Based Control Systems:</strong> In engineering, Kalman filters are integral to estimating system states that are not directly measurable, enabling precise control.</li> </ul> <p>It’s a beautiful example of how a deep understanding of mathematical principles can lead to practical, impactful solutions across diverse fields.</p> <h3 id="a-simple-mental-example">A Simple Mental Example</h3> <p>Imagine a self-driving car trying to know its exact location.</p> <ul> <li> <strong>Predict Step:</strong> Based on its previous known position, speed, and the steering angle it just applied, the car’s internal physics model predicts its new position and how much uncertainty there is in that prediction (e.g., “I <em>think</em> I’m here, $\pm$1 meter, due to tire slip and road imperfections.”).</li> <li> <strong>Update Step:</strong> A new GPS reading comes in (e.g., “The GPS says I’m here, $\pm$5 meters, because GPS is often inaccurate.”). The car’s camera might also see lane markings or landmarks, providing another measurement (e.g., “The camera says I’m here relative to the lane, $\pm$0.2 meters, but it might be a bit blurry.”).</li> <li> <strong>Kalman Gain’s Role:</strong> The Kalman Filter intelligently combines these. It gives more weight to the camera if it sees clear landmarks (low $R_k$ for camera, making $K_k$ large for camera input) and less weight to the GPS if it’s currently showing high error (high $R_k$ for GPS, making $K_k$ small for GPS input). The result is a much more accurate and stable estimate of the car’s true position than any single sensor could provide alone.</li> </ul> <h3 id="wrapping-up">Wrapping Up</h3> <p>My journey into the Kalman Filter has been incredibly rewarding. It demystifies how complex systems can maintain such precise awareness of their surroundings despite imperfect information. It’s a testament to the power of combining predictive modeling with real-world observations in an optimal, principled way.</p> <p>The elegance of its iterative “predict and update” cycle, governed by the dynamic weighting of the Kalman Gain, is truly inspiring. It teaches us that uncertainty isn’t something to fear or ignore, but rather a crucial piece of information that, when properly managed, can lead to remarkably robust and accurate estimations.</p> <p>If you’re embarking on your own data science or MLE journey, I highly encourage you to delve deeper into Kalman Filters. They open doors not just to understanding the math, but to appreciating the engineering marvels that underpin much of our modern technological world. Happy filtering!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>