<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Backpropagation: It's Not Magic, It's Math (But It Feels Like Magic!) | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/backpropagation-its-not-magic-its-math-but-it-feel/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Backpropagation: It's Not Magic, It's Math (But It Feels Like Magic!)</h1> <p class="post-meta"> Created on October 22, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/neural-networks"> <i class="fa-solid fa-hashtag fa-sm"></i> Neural Networks</a>   <a href="/blog/blog/tag/backpropagation"> <i class="fa-solid fa-hashtag fa-sm"></i> Backpropagation</a>   <a href="/blog/blog/tag/gradient-descent"> <i class="fa-solid fa-hashtag fa-sm"></i> Gradient Descent</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hello fellow data adventurers! Today, I want to pull back the curtain on one of the most fundamental and, dare I say, magical algorithms in the world of Artificial Intelligence: <strong>Backpropagation</strong>. If you’ve ever been amazed by what neural networks can do – recognizing faces, translating languages, generating art – then you’ve witnessed Backpropagation in action. It’s the engine that powers their learning.</p> <p>For a long time, the inner workings of neural networks felt like a black box to me. They took input, gave output, and somehow got better over time. But <em>how</em>? How does a network of interconnected “neurons” figure out what adjustments to make to its zillions of internal parameters to improve its performance? The answer, my friends, is Backpropagation. And as you’ll see, it’s not magic, but rather an elegant application of calculus – specifically, the <strong>chain rule</strong>.</p> <p>Let’s embark on this journey together to demystify it!</p> <h3 id="the-big-picture-neural-networks-and-their-mistakes">The Big Picture: Neural Networks and Their “Mistakes”</h3> <p>Before we dive into Backpropagation, let’s quickly recap what a neural network does. Imagine you’re teaching a child to ride a bike. They try, they wobble, they might fall. You give them feedback (“lean into the turn!,” “pedal faster!”). Over time, they learn.</p> <p>A neural network works similarly. It’s a series of layers, each containing artificial “neurons.” Each neuron takes inputs, performs a weighted sum, adds a bias, and then passes it through an activation function to produce an output. These outputs become inputs for the next layer, and so on, until we get the final output.</p> <ul> <li> <strong>Inputs:</strong> What we feed the network (e.g., pixels of an image, words in a sentence).</li> <li> <strong>Weights ($w$):</strong> These are the “strength” of the connections between neurons. Think of them as knobs you can turn.</li> <li> <strong>Biases ($b$):</strong> These are like an extra “nudge” for each neuron, allowing it to activate more easily or with more difficulty.</li> <li> <strong>Activation Functions ($\sigma$):</strong> Non-linear functions (like ReLU, Sigmoid, Tanh) that introduce complexity, allowing the network to learn non-linear relationships.</li> <li> <strong>Output:</strong> The network’s prediction (e.g., “this is a cat,” “the stock price will go up”).</li> </ul> <p>When we first initialize a neural network, its weights and biases are usually random. So, its initial predictions are likely way off, just like a child’s first attempt at riding a bike. This “offness” is what we call <strong>error</strong> or <strong>loss</strong>.</p> <p>We quantify this error using a <strong>loss function</strong> (e.g., Mean Squared Error, Cross-Entropy). A simple example of a loss function for a single output neuron might be:</p> <p>$L = \frac{1}{2}(y_{pred} - y_{true})^2$</p> <p>Where $y_{pred}$ is the network’s output and $y_{true}$ is the actual correct answer. Our ultimate goal? To <strong>minimize this loss</strong>. We want to adjust all those weights and biases so that $y_{pred}$ gets as close as possible to $y_{true}$.</p> <h3 id="the-quest-for-improvement-gradient-descent">The Quest for Improvement: Gradient Descent</h3> <p>How do we minimize the loss? This is where <strong>Gradient Descent</strong> comes in. Imagine you’re blindfolded on a mountainous terrain, and your goal is to find the lowest point (the minimum loss). You can’t see the whole landscape, but you can feel the slope right where you are. To go downhill, you take a step in the direction opposite to the steepest ascent.</p> <p>In mathematical terms, the “slope” is the <strong>gradient</strong>. The gradient tells us the direction of the steepest increase in the loss function. So, to <em>decrease</em> the loss, we move in the opposite direction of the gradient.</p> <p>For each weight $w$ and bias $b$ in our network, we need to know:</p> <ul> <li>How much does a tiny change in $w$ affect the total loss $L$? ($\frac{\partial L}{\partial w}$)</li> <li>How much does a tiny change in $b$ affect the total loss $L$? ($\frac{\partial L}{\partial b}$)</li> </ul> <p>These are partial derivatives. Once we have them, we update our weights and biases using a learning rate $\eta$:</p> <p>$w \leftarrow w - \eta \frac{\partial L}{\partial w}$ $b \leftarrow b - \eta \frac{\partial L}{\partial b}$</p> <p>The learning rate $\eta$ controls the size of our steps. Too small, and learning is slow; too large, and we might overshoot the minimum.</p> <h3 id="the-back-in-backpropagation-distributing-blame">The “Back” in Backpropagation: Distributing Blame</h3> <p>Here’s the tricky part: a typical neural network can have millions of weights and biases. How do we calculate these partial derivatives efficiently? This is where Backpropagation shines.</p> <p>If we simply tried to calculate $\frac{\partial L}{\partial w}$ for every single weight independently, it would be computationally impossible for large networks. Instead, Backpropagation leverages a clever trick: it computes the gradients layer by layer, starting from the output layer and moving <em>backward</em> to the input layer.</p> <p>Think of it like this: After a child crashes their bike, we don’t just say, “You crashed!” We try to figure out <em>why</em>. Was it the steering? The pedaling? The balance? We attribute blame. In a neural network, the output layer directly causes the error. But that error was influenced by the hidden layers before it, and those hidden layers were influenced by the layers before them, and so on.</p> <p>Backpropagation allows us to efficiently distribute the “blame” (the error signal) from the output back to every single weight and bias in the network, telling each one precisely how much it contributed to the final error.</p> <h3 id="the-chain-rule-our-superpower">The Chain Rule: Our Superpower!</h3> <p>At the heart of Backpropagation is the <strong>chain rule</strong> from calculus. If you recall, the chain rule tells us how to find the derivative of a composite function. If $y$ depends on $u$, and $u$ depends on $x$, then:</p> <p>$\frac{dy}{dx} = \frac{dy}{du} \cdot \frac{du}{dx}$</p> <p>This simple rule is incredibly powerful because it allows us to break down complex derivatives into simpler, manageable parts. In a neural network, the loss $L$ depends on the output of the final layer, which depends on the output of the previous layer, which depends on the weights and biases of that layer, and so on. It’s a long chain of dependencies!</p> <p>Let’s consider a single neuron. Its input is the weighted sum $z$, and its output is $a = \sigma(z)$. $z = \sum_k w_k a_k + b$ $a = \sigma(z)$</p> <p>Our goal is to find $\frac{\partial L}{\partial w}$ and $\frac{\partial L}{\partial b}$. Using the chain rule:</p> <p>$\frac{\partial L}{\partial w_k} = \frac{\partial L}{\partial a} \cdot \frac{\partial a}{\partial z} \cdot \frac{\partial z}{\partial w_k}$</p> <p>And similarly for bias:</p> <p>$\frac{\partial L}{\partial b} = \frac{\partial L}{\partial a} \cdot \frac{\partial a}{\partial z} \cdot \frac{\partial z}{\partial b}$</p> <p>Let’s break down each term:</p> <ul> <li>$\frac{\partial L}{\partial a}$: How much does the loss change with respect to this neuron’s output? This is the “error signal” coming from further down the chain.</li> <li>$\frac{\partial a}{\partial z}$: This is simply the derivative of our activation function, $\sigma’(z)$. It tells us how sensitive the neuron’s output is to its weighted input sum.</li> <li>$\frac{\partial z}{\partial w_k}$: From $z = \sum_k w_k a_k + b$, this is just $a_k$ (the input from the previous neuron).</li> <li>$\frac{\partial z}{\partial b}$: From $z = \sum_k w_k a_k + b$, this is just $1$.</li> </ul> <p>So, for a single neuron, the updates involve:</p> <p>$\frac{\partial L}{\partial w_k} = \frac{\partial L}{\partial a} \cdot \sigma’(z) \cdot a_k$ $\frac{\partial L}{\partial b} = \frac{\partial L}{\partial a} \cdot \sigma’(z)$</p> <p>The term $\frac{\partial L}{\partial a} \cdot \sigma’(z)$ is crucial; it’s often denoted as $\delta$ (delta) and represents the “error signal” for that specific neuron’s weighted input sum $z$.</p> <h3 id="walking-through-a-simple-network-the-essence-of-backpropagation">Walking Through a Simple Network (The Essence of Backpropagation)</h3> <p>Let’s formalize this for a multi-layered network. We’ll denote:</p> <ul> <li>$a^l$: the activation (output) of a neuron in layer $l$.</li> <li>$z^l$: the weighted sum (net input) of a neuron in layer $l$.</li> <li>$w^{l}$: the weights connecting layer $l-1$ to layer $l$.</li> <li>$b^{l}$: the biases for layer $l$.</li> </ul> <p>The forward pass is: $z^l = w^l a^{l-1} + b^l$ $a^l = \sigma(z^l)$</p> <p><strong>Step 1: Calculate Error at the Output Layer</strong></p> <p>This is our starting point. Let’s assume we have a simple loss function and a single output neuron. The error signal for the output layer $L$ (let’s say it’s layer 3) is:</p> <p>$\delta^3 = \frac{\partial L}{\partial z^3} = \frac{\partial L}{\partial a^3} \cdot \sigma’(z^3)$</p> <p>If $L = \frac{1}{2}(a^3 - y_{true})^2$, then $\frac{\partial L}{\partial a^3} = (a^3 - y_{true})$. So, $\delta^3 = (a^3 - y_{true}) \cdot \sigma’(z^3)$.</p> <p>Once we have $\delta^3$, we can calculate the gradients for the weights and biases connecting layer 2 to layer 3:</p> <p>$\frac{\partial L}{\partial w^3} = \delta^3 a^2$ $\frac{\partial L}{\partial b^3} = \delta^3$</p> <p><strong>Step 2: Propagate the Error Backward to Hidden Layers</strong></p> <p>Now, here’s the “back” part. We need to calculate the error signal $\delta^2$ for the hidden layer (layer 2). This error depends on the error in the <em>next</em> layer (layer 3) and how strongly layer 2’s outputs influenced layer 3.</p> <p>The key insight is that the error from layer $l+1$ is passed back to layer $l$, weighted by the connections $w^{l+1}$ that lead <em>from</em> layer $l$ <em>to</em> layer $l+1$.</p> <p>$\delta^l = \left( (w^{l+1})^T \delta^{l+1} \right) \odot \sigma’(z^l)$</p> <p>Let’s unpack this:</p> <ul> <li>$(w^{l+1})^T \delta^{l+1}$: This is the sum of the error signals from the next layer, weighted by the transposed weights. It’s essentially asking: “How much did my output contribute to the errors in the next layer, considering the strength of the connections?”</li> <li>$\odot$: This is the element-wise product (Hadamard product).</li> <li>$\sigma’(z^l)$: We multiply by the derivative of the activation function for layer $l$. This scales the error based on how steep the activation function was at that neuron’s weighted sum. If the neuron was “saturated” (e.g., in the flat part of a sigmoid), its output doesn’t change much even if its input changes, so its error signal will be small.</li> </ul> <p>Once we have $\delta^l$ for a hidden layer, we can calculate its gradients for weights and biases:</p> <p>$\frac{\partial L}{\partial w^l} = \delta^l (a^{l-1})^T$ $\frac{\partial L}{\partial b^l} = \delta^l$</p> <p>We repeat this process, calculating $\delta$ for each layer backward until we reach the first hidden layer.</p> <p><strong>Step 3: Update Weights and Biases</strong></p> <p>After calculating all the gradients ($\frac{\partial L}{\partial w}$ and $\frac{\partial L}{\partial b}$) for <em>all</em> layers, we then update all the weights and biases using our learning rate $\eta$ and the Gradient Descent rule:</p> <p>$w^{l} \leftarrow w^{l} - \eta \frac{\partial L}{\partial w^{l}}$ $b^{l} \leftarrow b^{l} - \eta \frac{\partial L}{\partial b^{l}}$</p> <p>This entire process – forward pass, calculate loss, backpropagate errors, update weights – constitutes one training iteration (or one “epoch” if done over the entire dataset). We repeat this millions or billions of times until the network’s loss is minimized and its predictions are accurate.</p> <h3 id="the-intuition-of-error-signals">The Intuition of Error Signals</h3> <p>The error signal $\delta_j^l$ (for neuron $j$ in layer $l$) essentially tells us two things:</p> <ol> <li> <strong>How much the output error would change if we slightly altered the weighted input $z_j^l$ to that neuron.</strong> This is the core “blame” measurement.</li> <li> <strong>How “active” that neuron is at its current state.</strong> The $\sigma’(z_j^l)$ term is crucial. If the neuron’s activation function is flat (e.g., a sigmoid neuron that’s outputting very close to 0 or 1), then even a large change in $z_j^l$ won’t significantly change its output $a_j^l$, and thus won’t greatly affect the loss. So, its contribution to the error (and its gradient) will be small. This is why ReLU activation functions became popular, as their derivative is simpler and doesn’t suffer from “vanishing gradients” as much as sigmoids.</li> </ol> <h3 id="why-backpropagation-matters">Why Backpropagation Matters</h3> <p>Backpropagation is not just a mathematical curiosity; it’s the bedrock upon which modern deep learning is built. Before Backpropagation was widely understood and efficiently implemented, training multi-layered neural networks was impractical. It provided the computational efficiency needed to:</p> <ul> <li> <strong>Train deep networks:</strong> Enabling networks with many hidden layers to learn complex features.</li> <li> <strong>Scale up:</strong> Allowing the use of massive datasets and millions of parameters.</li> <li> <strong>Unlock AI breakthroughs:</strong> Powering advancements in computer vision, natural language processing, speech recognition, and countless other fields.</li> </ul> <p>It’s the algorithm that transformed neural networks from a promising but limited idea into the dominant paradigm in AI.</p> <h3 id="conclusion">Conclusion</h3> <p>So, there you have it! Backpropagation, at its core, is an incredibly clever and efficient application of the chain rule from calculus. It’s the mechanism that allows a neural network to systematically calculate how much each of its internal parameters (weights and biases) contributed to its overall error, and then adjust those parameters to learn and improve.</p> <p>It’s not a magical learning spell; it’s elegant mathematics that, when applied iteratively over massive datasets, enables machines to “learn” in ways that once seemed unimaginable.</p> <p>I hope this journey into Backpropagation has demystified it a bit for you. The next time you see an AI performing an incredible feat, remember the humble but powerful algorithm working tirelessly behind the scenes, making it all possible.</p> <p>Now that you’ve grasped the core idea, I encourage you to explore further! Try to trace the calculations for a very small network by hand, or look for Python implementations of Backpropagation to see it in action. The more you play with it, the more intuitive it becomes! Happy learning!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>