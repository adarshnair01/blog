<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Beyond Accuracy: Unveiling Model Performance with ROC and AUC | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/beyond-accuracy-unveiling-model-performance-with-r/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Beyond Accuracy: Unveiling Model Performance with ROC and AUC</h1> <p class="post-meta"> Created on May 25, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/model-evaluation"> <i class="fa-solid fa-hashtag fa-sm"></i> Model Evaluation</a>   <a href="/blog/blog/tag/classification"> <i class="fa-solid fa-hashtag fa-sm"></i> Classification</a>   <a href="/blog/blog/tag/roc-curve"> <i class="fa-solid fa-hashtag fa-sm"></i> ROC Curve</a>   <a href="/blog/blog/tag/auc"> <i class="fa-solid fa-hashtag fa-sm"></i> AUC</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>Welcome back to my little corner of the data science world. Today, I want to talk about something fundamental, yet often misunderstood, in machine learning: evaluating our classification models. When I first started my journey, I thought accuracy was king. If my model was 95% accurate, surely it was amazing, right? Oh, how naive I was! It turns out, the world of binary classification is far more nuanced, and metrics like ROC (Receiver Operating Characteristic) curves and AUC (Area Under the Curve) are our guiding stars.</p> <p>Join me as we unravel the mysteries behind these powerful evaluation tools, making them accessible whether you’re just starting out or looking to solidify your understanding.</p> <h2 id="the-deceptive-simplicity-of-accuracy">The Deceptive Simplicity of Accuracy</h2> <p>Imagine you’re building a model to detect a rare disease that affects only 1% of the population. Your model proudly announces an accuracy of 99%. Sounds fantastic, right? But here’s the catch: a model that simply predicts “no disease” for everyone would also achieve 99% accuracy! It would miss every single positive case, yet still seem brilliant on paper.</p> <p>This little thought experiment highlights a critical issue: <strong>imbalanced datasets</strong>. In scenarios like fraud detection, medical diagnosis, or spam filtering, one class significantly outnumbers the other. In such cases, accuracy becomes a misleading metric, as it doesn’t differentiate between the types of errors your model is making. We need tools that give us a clearer picture of how well our model distinguishes between positive and negative classes.</p> <h2 id="getting-specific-the-confusion-matrix">Getting Specific: The Confusion Matrix</h2> <p>Before we dive deeper, let’s establish our foundation: the <strong>Confusion Matrix</strong>. This table helps us break down our model’s predictions against the actual outcomes. It’s truly a cornerstone for understanding performance.</p> <p>Let’s define the terms:</p> <ul> <li> <strong>True Positives (TP):</strong> Our model correctly predicted the positive class. (e.g., “It’s a cat!” and it truly was a cat.)</li> <li> <strong>True Negatives (TN):</strong> Our model correctly predicted the negative class. (e.g., “It’s not a cat!” and it truly wasn’t a cat.)</li> <li> <strong>False Positives (FP):</strong> Our model incorrectly predicted the positive class. (Type I error). (e.g., “It’s a cat!” but it was actually a dog. Oh dear.)</li> <li> <strong>False Negatives (FN):</strong> Our model incorrectly predicted the negative class. (Type II error). (e.g., “It’s not a cat!” but it actually was a cat. The model missed it!)</li> </ul> <table> <thead> <tr> <th style="text-align: left"> </th> <th style="text-align: left"><strong>Actual Positive</strong></th> <th style="text-align: left"><strong>Actual Negative</strong></th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Pred Positive</strong></td> <td style="text-align: left">TP</td> <td style="text-align: left">FP</td> </tr> <tr> <td style="text-align: left"><strong>Pred Negative</strong></td> <td style="text-align: left">FN</td> <td style="text-align: left">TN</td> </tr> </tbody> </table> <p>The costs associated with FP and FN errors can be vastly different depending on the application. In our rare disease example:</p> <ul> <li> <strong>FP:</strong> A healthy person is told they have the disease. This might cause stress and unnecessary further tests.</li> <li> <strong>FN:</strong> A sick person is told they are healthy. This is far more dangerous, potentially leading to delayed treatment and serious health consequences.</li> </ul> <p>Understanding these trade-offs is crucial.</p> <h2 id="beyond-simple-counts-sensitivity-and-specificity">Beyond Simple Counts: Sensitivity and Specificity</h2> <p>From the Confusion Matrix, we can derive more informative metrics that help us evaluate these trade-offs directly.</p> <h3 id="sensitivity-recall-or-true-positive-rate---tpr">Sensitivity (Recall or True Positive Rate - TPR)</h3> <p>Sensitivity measures the proportion of actual positive cases that our model correctly identified. It’s about completeness: out of all the real positives, how many did we catch?</p> <p>$ Sensitivity = Recall = TPR = \frac{TP}{TP + FN} $</p> <p>In our disease detection example, high sensitivity means the model is good at finding <em>all</em> the people who actually have the disease, minimizing false negatives. This is often paramount in medical screening, where missing a disease (FN) is more detrimental than a false alarm (FP).</p> <h3 id="specificity-true-negative-rate---tnr">Specificity (True Negative Rate - TNR)</h3> <p>Specificity measures the proportion of actual negative cases that our model correctly identified. It’s about precision for the negative class: out of all the real negatives, how many did we correctly identify as negative?</p> <p>$ Specificity = TNR = \frac{TN}{TN + FP} $</p> <p>High specificity means the model is good at ruling out the disease for healthy individuals, minimizing false positives. In some contexts, like approving loans, high specificity might be important to avoid lending money to high-risk individuals (FP).</p> <h3 id="the-inevitable-trade-off">The Inevitable Trade-off</h3> <p>Here’s the rub: improving sensitivity often comes at the cost of specificity, and vice-versa. Think about it: if you want to catch every single person with the disease (high sensitivity), you might lower your detection threshold so much that you also flag many healthy people (high false positives, low specificity). Conversely, if you want to be absolutely sure someone has the disease before flagging them (high specificity), you might miss some genuine cases (high false negatives, low sensitivity).</p> <p>So, how do we visualize and quantify this trade-off across <em>all</em> possible thresholds? Enter the ROC curve.</p> <h2 id="the-power-of-the-roc-curve">The Power of the ROC Curve</h2> <p>A classification model, especially one based on probabilities (like logistic regression or a neural network), doesn’t just output a “yes” or “no.” Instead, it outputs a probability score – say, the likelihood that an email is spam, or that a transaction is fraudulent. To convert this probability into a binary prediction, we use a <strong>threshold</strong>.</p> <ul> <li>If probability $\geq$ threshold, predict Positive.</li> <li>If probability $&lt;$ threshold, predict Negative.</li> </ul> <p>The magic of the ROC curve lies in showing us what happens to our model’s performance as we vary this threshold across all possible values (from 0 to 1).</p> <h3 id="plotting-the-roc-curve">Plotting the ROC Curve</h3> <p>The ROC curve plots two metrics against each other:</p> <ol> <li> <strong>True Positive Rate (TPR) on the Y-axis:</strong> This is our Sensitivity, defined as $ TPR = \frac{TP}{TP + FN} $.</li> <li> <strong>False Positive Rate (FPR) on the X-axis:</strong> This is related to Specificity, defined as $ FPR = \frac{FP}{FP + TN} = 1 - Specificity $.</li> </ol> <p>Each point on the ROC curve represents the (FPR, TPR) pair that results from a specific probability threshold.</p> <p>Let’s imagine you calculate (FPR, TPR) for a threshold of 0.1, then 0.2, then 0.3, and so on. Plotting all these points and connecting them gives you the ROC curve.</p> <h3 id="interpreting-the-roc-curve">Interpreting the ROC Curve</h3> <ul> <li> <strong>The Diagonal Line:</strong> A straight line from (0,0) to (1,1) represents a purely random classifier. It’s like flipping a coin for each prediction. If your model’s ROC curve is close to or below this line, your model is performing no better than, or even worse than, random chance.</li> <li> <strong>The Ideal Curve:</strong> The closer your curve is to the top-left corner (0,1), the better your classifier. A perfect classifier would have a curve that shoots straight up from (0,0) to (0,1) and then across to (1,1), meaning it achieves 100% TPR with 0% FPR – it correctly identifies all positives without making any false alarms.</li> <li> <strong>The Trade-off Visualized:</strong> As you move along the curve from left to right, you are effectively lowering your prediction threshold. This increases your TPR (you catch more positives), but also increases your FPR (you get more false alarms).</li> </ul> <p>The ROC curve provides a comprehensive visual summary of your model’s ability to discriminate between classes across all possible decision thresholds. It helps us see the full spectrum of the sensitivity-specificity trade-off.</p> <h2 id="quantifying-performance-the-auc-score">Quantifying Performance: The AUC Score</h2> <p>While the ROC curve gives us a fantastic visual, we often need a single, concise metric to compare different models or to report performance. This is where the <strong>Area Under the ROC Curve (AUC)</strong> comes in.</p> <p>The AUC is quite literally the area under the ROC curve. It quantifies the overall performance of a classification model.</p> <h3 id="interpreting-the-auc-score">Interpreting the AUC Score</h3> <ul> <li><strong>AUC ranges from 0 to 1.</strong></li> <li> <strong>AUC = 0.5:</strong> This means your model is performing no better than random chance. It’s like your diagonal line.</li> <li> <strong>AUC = 1.0:</strong> This indicates a perfect classifier, one that correctly separates all positive and negative instances.</li> <li> <strong>AUC &lt; 0.5:</strong> This is unusual and suggests your model is performing worse than random. It might mean your model is learning the inverse relationship, or perhaps your positive and negative labels are flipped!</li> </ul> <h3 id="why-auc-is-so-valuable">Why AUC is so Valuable</h3> <ol> <li> <strong>Threshold-Independent:</strong> Unlike accuracy, precision, or recall, AUC considers all possible classification thresholds. This means you don’t have to pick an arbitrary threshold to evaluate your model’s overall performance.</li> <li> <strong>Robust to Class Imbalance:</strong> Remember our disease detection problem? AUC is much more robust to imbalanced datasets than accuracy. Even if one class vastly outnumbers the other, AUC still gives a fair assessment of the model’s ability to distinguish between them.</li> <li> <strong>Probabilistic Interpretation:</strong> Perhaps the most intuitive interpretation of AUC is this: <strong>the AUC score represents the probability that a randomly chosen positive instance will be ranked higher (assigned a higher probability score) than a randomly chosen negative instance by the classifier.</strong> A higher AUC means the model is better at separating positive from negative examples.</li> </ol> <p>Let’s say a model has an AUC of 0.85. This means there’s an 85% chance that if you randomly pick one positive case and one negative case, the model will assign a higher probability to the positive case. Pretty neat, right?</p> <h2 id="a-quick-peek-into-implementation-conceptual">A Quick Peek into Implementation (Conceptual)</h2> <p>In Python, libraries like <code class="language-plaintext highlighter-rouge">scikit-learn</code> make calculating ROC curves and AUC scores incredibly straightforward:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_curve</span><span class="p">,</span> <span class="n">auc</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="c1"># (Assuming X_train, X_test, y_train, y_test are already defined)
</span>
<span class="c1"># 1. Train your model
</span><span class="n">model</span> <span class="o">=</span> <span class="nc">LogisticRegression</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># 2. Get predicted probabilities for the positive class
#    (The '1' in [:, 1] refers to the probability of the positive class)
</span><span class="n">y_pred_proba</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>

<span class="c1"># 3. Calculate FPR, TPR, and thresholds
</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> <span class="nf">roc_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_proba</span><span class="p">)</span>

<span class="c1"># 4. Calculate AUC
</span><span class="n">roc_auc</span> <span class="o">=</span> <span class="nf">auc</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">)</span>

<span class="c1"># 5. Plot the ROC curve
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">darkorange</span><span class="sh">'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">'</span><span class="s">ROC curve (area = </span><span class="si">{</span><span class="n">roc_auc</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">navy</span><span class="sh">'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Random Classifier</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">False Positive Rate</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">True Positive Rate</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Receiver Operating Characteristic</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="sh">"</span><span class="s">lower right</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">The AUC score for the model is: </span><span class="si">{</span><span class="n">roc_auc</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>This snippet conceptualizes how we take the predicted probabilities, calculate the FPR and TPR at various thresholds, and then plot them to get our beautiful ROC curve and its associated AUC score.</p> <h2 id="conclusion-mastering-your-models-true-story">Conclusion: Mastering Your Model’s True Story</h2> <p>My journey from being an accuracy-obsessed beginner to someone who deeply appreciates ROC and AUC has been incredibly insightful. These metrics aren’t just fancy terms; they are essential tools that give us a comprehensive, nuanced understanding of our classification models’ performance, especially in real-world scenarios where class imbalance is common and different types of errors carry different costs.</p> <p>Next time you evaluate a classification model, don’t just glance at the accuracy. Dig deeper. Plot that ROC curve. Understand that AUC score. It will tell you a much richer story about your model’s true capabilities and help you make more informed decisions about which model to deploy and at what operating point (threshold) to use it.</p> <p>Keep learning, keep exploring, and remember: the journey of a data scientist is a continuous quest for deeper understanding!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>