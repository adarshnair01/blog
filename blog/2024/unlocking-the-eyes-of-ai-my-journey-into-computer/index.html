<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Unlocking the Eyes of AI: My Journey into Computer Vision | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/unlocking-the-eyes-of-ai-my-journey-into-computer/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Unlocking the Eyes of AI: My Journey into Computer Vision</h1> <p class="post-meta"> Created on March 27, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/computer-vision"> <i class="fa-solid fa-hashtag fa-sm"></i> Computer Vision</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/image-processing"> <i class="fa-solid fa-hashtag fa-sm"></i> Image Processing</a>   <a href="/blog/blog/tag/artificial-intelligence"> <i class="fa-solid fa-hashtag fa-sm"></i> Artificial Intelligence</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>From the moment I first saw a self-driving car navigate a complex intersection, or an app identify a plant from a photo, I was hooked. How do these machines <em>see</em>? This burning question led me down a rabbit hole into the captivating world of Computer Vision – a field dedicated to enabling computers to understand and interpret visual information from the real world. It’s not just about taking pictures; it’s about making sense of what’s <em>in</em> them, giving AI the gift of sight.</p> <p>As a data science and machine learning enthusiast, diving into Computer Vision felt like unlocking a new superpower. It blends intricate mathematical concepts with the artistic challenge of teaching a machine intuition. If you’ve ever been curious about how your phone unlocks with your face, or how medical imaging spots anomalies, then buckle up! We’re about to embark on a journey to demystify how we teach computers to see.</p> <h3 id="the-world-through-a-computers-eyes-pixels-and-numbers">The World Through a Computer’s Eyes: Pixels and Numbers</h3> <p>Let’s start with the absolute basics. What does an image look like to a computer? Not a beautiful landscape or a smiling face, but rather a vast grid of numbers. Imagine a digital photograph as a gigantic spreadsheet. Each cell in this spreadsheet is a <strong>pixel</strong>, representing a tiny point of color.</p> <p>For a grayscale image, each pixel usually holds a single number, typically ranging from 0 (black) to 255 (white), representing varying shades of gray.</p> <p>But what about color images? They’re a bit more complex. Most color images use the <strong>RGB (Red, Green, Blue) model</strong>. This means for every pixel, there are three numbers, one for the intensity of red, one for green, and one for blue. Each of these values also typically ranges from 0 to 255.</p> <p>So, a simple 100x100 pixel color image isn’t just 10,000 numbers; it’s 10,000 pixels, each with 3 color channels, totaling 30,000 numbers! This numerical representation is the foundation upon which all Computer Vision tasks are built.</p> <p>A pixel’s value can be represented as $(R, G, B)$. For instance, pure red might be $(255, 0, 0)$, while white is $(255, 255, 255)$. To convert a color pixel to grayscale, a common formula is: \(L = 0.2989R + 0.5870G + 0.1140B\) where $L$ is the perceived luminance.</p> <h3 id="early-attempts-crafting-vision-rules-by-hand">Early Attempts: Crafting Vision Rules by Hand</h3> <p>In the early days of Computer Vision, researchers tried to explicitly program computers with rules to understand images. This involved techniques like:</p> <ul> <li> <strong>Edge Detection:</strong> Finding boundaries of objects. Algorithms like the Sobel operator calculate the intensity gradient of an image to highlight edges.</li> <li> <strong>Feature Extraction:</strong> Identifying specific points or patterns, like corners or blobs, that might indicate an object.</li> <li> <strong>Filtering:</strong> Applying mathematical operations to modify image pixels, for tasks like blurring or sharpening.</li> </ul> <p>One of the most fundamental operations here is <strong>convolution</strong>. Imagine a small window, called a <strong>kernel</strong> or <strong>filter</strong>, sliding across the entire image. At each position, it performs a mathematical operation (element-wise multiplication and summation) with the underlying pixels. This operation transforms the image, highlighting certain features.</p> <p>For example, a simple 3x3 kernel to detect horizontal edges might look like this: \(K = \begin{pmatrix} -1 &amp; -1 &amp; -1 \\ 0 &amp; 0 &amp; 0 \\ 1 &amp; 1 &amp; 1 \end{pmatrix}\) When this kernel slides over an image, it will produce a high positive value where there’s a transition from dark to light (bottom edge of an object) and a high negative value for light to dark (top edge).</p> <p>These traditional methods were ingenious, but they had a major limitation: they required humans to meticulously design these filters and rules for every specific task and type of image. They struggled with variations in lighting, angles, occlusions, and the sheer complexity of the real world. A cat looks very different when curled up, stretched out, or seen from above!</p> <h3 id="the-deep-learning-revolution-learning-to-see">The Deep Learning Revolution: Learning to See</h3> <p>The breakthrough came with <strong>Deep Learning</strong>, specifically with <strong>Convolutional Neural Networks (CNNs)</strong>. Instead of us painstakingly designing features and rules, CNNs learn them directly from data. This was a paradigm shift!</p> <p>At its core, a CNN is inspired by the human visual cortex. It processes information hierarchically, starting with simple features and gradually building up to more complex ones.</p> <p>Let’s break down the key layers of a typical CNN:</p> <h4 id="1-convolutional-layers">1. Convolutional Layers</h4> <p>This is the heart of a CNN. Similar to the traditional convolution we discussed, a convolutional layer uses kernels. However, these kernels are <em>not</em> hand-designed; they are <em>learnable parameters</em>.</p> <ul> <li> <strong>How it works:</strong> A set of small, learnable filters (e.g., 3x3 or 5x5) slides across the input image (or feature maps from previous layers). Each filter specializes in detecting a particular feature, like a vertical edge, a specific texture, or a corner.</li> <li> <strong>Output:</strong> Each filter produces an <strong>activation map</strong> (also called a feature map), showing where in the input that particular feature was detected.</li> <li> <strong>Key Idea:</strong> The network learns <em>which</em> filters are most useful for a given task during training. Early layers might learn simple features, while deeper layers combine these to learn more abstract concepts (e.g., an “eye,” a “wheel,” or a “cat’s ear”).</li> </ul> <h4 id="2-activation-functions">2. Activation Functions</h4> <p>After a convolution, the output often passes through a non-linear <strong>activation function</strong>. Without non-linearity, stacking multiple layers would simply result in a single linear transformation, limiting the network’s ability to learn complex patterns.</p> <p>A very popular choice is the <strong>Rectified Linear Unit (ReLU)</strong> function: \(f(x) = \max(0, x)\) ReLU is simple: if the input is positive, it outputs the input; otherwise, it outputs zero. This introduces non-linearity, allowing the network to learn more intricate relationships in the data.</p> <h4 id="3-pooling-layers">3. Pooling Layers</h4> <p>Pooling layers typically follow convolutional layers. Their main job is to reduce the spatial dimensions (width and height) of the feature maps, making the network more robust to variations and reducing computational load.</p> <ul> <li> <strong>Max Pooling:</strong> The most common type. It slides a small window (e.g., 2x2) over the feature map and simply takes the maximum value within that window.</li> <li> <strong>Benefits:</strong> <ul> <li> <strong>Dimensionality Reduction:</strong> Reduces the number of parameters and computation.</li> <li> <strong>Translation Invariance:</strong> Makes the network less sensitive to the exact position of a feature. If an edge shifts slightly, the max-pooled output might still capture it.</li> </ul> </li> </ul> <h4 id="4-fully-connected-layers">4. Fully Connected Layers</h4> <p>After several convolutional and pooling layers, the high-level features learned are “flattened” into a single vector. This vector is then fed into one or more <strong>fully connected layers</strong>, similar to a traditional neural network.</p> <ul> <li> <strong>Purpose:</strong> These layers take the abstract features extracted by the earlier layers and use them to perform the final classification (e.g., “is this a cat or a dog?”). Each neuron in a fully connected layer is connected to every neuron in the previous layer.</li> <li> <strong>Output Layer:</strong> The final fully connected layer usually has an output neuron for each class, with activation functions like <code class="language-plaintext highlighter-rouge">softmax</code> (for multi-class classification) that provide probabilities for each class.</li> </ul> <h4 id="the-learning-process">The Learning Process</h4> <p>How do these learnable filters and connections get their values? Through a process called <strong>training</strong>.</p> <ol> <li> <strong>Forward Pass:</strong> An image is fed through the network, and a prediction is made (e.g., “90% dog, 10% cat”).</li> <li> <strong>Loss Function:</strong> A <strong>loss function</strong> (e.g., cross-entropy for classification) quantifies how far off this prediction is from the actual truth.</li> <li> <strong>Backpropagation:</strong> This “error” is then propagated backward through the network.</li> <li> <strong>Optimization:</strong> An <strong>optimizer</strong> (like Stochastic Gradient Descent) uses this error information to slightly adjust the weights (the values in the kernels and connections) in a way that reduces the loss for the next prediction.</li> </ol> <p>This iterative process, repeated over thousands or millions of images, allows the CNN to “learn” the optimal features and relationships required to perform its task with high accuracy.</p> <p>Pioneering architectures like LeNet, AlexNet, VGG, ResNet, and Inception have continually pushed the boundaries of what’s possible, introducing innovations like deeper networks, skip connections, and more efficient computational blocks.</p> <h3 id="beyond-classification-what-else-can-computer-vision-do">Beyond Classification: What Else Can Computer Vision Do?</h3> <p>While image classification (identifying the main object in an image) is foundational, Computer Vision has evolved to tackle much more complex problems:</p> <ul> <li> <strong>Object Detection:</strong> Not just <em>what</em> is in the image, but <em>where</em> it is. This involves drawing <strong>bounding boxes</strong> around multiple objects and classifying each one. Think of self-driving cars identifying pedestrians, other vehicles, and traffic signs. Popular models include YOLO (You Only Look Once) and Faster R-CNN.</li> <li> <strong>Semantic Segmentation:</strong> This is vision at the pixel level. Every single pixel in an image is classified into a category. For example, painting every road pixel blue, every sky pixel green, and every tree pixel brown. This is crucial for understanding scenes in detail, like in augmented reality or medical image analysis. U-Net and FCN (Fully Convolutional Networks) are common architectures here.</li> <li> <strong>Instance Segmentation:</strong> Takes semantic segmentation a step further. If there are multiple objects of the same class (e.g., several cars), instance segmentation can identify and differentiate each individual instance, not just segmenting “all cars” as one blob. Mask R-CNN is a leading model for this.</li> <li> <strong>Image Generation and Style Transfer:</strong> Generative Adversarial Networks (GANs) can create realistic images from scratch or transfer the style of one image onto another, creating stunning artistic effects.</li> <li> <strong>Pose Estimation:</strong> Locating key points on a person or object to understand their posture or orientation.</li> <li> <strong>Optical Character Recognition (OCR):</strong> Converting images of text into machine-readable text.</li> </ul> <p>The applications are truly endless:</p> <ul> <li> <strong>Healthcare:</strong> Detecting diseases from X-rays or MRIs, surgical assistance.</li> <li> <strong>Manufacturing:</strong> Quality control, robotic guidance.</li> <li> <strong>Retail:</strong> Inventory management, customer behavior analysis.</li> <li> <strong>Security:</strong> Facial recognition, surveillance.</li> <li> <strong>Agriculture:</strong> Crop monitoring, yield prediction.</li> </ul> <h3 id="challenges-and-the-road-ahead">Challenges and the Road Ahead</h3> <p>Despite its incredible progress, Computer Vision is far from a solved problem. We still face challenges:</p> <ul> <li> <strong>Data Dependence:</strong> Deep learning models require massive amounts of annotated data, which can be expensive and time-consuming to acquire.</li> <li> <strong>Robustness:</strong> Models can be sensitive to variations in lighting, weather, viewpoint, and occlusions. Adversarial attacks can subtly modify images to fool models.</li> <li> <strong>Bias:</strong> If training data contains biases (e.g., underrepresentation of certain groups), the model will learn and perpetuate those biases.</li> <li> <strong>Explainability (XAI):</strong> Understanding <em>why</em> a model made a certain decision can be difficult, especially in critical applications like medicine or autonomous driving.</li> <li> <strong>Computational Cost:</strong> Training large models can be computationally intensive, requiring powerful GPUs.</li> </ul> <p>The future of Computer Vision is vibrant and exciting. We’re seeing advancements in:</p> <ul> <li> <strong>Self-supervised learning:</strong> Reducing the need for manual annotations.</li> <li> <strong>3D Vision:</strong> Better understanding of depth and spatial relationships.</li> <li> <strong>Video Understanding:</strong> Moving beyond single images to interpret dynamic scenes over time.</li> <li> <strong>Edge AI:</strong> Deploying complex CV models on low-power devices like smartphones and drones.</li> <li> <strong>Multimodal AI:</strong> Combining vision with other senses like language or audio for a more holistic understanding.</li> </ul> <h3 id="my-journey-continues">My Journey Continues</h3> <p>Exploring Computer Vision has been an exhilarating experience. It’s a field where theoretical breakthroughs quickly translate into real-world applications that impact our daily lives. From the simple elegance of a pixel grid to the sophisticated layers of a CNN, the journey from raw data to machine intelligence is nothing short of magical.</p> <p>As I continue to build my portfolio in data science and machine learning, Computer Vision remains a cornerstone of my passion. It’s a testament to human ingenuity and the boundless potential of AI. Whether you’re building the next generation of smart cameras or diagnosing diseases, the ability to grant machines sight opens up a universe of possibilities. I encourage you to delve deeper, experiment, and perhaps even teach a computer to see something new! The visionaries of tomorrow are building their foundations today.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>