<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Cracking the Code of \\\"Just For You\\\": A Deep Dive into Recommender Systems | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/cracking-the-code-of-just-for-you-a-deep-dive-into/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Cracking the Code of \\\"Just For You\\\": A Deep Dive into Recommender Systems</h1> <p class="post-meta"> Created on May 24, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/recommender-systems"> <i class="fa-solid fa-hashtag fa-sm"></i> Recommender Systems</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/collaborative-filtering"> <i class="fa-solid fa-hashtag fa-sm"></i> Collaborative Filtering</a>   <a href="/blog/blog/tag/content-based-filtering"> <i class="fa-solid fa-hashtag fa-sm"></i> Content-Based Filtering</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hello fellow data enthusiasts and curious minds!</p> <p>If you’re anything like me, your daily life is subtly, yet profoundly, influenced by algorithms. From the music Spotify suggests based on your mood, to the news articles Google curates just for you, we live in an era of personalized digital experiences. But have you ever paused and genuinely wondered, “How do they <em>know</em>?” How does a service, with millions of users and even more items, manage to pinpoint exactly what might pique <em>your</em> interest?</p> <p>Welcome to the captivating realm of <strong>Recommender Systems</strong>!</p> <p>Today, we’re embarking on a journey to demystify these powerful algorithms. My goal isn’t just to tell you <em>what</em> they are, but to explore <em>how</em> they work, the challenges they face, and why they’ve become an indispensable part of our digital landscape. So, grab your favorite beverage, get comfy, and let’s dive deep!</p> <h3 id="the-why-behind-the-what-information-overload-and-the-need-for-discovery">The “Why” Behind the “What”: Information Overload and the Need for Discovery</h3> <p>Imagine walking into a library with millions of books, or a music store with an endless catalog of songs. Without any guidance, finding something you’d genuinely enjoy would be like searching for a needle in a haystack – overwhelming, time-consuming, and often fruitless. This, my friends, is the problem of <strong>information overload</strong>.</p> <p>Recommender Systems are essentially our digital librarians, our personal shopping assistants, our taste-making DJs. They solve this problem by:</p> <ol> <li> <strong>Helping users discover new items:</strong> Introducing us to products, movies, or music we might not have found otherwise.</li> <li> <strong>Improving user experience:</strong> Making platforms more enjoyable and less frustrating.</li> <li> <strong>Boosting business metrics:</strong> Increasing engagement, sales, and retention for companies.</li> </ol> <p>At its core, a recommender system predicts a user’s preference for an item. This prediction can manifest as a numerical rating (e.g., how many stars you’d give a movie) or a binary decision (e.g., whether you’d click on an article).</p> <h3 id="the-big-two-content-based-vs-collaborative-filtering">The Big Two: Content-Based vs. Collaborative Filtering</h3> <p>While there are many sophisticated variations, most recommender systems generally fall into two main categories, or a combination of them.</p> <h4 id="1-content-based-filtering-if-you-liked-this-youll-like-that">1. Content-Based Filtering: “If you liked <em>this</em>, you’ll like <em>that</em>.”</h4> <p>Think of Content-Based Filtering like that friend who knows your tastes inside-out. If you tell them you loved a sci-fi movie directed by Christopher Nolan with mind-bending plots, they’ll instantly recommend other Nolan films, or perhaps other complex sci-fi thrillers.</p> <p><strong>How it works:</strong></p> <p>This approach relies on <strong>item features</strong> and <strong>user profiles</strong>.</p> <ul> <li> <strong>Item Profiles:</strong> Each item (movie, song, product) is described by its characteristics. For a movie, this could be its genre, director, actors, keywords, etc. We can represent these features as a vector.</li> <li> <strong>User Profiles:</strong> Built from the items a user has previously liked or interacted with. If you liked <em>Inception</em> and <em>Interstellar</em>, your user profile might emphasize “sci-fi,” “Nolan,” and “complex plots.”</li> </ul> <p>The system then looks for items whose features are <strong>similar</strong> to the items in the user’s profile.</p> <p><strong>The Math (Simplified): Cosine Similarity</strong></p> <p>To measure similarity between items or a user profile and an item, we often use metrics like <strong>Cosine Similarity</strong>. If we represent items as vectors in a multi-dimensional space (where each dimension is a feature), Cosine Similarity measures the cosine of the angle between two vectors. A smaller angle (closer to 0 degrees) means higher similarity.</p> <p>For two vectors $A$ and $B$, the cosine similarity is calculated as:</p> <table> <tbody> <tr> <td>$cos(\theta) = \frac{A \cdot B}{</td> <td> </td> <td>A</td> <td> </td> <td>\cdot</td> <td> </td> <td>B</td> <td> </td> <td>} = \frac{\sum_{i=1}^{n} A_i B_i}{\sqrt{\sum_{i=1}^{n} A_i^2} \sqrt{\sum_{i=1}^{n} B_i^2}}$</td> </tr> </tbody> </table> <p>Where $A_i$ and $B_i$ are components of vectors $A$ and $B$. This gives us a value between -1 and 1, with 1 indicating perfect similarity.</p> <p><strong>Pros:</strong></p> <ul> <li> <strong>No need for other users’ data:</strong> It can recommend to new users as long as their preferences are known.</li> <li> <strong>Good for niche tastes:</strong> Can recommend very specific items based on unique preferences.</li> <li> <strong>Transparency:</strong> Recommendations are easy to explain (“You liked X, and Y is similar because it shares these features…”).</li> </ul> <p><strong>Cons:</strong></p> <ul> <li> <strong>Limited novelty:</strong> Tends to recommend items very similar to what a user already likes, leading to “filter bubbles.”</li> <li> <strong>Feature engineering:</strong> Requires detailed, structured metadata for items, which can be hard to obtain or maintain.</li> <li> <strong>Cold Start for new users:</strong> If a new user hasn’t interacted with <em>any</em> items, there’s no profile to build upon.</li> </ul> <h4 id="2-collaborative-filtering-users-like-you-liked-this">2. Collaborative Filtering: “Users like you liked <em>this</em>.”</h4> <p>This is arguably the most famous and widely used type of recommender system. Instead of relying on item features, Collaborative Filtering leverages the <strong>collective wisdom</strong> of the crowd. It assumes that if two users have similar tastes in the past, they will likely have similar tastes in the future.</p> <p>Imagine you and your friend both love punk rock bands from the 80s. If your friend discovers a new 80s punk band and raves about it, you’re very likely to enjoy it too, even if you know nothing about the band’s specific members or sub-genres.</p> <p>There are two main sub-types of Collaborative Filtering:</p> <ul> <li> <strong>User-Based Collaborative Filtering (User-to-User):</strong> <ol> <li>Find users whose past behavior (ratings, purchases, views) is similar to yours.</li> <li>Recommend items that these “similar users” liked but you haven’t yet seen.</li> </ol> <p>While conceptually simple, this can be computationally intensive for systems with millions of users, as finding “similar users” for every active user in real-time is a huge task.</p> </li> <li> <strong>Item-Based Collaborative Filtering (Item-to-Item):</strong> <ol> <li>Identify items that are similar to the items you’ve liked. Item similarity here is based on how frequently other users liked/interacted with <em>both</em> items. For example, if many users who bought Item A also bought Item B, then A and B are considered similar.</li> <li>Recommend those similar items to you.</li> </ol> <p>This approach is often favored by large-scale commercial systems (like Amazon’s “Customers who viewed this item also viewed…”) because item similarity tends to be more stable than user similarity and can be pre-computed offline.</p> </li> </ul> <p><strong>Pros:</strong></p> <ul> <li> <strong>Discover new and diverse items:</strong> Can recommend items that are very different from what a user has seen, relying on the varied tastes of similar users.</li> <li> <strong>No feature engineering required:</strong> Doesn’t need explicit item metadata; it learns relationships purely from user-item interactions.</li> <li> <strong>Handles complex items:</strong> Works well even if items are hard to describe with features (e.g., abstract art, jokes).</li> </ul> <p><strong>Cons:</strong></p> <ul> <li> <strong>Cold Start Problem (for new users AND new items):</strong> A new user has no past interactions, so there are no similar users to find. A new item has no interactions, so its similarity to other items can’t be computed.</li> <li> <strong>Sparsity:</strong> Most users only interact with a tiny fraction of available items. The user-item interaction matrix is often very sparse, making it hard to find reliable similarities.</li> <li> <strong>Scalability:</strong> Especially user-based CF can struggle with huge datasets.</li> </ul> <h4 id="3-matrix-factorization-unveiling-hidden-patterns">3. Matrix Factorization: Unveiling Hidden Patterns</h4> <p>To address the scalability and sparsity issues of traditional Collaborative Filtering, techniques like <strong>Matrix Factorization</strong> emerged. This is where things get a bit more advanced and incredibly powerful.</p> <p>Imagine a giant table (matrix) where rows are users and columns are items. The cells contain ratings or interactions. Most cells are empty (sparse). Matrix Factorization tries to “fill in” these empty cells by representing both users and items in a lower-dimensional “latent space.”</p> <p><strong>The Core Idea:</strong> Instead of explicitly saying a movie is “sci-fi” or “comedy,” we assume there are $k$ hidden factors (latent factors) that determine how much a user likes an item. These factors could represent abstract “genres,” “themes,” or “styles” that we don’t explicitly define.</p> <p>We decompose the original large, sparse user-item interaction matrix ($R$) into two smaller, dense matrices:</p> <ol> <li>A <strong>User-Factor matrix ($P$)</strong>: Each row represents a user, and each column represents their affinity for one of the $k$ latent factors.</li> <li>A <strong>Item-Factor matrix ($Q$)</strong>: Each row represents an item, and each column represents how much that item embodies one of the $k$ latent factors.</li> </ol> <p>The prediction for a user $u$ for an item $i$ is then simply the dot product of their respective latent factor vectors:</p> <p>$\hat{r}<em>{ui} = p_u \cdot q_i^T = \sum</em>{k=1}^{K} p_{uk} q_{ik}$</p> <p>Here, $p_u$ is the $u$-th row of $P$ (user $u$’s latent vector) and $q_i$ is the $i$-th row of $Q$ (item $i$’s latent vector). The goal is to find $P$ and $Q$ such that their product closely approximates the original interaction matrix $R$. This is often achieved using algorithms like <strong>Singular Value Decomposition (SVD)</strong> or <strong>Alternating Least Squares (ALS)</strong>, minimizing the error between predicted and actual ratings.</p> <p><strong>Pros:</strong></p> <ul> <li> <strong>Handles sparsity:</strong> By learning latent representations, it can make good predictions even with few observed ratings.</li> <li> <strong>Scalability:</strong> Once the latent factors are learned, predictions are fast.</li> <li> <strong>Discovers hidden features:</strong> Can uncover subtle relationships that aren’t obvious from explicit item features.</li> </ul> <p><strong>Cons:</strong></p> <ul> <li> <strong>Interpretability:</strong> What do those $k$ latent factors <em>actually</em> mean? It’s often hard to explain in human terms.</li> <li> <strong>Cold Start:</strong> Still struggles with brand new users or items, as they have no interactions to learn latent factors from.</li> </ul> <h4 id="4-hybrid-recommender-systems-the-best-of-both-worlds">4. Hybrid Recommender Systems: The Best of Both Worlds</h4> <p>Given the strengths and weaknesses of Content-Based and Collaborative Filtering, it makes sense to combine them. <strong>Hybrid Recommender Systems</strong> do just that, aiming to mitigate the limitations of individual approaches. For example, a hybrid system might:</p> <ul> <li>Use content-based methods to handle the cold start problem for new users, then switch to collaborative filtering once enough interaction data is gathered.</li> <li>Combine content features with collaborative latent factors in a single model.</li> </ul> <p>Most state-of-the-art recommender systems in production today are hybrids.</p> <h3 id="the-real-world-grind-challenges-in-building-recommenders">The Real-World Grind: Challenges in Building Recommenders</h3> <p>Building a recommender system isn’t just about picking an algorithm; it’s about navigating a landscape of real-world challenges:</p> <ul> <li> <strong>Cold Start Problem:</strong> As mentioned, how do you recommend to a new user with no history, or recommend a brand-new item? Solutions include recommending popular items, asking for initial preferences, or using demographic data.</li> <li> <strong>Sparsity:</strong> Most users interact with a tiny fraction of items, leading to very sparse data matrices. This can make similarity calculations unreliable.</li> <li> <strong>Scalability:</strong> For platforms with millions of users and items, algorithms must be extremely efficient.</li> <li> <strong>Serendipity, Diversity, and Novelty:</strong> Recommending only highly similar items can lead to a “filter bubble.” Good recommenders balance relevance with the occasional surprising, diverse, or novel item.</li> <li> <strong>Fairness and Bias:</strong> Recommenders can inadvertently amplify existing biases in the data (e.g., showing only male leads for certain genres if historical data reflects that bias). Ensuring fairness is a growing ethical concern.</li> <li> <strong>Shilling Attacks:</strong> Malicious actors trying to manipulate recommendations for personal gain.</li> </ul> <h3 id="how-do-we-know-its-working-evaluating-recommender-systems">How Do We Know It’s Working? Evaluating Recommender Systems</h3> <p>Once you’ve built a recommender, how do you measure its success? We use various metrics:</p> <ul> <li> <strong>For Rating Prediction (e.g., predicting star ratings):</strong> <ul> <li> <strong>Root Mean Squared Error (RMSE):</strong> The square root of the average of the squared differences between predicted and actual ratings. Lower is better. $RMSE = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (r_i - \hat{r_i})^2}$</li> <li> <strong>Mean Absolute Error (MAE):</strong> The average of the absolute differences between predicted and actual ratings. Lower is better. $MAE = \frac{1}{N} \sum_{i=1}^{N} |r_i - \hat{r_i}|$</li> </ul> </li> <li> <strong>For Item Ranking/Recommendation (e.g., predicting which items a user will click on):</strong> <ul> <li> <strong>Precision, Recall, F1-score:</strong> Standard classification metrics, adapted for recommendations (e.g., “precision@k” measures precision among the top $k$ recommended items).</li> <li> <strong>MAP (Mean Average Precision):</strong> A ranking-aware metric.</li> <li> <strong>NDCG (Normalized Discounted Cumulative Gain):</strong> Another ranking metric that accounts for the position of relevant items.</li> </ul> </li> </ul> <p>Beyond these quantitative metrics, <strong>A/B testing</strong> is crucial. Deploy different recommender versions to small user groups and see which performs better on real-world business metrics like click-through rates, conversion rates, or retention.</p> <h3 id="your-turn-building-your-own-recommender">Your Turn: Building Your Own Recommender!</h3> <p>The beauty of Data Science and Machine Learning is that you don’t need to be a giant tech company to experiment. Libraries like <code class="language-plaintext highlighter-rouge">Surprise</code> in Python make it incredibly easy to get started with various collaborative filtering algorithms, including SVD. <code class="language-plaintext highlighter-rouge">LightFM</code> offers a hybrid approach, and for deep learning enthusiasts, building recommenders with TensorFlow or PyTorch is a popular path.</p> <p>My advice? Find a dataset (MovieLens is a classic starting point!), pick an algorithm, and start coding! The best way to understand these systems is to build one yourself.</p> <h3 id="the-future-is-personalized">The Future is Personalized</h3> <p>Recommender Systems are a testament to the power of data and algorithms to enhance our daily lives. From simple similarity metrics to complex deep learning models incorporating sequence information and reinforcement learning (learning from user feedback in real-time), this field is constantly evolving.</p> <p>As we continue to generate more data, and as computational power grows, recommender systems will only become more sophisticated, intuitive, and, hopefully, more ethical and transparent. They are not just about suggesting the next product; they are about shaping our digital experiences, helping us discover, learn, and connect in an increasingly vast and complex world.</p> <p>So, the next time Netflix asks, “Are you still watching?”, give a nod to the brilliant algorithms working tirelessly behind the scenes, anticipating your desires and guiding your discovery journey. It’s a field brimming with innovation, and there’s never been a better time to explore it!</p> <p>Keep learning, keep building, and keep being curious!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>