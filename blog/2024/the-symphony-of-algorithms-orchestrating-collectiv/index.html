<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Symphony of Algorithms: Orchestrating Collective Intelligence with Ensemble Learning | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/the-symphony-of-algorithms-orchestrating-collectiv/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Symphony of Algorithms: Orchestrating Collective Intelligence with Ensemble Learning</h1> <p class="post-meta"> Created on August 12, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a> ¬† ¬∑ ¬† <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a> ¬† <a href="/blog/blog/tag/ensemble-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Ensemble Learning</a> ¬† <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a> ¬† <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a> ¬† <a href="/blog/blog/tag/model-performance"> <i class="fa-solid fa-hashtag fa-sm"></i> Model Performance</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone! üëã</p> <p>Today, I want to talk about one of the coolest, most intuitive, and incredibly powerful concepts in machine learning: <strong>Ensemble Learning</strong>. If you‚Äôve ever felt stuck with a model that just wasn‚Äôt performing quite right, or if you‚Äôre curious about how top data scientists achieve those elusive high scores, then you‚Äôre in the right place. Think of it not just as a technique, but as a philosophy ‚Äì the idea that <em>more heads are often better than one</em>.</p> <h2 id="the-power-of-we">The Power of ‚ÄúWe‚Äù</h2> <p>Imagine you‚Äôre trying to predict the outcome of a complex situation, say, the winner of a major sports tournament. You could ask one expert, a lone analyst, to give their prediction. They might be very good, but they‚Äôre still just one person with one perspective. Now, imagine you ask <em>a whole panel</em> of experts, each with slightly different specializations, experiences, and even biases. Some might be great at analyzing player form, others at team strategy, and some at historical data. If you combine their predictions intelligently, weighing their strengths and weaknesses, chances are your collective prediction will be far more accurate and robust than any single expert‚Äôs guess.</p> <p>This, in essence, is the heart of Ensemble Learning. Instead of relying on a single ‚Äúexpert‚Äù (a single machine learning model), we build and combine multiple models (our ‚Äúpanel of experts‚Äù) to make a final, often superior, prediction.</p> <h3 id="why-not-just-one-super-model">Why Not Just One Super Model?</h3> <p>You might ask, ‚ÄúWhy bother with multiple models when I can just train one really complex, powerful model?‚Äù That‚Äôs a great question! Single, complex models, while capable, often come with their own set of challenges:</p> <ul> <li> <strong>Overfitting</strong>: A single model might become too specialized in the training data, learning the noise alongside the signal, and performing poorly on new, unseen data.</li> <li> <strong>Underfitting</strong>: A simple model might not capture the underlying patterns in the data effectively, leading to high error rates.</li> <li> <strong>Sensitivity to Noise</strong>: A single model can be easily swayed by outliers or noisy data points.</li> <li> <strong>Lack of Robustness</strong>: Its performance can vary significantly with small changes in the training data.</li> </ul> <p>Ensemble methods tackle these issues by leveraging the ‚Äúwisdom of crowds‚Äù principle. While individual models might make errors, they often make <em>different</em> errors. By combining them, these errors can cancel each other out, leading to a more generalized, robust, and accurate overall prediction.</p> <h2 id="diving-into-the-ensemble-arsenal">Diving into the Ensemble Arsenal</h2> <p>There are many ways to combine models, but three prominent techniques form the bedrock of ensemble learning: <strong>Bagging</strong>, <strong>Boosting</strong>, and <strong>Stacking</strong>. Let‚Äôs unpack each one.</p> <h3 id="1-bagging-bootstrap-aggregating-reducing-variance-with-parallel-wisdom">1. Bagging (Bootstrap Aggregating): Reducing Variance with Parallel Wisdom</h3> <p><strong>Analogy:</strong> Imagine a classroom where the teacher wants to assess students‚Äô understanding of a large textbook. Instead of having one student read the entire book and take a test, the teacher makes multiple <em>slightly different</em> versions of the textbook by omitting or re-emphasizing certain chapters for different students. Each student studies their version and takes a test. To get the final ‚Äúclass score,‚Äù the teacher averages all the individual student scores. This helps smooth out the individual quirks or misunderstandings of any single student.</p> <p><strong>The Gist:</strong> Bagging focuses on <strong>reducing variance</strong>. It works by training multiple models of the <em>same type</em> (e.g., multiple decision trees) on different, random subsets of the training data. Each model then makes a prediction, and these predictions are combined (e.g., by averaging for regression or majority voting for classification) to get the final output.</p> <p><strong>How it Works (Step-by-Step):</strong></p> <ol> <li> <strong>Bootstrap Sampling</strong>: From your original training dataset of size $N$, you create $B$ new datasets. Each new dataset is created by randomly sampling $N$ data points from the original dataset <em>with replacement</em>. This means some original data points might appear multiple times in a new dataset, while others might not appear at all. This introduces diversity among the training sets.</li> <li> <strong>Parallel Training</strong>: You train $B$ independent base models (often called ‚Äúbase learners‚Äù or ‚Äúweak learners‚Äù) ‚Äì typically decision trees ‚Äì one on each of the $B$ bootstrap samples. Because the samples are different, each model will learn slightly different patterns and make different errors.</li> <li> <strong>Aggregation</strong>: <ul> <li> <strong>For Regression</strong>: The final prediction is the average of the predictions from all $B$ base models: \(\hat{f}_{\text{bag}}(x) = \frac{1}{B} \sum_{b=1}^{B} \hat{f}\_b(x)\) where $\hat{f}_b(x)$ is the prediction of the $b$-th model for input $x$.</li> <li> <strong>For Classification</strong>: The final prediction is determined by majority voting among the $B$ base models. If 51 out of 100 models predict ‚Äúcat‚Äù and 49 predict ‚Äúdog‚Äù, the ensemble predicts ‚Äúcat‚Äù.</li> </ul> </li> </ol> <p><strong>A Star Example: Random Forest</strong></p> <p>The most famous example of bagging is the <strong>Random Forest</strong>. It‚Äôs essentially bagging applied to decision trees, with an added twist: when building each tree, instead of considering all features for splitting, it only considers a random subset of features. This further decorrelates the trees, making the ensemble even more robust and powerful. Random Forests are incredibly versatile and often serve as a strong baseline model for many tasks.</p> <p><strong>Pros of Bagging:</strong></p> <ul> <li> <strong>Reduces Variance</strong>: By averaging or voting, the impact of individual model‚Äôs noisy predictions is smoothed out.</li> <li> <strong>Robustness</strong>: Less prone to overfitting compared to a single complex model.</li> <li> <strong>Parallelizable</strong>: Each base model can be trained independently, making it computationally efficient.</li> </ul> <p><strong>Cons of Bagging:</strong></p> <ul> <li> <strong>Interpretability</strong>: The final model becomes a black box, harder to interpret than a single decision tree.</li> <li> <strong>Bias</strong>: If the base models are inherently biased, bagging might not significantly reduce this bias.</li> </ul> <h3 id="2-boosting-sequential-learning-from-mistakes">2. Boosting: Sequential Learning from Mistakes</h3> <p><strong>Analogy:</strong> Think of a student who takes a series of quizzes. After each quiz, they review their mistakes, focus on the concepts they struggled with, and study those areas more intensely before the next quiz. Over time, by iteratively correcting their weaknesses, their overall understanding and performance significantly improve.</p> <p><strong>The Gist:</strong> Boosting focuses on <strong>reducing bias</strong> and transforming weak learners into strong ones. Unlike bagging, where models are trained independently, boosting trains models <strong>sequentially</strong>. Each new model in the sequence is trained to correct the errors made by the previous models. It pays more attention to the data points that the previous models misclassified or predicted poorly.</p> <p><strong>How it Works (Step-by-Step, conceptually):</strong></p> <ol> <li> <strong>Initial Model</strong>: A simple base model (often a ‚Äúweak learner,‚Äù like a shallow decision tree) is trained on the original dataset.</li> <li> <strong>Error Identification</strong>: The model makes predictions, and its errors (misclassifications or large prediction residuals) are identified.</li> <li> <strong>Weight Adjustment</strong>: Data points that were difficult to classify or predict accurately by the previous model are given higher ‚Äúweights‚Äù or emphasis for the next model. Conversely, easily classified points get less weight.</li> <li> <strong>Sequential Training</strong>: A new base model is trained on this re-weighted dataset, focusing more on the ‚Äúhard‚Äù examples.</li> <li> <strong>Iteration</strong>: Steps 2-4 are repeated for many iterations, with each new model trying to improve upon the collective performance of the previous ones.</li> <li> <strong>Weighted Combination</strong>: The final prediction is a weighted sum of the predictions from all base models. Models that performed better on harder examples might have more influence.</li> </ol> <p><strong>Popular Boosting Algorithms:</strong></p> <ul> <li> <strong>AdaBoost (Adaptive Boosting)</strong>: One of the earliest boosting algorithms. It specifically re-weights data points based on previous errors and gives more weight to more accurate models.</li> <li> <strong>Gradient Boosting</strong>: A more generalized boosting approach that minimizes a loss function by iteratively adding weak learners that ‚Äústep‚Äù in the direction of the negative gradient of the loss function. It literally builds on the residuals (errors). <ul> <li> <strong>XGBoost, LightGBM, CatBoost</strong>: These are highly optimized and incredibly popular implementations of gradient boosting, known for their speed and state-of-the-art performance in structured data competitions.</li> </ul> </li> </ul> <p><strong>Pros of Boosting:</strong></p> <ul> <li> <strong>High Accuracy</strong>: Often achieves excellent performance, especially with powerful implementations like XGBoost.</li> <li> <strong>Reduces Bias</strong>: Effectively converts weak learners into strong learners.</li> <li> <strong>Handles Complex Relationships</strong>: Can model complex non-linear relationships in the data.</li> </ul> <p><strong>Cons of Boosting:</strong></p> <ul> <li> <strong>Sequential Nature</strong>: Training is sequential, making it slower and harder to parallelize than bagging.</li> <li> <strong>Sensitivity to Noise/Outliers</strong>: Because it focuses on ‚Äúhard‚Äù examples, noisy data or outliers can be given excessive weight, potentially leading to overfitting.</li> <li> <strong>Complex Hyperparameters</strong>: Requires careful tuning of parameters to avoid overfitting.</li> </ul> <h3 id="3-stacking-stacked-generalization-the-meta-learners-synthesis">3. Stacking (Stacked Generalization): The Meta-Learner‚Äôs Synthesis</h3> <p><strong>Analogy:</strong> Imagine a highly specialized committee. You have an economist, a sociologist, and a political scientist, each providing their expert opinion on a complex societal issue. Instead of just averaging their opinions, you have a <strong>Chief Strategist</strong> (the meta-learner) whose job it is to <em>understand how each expert thinks</em>, synthesize their individual viewpoints, and then make the final, most informed decision. The Chief Strategist doesn‚Äôt just average; they learn <em>from the predictions of the experts themselves</em>.</p> <p><strong>The Gist:</strong> Stacking combines predictions from diverse models using another machine learning model, called a <strong>meta-learner</strong> or <strong>blender</strong>. It leverages the strengths of different types of models.</p> <p><strong>How it Works (Step-by-Step, Simplified):</strong></p> <ol> <li> <strong>Level 0 Models (Base-Learners)</strong>: You train several different types of models (e.g., a Logistic Regression, a Support Vector Machine, and a Random Forest) on your original training data. These are your ‚Äúexpert opinions.‚Äù</li> <li> <strong>Generate Predictions</strong>: Each Level 0 model makes predictions on a <em>separate</em> validation set (or, more commonly, uses k-fold cross-validation on the training data to generate ‚Äúout-of-fold‚Äù predictions). These predictions are then used as features for the next level.</li> <li> <strong>Level 1 Model (Meta-Learner)</strong>: A new model (the meta-learner) is trained. Its input features are the <em>predictions</em> generated by the Level 0 models, and its target variable is the original target variable from your dataset. This meta-learner learns the optimal way to combine the predictions of the base models.</li> <li> <strong>Final Prediction</strong>: When you have new, unseen data, you first pass it through all your Level 0 models to get their predictions. Then, you feed these Level 0 predictions into your trained Level 1 meta-learner, which makes the ultimate final prediction.</li> </ol> <p><strong>A Crucial Detail: Preventing Data Leakage</strong></p> <p>A common mistake in stacking is to train Level 0 models and then immediately use their predictions on the <em>same</em> training data to train the Level 1 model. This is data leakage! The Level 0 models would have ‚Äúseen‚Äù the answers for those predictions, making the Level 1 model overfit. To avoid this, techniques like k-fold cross-validation are used:</p> <ul> <li>The training data is split into $k$ folds.</li> <li>For each fold, a Level 0 model is trained on the other $k-1$ folds and makes predictions on the current fold.</li> <li>These ‚Äúout-of-fold‚Äù predictions for the entire training set (where each base model has only predicted on data it hasn‚Äôt seen during its training) then become the features for the Level 1 meta-learner.</li> </ul> <p><strong>Pros of Stacking:</strong></p> <ul> <li> <strong>Potentially Highest Accuracy</strong>: Often achieves state-of-the-art results by leveraging the complementary strengths of diverse models.</li> <li> <strong>Versatility</strong>: Can combine virtually any type of model.</li> </ul> <p><strong>Cons of Stacking:</strong></p> <ul> <li> <strong>Complexity</strong>: More involved to implement and tune than bagging or boosting.</li> <li> <strong>Computational Cost</strong>: Requires training multiple models and then another model on their outputs.</li> <li> <strong>Risk of Overfitting</strong>: If not implemented carefully (especially regarding data leakage), the meta-learner can overfit.</li> </ul> <h2 id="when-to-use-which-a-quick-guide">When to Use Which? A Quick Guide</h2> <ul> <li> <strong>Bagging (e.g., Random Forest)</strong>: When your base models are prone to high variance or overfitting (like deep decision trees). Great for robustness and parallel processing. It works well if your individual models are good but too ‚Äúfinicky.‚Äù</li> <li> <strong>Boosting (e.g., XGBoost, LightGBM)</strong>: When you need high accuracy and your base models are weak learners (like shallow decision trees). Excellent for complex datasets where you need to reduce bias. Be mindful of overfitting and tuning.</li> <li> <strong>Stacking</strong>: When you want to combine the unique strengths of fundamentally <em>different</em> types of models and push for the absolute highest performance, often in competitive scenarios. It‚Äôs the most sophisticated and often the most powerful, but also the most complex.</li> </ul> <h2 id="my-takeaway-and-your-next-step">My Takeaway and Your Next Step</h2> <p>My journey into machine learning deepened significantly when I truly grasped the ‚Äúwhy‚Äù behind ensemble methods. It‚Äôs not just a trick; it‚Äôs a profound application of collective intelligence. Understanding these techniques empowers you to move beyond single-model limitations and build truly robust and high-performing systems.</p> <p>So, what‚Äôs your next step? I encourage you to:</p> <ol> <li> <strong>Experiment</strong>: Pick a dataset, train a simple decision tree, then try a Random Forest. Observe the performance difference.</li> <li> <strong>Explore</strong>: Dive into a Gradient Boosting library like XGBoost. See how its parameters influence its performance.</li> <li> <strong>Build</strong>: Challenge yourself to implement a basic stacking ensemble. It‚Äôs a fantastic learning experience!</li> </ol> <p>Ensemble learning isn‚Äôt just a powerful tool; it‚Äôs a testament to the idea that sometimes, the greatest strength lies in collaboration. Happy modeling!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>