<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Bayesian Way: How to Smartly Update Your Beliefs with Data | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/the-bayesian-way-how-to-smartly-update-your-belief/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Bayesian Way: How to Smartly Update Your Beliefs with Data</h1> <p class="post-meta"> Created on June 30, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/bayesian-statistics"> <i class="fa-solid fa-hashtag fa-sm"></i> Bayesian Statistics</a>   <a href="/blog/blog/tag/probability"> <i class="fa-solid fa-hashtag fa-sm"></i> Probability</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/uncertainty-quantification"> <i class="fa-solid fa-hashtag fa-sm"></i> Uncertainty Quantification</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>As a budding data scientist, one of the most exciting aspects of this field is the constant evolution of how we interpret and learn from data. Early on, like many, I was introduced to the frequentist school of thought — p-values, confidence intervals, and null hypothesis testing. They’re fundamental, no doubt. But then I stumbled upon something different, something that felt… more human, more intuitive: Bayesian statistics.</p> <p>It wasn’t just a new set of formulas; it was a shift in perspective. Instead of seeing probabilities as long-run frequencies of events, the Bayesian approach invited me to think of probabilities as <em>degrees of belief</em>. And critically, it provided a rigorous mathematical framework to <em>update</em> those beliefs as new evidence emerged. It felt like learning how to reason under uncertainty, systematically, just like we often do in our daily lives, but with the power of mathematics.</p> <h3 id="the-curious-case-of-the-unknown">The Curious Case of the Unknown</h3> <p>Imagine you’re trying to figure out if a coin is fair. If you flip it 100 times and get 52 heads, what do you conclude? A frequentist might tell you that, based on this data, we can’t reject the null hypothesis that the coin is fair, as a p-value might indicate this outcome is reasonably likely under a fair coin assumption. They’d focus on the probability of seeing this data <em>given</em> a fair coin.</p> <p>Now, let’s bring in a twist. What if you <em>know</em> this particular coin comes from a batch where 90% of coins are fair, and 10% are heavily biased towards heads (say, 80% heads)? Does that prior knowledge change your interpretation of those 52 heads out of 100 flips? Absolutely! You’d likely still lean towards it being a fair coin, given its origin, despite the slight lean towards heads in your experiment.</p> <p>This is the essence of Bayesian thinking: <strong>incorporating prior knowledge or beliefs and updating them with new data to form a new, more informed belief.</strong></p> <h3 id="frequentist-vs-bayesian-a-philosophical-divide">Frequentist vs. Bayesian: A Philosophical Divide</h3> <p>To truly appreciate Bayesian statistics, it helps to briefly contrast it with its elder sibling, the Frequentist approach:</p> <ul> <li> <strong>Frequentist Perspective:</strong> <ul> <li> <strong>Probability:</strong> Defined as the long-run frequency of an event if an experiment were repeated many times.</li> <li> <strong>Parameters:</strong> Assumed to be fixed, but unknown, constants. We try to estimate them using data.</li> <li> <strong>Focus:</strong> On the probability of observing the data <em>given</em> a specific hypothesis (e.g., p-values, confidence intervals). “What’s the probability of observing 52 heads in 100 flips <em>if</em> the coin is fair?”</li> </ul> </li> <li> <strong>Bayesian Perspective:</strong> <ul> <li> <strong>Probability:</strong> Defined as a degree of belief or confidence in an event.</li> <li> <strong>Parameters:</strong> Treated as random variables that have probability distributions. We’re interested in the <em>distribution of plausible parameter values</em>.</li> <li> <strong>Focus:</strong> On the probability of a hypothesis <em>given</em> the observed data (e.g., posterior distributions). “What’s the probability that the coin is fair <em>given</em> I observed 52 heads in 100 flips?”</li> </ul> </li> </ul> <p>The key difference is what we consider “random” and what we consider “fixed.” For Frequentists, data is random, parameters are fixed. For Bayesians, data is fixed (once observed), and parameters are random (we have beliefs about them).</p> <h3 id="the-heart-of-bayesianism-bayes-theorem">The Heart of Bayesianism: Bayes’ Theorem</h3> <p>At the core of all Bayesian statistics lies a remarkably simple yet profound formula: <strong>Bayes’ Theorem</strong>. It’s not new; Thomas Bayes first developed it in the 18th century. What’s new is our computational power to make it practical.</p> <p>The theorem is expressed as:</p> \[P(H|E) = \frac{P(E|H) P(H)}{P(E)}\] <p>Let’s break down what each term means:</p> <ul> <li> <table> <tbody> <tr> <td>**$P(H</td> <td>E)$ (Posterior Probability):** This is what we <em>want</em> to know. It’s the probability of our <strong>Hypothesis (H)</strong> being true <em>given</em> the <strong>Evidence (E)</strong> we’ve observed. This is our updated belief after seeing the data.</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>**$P(E</td> <td>H)$ (Likelihood):** This tells us how likely we were to observe the <strong>Evidence (E)</strong> <em>if</em> our <strong>Hypothesis (H)</strong> were true. It quantifies how well the hypothesis explains the observed data.</td> </tr> </tbody> </table> </li> <li> <p><strong>$P(H)$ (Prior Probability):</strong> This represents our initial belief in the <strong>Hypothesis (H)</strong> <em>before</em> we’ve seen any new evidence. This is where our prior knowledge, or even an educated guess, comes into play.</p> </li> <li> <strong>$P(E)$ (Marginal Likelihood or Evidence):</strong> This is the overall probability of observing the <strong>Evidence (E)</strong>, regardless of whether our hypothesis is true or not. It acts as a normalizing constant, ensuring that our posterior probabilities sum to 1. For practical purposes, especially when comparing hypotheses, we often just focus on the numerator and consider $P(E)$ as a scaling factor. <ul> <li> <table> <tbody> <tr> <td>$P(E) = \sum_{i} P(E</td> <td>H_i) P(H_i)$ (sum over all possible hypotheses $H_i$)</td> </tr> </tbody> </table> </li> </ul> </li> </ul> <p>Essentially, Bayes’ Theorem states: <strong>Posterior is proportional to Likelihood times Prior.</strong></p> <p>Let’s illustrate with a classic example: a medical test.</p> <h3 id="a-real-world-walkthrough-the-medical-test-dilemma">A Real-World Walkthrough: The Medical Test Dilemma</h3> <p>Imagine a rare disease that affects <strong>1 in 1000</strong> people in the general population. There’s a test for this disease that’s pretty good:</p> <ul> <li>It correctly identifies the disease <strong>99%</strong> of the time (True Positive Rate).</li> <li>It incorrectly gives a positive result to healthy people <strong>5%</strong> of the time (False Positive Rate).</li> </ul> <p>Now, suppose you test positive. How worried should you be? What’s the probability that you actually have the disease, given your positive test result?</p> <p>Let’s define our terms for Bayes’ Theorem:</p> <ul> <li> <strong>H:</strong> You have the disease.</li> <li> <strong>$\neg H$:</strong> You do <em>not</em> have the disease.</li> <li> <strong>E:</strong> You test positive.</li> </ul> <ol> <li> <strong>Prior Probability, $P(H)$:</strong> Our initial belief that you have the disease, before the test. <ul> <li>$P(H) = 1/1000 = 0.001$ (since it affects 1 in 1000 people).</li> <li>$P(\neg H) = 1 - P(H) = 0.999$</li> </ul> </li> <li> <table> <tbody> <tr> <td>**Likelihood, $P(E</td> <td>H)$:** The probability of testing positive <em>if</em> you have the disease.</td> </tr> </tbody> </table> <ul> <li> <table> <tbody> <tr> <td>$P(E</td> <td>H) = 0.99$ (True Positive Rate).</td> </tr> </tbody> </table> </li> </ul> </li> <li> <table> <tbody> <tr> <td>**Likelihood, $P(E</td> <td>\neg H)$:** The probability of testing positive <em>if</em> you do <em>not</em> have the disease (False Positive Rate).</td> </tr> </tbody> </table> <ul> <li> <table> <tbody> <tr> <td>$P(E</td> <td>\neg H) = 0.05$</td> </tr> </tbody> </table> </li> </ul> </li> </ol> <p>Now, let’s calculate the <strong>Marginal Likelihood, $P(E)$</strong>, the overall probability of a positive test result, considering both scenarios (having the disease or not): \(P(E) = P(E|H)P(H) + P(E|\neg H)P(\neg H)\) \(P(E) = (0.99)(0.001) + (0.05)(0.999)\) \(P(E) = 0.00099 + 0.04995\) \(P(E) = 0.05094\)</p> <table> <tbody> <tr> <td>Finally, we can calculate the **Posterior Probability, $P(H</td> <td>E)$**:</td> <td> </td> </tr> <tr> <td>$$ P(H</td> <td>E) = \frac{P(E</td> <td>H) P(H)}{P(E)} $$</td> </tr> <tr> <td>$$ P(H</td> <td>E) = \frac{(0.99)(0.001)}{0.05094} $$</td> <td> </td> </tr> <tr> <td>$$ P(H</td> <td>E) = \frac{0.00099}{0.05094} $$</td> <td> </td> </tr> <tr> <td>$$ P(H</td> <td>E) \approx 0.0194 $$</td> <td> </td> </tr> </tbody> </table> <p>So, even with a positive test result from a “99% accurate” test, your probability of actually having the disease is only about <strong>1.94%</strong>!</p> <p>This result often surprises people. Why so low? Because the disease is so rare (low prior probability). The high number of false positives among the vast majority of healthy people overwhelms the few true positives. Your prior belief (very low chance of having the disease) heavily influences the updated belief. This example powerfully demonstrates how Bayesian reasoning allows us to integrate all available information, leading to more nuanced and often counter-intuitive, yet correct, conclusions.</p> <h3 id="why-bayesian-statistics-for-data-science-and-machine-learning">Why Bayesian Statistics for Data Science and Machine Learning?</h3> <p>For those of us in Data Science and Machine Learning, Bayesian statistics offers a powerful toolkit, especially when dealing with complex problems:</p> <ol> <li> <p><strong>Incorporating Prior Knowledge:</strong> In many real-world scenarios, we aren’t starting from scratch. We have domain expertise, historical data, or expert opinions. Bayesian methods provide a natural way to integrate this prior information directly into our models. This is invaluable in fields like drug trials (where previous research is vital), personalized medicine, or when dealing with rare events.</p> </li> <li> <p><strong>Quantifying Uncertainty:</strong> Instead of just getting a single “best estimate” for a parameter (like a mean or a regression coefficient), Bayesian methods give us an entire probability distribution for that parameter. This “posterior distribution” tells us not just what the most likely value is, but also the <em>range of plausible values</em> and how likely each value is. This is crucial for robust decision-making, allowing us to say, “There’s a 95% probability that the conversion rate is between 1.2% and 1.8%,” rather than just, “The conversion rate is 1.5%.”</p> </li> <li> <p><strong>Small Data Problems:</strong> When data is scarce, frequentist methods can struggle, leading to unstable estimates or overly wide confidence intervals. Bayesian methods can shine here because the prior acts as a form of regularization, helping to constrain the possible parameter values and provide more reasonable estimates, even with limited data.</p> </li> <li> <p><strong>Interpretability:</strong> Bayesian conclusions are often more intuitive to interpret. We can directly make probability statements about hypotheses or parameter values (e.g., “The probability that Model A is better than Model B is 80%”). This contrasts with frequentist p-values, which are statements about the data given a null hypothesis, and are often misinterpreted.</p> </li> <li> <p><strong>Hierarchical Models:</strong> Bayesian statistics provides a natural framework for building hierarchical models, which are excellent for handling complex data structures with multiple levels (e.g., students nested within schools, nested within districts). We can model how parameters vary across these different levels while still sharing information.</p> </li> <li> <p><strong>Model Comparison:</strong> Tools like Bayes Factors offer a principled way to compare different models, directly quantifying the evidence in favor of one model over another.</p> </li> </ol> <h3 id="the-road-ahead-challenges-and-tools">The Road Ahead: Challenges and Tools</h3> <p>While incredibly powerful, Bayesian statistics does come with its own set of considerations:</p> <ul> <li> <strong>Choosing Priors:</strong> Selecting an appropriate prior can sometimes feel subjective. However, for many problems, there are established guidelines (e.g., weakly informative priors, conjugate priors), and with enough data, the likelihood often dominates the prior anyway.</li> <li> <strong>Computational Complexity:</strong> For many models, especially those with many parameters or complex structures, directly calculating the posterior distribution is mathematically intractable. This is where modern computational methods like <strong>Markov Chain Monte Carlo (MCMC)</strong> come to the rescue. Algorithms like Metropolis-Hastings and Hamiltonian Monte Carlo allow us to <em>sample</em> from the posterior distribution, giving us an approximation. These methods can be computationally intensive and require careful tuning.</li> </ul> <p>Fortunately, the ecosystem for Bayesian modeling has matured significantly. Libraries like <strong>PyMC</strong> (in Python) and <strong>Stan</strong> (with interfaces for Python, R, and Julia) make it accessible to specify, fit, and analyze complex Bayesian models without having to write MCMC samplers from scratch. These tools have democratized Bayesian methods, making them practical for a wide range of real-world data science problems.</p> <h3 id="my-bayesian-journey-continues">My Bayesian Journey Continues…</h3> <p>Embracing Bayesian statistics has been a profound shift in how I approach data problems. It’s not just about the numbers; it’s about a more holistic, adaptive way of thinking about knowledge and uncertainty. It teaches you to be explicit about your assumptions, to quantify your doubt, and to always be ready to update your worldview when new evidence comes knocking.</p> <p>If you’re a student or practitioner in data science, I strongly encourage you to dive deeper into the Bayesian world. Start with the basics, work through some examples, and then experiment with the powerful libraries available. You’ll find it’s not just a mathematical framework; it’s a philosophy that makes you a more thoughtful and effective data scientist, ready to tackle the complex uncertainties of the real world, one updated belief at a time.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>