<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> From Noise to Masterpiece: A Journey into Diffusion Models | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/from-noise-to-masterpiece-a-journey-into-diffusion/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">From Noise to Masterpiece: A Journey into Diffusion Models</h1> <p class="post-meta"> Created on September 21, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/generative-ai"> <i class="fa-solid fa-hashtag fa-sm"></i> Generative AI</a>   <a href="/blog/blog/tag/diffusion-models"> <i class="fa-solid fa-hashtag fa-sm"></i> Diffusion Models</a>   <a href="/blog/blog/tag/image-synthesis"> <i class="fa-solid fa-hashtag fa-sm"></i> Image Synthesis</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>My journey into the world of artificial intelligence has been a constant source of wonder, but few concepts have captivated my imagination quite like Diffusion Models. When I first saw images generated by models like DALL-E or Midjourney, my jaw literally dropped. The sheer creativity, the photorealism, the ability to conjure almost anything from a simple text prompt – it felt like peering into the future. And at the heart of much of this magic? Diffusion Models.</p> <p>For those of you who, like me, are fascinated by how AI can create, and perhaps even for those just starting to dip their toes into the vast ocean of machine learning, I want to share my understanding of these incredible models. Forget complex jargon for a moment; let’s think about something simpler: the act of “un-noising.”</p> <h3 id="the-art-of-un-noising-an-intuitive-start">The Art of Un-Noising: An Intuitive Start</h3> <p>Imagine you have a beautiful, crystal-clear photograph. Now, I slowly start adding static, pixel by pixel, until the image is completely obscured, turning into pure, random visual noise – like an old TV screen showing nothing but “snow.”</p> <p>Now, for the tricky part: can you reverse that process? If I showed you the completely noisy image, could you figure out how to <em>remove</em> the static, little by little, until the original photo reappeared? Sounds impossible for a human, right? But this, in essence, is what Diffusion Models learn to do. They are masters of “un-noising.”</p> <h3 id="part-1-the-forward-pass--embracing-chaos">Part 1: The Forward Pass – Embracing Chaos</h3> <p>The first step in understanding Diffusion Models is what we call the <strong>forward diffusion process</strong>. This is the part where we <em>deliberately</em> add noise to an image. It’s a simple, predictable, and fixed process. Think of it like this:</p> <p>We start with our original, pristine image, let’s call it $x_0$. At each step $t$ (from $t=1$ up to a maximum $T$, say 1000 steps), we add a tiny bit of Gaussian noise to the image from the previous step, $x_{t-1}$.</p> <p>The mathematical way to express this incremental addition of noise is: \(x_t = \sqrt{1 - \beta_t} x_{t-1} + \sqrt{\beta_t} \epsilon\)</p> <p>Let’s break that down:</p> <ul> <li>$x_t$: The image at time step $t$.</li> <li>$x_{t-1}$: The image at the previous time step.</li> <li>$\beta_t$: A small value (the “variance schedule” or “noise schedule”) that dictates <em>how much</em> noise is added at step $t$. It usually increases slightly over time, meaning more noise is added in later steps.</li> <li>$\epsilon$: This is random noise, sampled from a standard normal distribution $\mathcal{N}(0, \mathbf{I})$ (meaning a bell-curve distribution with a mean of 0 and a standard deviation of 1). It’s the “pure static” we’re adding.</li> <li>The $\sqrt{1 - \beta_t}$ and $\sqrt{\beta_t}$ terms ensure that the overall variance (or “energy”) of the image remains stable as noise is added, preventing the image values from exploding or disappearing.</li> </ul> <p>This process continues for many steps ($T$), gradually transforming $x_0$ into $x_T$, which is practically pure, random Gaussian noise. It’s like slowly dissolving a perfect painting in water until only murky, colorless liquid remains. The beauty of this forward process is that it’s completely deterministic – we know exactly how much noise we added at each step, and we can even jump directly to any noisy version $x_t$ from $x_0$ using a clever mathematical trick. This direct sampling ability is crucial for efficient training!</p> <h3 id="part-2-the-reverse-pass--learning-to-denoise">Part 2: The Reverse Pass – Learning to Denoise</h3> <p>Now for the real magic! If the forward process is about turning an image into noise, the <strong>reverse diffusion process</strong> is about turning noise back into an image. This is where our deep learning model comes in.</p> <table> <tbody> <tr> <td>Our goal is to learn how to gradually remove the noise, step by step, starting from pure noise $x_T$ and going back to a clean image $x_0$. Mathematically, we want to learn the probability distribution $p_\theta(x_{t-1}</td> <td>x_t)$ – that is, given a noisy image $x_t$, what’s the likelihood of it being transformed into the slightly less noisy image $x_{t-1}$?</td> </tr> </tbody> </table> <p>This reverse step is much harder than the forward step because we don’t <em>know</em> how to perfectly subtract the exact noise that was added. The model needs to <em>predict</em> it.</p> <p>Instead of directly predicting $x_{t-1}$, it turns out to be much more effective for the model to predict the <em>noise itself</em> ($\epsilon$) that was added to create $x_t$ from some hypothetical cleaner version.</p> <p>So, our neural network, often a type of architecture called a <strong>U-Net</strong> (which is excellent at processing images and maintaining spatial details), takes two inputs:</p> <ol> <li>The current noisy image $x_t$.</li> <li>The current time step $t$ (this is important, as the amount and type of noise change over time).</li> </ol> <p>And its output? An estimation of the noise that needs to be removed from $x_t$ to get to a cleaner state. Let’s call this predicted noise $\epsilon_\theta(x_t, t)$, where $\theta$ represents all the learnable parameters of our neural network.</p> <p>Once we have this predicted noise, we can use it to take a step back: \(x_{t-1} = \frac{1}{\sqrt{1 - \beta_t}} \left( x_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_\theta(x_t, t) \right) + \sigma_t z\) where $\bar{\alpha}<em>t = \prod</em>{s=1}^t (1 - \beta_s)$, $\sigma_t$ is a specific variance term, and $z \sim \mathcal{N}(0, \mathbf{I})$ is additional random noise to ensure diversity in generations. Don’t get bogged down by the exact formula here; the key idea is that the model <em>predicts the noise component</em>, and we use that prediction to iteratively subtract noise and clarify the image.</p> <h3 id="training-the-noise-whisperer">Training the Noise Whisperer</h3> <p>How does our model learn to be such a good “noise whisperer”? This is the elegant part of the training process.</p> <ol> <li> <strong>Start with a real image ($x_0$)</strong>: We pick an image from our dataset (e.g., a photo of a cat).</li> <li> <strong>Pick a random time step ($t$)</strong>: We randomly choose a number between 1 and $T$ (e.g., $t=350$).</li> <li> <strong>Generate a noisy version ($x_t$)</strong>: Using the <em>known</em> forward diffusion process, we directly create $x_t$ from $x_0$ by adding the correct amount of noise for step $t$. Importantly, we also know the <em>exact noise</em> $\epsilon$ that was added to get to $x_t$. This direct sampling from $x_0$ to $x_t$ is a crucial optimization that makes training much faster than step-by-step forward diffusion.</li> <li> <strong>Model predicts noise</strong>: We feed this noisy image $x_t$ and the time step $t$ into our neural network. The network tries to predict the noise, giving us $\epsilon_\theta(x_t, t)$.</li> <li> <strong>Calculate the error (Loss Function)</strong>: We compare the noise predicted by our model ($\epsilon_\theta(x_t, t)$) with the <em>actual noise</em> ($\epsilon$) that we added to create $x_t$. The difference between these two is our error. We typically use a simple Mean Squared Error (MSE) loss: \(L = E_{t \sim [1,T], x_0 \sim q(x_0), \epsilon \sim \mathcal{N}(0, \mathbf{I})} [ \| \epsilon - \epsilon_\theta(x_t, t) \|^2 ]\) This formula essentially says: “On average, across all possible time steps $t$, all original images $x_0$, and all possible random noises $\epsilon$, we want the squared difference between the noise we <em>actually</em> added and the noise our model <em>predicted</em> to be as small as possible.”</li> </ol> <p>We repeat this process millions of times with countless images and random noise steps. Through this iterative learning, the model gets incredibly good at predicting the noise component at any given step $t$. It learns the intricate dance of pixel manipulation required to reverse the diffusion process.</p> <h3 id="generating-a-masterpiece-the-sampling-process">Generating a Masterpiece: The Sampling Process</h3> <p>Once our model is trained, the fun begins – generation!</p> <ol> <li> <strong>Start with pure noise</strong>: We begin with a tensor of pure Gaussian noise, $x_T$, which is essentially a blank, chaotic canvas.</li> <li> <strong>Iterative Denoising</strong>: We then iterate backwards from $t=T$ down to $t=1$. <ul> <li>At each step $t$, we feed the current noisy image $x_t$ and the time step $t$ into our trained neural network.</li> <li>The model predicts the noise $\epsilon_\theta(x_t, t)$.</li> <li>We use this predicted noise (and the specific reverse step formula mentioned earlier) to calculate $x_{t-1}$, a slightly less noisy version of the image.</li> </ul> </li> <li> <strong>Voila!</strong>: After $T$ steps, we are left with $x_0$, a brand new, generated image that never existed before!</li> </ol> <p>This process is slow because it’s iterative, taking hundreds or thousands of steps for a single image. However, the quality it produces is often worth the wait.</p> <h3 id="why-diffusion-models-are-revolutionizing-generative-ai">Why Diffusion Models are Revolutionizing Generative AI</h3> <p>My fascination with Diffusion Models isn’t just because they produce pretty pictures; it’s because they bring significant advantages:</p> <ul> <li> <strong>Unparalleled Quality</strong>: They consistently generate incredibly high-quality, diverse, and photorealistic images. This is largely due to their stable training process and the way they learn to navigate the data distribution.</li> <li> <strong>Stable Training</strong>: Unlike Generative Adversarial Networks (GANs), which can be notoriously tricky to train due to their adversarial nature (two networks competing), Diffusion Models have a simple, well-defined loss function (MSE) that makes them much more stable and predictable to optimize.</li> <li> <strong>Diverse Outputs, Less Mode Collapse</strong>: Diffusion models are less prone to “mode collapse,” a problem where GANs might only learn to generate a limited variety of outputs. Because Diffusion Models learn to reconstruct <em>any</em> image from noise, they effectively explore the entire possible data landscape, leading to a wider range of diverse and creative generations.</li> <li> <strong>Versatility</strong>: Beyond images, diffusion models are being adapted for video generation, audio synthesis, 3D model creation, and even scientific applications like drug discovery.</li> </ul> <h3 id="a-glimpse-beyond-the-canvas">A Glimpse Beyond the Canvas</h3> <p>My journey into diffusion models has been nothing short of fascinating. While earlier generative models like Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs) laid crucial groundwork, diffusion models represent a significant leap forward in generating high-fidelity, diverse content. VAEs often produce blurrier images, and GANs, while capable of stunning results, suffer from training instability and mode collapse. Diffusion models have found a sweet spot, albeit with the tradeoff of slower sampling speeds.</p> <p>But even that “slow sampling” is being rapidly addressed! Researchers are developing innovative techniques like DDIM (Denoising Diffusion Implicit Models) and Consistency Models, which can drastically speed up the generation process, making diffusion models even more practical for real-world applications. The integration of conditioning (like text prompts in Stable Diffusion or DALL-E) has unlocked unprecedented control over generation, transforming these models into powerful creative assistants.</p> <p>As I continue to explore the capabilities of these models, I’m filled with excitement for what’s next. We’re truly living in an era where AI is not just analyzing data but actively creating new realities, and Diffusion Models are at the forefront of this artistic revolution. If you’re looking for a cutting-edge area in AI to dive into, I wholeheartedly recommend exploring the art of un-noising. It’s a journey from chaos to creation, one noise prediction at a time.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>