<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Untangling the Data Web: My Journey with Principal Component Analysis (PCA) | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/untangling-the-data-web-my-journey-with-principal/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Untangling the Data Web: My Journey with Principal Component Analysis (PCA)</h1> <p class="post-meta"> Created on April 26, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/dimensionality-reduction"> <i class="fa-solid fa-hashtag fa-sm"></i> Dimensionality Reduction</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/pca"> <i class="fa-solid fa-hashtag fa-sm"></i> PCA</a>   <a href="/blog/blog/tag/statistics"> <i class="fa-solid fa-hashtag fa-sm"></i> Statistics</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>As a budding data scientist, there are moments when you feel like a detective, sifting through mountains of evidence. And then there are moments when you feel like you’re drowning in it. I distinctly remember one project where I was trying to predict housing prices. I had everything: square footage, number of bedrooms, bathrooms, year built, ZIP code, school district ratings, proximity to parks, crime rates, average income, renovation history, solar panel presence, type of heating system, roof material… the list went on, exceeding 50 features! My models were slow, prone to overfitting, and honestly, even I couldn’t keep track of what was going on.</p> <p>That’s when I first truly appreciated the magic of <strong>Principal Component Analysis (PCA)</strong>. It felt like someone handed me a powerful magnifying glass that also knew how to declutter. Instead of trying to juggle all 50 features, PCA helped me find the <em>essence</em> of the data, allowing my models to breathe, perform better, and even become more understandable.</p> <h3 id="the-curse-of-dimensionality-why-less-can-be-more">The Curse of Dimensionality: Why Less Can Be More</h3> <p>Before we dive into PCA, let’s talk about the problem it solves: the “curse of dimensionality.” Imagine you’re trying to describe a point in 1 dimension (a line). Easy! You just need one number. In 2 dimensions (a flat surface), you need two numbers. In 3 dimensions (our world), you need three. Now imagine trying to describe something in 50, 100, or even 1000 dimensions!</p> <p>As the number of features (dimensions) in our dataset grows:</p> <ul> <li> <strong>Data becomes sparse:</strong> The data points become incredibly spread out, making it hard to find meaningful relationships. It’s like trying to find specific grains of sand on an infinitely expanding beach.</li> <li> <strong>Increased computational cost:</strong> More dimensions mean more calculations, leading to slower training times for machine learning models.</li> <li> <strong>Overfitting:</strong> Models can get too good at learning the noise in high-dimensional data, performing poorly on new, unseen data.</li> <li> <strong>Difficulty in visualization:</strong> Try plotting data with more than three features! It’s impossible for our human brains to comprehend.</li> </ul> <p><strong>Dimensionality reduction</strong> is the hero here. It’s the process of reducing the number of random variables under consideration by obtaining a set of principal variables. Think of it like compressing a large file without losing the critical information, or projecting a 3D object’s shadow onto a 2D wall – you lose some detail, but you get a clearer, more manageable representation.</p> <h3 id="pca-finding-the-most-important-directions">PCA: Finding the “Most Important” Directions</h3> <p>At its heart, PCA is a technique for <strong>linear dimensionality reduction</strong>. It transforms our data from its original high-dimensional space into a new, lower-dimensional space. But it doesn’t just randomly drop features. Instead, it creates entirely <em>new</em> features, called <strong>Principal Components (PCs)</strong>, which are linear combinations of the original features. These new features are special: they capture the maximum amount of variance (spread/information) in the data.</p> <p>Let’s use an analogy. Imagine you have a scatter plot of data points in 2D (say, height vs. weight). The data might look like an elongated cloud.</p> <ol> <li> <strong>First Principal Component (PC1):</strong> PCA tries to find a new axis (a line) that runs through the longest part of this cloud. This line captures the <em>most</em> variance in the data. If you project all data points onto this line, they would be maximally spread out. This PC1 is the most important direction.</li> <li> <strong>Second Principal Component (PC2):</strong> After finding PC1, PCA then looks for another axis that captures the <em>remaining</em> variance. Crucially, this second axis must be <strong>orthogonal</strong> (perpendicular) to the first one. In our 2D example, PC2 would be a line perpendicular to PC1, representing the “width” of the cloud.</li> </ol> <p>If we had 3D data, we’d find PC1, then PC2 (perpendicular to PC1), and then PC3 (perpendicular to both PC1 and PC2). Each successive principal component captures less and less of the total variance in the data. The magic is that we can decide to keep only the first few principal components that capture, say, 95% of the total variance, effectively reducing our dimensions while retaining most of the information.</p> <h3 id="the-math-behind-the-magic-a-peek-under-the-hood">The Math Behind the Magic (A Peek Under the Hood)</h3> <p>Okay, let’s get a little technical. Don’t worry, we’ll keep it as intuitive as possible. The core of PCA relies on <strong>eigenvectors</strong> and <strong>eigenvalues</strong> of the data’s <strong>covariance matrix</strong>.</p> <p>Here’s a step-by-step breakdown:</p> <h4 id="step-1-standardize-the-data">Step 1: Standardize the Data</h4> <p>Imagine one feature is “housing price” in millions, and another is “number of bedrooms” (from 1-5). Their scales are vastly different. If we don’t standardize, the feature with the larger scale (housing price) will dominate the variance calculation, making PCA biased. So, we <strong>scale</strong> our data, typically to have a mean of 0 and a standard deviation of 1. $x’ = \frac{x - \mu}{\sigma}$</p> <h4 id="step-2-calculate-the-covariance-matrix">Step 2: Calculate the Covariance Matrix</h4> <p>The covariance matrix tells us how much two variables change together.</p> <ul> <li>A positive covariance means if one variable increases, the other tends to increase.</li> <li>A negative covariance means if one variable increases, the other tends to decrease.</li> <li>A covariance close to zero means they don’t have much of a linear relationship.</li> </ul> <p>For a dataset with $p$ features, the covariance matrix $C$ will be a $p \times p$ matrix. The diagonal elements are the variances of individual features, and the off-diagonal elements are the covariances between pairs of features. For a dataset $\mathbf{X}$ (where each column is a feature), the covariance matrix can be calculated as: $C = \frac{1}{n-1} \mathbf{X}^T \mathbf{X}$ (assuming $\mathbf{X}$ is mean-centered).</p> <h4 id="step-3-compute-eigenvectors-and-eigenvalues">Step 3: Compute Eigenvectors and Eigenvalues</h4> <p>This is where the real “magic” happens.</p> <ul> <li> <strong>Eigenvectors</strong> are special vectors that, when a linear transformation (like our covariance matrix) is applied to them, only change by a scalar factor (they don’t change direction). In PCA, the eigenvectors of the covariance matrix are our <strong>Principal Components</strong>. They represent the directions of maximum variance in the data.</li> <li> <strong>Eigenvalues</strong> are the scalar factors by which the eigenvectors are scaled. They tell us the <strong>magnitude</strong> of variance captured along their corresponding eigenvector direction. A larger eigenvalue means that eigenvector captures more variance.</li> </ul> <p>Mathematically, for a square matrix $A$, a vector $\mathbf{v}$ is an eigenvector if $A\mathbf{v} = \lambda\mathbf{v}$, where $\lambda$ is the eigenvalue. In our case, $A$ is the covariance matrix $C$. So, $C\mathbf{v} = \lambda\mathbf{v}$.</p> <h4 id="step-4-select-principal-components">Step 4: Select Principal Components</h4> <p>We sort the eigenvalues in descending order. The eigenvector corresponding to the largest eigenvalue is PC1 (the direction of most variance), the next largest is PC2, and so on. We then choose how many principal components ($k$) to keep. A common way is to look at the <strong>explained variance ratio</strong>, which tells us the proportion of total variance explained by each principal component. We might decide to keep enough components to explain, say, 95% of the total variance.</p> <h4 id="step-5-project-data">Step 5: Project Data</h4> <p>Finally, we create a <strong>projection matrix</strong> (also called a feature vector or transformation matrix) using the top $k$ eigenvectors. We then multiply our original (standardized) data matrix by this projection matrix to transform our data into the new, lower-dimensional space. If $\mathbf{W}_k$ is the matrix formed by the top $k$ eigenvectors, and $\mathbf{X}$ is our original standardized data, the new data $\mathbf{Y}$ in the reduced space is: $\mathbf{Y} = \mathbf{X}\mathbf{W}_k$</p> <p>And voilà! Our high-dimensional data is now represented by fewer dimensions, without losing too much of its original information.</p> <h3 id="when-to-use-pca-and-when-to-be-cautious">When to Use PCA (and When to Be Cautious)</h3> <p>PCA is a fantastic tool, but it’s not a silver bullet.</p> <p><strong>Benefits:</strong></p> <ul> <li> <strong>Dimensionality Reduction:</strong> Reduces features, making models faster and less prone to overfitting.</li> <li> <strong>Noise Reduction:</strong> By focusing on directions of maximum variance, PCA can implicitly filter out some noise present in less important dimensions.</li> <li> <strong>Visualization:</strong> Reduces data to 2 or 3 dimensions, making it plottable and understandable.</li> <li> <strong>Improved Performance:</strong> Can sometimes improve model accuracy by removing redundant or noisy features.</li> </ul> <p><strong>Limitations:</strong></p> <ul> <li> <strong>Loss of Interpretability:</strong> The new principal components are linear combinations of original features. This means PC1 might be “0.3<em>bedrooms + 0.6</em>square_footage - 0.2*crime_rate”. It’s harder to explain what PC1 *means* in real-world terms compared to an original feature like “square footage.”</li> <li> <strong>Assumes Linearity:</strong> PCA is a linear transformation. If the true relationships between your features are non-linear, PCA might not capture them effectively.</li> <li> <strong>Sensitive to Scaling:</strong> As discussed, feature scaling is crucial. If not scaled, features with larger ranges will dominate the principal components.</li> <li> <strong>Variance != Importance:</strong> PCA focuses on maximizing variance. Sometimes, a feature with low variance might still be very important for your prediction task (e.g., a rare disease indicator). PCA might reduce its impact.</li> </ul> <h3 id="pca-in-action-a-glimpse-with-python">PCA in Action (A Glimpse with Python)</h3> <p>Implementing PCA is surprisingly straightforward with libraries like scikit-learn in Python.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Assume 'df' is your DataFrame with numerical features
# (You'd typically separate features X from target y)
</span><span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="sh">'</span><span class="s">target_column</span><span class="sh">'</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Step 1: Standardize the data
</span><span class="n">scaler</span> <span class="o">=</span> <span class="nc">StandardScaler</span><span class="p">()</span>
<span class="n">X_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Step 2: Apply PCA
# We can specify the number of components or the variance to explain
</span><span class="n">pca</span> <span class="o">=</span> <span class="nc">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span> <span class="c1"># Keep components that explain 95% of variance
# OR: pca = PCA(n_components=2) # To reduce to 2 dimensions for visualization
</span>
<span class="n">X_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>

<span class="c1"># Check explained variance ratio
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Explained variance ratio per component:</span><span class="sh">"</span><span class="p">,</span> <span class="n">pca</span><span class="p">.</span><span class="n">explained_variance_ratio_</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Cumulative explained variance:</span><span class="sh">"</span><span class="p">,</span> <span class="nf">sum</span><span class="p">(</span><span class="n">pca</span><span class="p">.</span><span class="n">explained_variance_ratio_</span><span class="p">))</span>

<span class="c1"># You can now use X_pca (your transformed data) for modeling
# For visualization, if you chose n_components=2:
# plt.scatter(X_pca[:, 0], X_pca[:, 1], c=df['target_column'])
# plt.xlabel('Principal Component 1')
# plt.ylabel('Principal Component 2')
# plt.title('Data in 2 Principal Components')
# plt.show()
</span></code></pre></div></div> <p>This snippet demonstrates how easily you can apply PCA. The <code class="language-plaintext highlighter-rouge">PCA</code> object handles the covariance matrix calculation, eigenvector/eigenvalue decomposition, and projection for you. The <code class="language-plaintext highlighter-rouge">n_components</code> parameter is incredibly useful for balancing dimensionality reduction with information retention.</p> <h3 id="my-personal-takeaway">My Personal Takeaway</h3> <p>Learning PCA felt like unlocking a new level in my data science journey. It transformed my perspective on handling complex datasets. No longer do I dread the thought of 100+ features; instead, I see an opportunity for PCA to reveal their underlying structure. It’s a testament to the elegance of mathematics and statistics to solve practical, real-world problems.</p> <p>If you’re just starting out, don’t be intimidated by terms like “eigenvectors.” Focus on the intuition first: PCA finds the most important directions of spread in your data. Then, slowly peel back the layers to understand the math. It’s a rewarding process that will equip you with a powerful tool for your data science arsenal.</p> <p>So, the next time you find yourself tangled in a web of too many features, remember PCA. It might just be the guiding light you need to untangle the mess and discover the true story hidden within your data.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>