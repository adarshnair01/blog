<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Random Forests: Navigating the Wild Woods of Data with Collective Wisdom | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/random-forests-navigating-the-wild-woods-of-data-w/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Random Forests: Navigating the Wild Woods of Data with Collective Wisdom</h1> <p class="post-meta"> Created on October 05, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/ensemble-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Ensemble Learning</a>   <a href="/blog/blog/tag/random-forests"> <i class="fa-solid fa-hashtag fa-sm"></i> Random Forests</a>   <a href="/blog/blog/tag/decision-trees"> <i class="fa-solid fa-hashtag fa-sm"></i> Decision Trees</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey there, fellow data explorer!</p> <p>Today, I want to share a story from my own journey through the fascinating landscape of machine learning. It’s about a technique that often feels like magic, yet is built on surprisingly intuitive principles: <strong>Random Forests</strong>. When I first encountered them, I was struck by their elegance and sheer power in tackling real-world problems. They’re a staple in almost every data scientist’s toolkit, and for good reason.</p> <p>Imagine you’re trying to make a really important decision, like predicting the stock market, diagnosing a disease, or even figuring out if a customer will churn. You could ask one expert for their opinion. That expert might be brilliant, but they also might have their own biases or blind spots. What if you could ask <em>hundreds</em> of experts, each with a slightly different perspective, and then combine their wisdom? That’s essentially the core idea behind Random Forests.</p> <h3 id="the-lone-tree-our-starting-point">The Lone Tree: Our Starting Point</h3> <p>To truly appreciate Random Forests, let’s first understand their foundational element: the <strong>Decision Tree</strong>.</p> <p>Think of a Decision Tree as a flowchart. You start at the “root” node, ask a question (e.g., “Is the customer’s age &gt; 30?”), and based on the answer, you go down a specific branch. You keep asking questions until you reach a “leaf” node, which gives you the final prediction or classification.</p> <pre><code class="language-mermaid">graph TD
    A[Root: Is it raining?] --&gt;|Yes| B[Go back inside];
    A --&gt;|No| C{Is it windy?};
    C --&gt;|Yes| D[Take a kite];
    C --&gt;|No| E[Go for a walk];
</code></pre> <p>Decision trees are incredibly intuitive and easy to interpret. You can literally trace the path a decision takes. I loved them for their transparency! However, in my early days, I quickly learned they have a significant weakness: they’re prone to <strong>overfitting</strong>. A single tree, left unchecked, can become overly complex, memorizing the training data perfectly but failing miserably on new, unseen data. It’s like an expert who’s brilliant at recalling specific past events but can’t generalize their knowledge to new situations. They’re also quite <strong>unstable</strong> – a tiny change in the input data can lead to a drastically different tree structure.</p> <p>This is where the idea of an “ensemble” comes into play.</p> <h3 id="the-wisdom-of-the-crowd-ensemble-learning">The Wisdom of the Crowd: Ensemble Learning</h3> <p>The concept of “ensemble learning” is simple but profound: combine the predictions of multiple individual models to achieve better performance than any single model could on its own. It’s the machine learning equivalent of “two heads are better than one,” or even “a hundred heads are better than one!”</p> <p>One common ensemble technique is <strong>Bagging</strong> (Bootstrap Aggregating). Here’s how it works:</p> <ol> <li>We take our original dataset.</li> <li>We create multiple new datasets by <strong>bootstrapping</strong>. This means sampling our original data <em>with replacement</em>. So, each new dataset might have some original data points repeated, and some left out. This introduces variation.</li> <li>We train a separate model (e.g., a Decision Tree) on each of these bootstrapped datasets.</li> <li>Finally, we aggregate their predictions: for classification, we take a majority vote; for regression, we average their outputs.</li> </ol> <p>Bagging helps reduce <strong>variance</strong> (how much the model’s performance changes with different training data) and generally leads to more stable and accurate predictions. However, there’s still a catch. If all the individual models in our bag are very similar, even if they’re trained on different bootstrapped samples, their combined wisdom might not be as diverse as we’d hope. Imagine if all our experts had the same core bias – their combined vote might still be flawed.</p> <p>This brings us to the true innovation of Random Forests.</p> <h3 id="entering-the-forest-how-random-forests-grow">Entering the Forest: How Random Forests Grow</h3> <p>Random Forests take the concept of Bagging and add a crucial twist that makes them incredibly powerful: <strong>randomness at the feature level</strong>. This brilliant addition ensures that the individual trees are not only diverse due to different data samples but also due to different perspectives on the features.</p> <p>Here’s my step-by-step mental image of how a Random Forest is built:</p> <ol> <li> <p><strong>Planting Many Seeds (Bootstrapping Data):</strong> Just like Bagging, we start by creating many bootstrapped subsets of our original training data. Each tree in our forest will be grown on one of these unique subsets. This introduces diversity in the <em>data samples</em> each tree sees.</p> </li> <li> <strong>Selective Vision for Each Split (Feature Subsetting):</strong> This is the game-changer! When each individual tree is being built, and it’s trying to decide the <em>best feature to split on</em> at any given node, it doesn’t consider <em>all</em> the available features. Instead, it randomly selects a <em>subset</em> of features. For example, if you have 100 features, a tree might only consider 10 of them for a particular split. <ul> <li> <strong>Why is this so important?</strong> Imagine you have one overwhelmingly strong feature (e.g., “credit score” for loan approval). If every tree were allowed to consider all features, almost every tree would likely pick “credit score” as its top split. This would make all the trees very similar (correlated), defeating the purpose of combining diverse opinions. By forcing each tree to consider only a random subset of features, we encourage it to explore other, potentially less obvious, relationships in the data. This <strong>decorrelates</strong> the trees significantly.</li> </ul> </li> <li> <p><strong>Growing Deep, Independent Trees:</strong> Each tree is typically grown to its maximum depth without pruning (or with very minimal pruning). Because of the randomness introduced at both the data and feature levels, these “overfitted” individual trees become part of a larger, robust system.</p> </li> <li> <strong>Collective Prediction (Voting/Averaging):</strong> Once all the trees in the forest are grown, when a new data point comes in for prediction: <ul> <li>For <strong>classification</strong>: Each tree makes its own class prediction, and the forest aggregates these predictions by taking a <strong>majority vote</strong>. The class that gets the most votes wins!</li> <li>For <strong>regression</strong>: Each tree predicts a numerical value, and the forest takes the <strong>average</strong> of these predictions.</li> </ul> </li> </ol> <h3 id="the-magic-behind-the-forests-wisdom">The Magic Behind the Forest’s Wisdom</h3> <p>So, why does this two-pronged approach to randomness work so incredibly well?</p> <ul> <li> <strong>Reduced Overfitting:</strong> By averaging many deep, slightly overfitted trees, the Random Forest cancels out their individual errors and biases. The “wisdom of the crowd” prevails; the collective generalization power is far superior to any single tree.</li> <li> <strong>Lower Variance:</strong> The bootstrapping and averaging significantly reduce the model’s variance. While individual trees might be sensitive to small changes in the data, the ensemble’s overall prediction remains stable.</li> <li> <strong>Handles High Dimensionality:</strong> Random Forests can effectively work with datasets that have a very large number of features, thanks to the feature subsetting at each split.</li> <li> <strong>Robustness to Noise:</strong> Because each tree sees a different sample of data and features, the model becomes more resilient to noisy data or outliers.</li> <li> <strong>Implicit Feature Importance:</strong> A fantastic side benefit of Random Forests is their ability to tell you which features were most influential in making decisions across the entire forest. Features that consistently lead to better splits (reducing impurity) across many trees are deemed more important.</li> </ul> <h3 id="a-peek-under-the-hood-mathematical-intuition">A Peek Under the Hood: Mathematical Intuition</h3> <p>While Random Forests might seem like a complex black box, the core ideas for splitting within each tree are rooted in simple information theory. For classification, trees typically use metrics like <strong>Gini Impurity</strong> or <strong>Entropy</strong> to decide the best split.</p> <ul> <li> <p><strong>Gini Impurity</strong> measures how “pure” a node is. A Gini of 0 means all samples in that node belong to the same class (perfectly pure). $G = 1 - \sum_{i=1}^{C} p_i^2$ where $C$ is the number of classes, and $p_i$ is the proportion of samples belonging to class $i$ in the node. The goal is to find a split that maximizes the <em>reduction</em> in Gini impurity.</p> </li> <li> <p><strong>Entropy</strong> is another measure of disorder or uncertainty. A node with high entropy is very mixed. $H = - \sum_{i=1}^{C} p_i \log_2(p_i)$ The goal here is to maximize <strong>Information Gain</strong>, which is the reduction in entropy after a split.</p> </li> </ul> <p>For regression tasks, trees often aim to minimize the <strong>Mean Squared Error (MSE)</strong> or similar metrics, making splits that result in child nodes with samples whose target values are as close to each other as possible.</p> <p>Feature importance, mentioned earlier, is often calculated by summing the decrease in Gini impurity (or MSE for regression) across all nodes where a particular feature is used for splitting, averaged over all trees in the forest. This gives us a numerical score for each feature’s contribution.</p> <h3 id="tuning-the-forest-hyperparameters">Tuning the Forest: Hyperparameters</h3> <p>Like any powerful tool, Random Forests have a few “dials” we can turn to optimize their performance, called <strong>hyperparameters</strong>:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">n_estimators</code>: The number of trees in the forest. More trees generally lead to better performance but also increase computation time. There’s usually a point of diminishing returns.</li> <li> <code class="language-plaintext highlighter-rouge">max_features</code>: The number of features to consider when looking for the best split. This is crucial for decorrelating the trees. Common choices are $\sqrt{p}$ for classification and $p/3$ for regression, where $p$ is the total number of features.</li> <li> <code class="language-plaintext highlighter-rouge">max_depth</code>: The maximum depth of each tree. While individual trees are often grown deep, limiting depth can sometimes reduce individual tree variance without sacrificing much ensemble performance.</li> <li> <code class="language-plaintext highlighter-rouge">min_samples_split</code> / <code class="language-plaintext highlighter-rouge">min_samples_leaf</code>: These control the minimum number of samples required to split an internal node or to be at a leaf node, respectively. They help prevent individual trees from becoming <em>too</em> specific.</li> </ul> <h3 id="where-do-random-forests-shine-applications">Where Do Random Forests Shine? (Applications)</h3> <p>Random Forests are incredibly versatile and have found their way into countless applications:</p> <ul> <li> <strong>Healthcare</strong>: Diagnosing diseases (e.g., predicting heart disease, cancer), analyzing medical images.</li> <li> <strong>Finance</strong>: Fraud detection, credit risk assessment, stock market prediction.</li> <li> <strong>E-commerce</strong>: Customer churn prediction, recommendation systems.</li> <li> <strong>Image Recognition</strong>: Classifying objects in images.</li> <li> <strong>Bioinformatics</strong>: Gene expression analysis.</li> </ul> <p>Their robustness, accuracy, and ability to handle various data types make them a go-to choice for many real-world problems.</p> <h3 id="the-occasional-thicket-limitations">The Occasional Thicket: Limitations</h3> <p>While powerful, Random Forests aren’t without their drawbacks:</p> <ul> <li> <strong>Interpretability</strong>: While better than neural networks, a Random Forest isn’t as transparent as a single Decision Tree. Understanding <em>why</em> a specific prediction was made requires inspecting multiple trees.</li> <li> <strong>Computational Cost</strong>: Training many trees, especially with very large datasets or many features, can be computationally expensive and time-consuming.</li> <li> <strong>Memory Usage</strong>: Storing many trees can require significant memory.</li> </ul> <h3 id="my-journeys-takeaway">My Journey’s Takeaway</h3> <p>Learning about Random Forests was a significant step in my machine learning journey. It taught me the profound lesson of leveraging collective intelligence and the power of controlled randomness. They beautifully balance complexity and performance, providing a robust solution that often performs exceptionally well right out of the box.</p> <p>If you’re embarking on your own data science adventure, understanding and implementing Random Forests will undoubtedly equip you with a formidable tool. So, go forth, grow your own forests, and uncover the hidden wisdom within your data!</p> <p>Keep exploring, keep learning. The data wilderness awaits!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>