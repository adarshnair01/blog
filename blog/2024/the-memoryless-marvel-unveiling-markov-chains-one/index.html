<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Memoryless Marvel: Unveiling Markov Chains, One Step at a Time | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/the-memoryless-marvel-unveiling-markov-chains-one/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Memoryless Marvel: Unveiling Markov Chains, One Step at a Time</h1> <p class="post-meta"> Created on September 10, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/markov-chains"> <i class="fa-solid fa-hashtag fa-sm"></i> Markov Chains</a>   <a href="/blog/blog/tag/probability"> <i class="fa-solid fa-hashtag fa-sm"></i> Probability</a>   <a href="/blog/blog/tag/stochastic-processes"> <i class="fa-solid fa-hashtag fa-sm"></i> Stochastic Processes</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hello fellow data explorers!</p> <p>Have you ever found yourself trying to predict what’s next? Maybe you’re watching a chess game and trying to guess your opponent’s next move, or perhaps you’re checking the weather forecast for tomorrow. In many such scenarios, our best guess about the future often depends heavily on what’s happening <em>right now</em>. The past, while interesting, might not always be the most critical piece of information for predicting the immediate next step.</p> <p>This intuitive idea – that the future depends only on the present, not on the sequence of events that led to it – is the cornerstone of a remarkably powerful mathematical concept: <strong>Markov Chains</strong>.</p> <p>As a data scientist, I find Markov Chains absolutely captivating. They’re simple enough to grasp, yet profound enough to be applied to an astonishing range of real-world problems. Let’s peel back the layers and discover what makes these “memoryless marvels” so special.</p> <h3 id="what-exactly-is-a-markov-chain">What Exactly is a Markov Chain?</h3> <p>At its heart, a Markov Chain is a <strong>stochastic process</strong> – a sequence of random variables that describe the evolution of some system over time. But not just any stochastic process! What makes a Markov Chain unique is its adherence to the <strong>Markov Property</strong>.</p> <p>Imagine you’re tracking the weather. You know it’s sunny today. To predict tomorrow’s weather, do you really need to know if it was rainy three days ago, then cloudy two days ago, then sunny yesterday? Or is today’s sunny state enough information?</p> <p>The Markov Property states that the probability of moving to any given next state depends <em>only</em> on the current state and <em>not</em> on the sequence of events that preceded it. In mathematical terms, if we denote the state of our system at time $n$ as $X_n$, then:</p> <table> <tbody> <tr> <td>$P(X_{n+1} = x</td> <td>X_n, X_{n-1}, \dots, X_0) = P(X_{n+1} = x</td> <td>X_n)$</td> </tr> </tbody> </table> <p>This “memoryless” property is what gives Markov Chains their elegant simplicity and makes them computationally tractable.</p> <h3 id="deconstructing-the-chain-states-and-transitions">Deconstructing the Chain: States and Transitions</h3> <p>To really understand Markov Chains, let’s break down their core components:</p> <ol> <li> <strong>States:</strong> These are the possible conditions or configurations that our system can be in. Think of them as discrete “buckets” the system can reside in. <ul> <li> <em>Weather Example:</em> Sunny, Cloudy, Rainy.</li> <li> <em>Board Game Example:</em> Position on the board.</li> <li> <em>Language Example:</em> The current word in a sentence.</li> </ul> </li> <li> <p><strong>Transitions:</strong> These are the movements or changes from one state to another. The system transitions between states over time.</p> </li> <li> <strong>Transition Probabilities:</strong> This is where the “randomness” comes in. For every possible move from state A to state B, there’s a specific probability associated with it. These probabilities dictate the likelihood of moving from one state to another.</li> </ol> <p>Let’s stick with our weather example. Suppose we’ve observed the weather for a long time and compiled the following probabilities for transitions between states:</p> <ul> <li>If it’s <strong>Sunny</strong> today: <ul> <li>30% chance it’s Sunny tomorrow</li> <li>60% chance it’s Cloudy tomorrow</li> <li>10% chance it’s Rainy tomorrow</li> </ul> </li> <li>If it’s <strong>Cloudy</strong> today: <ul> <li>40% chance it’s Sunny tomorrow</li> <li>40% chance it’s Cloudy tomorrow</li> <li>20% chance it’s Rainy tomorrow</li> </ul> </li> <li>If it’s <strong>Rainy</strong> today: <ul> <li>20% chance it’s Sunny tomorrow</li> <li>50% chance it’s Cloudy tomorrow</li> <li>30% chance it’s Rainy tomorrow</li> </ul> </li> </ul> <p>We can represent these transition probabilities in a neat little table called a <strong>Transition Matrix ($P$)</strong>:</p> <p>Let S=Sunny, C=Cloudy, R=Rainy. We order our states (S, C, R) for rows and columns.</p> <p>$P = \begin{pmatrix} P_{SS} &amp; P_{SC} &amp; P_{SR} <br> P_{CS} &amp; P_{CC} &amp; P_{CR} <br> P_{RS} &amp; P_{RC} &amp; P_{RR} \end{pmatrix} = \begin{pmatrix} 0.3 &amp; 0.6 &amp; 0.1 <br> 0.4 &amp; 0.4 &amp; 0.2 <br> 0.2 &amp; 0.5 &amp; 0.3 \end{pmatrix}$</p> <p>Notice a crucial property of this matrix: the sum of probabilities in each row must always equal 1. Why? Because if you are in a particular state (say, Sunny), you <em>must</em> transition to <em>one</em> of the possible next states (Sunny, Cloudy, or Rainy). There are no other options!</p> <h3 id="predicting-the-future-sort-of">Predicting the Future (Sort Of!)</h3> <p>With our transition matrix, we can start asking interesting questions:</p> <p><strong>1. What’s the probability of a specific sequence of events?</strong></p> <p>Let’s say it’s Sunny today (Day 0). What’s the probability of having Cloudy weather tomorrow (Day 1) and Rainy weather the day after (Day 2)?</p> <table> <tbody> <tr> <td>$P(X_0=S, X_1=C, X_2=R) = P(X_0=S) \times P(X_1=C</td> <td>X_0=S) \times P(X_2=R</td> <td>X_1=C)$</td> </tr> </tbody> </table> <p>Assuming we start Sunny ($P(X_0=S)=1$): $P(X_0=S, X_1=C, X_2=R) = 1 \times P_{SC} \times P_{CR} = 1 \times 0.6 \times 0.2 = 0.12$</p> <p>So, there’s a 12% chance of that specific weather sequence.</p> <p><strong>2. What’s the probability of being in a particular state after <em>k</em> steps?</strong></p> <p>This is where the magic of matrix multiplication comes in! If $P$ represents the probabilities for 1 step, then $P^2$ represents the probabilities for 2 steps, $P^3$ for 3 steps, and so on.</p> <p>Let $\pi_0$ be our initial probability distribution over the states. If we are <em>certain</em> it’s Sunny today, then $\pi_0 = \begin{pmatrix} 1 &amp; 0 &amp; 0 \end{pmatrix}$ (100% Sunny, 0% Cloudy, 0% Rainy).</p> <p>To find the probability distribution after one day ($\pi_1$): $\pi_1 = \pi_0 P = \begin{pmatrix} 1 &amp; 0 &amp; 0 \end{pmatrix} \begin{pmatrix} 0.3 &amp; 0.6 &amp; 0.1 <br> 0.4 &amp; 0.4 &amp; 0.2 <br> 0.2 &amp; 0.5 &amp; 0.3 \end{pmatrix} = \begin{pmatrix} 0.3 &amp; 0.6 &amp; 0.1 \end{pmatrix}$</p> <p>This makes sense: if it’s Sunny today, there’s a 30% chance of Sunny tomorrow, 60% Cloudy, 10% Rainy.</p> <p>Now, for two days ($\pi_2$): $\pi_2 = \pi_1 P = (\pi_0 P) P = \pi_0 P^2$</p> <p>Let’s calculate $P^2$: $P^2 = \begin{pmatrix} 0.3 &amp; 0.6 &amp; 0.1 <br> 0.4 &amp; 0.4 &amp; 0.2 <br> 0.2 &amp; 0.5 &amp; 0.3 \end{pmatrix} \begin{pmatrix} 0.3 &amp; 0.6 &amp; 0.1 <br> 0.4 &amp; 0.4 &amp; 0.2 <br> 0.2 &amp; 0.5 &amp; 0.3 \end{pmatrix} = \begin{pmatrix} (0.3<em>0.3 + 0.6</em>0.4 + 0.1<em>0.2) &amp; (0.3</em>0.6 + 0.6<em>0.4 + 0.1</em>0.5) &amp; (0.3<em>0.1 + 0.6</em>0.2 + 0.1<em>0.3) <br> (0.4</em>0.3 + 0.4<em>0.4 + 0.2</em>0.2) &amp; (0.4<em>0.6 + 0.4</em>0.4 + 0.2<em>0.5) &amp; (0.4</em>0.1 + 0.4<em>0.2 + 0.2</em>0.3) <br> (0.2<em>0.3 + 0.5</em>0.4 + 0.3<em>0.2) &amp; (0.2</em>0.6 + 0.5<em>0.4 + 0.3</em>0.5) &amp; (0.2<em>0.1 + 0.5</em>0.2 + 0.3*0.3) \end{pmatrix} = \begin{pmatrix} 0.35 &amp; 0.47 &amp; 0.18 <br> 0.32 &amp; 0.48 &amp; 0.20 <br> 0.32 &amp; 0.47 &amp; 0.21 \end{pmatrix}$</p> <p>Now, to find $\pi_2$: $\pi_2 = \begin{pmatrix} 1 &amp; 0 &amp; 0 \end{pmatrix} \begin{pmatrix} 0.35 &amp; 0.47 &amp; 0.18 <br> 0.32 &amp; 0.48 &amp; 0.20 <br> 0.32 &amp; 0.47 &amp; 0.21 \end{pmatrix} = \begin{pmatrix} 0.35 &amp; 0.47 &amp; 0.18 \end{pmatrix}$</p> <p>So, if it’s Sunny today, there’s a 35% chance it will be Sunny in two days, 47% chance Cloudy, and 18% chance Rainy.</p> <h3 id="the-long-run-stationary-distribution">The Long Run: Stationary Distribution</h3> <p>What happens if we keep multiplying the matrix? As $k$ gets very large, $P^k$ often converges to a state where all its rows are identical. This stable probability distribution is known as the <strong>stationary distribution</strong> (or equilibrium distribution). It tells us the long-term probabilities of being in each state, regardless of the initial starting state.</p> <p>In our weather example, after a very long time, the probability of it being Sunny, Cloudy, or Rainy on any given day will settle into a fixed proportion. This tells us the typical “climate” described by our Markov Chain model. Mathematically, the stationary distribution $\pi_{stable}$ satisfies $\pi_{stable} P = \pi_{stable}$.</p> <h3 id="where-do-markov-chains-live-in-the-real-world">Where Do Markov Chains Live in the Real World?</h3> <p>The simple elegance of Markov Chains makes them incredibly versatile. You’ll find them lurking behind the scenes in many data science and machine learning applications:</p> <ol> <li> <strong>Natural Language Processing (NLP):</strong> <ul> <li> <strong>Text Generation:</strong> Early language models used Markov Chains to predict the next word in a sentence based on the current word (or pair of words for higher-order Markov models).</li> <li> <strong>Part-of-Speech Tagging:</strong> Determining the grammatical role of words in a sentence.</li> <li> <strong>Spam Filtering:</strong> Analyzing sequences of words in emails.</li> </ul> </li> <li> <strong>Web Search and Ranking:</strong> <ul> <li> <strong>Google PageRank:</strong> One of the most famous applications! PageRank models the web as a Markov Chain where web pages are states, and hyperlinks are transitions. The stationary distribution of this chain gives each page a “rank” or importance score.</li> </ul> </li> <li> <strong>Finance:</strong> <ul> <li> <strong>Modeling Stock Prices:</strong> While highly simplified, some models use Markov Chains to predict market state changes (e.g., bull, bear, stable).</li> <li> <strong>Credit Risk Assessment:</strong> Modeling the transition of individuals or companies between different credit ratings.</li> </ul> </li> <li> <strong>Biology and Genomics:</strong> <ul> <li> <strong>DNA Sequence Analysis:</strong> Modeling the sequence of base pairs (A, T, C, G) and predicting patterns.</li> <li> <strong>Protein Folding:</strong> Simulating the conformational changes of proteins.</li> </ul> </li> <li> <strong>Reinforcement Learning:</strong> <ul> <li> <strong>Markov Decision Processes (MDPs):</strong> These are a powerful extension of Markov Chains used in AI to model environments where an agent makes decisions to maximize rewards. Markov Chains form the underlying process for the environment’s dynamics.</li> </ul> </li> <li> <strong>Simulation and Modeling:</strong> <ul> <li> <strong>Queuing Theory:</strong> Modeling customer flow in stores or call centers.</li> <li> <strong>Disease Spread:</strong> Simulating how infections might spread through a population.</li> </ul> </li> </ol> <h3 id="limitations-and-what-comes-next">Limitations and What Comes Next</h3> <p>While powerful, Markov Chains aren’t a silver bullet. Their primary limitation stems from their greatest strength: the <strong>memoryless property</strong>. In many real-world scenarios, the future <em>does</em> depend on more than just the immediate past. For instance, predicting stock prices usually requires looking at trends over a longer period than just “yesterday’s price.”</p> <p>This is where more advanced techniques come in, such as:</p> <ul> <li> <strong>Hidden Markov Models (HMMs):</strong> Where the underlying states aren’t directly observable, but we can infer them from observable outputs (e.g., inferring a speaker’s intention from their speech).</li> <li> <strong>Recurrent Neural Networks (RNNs) / LSTMs / Transformers:</strong> These deep learning architectures are specifically designed to capture long-term dependencies in sequential data, effectively having a “memory” that extends far beyond a single step.</li> </ul> <p>However, understanding Markov Chains is a fundamental stepping stone to grasping these more complex models. They provide a clear, interpretable framework for thinking about sequential processes.</p> <h3 id="wrapping-up">Wrapping Up</h3> <p>Markov Chains are a beautiful blend of simplicity and power. They allow us to model complex systems, predict future states, and understand long-term behavior, all by making the single, elegant assumption of “memorylessness.” From the weather outside your window to the search results on your screen, Markov Chains are silently at work, helping us make sense of a dynamic, uncertain world.</p> <p>So, the next time you see a prediction, pause for a moment. Could there be a “memoryless marvel” at play, simply looking at the present to tell you what’s next?</p> <p>Keep exploring, keep learning, and keep building awesome things with data!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>