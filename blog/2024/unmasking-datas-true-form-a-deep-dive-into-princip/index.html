<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Unmasking Data's True Form: A Deep Dive into Principal Component Analysis (PCA) | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/2024/unmasking-datas-true-form-a-deep-dive-into-princip/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Unmasking Data's True Form: A Deep Dive into Principal Component Analysis (PCA)</h1> <p class="post-meta"> Created on April 11, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/dimensionality-reduction"> <i class="fa-solid fa-hashtag fa-sm"></i> Dimensionality Reduction</a>   <a href="/blog/blog/tag/pca"> <i class="fa-solid fa-hashtag fa-sm"></i> PCA</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/statistics"> <i class="fa-solid fa-hashtag fa-sm"></i> Statistics</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>I remember vividly my early days diving into machine learning. One moment I was thrilled, dreaming of building the next revolutionary AI, and the next, I was staring at a spreadsheet with hundreds of columns, each representing a “feature” of my data. My brain just screamed, “Too many variables!” The sheer complexity felt like trying to understand a bustling city by looking at every single brick. This, my friends, is the infamous “curse of dimensionality.” High-dimensional data is hard to visualize, computationally expensive to process, and often leads to models that overfit and generalize poorly.</p> <p>But then, I met Principal Component Analysis, or PCA. It felt like finding a secret map to navigate that complex city, showing me the main roads and landmarks instead of every single brick. PCA isn’t just a fancy algorithm; it’s a fundamental concept that elegantly simplifies data while preserving its most important characteristics. It’s about finding the “essence” of your data.</p> <p>Ready to uncover how PCA works its magic? Let’s dive in!</p> <h3 id="what-is-pca-the-core-idea">What is PCA? The Core Idea</h3> <p>Imagine you have a crumpled piece of paper. This paper, in its crumpled state, is a complex 3D object. But perhaps its fundamental <em>shape</em> is just a flat piece of paper. PCA tries to “uncrumple” your data, or rather, it tries to find the most informative “flat surfaces” or directions where your data primarily lies.</p> <p>Think of it like this: You have a 3D object, say a banana, floating in a room. If you shine a light from one direction, you’ll see a shadow on the wall. If you shine it from another, you’ll see a different shadow. PCA’s goal is to find the angle from which to shine the light such that the shadow (a 2D representation) captures <em>as much of the original banana’s shape and spread as possible</em>. It wants the “best” shadow.</p> <p>In data terms, PCA identifies new axes, called <strong>Principal Components (PCs)</strong>, along which the data shows the <strong>most variance</strong> (spread). It then projects the original data onto these new axes. The first principal component (PC1) captures the most variance, the second (PC2) captures the second most variance and is <em>orthogonal</em> (uncorrelated) to PC1, and so on.</p> <p>The beauty? You can choose to keep only the top few principal components, effectively reducing the number of dimensions while retaining most of the important information.</p> <h3 id="the-math-behind-the-magic-step-by-step-unveiling">The Math Behind the Magic: Step-by-Step Unveiling</h3> <p>Don’t let the “math” scare you! We’ll walk through it step-by-step, building intuition along the way.</p> <h4 id="1-centering-the-data">1. Centering the Data</h4> <p>Before anything else, PCA prefers that your data is centered around the origin. This means subtracting the mean of each feature from all its values, so the mean of each feature becomes zero. This simplifies calculations later and ensures that the principal components truly capture variance, not just the overall location of the data.</p> <p>Mathematically, for each feature $x_i$: $x’_{\text{new},i} = x_i - \mu_i$</p> <p>Where $\mu_i$ is the mean of feature $i$.</p> <h4 id="2-the-covariance-matrix-measuring-relationships">2. The Covariance Matrix: Measuring Relationships</h4> <p>This is where things start to get interesting. PCA doesn’t just care about individual features; it cares about how they relate to each other. This relationship is captured by the <strong>covariance matrix</strong>.</p> <ul> <li> <strong>Variance</strong> measures how much a single variable varies from its mean. A high variance means the data points are very spread out.</li> <li> <strong>Covariance</strong> measures how two variables change together. <ul> <li>Positive covariance: If one variable increases, the other tends to increase.</li> <li>Negative covariance: If one variable increases, the other tends to decrease.</li> <li>Zero covariance: The variables are independent or unrelated.</li> </ul> </li> </ul> <p>The <strong>covariance matrix</strong> is a square matrix where:</p> <ul> <li>The elements on the main diagonal are the variances of each feature.</li> <li>The off-diagonal elements are the covariances between pairs of features.</li> </ul> <p>For centered data $X$ (where rows are observations and columns are features), the covariance matrix $\Sigma$ can be calculated as:</p> <p>$\Sigma = \frac{1}{n-1} X^T X$</p> <p>Where $n$ is the number of observations. This matrix is crucial because it encapsulates all the interrelationships and the spread of your data. It’s the blueprint PCA uses to find the directions of maximum variance.</p> <h4 id="3-eigenvalues-and-eigenvectors-the-heartbeat-of-pca">3. Eigenvalues and Eigenvectors: The Heartbeat of PCA</h4> <p>This is the core concept, and it sounds scarier than it is!</p> <p>Imagine our covariance matrix $\Sigma$ as a transformation that “stretches” and “rotates” vectors in space. <strong>Eigenvectors</strong> are special vectors that, when transformed by $\Sigma$, only get <em>scaled</em> (stretched or shrunk), but <em>don’t change their direction</em>. They are the “preferred directions” of the transformation.</p> <p><strong>Eigenvalues</strong> are the scalar factors by which the eigenvectors are scaled. A larger eigenvalue means that its corresponding eigenvector is stretched more, indicating a direction where the data has more variance.</p> <p>The mathematical relationship is: $\Sigma \mathbf{v} = \lambda \mathbf{v}$</p> <p>Where:</p> <ul> <li>$\Sigma$ is our covariance matrix.</li> <li>$\mathbf{v}$ is an eigenvector.</li> <li>$\lambda$ is the corresponding eigenvalue.</li> </ul> <p>In the context of PCA:</p> <ul> <li>The <strong>eigenvectors</strong> of the covariance matrix are our <strong>Principal Components</strong>. They define the new axes.</li> <li>The <strong>eigenvalues</strong> tell us the <strong>amount of variance</strong> captured along each principal component.</li> </ul> <p>So, by calculating the eigenvectors and eigenvalues of our covariance matrix, we effectively find the directions (principal components) along which our data varies the most, and how much variance is explained by each direction.</p> <h4 id="4-selecting-the-principal-components-the-essence-of-reduction">4. Selecting the Principal Components: The Essence of Reduction</h4> <p>Once we have all the eigenvectors and their corresponding eigenvalues, we sort them in <strong>descending order</strong> based on their eigenvalues.</p> <ul> <li>The eigenvector with the largest eigenvalue is our <strong>first Principal Component (PC1)</strong>. It captures the most variance in the data.</li> <li>The eigenvector with the second-largest eigenvalue is our <strong>second Principal Component (PC2)</strong>, and so on. Importantly, PC2 is always orthogonal (at a 90-degree angle, meaning uncorrelated) to PC1, ensuring it captures new, independent information.</li> </ul> <p>Now, the big question: How many principal components should we keep?</p> <p>We can use a few heuristics:</p> <ul> <li> <strong>Explained Variance Ratio:</strong> Each eigenvalue represents the amount of variance explained by its corresponding principal component. We can calculate the proportion of total variance explained by each component: $\text{Explained Variance Ratio}<em>k = \frac{\lambda_k}{\sum</em>{i=1}^p \lambda_i}$ Where $\lambda_k$ is the eigenvalue for the $k$-th component, and $p$ is the total number of features.</li> <li> <strong>Cumulative Explained Variance:</strong> We sum these ratios to see how much total variance is explained by the top $k$ components. Often, we aim to retain 90-95% of the total variance.</li> <li> <strong>Scree Plot:</strong> This is a plot of eigenvalues in descending order. We look for an “elbow” point where the eigenvalues start to drop off significantly. Components before the elbow are usually kept.</li> </ul> <p>By selecting only the top $k$ principal components (where $k &lt; p$), we reduce the dimensionality of our data.</p> <h4 id="5-projecting-data-seeing-the-new-world">5. Projecting Data: Seeing the New World</h4> <p>Finally, we take our chosen top $k$ eigenvectors and form a <strong>projection matrix</strong> (let’s call it $W$). Each column of $W$ is one of our selected principal components.</p> <p>We then multiply our original, centered data matrix $X$ by this projection matrix $W$:</p> <p>$Y = XW$</p> <p>The resulting matrix $Y$ is our new dataset. It has $n$ observations (rows) but only $k$ features (columns), representing the principal components. Each row in $Y$ is the original observation, now expressed in terms of its scores on the principal components. We’ve successfully transformed our high-dimensional data into a lower-dimensional space!</p> <h3 id="why-pca-is-your-datas-best-friend">Why PCA is Your Data’s Best Friend</h3> <p>PCA isn’t just a mathematical curiosity; it’s an incredibly practical tool in data science:</p> <ol> <li> <strong>Visualization:</strong> It’s almost impossible to visualize data with more than 3 dimensions. PCA allows us to reduce high-dimensional data (e.g., 100 features) down to 2 or 3 principal components, which we can then plot to uncover hidden clusters, outliers, or patterns.</li> <li> <strong>Noise Reduction:</strong> Often, the lower-variance principal components capture noise rather than meaningful signal. By discarding these components, PCA can effectively denoise your data, leading to cleaner inputs for models.</li> <li> <strong>Feature Extraction and Engineering:</strong> Instead of individual features, PCA creates new, uncorrelated “synthetic” features (the principal components) that are linear combinations of the original ones. These new features can sometimes be more informative and useful for downstream tasks.</li> <li> <strong>Improved Model Performance:</strong> <ul> <li> <strong>Reduced Computational Cost:</strong> Fewer dimensions mean faster training times for most machine learning algorithms.</li> <li> <strong>Mitigating the Curse of Dimensionality:</strong> With fewer features, models are less likely to overfit, leading to better generalization on unseen data.</li> <li> <strong>Addressing Multicollinearity:</strong> Since principal components are orthogonal, they are uncorrelated, which can be beneficial for models sensitive to multicollinearity (like linear regression).</li> </ul> </li> </ol> <h3 id="a-word-of-caution-pcas-limitations">A Word of Caution: PCA’s Limitations</h3> <p>While powerful, PCA isn’t a silver bullet for every data problem:</p> <ol> <li> <strong>Linearity Assumption:</strong> PCA assumes that the principal components are linear combinations of the original features. If your data has complex non-linear relationships (imagine data points spiraling on a 3D plane), PCA might struggle to capture its true structure. For such cases, techniques like Kernel PCA or t-SNE might be more appropriate.</li> <li> <strong>Scale Sensitivity:</strong> Remember how we talked about variance? PCA is heavily influenced by the scales of your features. Features with larger ranges or higher magnitudes will naturally have higher variances and might dominate the principal components. This is why <strong>standardization (or scaling)</strong> of your data before applying PCA is almost always a critical first step.</li> <li> <strong>Interpretability:</strong> The principal components are abstract. They are linear combinations of <em>all</em> original features. While PC1 might represent “overall health” in a medical dataset, it’s not as straightforward to interpret as a single original feature like “blood pressure.” This can make explaining model decisions harder.</li> <li> <strong>Information Loss:</strong> When you reduce dimensions, you <em>are</em> discarding some information. The hope is that the discarded information is primarily noise or redundant. However, if crucial information is concentrated in the lower-variance components you remove, PCA might lead to a loss of important signal.</li> </ol> <h3 id="my-takeaway-embracing-the-elegance">My Takeaway: Embracing the Elegance</h3> <p>PCA, to me, is more than just an algorithm; it’s a testament to the elegance of mathematics in solving real-world problems. It distills complexity into clarity, making overwhelming datasets approachable and understandable. It allows us to peek beyond the raw numbers and see the underlying structure that drives our data.</p> <p>Understanding the mechanics behind PCA, from covariance matrices to eigenvectors, gives you a profound appreciation for why it works and when to apply it. It empowers you to not just use a library function but to truly <em>comprehend</em> the transformation your data undergoes.</p> <p>So, the next time you face a high-dimensional dataset, remember PCA. It’s not just reducing features; it’s revealing the most significant stories your data has to tell. Go forth, explore, and let PCA help you uncover those hidden dimensions!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>