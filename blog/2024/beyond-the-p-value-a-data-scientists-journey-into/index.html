<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Beyond the P-Value: A Data Scientist's Journey into Bayesian Thinking | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/beyond-the-p-value-a-data-scientists-journey-into/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Beyond the P-Value: A Data Scientist's Journey into Bayesian Thinking</h1> <p class="post-meta"> Created on June 06, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/bayesian-statistics"> <i class="fa-solid fa-hashtag fa-sm"></i> Bayesian Statistics</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/probability"> <i class="fa-solid fa-hashtag fa-sm"></i> Probability</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/statistical-inference"> <i class="fa-solid fa-hashtag fa-sm"></i> Statistical Inference</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h3 id="the-detectives-dilemma-my-first-encounter-with-uncertainty">The Detective’s Dilemma: My First Encounter with Uncertainty</h3> <p>I remember staring at my laptop screen, trying to make sense of A/B test results. “The p-value is 0.04,” my colleague announced, “so we reject the null hypothesis!” I nodded, pretending to understand, but a nagging question lingered: <em>What does that even mean for our product?</em> It felt like I was being told a coin was probably biased, but not by how much, or how confident I should be. My internal monologue was shouting, “But what if the change <em>isn’t</em> better? How likely is <em>that</em>?”</p> <p>This was my introduction to the rigid, often counter-intuitive world of Frequentist statistics, the dominant paradigm taught in most schools. It’s powerful, don’t get me wrong, but it left me craving a more intuitive way to think about data and uncertainty. That’s when I stumbled upon <strong>Bayesian Statistics</strong>, and it felt like finding a secret decoder ring for the universe.</p> <p>Imagine you’re a detective. You have a hunch (a <em>prior belief</em>) about who committed a crime. Then, new evidence (your <em>data</em>) comes in. Do you discard your hunch and start fresh? No! You update your hunch based on the new evidence. Bayesian statistics is exactly that: a mathematical framework for updating your beliefs as new evidence emerges. It’s a natural, human way of thinking, codified into elegant mathematics.</p> <h3 id="frequentist-vs-bayesian-a-tale-of-two-philosophies">Frequentist vs. Bayesian: A Tale of Two Philosophies</h3> <p>Before we dive deep, let’s quickly contrast it with its frequentist cousin, which you’ve likely encountered:</p> <ul> <li> <strong>Frequentist Statistics:</strong> Focuses on the <em>frequency</em> of events in repeated trials. It asks: “Given that the null hypothesis is true, how often would we observe data as extreme as ours?” The “true” parameter is a fixed, unknown constant.</li> <li> <strong>Bayesian Statistics:</strong> Treats parameters as random variables with probability distributions. It asks: “Given the data we’ve observed, what is the probability distribution of our parameter?” It incorporates prior beliefs about the parameters before seeing any data.</li> </ul> <p>The key difference? Frequentists typically provide point estimates (e.g., “the average is 10”) and confidence intervals (e.g., “we are 95% confident the true average is between 8 and 12”). Bayesians, however, give you a <em>full probability distribution</em> over the possible values of your parameter (e.g., “there’s a 90% chance the average is between 9 and 11, with the most likely value being 10”). This distribution allows you to answer questions like: “What’s the probability that the new product feature is actually better?” — a question frequentist p-values famously can’t answer.</p> <h3 id="the-heart-of-bayesianism-bayes-theorem">The Heart of Bayesianism: Bayes’ Theorem</h3> <p>At the core of all Bayesian magic lies a simple yet profound formula: <strong>Bayes’ Theorem</strong>. It’s named after the Reverend Thomas Bayes, an 18th-century statistician and philosopher.</p> <p>Let’s break it down:</p> \[P(H|E) = \frac{P(E|H) \cdot P(H)}{P(E)}\] <p>This equation looks intimidating at first, but let’s translate it into our detective story:</p> <ul> <li> <table> <tbody> <tr> <td>$P(H</td> <td>E)$: This is your <strong>Posterior Probability</strong>. It’s your <em>updated belief</em> in a hypothesis ($H$) <em>after</em> observing the evidence ($E$). This is what we want to calculate!</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>$P(E</td> <td>H)$: This is the <strong>Likelihood</strong>. It’s the probability of observing the evidence ($E$) <em>if</em> your hypothesis ($H$) were true. How well does the evidence fit your theory?</td> </tr> </tbody> </table> </li> <li>$P(H)$: This is your <strong>Prior Probability</strong>. It’s your <em>initial belief</em> in the hypothesis ($H$) <em>before</em> seeing any new evidence. This is where you bake in existing knowledge, intuition, or expert opinion.</li> <li>$P(E)$: This is the <strong>Marginal Likelihood</strong> (also called “Evidence”). It’s the overall probability of observing the evidence ($E$), regardless of whether your hypothesis is true or not. In many practical scenarios, you can think of this as a normalizing constant that ensures your posterior probabilities sum up to 1. For relative comparisons of hypotheses, we often ignore it because it’s constant for all hypotheses given the same evidence.</li> </ul> <p>So, in plain English, Bayes’ Theorem says:</p> <p><strong>“Your updated belief about a hypothesis is proportional to how well the evidence supports that hypothesis, weighted by how strongly you believed in the hypothesis to begin with.”</strong></p> <p>Powerful, right?</p> <h3 id="a-practical-example-the-biased-coin">A Practical Example: The Biased Coin</h3> <p>Let’s bring this to life with a classic example: determining if a coin is fair.</p> <p>Imagine you pick up a coin. You want to know its probability of landing heads, which we’ll call $\theta$.</p> <ul> <li>A fair coin would have $\theta = 0.5$.</li> <li>A biased coin might have $\theta = 0.6$, $0.2$, or anything else between 0 and 1.</li> </ul> <p><strong>Step 1: Formulate Your Prior ($P(H)$)</strong></p> <p>Before you even flip the coin, what do you believe about $\theta$?</p> <ul> <li> <strong>Option A (No strong belief):</strong> You might assume all values of $\theta$ between 0 and 1 are equally likely. This is a <em>uniform prior</em>. We can represent this with a Beta distribution: $\text{Beta}(1,1)$. Its probability density function is flat.</li> <li> <strong>Option B (Some belief):</strong> You might suspect most coins are roughly fair. So, you might put more probability around $\theta = 0.5$. A $\text{Beta}(10,10)$ distribution would reflect this, peaking strongly at 0.5. The parameters of a Beta distribution ($\alpha$, $\beta$) can be thought of as “pseudo-counts” of heads and tails you’ve observed <em>before</em> starting your actual experiment.</li> </ul> <p>Let’s choose <strong>Option A</strong> for simplicity: a uniform prior, $\text{Beta}(1,1)$. This means we have no pre-existing bias about the coin’s fairness.</p> <table> <tbody> <tr> <td>**Step 2: Define Your Likelihood ($P(E</td> <td>H)$)**</td> </tr> </tbody> </table> <p>Now, you flip the coin. Each flip is a Bernoulli trial. If you flip the coin $N$ times and get $k$ heads, the probability of observing this sequence of flips, <em>given</em> a specific coin bias $\theta$, follows a Binomial distribution.</p> \[P(k \text{ heads in } N \text{ flips } | \theta) = \binom{N}{k} \theta^k (1-\theta)^{N-k}\] <p>This is our likelihood. It tells us how probable our observed data is for any given value of $\theta$.</p> <p><strong>Step 3: Collect Data (The Evidence $E$)</strong></p> <p>You flip the coin 10 times. You get 7 heads and 3 tails. So, $N=10$, $k=7$.</p> <table> <tbody> <tr> <td>**Step 4: Calculate the Posterior ($P(H</td> <td>E)$)**</td> </tr> </tbody> </table> <p>Now we combine our prior belief with the observed data using Bayes’ Theorem. For the Beta-Binomial conjugate pair (a fancy way of saying “when your prior and likelihood play nicely together, your posterior will be of the same family as your prior”), the math is surprisingly elegant.</p> <p>If your prior is $\text{Beta}(\alpha, \beta)$ and you observe $k$ heads in $N$ flips, your posterior distribution for $\theta$ will be:</p> \[\text{Posterior} \sim \text{Beta}(\alpha + k, \beta + N - k)\] <p>In our case, with a $\text{Beta}(1,1)$ prior and observing 7 heads ($k=7$) out of 10 flips ($N=10$):</p> \[\text{Posterior} \sim \text{Beta}(1 + 7, 1 + (10 - 7)) = \text{Beta}(8, 4)\] <p><strong>What does $\text{Beta}(8,4)$ mean?</strong> It’s a probability distribution over the possible values of $\theta$. This distribution now peaks around $\frac{8}{8+4} = \frac{8}{12} \approx 0.67$.</p> <ul> <li>Our initial uniform prior ($\text{Beta}(1,1)$) was flat, indicating no strong belief.</li> <li>After 10 flips (7 heads, 3 tails), our belief has <em>shifted</em>. The posterior distribution ($\text{Beta}(8,4)$) now strongly suggests that the coin’s true bias ($\theta$) is closer to 0.67, with a range of likely values. We have updated our belief!</li> </ul> <p>If we were to flip the coin 100 more times and get 60 heads, our posterior would update further to $\text{Beta}(8+60, 4+40) = \text{Beta}(68, 44)$. The more data we observe, the narrower our posterior distribution becomes, and the more confident we are in our estimate of $\theta$.</p> <h3 id="why-bayesian-statistics-is-a-game-changer-for-data-scientists-and-mles">Why Bayesian Statistics Is a Game-Changer for Data Scientists and MLEs</h3> <ol> <li> <strong>Incorporates Prior Knowledge:</strong> This is its superpower. Whether it’s expert opinion, historical data, or even just a reasonable guess, Bayesian methods allow you to bake this information directly into your model. This is especially valuable in fields with limited data or where specific domain knowledge is crucial (e.g., rare disease diagnosis, drug discovery).</li> <li> <strong>Intuitive Interpretation:</strong> Instead of abstract p-values, you get direct answers to questions like “What is the probability that model A performs better than model B?” or “What’s the most probable range for this parameter?” This makes communicating results to stakeholders much clearer.</li> <li> <strong>Full Probability Distributions:</strong> Bayesians don’t just give you a single “best estimate”; they give you a full probability distribution for your parameters. This distribution quantifies uncertainty directly. You can say, “There’s a 95% probability that the conversion rate is between 2.1% and 2.5%,” which is far more informative than a frequentist confidence interval statement.</li> <li> <strong>Updates Naturally with New Data:</strong> As we saw with the coin example, Bayesian models are designed to learn continuously. This is perfect for dynamic environments like online A/B testing, fraud detection, or recommendation systems, where models need to adapt as new data streams in.</li> <li> <strong>Handles Small Datasets Gracefully:</strong> When you have very little data, frequentist methods can struggle or produce unstable results. Bayesian methods, by leveraging prior information, can often provide more robust and sensible conclusions even with sparse data.</li> <li> <strong>Foundation for Advanced ML:</strong> Bayesian thinking is not just for basic inference. It underpins powerful machine learning techniques like Bayesian Optimization (efficiently finding optimal hyperparameters), Gaussian Processes (probabilistic modeling for complex functions), and Bayesian Neural Networks (quantifying uncertainty in deep learning predictions).</li> </ol> <h3 id="challenges-and-considerations">Challenges and Considerations</h3> <p>Of course, no method is a silver bullet:</p> <ul> <li> <strong>Choosing Priors:</strong> While a strength, choosing a prior can also be a challenge. A “bad” or overly strong prior can skew results, especially with limited data. However, robust analysis often involves testing different reasonable priors to see how sensitive your conclusions are.</li> <li> <strong>Computational Intensity:</strong> For complex models, calculating the posterior distribution directly can be mathematically intractable. This is where modern computational methods like Markov Chain Monte Carlo (MCMC) come in. Tools like PyMC3, Stan, and Pyro make MCMC accessible, allowing us to approximate these complex posteriors. While fascinating, delving into MCMC is a topic for another blog post!</li> </ul> <h3 id="my-takeaway-embrace-the-uncertainty">My Takeaway: Embrace the Uncertainty</h3> <p>My journey into Bayesian statistics fundamentally changed how I approach data problems. It shifted my focus from merely rejecting or failing to reject a null hypothesis to truly <em>understanding the spectrum of possibilities</em> and <em>how strongly I should believe in each</em>.</p> <p>As data scientists and machine learning engineers, our job isn’t just to make predictions, but to quantify the uncertainty around those predictions. Bayesian statistics provides a remarkably elegant and intuitive framework for doing just that. It encourages a nuanced, adaptive approach to problem-solving, where every new piece of information refines our understanding, much like a detective piecing together clues.</p> <p>So, the next time you’re faced with an uncertain situation, remember Bayes’ Theorem. It’s more than just a formula; it’s a way of thinking, a philosophy for learning and decision-making in an inherently uncertain world. And for me, that’s a truly beautiful thing.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>