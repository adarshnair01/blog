<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Deep Learning Dance-Off: PyTorch vs TensorFlow, A Personal Odyssey | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/2024/the-deep-learning-dance-off-pytorch-vs-tensorflow/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Deep Learning Dance-Off: PyTorch vs TensorFlow, A Personal Odyssey</h1> <p class="post-meta"> Created on March 12, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/pytorch"> <i class="fa-solid fa-hashtag fa-sm"></i> PyTorch</a>   <a href="/blog/blog/tag/tensorflow"> <i class="fa-solid fa-hashtag fa-sm"></i> TensorFlow</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>As a young explorer in the vast landscape of data science and machine learning, I quickly learned that one of the first “holy wars” you encounter isn’t about programming languages or operating systems, but about deep learning frameworks. The contenders? PyTorch and TensorFlow. I remember feeling overwhelmed, seeing seasoned professionals passionately argue their chosen framework’s superiority. It felt like picking a sports team before even understanding the rules of the game!</p> <p>But here’s the secret I wish I knew then: it’s not about choosing a “winner” in an ultimate battle. It’s about understanding the unique strengths and philosophies of each, how they’ve evolved, and ultimately, which one best suits a particular task, team, or personal style. Think of it as learning to drive both an agile sports car and a robust SUV – both get you to your destination, but the journey and optimal use cases differ.</p> <p>So, let’s embark on this journey together. We’ll peel back the layers, understand their core mechanics, and see how these two titans shape the world of AI.</p> <h3 id="the-bedrock-tensors-and-gradients">The Bedrock: Tensors and Gradients</h3> <p>Before we dive into the specifics, let’s establish some common ground. At the heart of both PyTorch and TensorFlow are two fundamental concepts:</p> <ol> <li> <p><strong>Tensors</strong>: If you’ve worked with NumPy, you’re already familiar with tensors. They are simply multi-dimensional arrays, the fundamental data structure used to represent all data (inputs, outputs, model parameters) in deep learning. From a single number (a scalar, 0-D tensor) to a vector (1-D tensor), a matrix (2-D tensor), and beyond, tensors are the universal language. For example, an image can be represented as a 3-D tensor (height, width, color channels), and a batch of images would be a 4-D tensor (batch size, height, width, color channels).</p> </li> <li> <p><strong>Automatic Differentiation (Autograd)</strong>: This is the magic sauce that makes deep learning possible. Training neural networks involves finding the right set of weights and biases that minimize a ‘loss’ function. We do this using optimization algorithms like Gradient Descent, which require calculating the gradient (the direction and magnitude of the steepest ascent) of the loss function with respect to each parameter. Manually calculating these derivatives for millions of parameters would be impossible. Both PyTorch and TensorFlow provide an automatic differentiation engine (PyTorch calls its <code class="language-plaintext highlighter-rouge">autograd</code>, TensorFlow integrates it into its graph execution) that efficiently computes these gradients. It essentially keeps track of all operations performed on tensors and, when requested, computes the derivatives using the chain rule. If we have a loss function $L(y, \hat{y})$ where $\hat{y} = f(x; W)$ is our model’s prediction and $W$ represents its weights, Autograd helps us calculate $\frac{\partial L}{\partial W}$.</p> </li> </ol> <p>Now that we have our foundation, let’s meet the contenders!</p> <h3 id="pytorch-the-pythonic-research-darling">PyTorch: The Pythonic Research Darling</h3> <p>My first deep dive into PyTorch felt incredibly intuitive. It’s often hailed as the “Pythonic” framework, and for good reason. If you’re comfortable with Python and NumPy, PyTorch will feel like a natural extension.</p> <h4 id="philosophy-and-origins">Philosophy and Origins</h4> <p>PyTorch was developed by Facebook’s AI Research lab (FAIR). Its design philosophy was heavily influenced by the needs of researchers: flexibility, ease of use, and dynamic behavior. It prioritizes a familiar, imperative programming style that integrates seamlessly with the Python ecosystem.</p> <h4 id="the-dynamic-computational-graph-define-by-run">The Dynamic Computational Graph (Define-by-Run)</h4> <p>This is PyTorch’s defining characteristic. Imagine you’re building a complex LEGO structure.</p> <ul> <li> <strong>PyTorch’s approach:</strong> You pick up a piece, attach it, then decide which piece to pick up next based on how the structure looks <em>right now</em>. If you make a mistake, you can immediately see it and change your next step. This is <strong>Define-by-Run</strong>. The computational graph (the sequence of operations) is built <em>on the fly</em> as your code executes.</li> </ul> <p>Why is this a big deal?</p> <ul> <li> <strong>Flexibility</strong>: Especially useful for models with dynamic architectures, like Recurrent Neural Networks (RNNs) that process sequences of varying lengths, or models where control flow (if/else statements, loops) depends on input data.</li> <li> <strong>Easier Debugging</strong>: Because the graph is built step-by-step, you can use standard Python debugging tools (like <code class="language-plaintext highlighter-rouge">pdb</code>) to inspect intermediate tensors and trace errors directly in your code. It feels like debugging any other Python script.</li> <li> <strong>Intuitive Control Flow</strong>: Writing conditional logic or loops inside your model architecture is straightforward, just like regular Python.</li> </ul> <h4 id="example-conceptual">Example (Conceptual):</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">MyDynamicModel</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">x</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span> <span class="c1"># Dynamic decision based on data
</span>            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">linear2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <p>In this simplified example, the path of execution through the <code class="language-plaintext highlighter-rouge">forward</code> method changes based on the data. PyTorch handles this effortlessly because it constructs the graph for each forward pass.</p> <h4 id="key-features--ecosystem">Key Features &amp; Ecosystem:</h4> <ul> <li> <strong><code class="language-plaintext highlighter-rouge">torch.nn</code></strong>: A powerful module for building neural network layers.</li> <li> <strong><code class="language-plaintext highlighter-rouge">torch.optim</code></strong>: Implementations of various optimization algorithms (SGD, Adam, etc.).</li> <li> <strong><code class="language-plaintext highlighter-rouge">DataLoader</code></strong>: Efficiently loads and batches data, often with multi-processing.</li> <li> <strong>TorchScript</strong>: A way to serialize PyTorch models into a static graph representation that can be run independently of Python, enabling deployment in production environments (C++, mobile, edge devices). This feature addresses one of PyTorch’s initial weaknesses compared to TensorFlow.</li> </ul> <h3 id="tensorflow-the-production-powerhouse">TensorFlow: The Production Powerhouse</h3> <p>TensorFlow, developed by Google, has been around longer and traditionally focused on scalability, deployment, and a broader ecosystem. It started with a steeper learning curve but has evolved significantly.</p> <h4 id="philosophy-and-origins-1">Philosophy and Origins</h4> <p>TensorFlow was designed with production deployments, large-scale training, and cross-platform capabilities in mind. Google’s vast infrastructure and diverse AI applications (from search to self-driving cars) heavily influenced its design.</p> <h4 id="the-static-computational-graph-define-and-run---historically">The Static Computational Graph (Define-and-Run - Historically)</h4> <p>Historically, TensorFlow’s core paradigm was <strong>Define-and-Run</strong>.</p> <ul> <li> <strong>TensorFlow’s traditional approach:</strong> Before you start building your LEGO structure, you first draw a complete, detailed blueprint of every single piece and connection. Only once the entire blueprint is done do you start assembling. This meant you would first define the entire computational graph as a static structure. Then, you would “feed” data into this graph within a <code class="language-plaintext highlighter-rouge">tf.Session</code> to execute it.</li> </ul> <p>Why this approach?</p> <ul> <li> <strong>Optimization</strong>: A static graph allows the framework to perform global optimizations <em>before</em> execution, like pruning unused nodes or fusing operations, leading to highly optimized code.</li> <li> <strong>Deployment</strong>: A pre-compiled graph is easier to deploy to various environments (CPUs, GPUs, TPUs, mobile devices, web browsers) without needing the Python interpreter.</li> <li> <strong>Scalability</strong>: Easier to distribute across multiple servers or devices because the graph is fixed.</li> </ul> <p>The downsides, especially for beginners:</p> <ul> <li> <strong>Debugging</strong>: Harder to debug since errors would often appear during session execution, not necessarily at the point of definition. You couldn’t easily inspect intermediate tensors mid-graph construction.</li> <li> <strong>Flexibility</strong>: Dynamic control flow was cumbersome, requiring special <code class="language-plaintext highlighter-rouge">tf.cond</code> and <code class="language-plaintext highlighter-rouge">tf.while_loop</code> operations that didn’t feel like standard Python.</li> </ul> <h4 id="tensorflow-2x-and-eager-execution-a-game-changer">TensorFlow 2.x and Eager Execution: A Game Changer!</h4> <p>This is crucial: TensorFlow <em>learned</em> from PyTorch! With TensorFlow 2.x, the default execution mode is <strong>Eager Execution</strong>, which largely mirrors PyTorch’s Define-by-Run approach. Now, operations are executed immediately, and the computational graph is built dynamically.</p> <p>This means:</p> <ul> <li> <strong>Much easier to use and debug</strong>: You can inspect values, use <code class="language-plaintext highlighter-rouge">pdb</code>, and write standard Python control flow.</li> <li> <strong>Familiarity</strong>: It feels much more like PyTorch or NumPy, significantly lowering the barrier to entry.</li> </ul> <p>But TF still retains its production DNA. How?</p> <ul> <li> <strong><code class="language-plaintext highlighter-rouge">tf.function</code></strong>: You can decorate a Python function with <code class="language-plaintext highlighter-rouge">@tf.function</code>. This tells TensorFlow to “trace” the function once, convert it into a static, optimized computational graph, and then execute that graph for subsequent calls. This brings back the performance and deployment benefits of static graphs, but only <em>after</em> you’ve developed and debugged your model eagerly.</li> </ul> <h4 id="keras-the-user-friendly-wrapper">Keras: The User-Friendly Wrapper</h4> <p>Keras is a high-level API for building and training deep learning models. It was initially a standalone project but is now the official high-level API for TensorFlow.</p> <ul> <li> <strong>Simplicity</strong>: Keras makes building complex networks incredibly simple and intuitive.</li> <li> <strong>Accessibility</strong>: It’s a fantastic entry point for beginners, abstracting away much of the underlying complexity.</li> <li> <strong><code class="language-plaintext highlighter-rouge">model.fit()</code></strong>: Keras provides a convenient <code class="language-plaintext highlighter-rouge">fit()</code> method for training models, handling the training loop, validation, and callbacks.</li> </ul> <h4 id="key-features--ecosystem-1">Key Features &amp; Ecosystem:</h4> <ul> <li> <strong>TensorBoard</strong>: A powerful visualization tool for monitoring training, visualizing graphs, and embedding projections.</li> <li> <strong>TensorFlow Extended (TFX)</strong>: An end-to-end platform for deploying production ML pipelines.</li> <li> <strong>TensorFlow Lite</strong>: For mobile and edge devices.</li> <li> <strong>TensorFlow.js</strong>: For running ML models in browsers and Node.js.</li> <li> <strong>TPU Support</strong>: Native support for Google’s Tensor Processing Units.</li> </ul> <h3 id="the-converging-paths-pytorch-vs-tensorflow-in-the-modern-era">The Converging Paths: PyTorch vs. TensorFlow in the Modern Era</h3> <p>The “holy war” isn’t as fierce as it once was, largely because both frameworks have converged on many best practices.</p> <ul> <li> <strong>Debugging</strong>: PyTorch still feels marginally more straightforward for immediate debugging, but TF2.x with Eager Execution has drastically improved its debugging experience.</li> <li> <strong>Flexibility vs. Optimization</strong>: Both offer the best of both worlds. PyTorch’s TorchScript allows for graph optimization and deployment, while TF2.x’s <code class="language-plaintext highlighter-rouge">tf.function</code> allows for dynamic development followed by static graph compilation.</li> <li> <strong>Learning Curve</strong>: For a Pythonista, PyTorch might still feel a tiny bit more intuitive from scratch. However, Keras makes TensorFlow incredibly accessible, especially for beginners focusing on standard architectures.</li> <li> <strong>Community and Resources</strong>: Both have massive, supportive communities. TensorFlow benefits from Google’s extensive resources and widespread industry adoption, while PyTorch has become the dominant force in academic research and cutting-edge publications.</li> <li> <strong>Deployment</strong>: TensorFlow historically had a stronger edge here with its comprehensive ecosystem for production (TFX, TF Lite, TF.js). PyTorch is catching up rapidly with TorchScript and production-oriented features.</li> <li> <strong>Performance</strong>: For most standard tasks, performance differences are negligible. Highly optimized low-level operations might offer slight advantages to one or the other in very specific scenarios.</li> </ul> <h3 id="so-which-one-should-you-choose">So, Which One Should You Choose?</h3> <p>Here’s my personal take, based on various scenarios:</p> <ul> <li> <strong>For Academic Research &amp; Rapid Prototyping</strong>: <strong>PyTorch</strong> often wins. Its dynamic graph and Pythonic nature make it ideal for experimenting with novel architectures, quickly iterating on ideas, and easily implementing complex, non-standard models. Most cutting-edge research papers often release their code in PyTorch.</li> <li> <strong>For Enterprise-Level Production &amp; Scalability</strong>: <strong>TensorFlow</strong> (especially with its full ecosystem like TFX) can be the stronger choice. If you’re building robust, production-ready systems that need to scale, deploy to diverse environments (mobile, web, edge devices), and integrate with a mature MLOps pipeline, TensorFlow’s comprehensive suite of tools might be more appealing.</li> <li> <strong>For Beginners</strong>: This is a tough one now! If you’re coming from a strong Python background and value explicit control, <strong>PyTorch</strong> might resonate more. If you prefer a high-level, opinionated API that gets you building models quickly, <strong>TensorFlow with Keras</strong> is an excellent entry point.</li> <li> <strong>If Your Team Already Uses One</strong>: Use that one! The benefits of consistency, shared knowledge, and existing infrastructure almost always outweigh marginal technical differences.</li> <li> <strong>The Best Answer</strong>: Learn both! Truly understanding deep learning means transcending the framework. The concepts of tensors, computational graphs, backpropagation, model architectures, and optimization are universal. Being proficient in both allows you to read research papers, contribute to different projects, and leverage the strengths of each as needed.</li> </ul> <h3 id="conclusion-embrace-the-evolution">Conclusion: Embrace the Evolution</h3> <p>The story of PyTorch vs. TensorFlow isn’t a stagnant rivalry; it’s a dynamic tale of innovation, convergence, and mutual learning. Both frameworks are incredible pieces of engineering that have democratized deep learning, making it accessible to millions.</p> <p>My journey from being confused about which to pick to appreciating the unique beauty of each has been incredibly rewarding. It taught me that the tools are powerful, but the most powerful asset is your understanding of the underlying principles.</p> <p>So, don’t get caught up in the “holy war.” Instead, pick one, get comfortable, build some amazing models, and then dare to explore the other. Your data science portfolio will thank you for the versatility, and your understanding of deep learning will deepen immensely. Happy coding!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>