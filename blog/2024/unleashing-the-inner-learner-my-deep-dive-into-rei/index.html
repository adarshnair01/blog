<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Unleashing the Inner Learner: My Deep Dive into Reinforcement Learning | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/unleashing-the-inner-learner-my-deep-dive-into-rei/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Unleashing the Inner Learner: My Deep Dive into Reinforcement Learning</h1> <p class="post-meta"> Created on June 16, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/reinforcement-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Reinforcement Learning</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>As a kid, I was always fascinated by how we, as humans, learn. We touch a hot stove once, and we learn not to do it again. We try different ways to solve a puzzle, and eventually, we find the optimal path. This process of learning through interaction, feedback, and striving for a goal isn’t just for us; it’s also at the heart of one of the most exciting branches of Artificial Intelligence: <strong>Reinforcement Learning (RL)</strong>.</p> <p>For my data science and MLE portfolio, diving deep into RL wasn’t just a requirement; it was a revelation. It connected so many dots in my understanding of intelligent systems. This post is my attempt to share that journey with you, breaking down the magic of RL into digestible pieces, just as I wished someone had done for me.</p> <h3 id="the-core-idea-learning-by-doing">The Core Idea: Learning by Doing</h3> <p>Imagine you’re training a dog. You give a command, the dog performs an action, and based on that action, you give it a treat (positive reinforcement) or a gentle “no” (negative reinforcement). Over time, the dog learns which actions lead to treats and which don’t, eventually forming a strategy to maximize its treat intake.</p> <p>Reinforcement Learning works on this very principle. We have an <strong>agent</strong> (the learner, like our dog) interacting with an <strong>environment</strong> (the world around it). The agent takes <strong>actions</strong>, observes the <strong>state</strong> of the environment, and receives a <strong>reward</strong> (or penalty). Its ultimate goal? To learn a <strong>policy</strong> – a strategy – that tells it what action to take in any given state to maximize the <em>total</em> accumulated reward over the long run.</p> <p>This paradigm is incredibly powerful. It’s how AlphaGo conquered the world’s best Go players, how AI agents master complex video games like Atari and Dota 2, and how robots are learning to navigate and manipulate objects in the real world.</p> <h3 id="the-pillars-of-reinforcement-learning">The Pillars of Reinforcement Learning</h3> <p>Let’s break down the fundamental components that make up any RL system. Think of them as the characters in our learning story:</p> <ol> <li> <strong>Agent:</strong> This is our decision-maker, the “brain” learning from experience. It observes the environment and decides what to do next.</li> <li> <strong>Environment:</strong> This is the world the agent lives in. It could be a chess board, a virtual maze, a robotic arm, or even a financial market. It responds to the agent’s actions and provides feedback.</li> <li> <strong>State ($S_t$):</strong> At any given moment $t$, the state describes the current situation of the environment. If our agent is a robot navigating a room, the state might include its current coordinates, orientation, and sensor readings.</li> <li> <strong>Action ($A_t$):</strong> This is what the agent chooses to do in a particular state. For our robot, actions could be “move forward,” “turn left,” “pick up object.”</li> <li> <strong>Reward ($R_t$):</strong> After taking an action in a state, the environment provides a reward signal. This is a scalar value that tells the agent how good or bad its last action was. A positive reward encourages the action, a negative one discourages it. Crucially, RL agents aim to maximize <em>cumulative</em> reward, not just immediate reward. This is key for solving complex, long-term problems.</li> <li> <strong>Policy ($\pi$):</strong> This is the agent’s strategy or “brain.” A policy is essentially a mapping from states to actions, telling the agent what to do in any given situation. It can be deterministic (always take action <code class="language-plaintext highlighter-rouge">a</code> in state <code class="language-plaintext highlighter-rouge">s</code>) or stochastic (take action <code class="language-plaintext highlighter-rouge">a</code> with probability <code class="language-plaintext highlighter-rouge">p</code> in state <code class="language-plaintext highlighter-rouge">s</code>). The ultimate goal of RL is to find an <em>optimal policy</em> ($\pi^*$).</li> <li> <strong>Value Function ($V(s)$ or $Q(s,a)$):</strong> While rewards tell us about the immediate goodness of an action, value functions tell us about the <em>long-term</em> goodness. <ul> <li>$V(s)$ (Value of a State): Represents the expected total cumulative reward an agent can expect to get starting from state $s$ and following a particular policy $\pi$.</li> <li>$Q(s,a)$ (Action-Value of a State-Action Pair): Represents the expected total cumulative reward an agent can expect to get starting from state $s$, taking action $a$, and then following policy $\pi$ thereafter. $Q$-values are often more useful for decision-making.</li> </ul> </li> </ol> <h3 id="the-challenge-of-time-discount-factor">The Challenge of Time: Discount Factor</h3> <p>Remember how I said RL agents maximize <em>cumulative</em> reward? This brings up an interesting question: Are future rewards as important as immediate rewards? In most real-world scenarios, we prefer immediate gratification. A reward received now is generally better than the same reward received a year from now.</p> <p>This concept is captured by the <strong>discount factor ($\gamma$)</strong>, a value between 0 and 1. \(G*t = R*{t+1} + \gamma R*{t+2} + \gamma^2 R*{t+3} + \dots = \sum*{k=0}^{\infty} \gamma^k R*{t+k+1}\) Here, $G_t$ is the total discounted cumulative reward (return) from time $t$. A $\gamma$ close to 0 means the agent is “myopic,” caring mostly about immediate rewards. A $\gamma$ close to 1 means it’s “farsighted,” valuing future rewards almost as much as immediate ones. Choosing the right $\gamma$ is crucial for shaping the agent’s behavior.</p> <h3 id="the-mathematical-framework-markov-decision-processes-mdps">The Mathematical Framework: Markov Decision Processes (MDPs)</h3> <p>To formalize the RL problem, we often use a mathematical framework called a <strong>Markov Decision Process (MDP)</strong>. An MDP describes an environment where the agent’s future state depends <em>only</em> on the current state and the action taken, not on the entire history of actions and states. This is known as the <strong>Markov Property</strong>.</p> <p>An MDP is defined by:</p> <ul> <li>A set of states $\mathcal{S}$</li> <li>A set of actions $\mathcal{A}$</li> <li> <table> <tbody> <tr> <td>A state transition probability function $P(s’</td> <td>s, a)$: The probability of transitioning to state $s’$ given that the agent takes action $a$ in state $s$.</td> </tr> </tbody> </table> </li> <li>A reward function $R(s, a, s’)$: The expected immediate reward received after transitioning from state $s$ to state $s’$ via action $a$.</li> <li>A discount factor $\gamma \in [0, 1)$</li> </ul> <p>The core of solving an MDP lies in the <strong>Bellman Equations</strong>. These equations recursively define the optimal value function and, consequently, the optimal policy.</p> <p>The <strong>optimal state-value function</strong> $V^<em>(s)$ for any state $s$ is the maximum expected return achievable from $s$ under any policy: \(V^_(s) = \max*a \sum*{s'} P(s' | s, a) [R(s, a, s') + \gamma V^_(s')]\) And the <strong>optimal action-value function</strong> $Q^</em>(s,a)$ for any state-action pair $(s,a)$ is: \(Q^_(s,a) = \sum*{s'} P(s' | s, a) [R(s, a, s') + \gamma \max*{a'} Q^_(s', a')]\) These equations look intimidating, but their essence is profound: The optimal value of being in a state (or taking an action in a state) is the expected immediate reward <em>plus</em> the discounted optimal value of the <em>next</em> state (or the best action in the next state). By iteratively solving these equations, an agent can eventually find the optimal policy.</p> <h3 id="exploration-vs-exploitation-the-eternal-dilemma">Exploration vs. Exploitation: The Eternal Dilemma</h3> <p>One of the central challenges in RL is balancing <strong>exploration</strong> (trying new things to discover better rewards) with <strong>exploitation</strong> (using what you already know to get the most rewards).</p> <p>Imagine our robot agent trying to find its way out of a complex maze.</p> <ul> <li> <strong>Exploitation:</strong> It could always follow the path it <em>knows</em> leads to some reward, even if it’s not the best one.</li> <li> <strong>Exploration:</strong> It could occasionally try a new, unknown path, which might lead to a dead end, or it might discover a shortcut to an even bigger reward.</li> </ul> <p>Too much exploitation, and the agent might get stuck in a locally optimal solution, never finding the true best path. Too much exploration, and it might waste too much time trying suboptimal actions, even when a good path is known.</p> <p>A common strategy to balance this is <strong>$\epsilon$-greedy exploration</strong>. With probability $\epsilon$ (a small number like 0.1), the agent chooses a random action (explores). With probability $1-\epsilon$, it chooses the action it currently believes is best (exploits). Over time, $\epsilon$ is often decayed, allowing the agent to explore more initially and then exploit more as its knowledge improves.</p> <h3 id="diving-into-algorithms-q-learning">Diving into Algorithms: Q-Learning</h3> <p>There are many types of RL algorithms, often categorized as:</p> <ul> <li> <strong>Model-Based vs. Model-Free:</strong> Does the agent learn/know the environment’s dynamics (P, R) (model-based) or does it learn directly from experience (model-free)?</li> <li> <strong>Value-Based vs. Policy-Based:</strong> Does it learn an optimal value function and derive the policy (value-based) or does it learn the policy directly (policy-based)?</li> </ul> <p>One of the most foundational and popular model-free, value-based algorithms is <strong>Q-Learning</strong>.</p> <p><strong>Q-Learning</strong> aims to learn the optimal action-value function, $Q^*(s,a)$, without explicitly modeling the environment’s transitions or rewards. It’s an <strong>off-policy</strong> algorithm, meaning it can learn the optimal policy even while following a different (e.g., exploratory) policy.</p> <p>The core of Q-Learning is its update rule: \(Q(s,a) \leftarrow Q(s,a) + \alpha [R_{t+1} + \gamma \max_{a'} Q(s',a') - Q(s,a)]\) Let’s break this down:</p> <ul> <li>$Q(s,a)$: The current estimated Q-value for taking action $a$ in state $s$.</li> <li>$\alpha$: The <strong>learning rate</strong> (between 0 and 1). It determines how much we update our Q-value based on the new information. A higher $\alpha$ means faster, potentially unstable, learning.</li> <li>$R_{t+1}$: The immediate reward received after taking action $a$ in state $s$ and transitioning to state $s’$.</li> <li>$\gamma$: The discount factor.</li> <li>$\max_{a’} Q(s’,a’)$: This is the estimated optimal future Q-value from the <em>next</em> state $s’$, assuming the agent takes the best possible action $a’$ from there.</li> <li>$[R_{t+1} + \gamma \max_{a’} Q(s’,a’) - Q(s,a)]$: This entire term is called the <strong>TD Error</strong> (Temporal Difference Error). It represents the difference between the agent’s current estimate of the Q-value and a more “up-to-date” estimate based on the experience just gained.</li> </ul> <p>In essence, Q-Learning repeatedly adjusts its $Q(s,a)$ estimates by learning from each interaction with the environment, gradually converging to the optimal $Q^<em>$ values. Once it has optimal $Q^</em>$ values, the optimal policy is simply to take the action $a$ that maximizes $Q^*(s,a)$ for any given state $s$.</p> <h3 id="the-rise-of-deep-reinforcement-learning-drl">The Rise of Deep Reinforcement Learning (DRL)</h3> <p>While Q-Learning works beautifully for environments with a small, discrete number of states and actions (like gridworlds), it struggles when the state space becomes enormous or continuous (e.g., raw pixel data from a game, sensor readings from a complex robot). Imagine trying to store a $Q$-table for every pixel combination in an Atari game!</p> <p>This is where <strong>Deep Reinforcement Learning (DRL)</strong> comes into play. DRL combines the principles of RL with the power of deep neural networks. Instead of using a tabular approach to store $Q$-values, we use a deep neural network to <em>approximate</em> the $Q$-function or the policy.</p> <p>The breakthrough came with <strong>Deep Q-Networks (DQN)</strong>, where Google DeepMind successfully trained an agent to play Atari games directly from raw pixel data, often surpassing human performance. The neural network takes the game screen (state) as input and outputs the Q-values for all possible actions.</p> <p>DRL has since led to incredible advancements, powering systems like AlphaGo, which defeated human world champions in Go, and sophisticated robotic control systems.</p> <h3 id="real-world-impact-and-my-enthusiasm">Real-World Impact and My Enthusiasm</h3> <p>The applications of Reinforcement Learning are vast and continue to expand:</p> <ul> <li> <strong>Gaming:</strong> From classic Atari games to complex strategy games like StarCraft II and Dota 2, RL agents are redefining what’s possible.</li> <li> <strong>Robotics:</strong> Learning complex motor skills, grasping objects, and navigating dynamic environments.</li> <li> <strong>Autonomous Driving:</strong> Training self-driving cars to make safe and efficient decisions.</li> <li> <strong>Resource Management:</strong> Optimizing energy consumption in data centers or managing traffic flow in cities.</li> <li> <strong>Finance:</strong> Developing trading strategies.</li> <li> <strong>Healthcare:</strong> Optimizing treatment plans.</li> </ul> <p>My journey through Reinforcement Learning has been nothing short of inspiring. It provides a framework for building truly intelligent agents that learn and adapt, much like we do. From the elegant simplicity of the reward signal to the mathematical rigor of Bellman equations and the power of deep neural networks, RL is a field that sits at the cutting edge of AI.</p> <p>The challenges are still there – sample efficiency, safe exploration, and designing effective reward functions are ongoing research areas. But the potential is immense. As I continue to build my portfolio and explore complex AI systems, I’m confident that the principles of Reinforcement Learning will be a cornerstone of my work.</p> <p>If you’re looking for a field that truly embodies the spirit of learning and discovery in artificial intelligence, I highly encourage you to dive into Reinforcement Learning. The future is being learned, one reward at a time!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>