<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Teaching Computers to "See": A Deep Dive into Convolutional Neural Networks | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/teaching-computers-to-see-a-deep-dive-into-convolu/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Teaching Computers to "See": A Deep Dive into Convolutional Neural Networks</h1> <p class="post-meta"> Created on October 27, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/computer-vision"> <i class="fa-solid fa-hashtag fa-sm"></i> Computer Vision</a>   <a href="/blog/blog/tag/cnns"> <i class="fa-solid fa-hashtag fa-sm"></i> CNNs</a>   <a href="/blog/blog/tag/artificial-intelligence"> <i class="fa-solid fa-hashtag fa-sm"></i> Artificial Intelligence</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hello fellow explorers of the digital frontier!</p> <p>Today, I want to take you on a journey into one of the most fascinating and impactful areas of artificial intelligence: <strong>Convolutional Neural Networks (CNNs)</strong>. If you’ve ever seen AI classify images, detect objects in a photo, or even power the visual recognition in self-driving cars, you’ve witnessed the marvel of CNNs in action.</p> <p>As a budding data scientist, I remember first encountering the sheer volume of data in images. A simple 100x100 pixel grayscale image has 10,000 data points. A color image? Multiply that by three for red, green, and blue channels! Imagine trying to feed that into a traditional neural network – the number of connections and parameters would explode, making training computationally impossible and prone to overfitting.</p> <p>This is where CNNs swoop in, armed with an ingenious approach inspired by our own biological visual system. They don’t just “see” pixels; they learn to <em>understand</em> what those pixels collectively represent. Let’s peel back the layers and see how they do it.</p> <h3 id="the-aha-moment-convolution">The “Aha!” Moment: Convolution</h3> <p>Before CNNs, standard neural networks treated image pixels like any other numerical data. There was no inherent understanding that pixels close to each other were related, forming shapes and textures. Our brains, however, don’t analyze every single light photon individually; they process patterns. This biological inspiration led to the core idea of convolution.</p> <p>Imagine you’re trying to identify edges in an image. You don’t need to know the color of every single pixel; you just need to spot where there’s a sharp change in brightness. A human might mentally slide a small “feature detector” over the image, looking for these changes. That’s precisely what a CNN does with a <strong>kernel</strong> (also known as a filter).</p> <p>A kernel is a small matrix of numbers (e.g., 3x3 or 5x5). The <strong>convolution operation</strong> involves sliding this kernel across the entire image, pixel by pixel. At each position, we perform an element-wise multiplication between the kernel and the corresponding patch of the image, then sum up all the results. This sum becomes a single pixel in a new image, which we call a <strong>feature map</strong>.</p> <p>Let’s look at a simple example with a 5x5 image and a 3x3 kernel:</p> <p>Original Image (I):</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[1, 1, 1, 0, 0],
 [0, 1, 1, 1, 0],
 [0, 0, 1, 1, 1],
 [0, 0, 1, 1, 0],
 [0, 1, 1, 0, 0]]
</code></pre></div></div> <p>Kernel (K) (an example for detecting vertical edges):</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[-1, 0, 1],
 [-1, 0, 1],
 [-1, 0, 1]]
</code></pre></div></div> <p>When we convolve (slide and multiply-add) this kernel over the image, say over the top-left 3x3 patch:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[1, 1, 1],
 [0, 1, 1],
 [0, 0, 1]]
</code></pre></div></div> <p>The calculation would be: $(1 \times -1) + (1 \times 0) + (1 \times 1) +$ $(0 \times -1) + (1 \times 0) + (1 \times 1) +$ $(0 \times -1) + (0 \times 0) + (1 \times 1) = -1 + 0 + 1 + 0 + 0 + 1 + 0 + 0 + 1 = 2$</p> <p>This ‘2’ would be the first pixel in our resulting feature map. The kernel then slides one pixel to the right, and the process repeats.</p> <p>Mathematically, the convolution operation $(I * K)(i, j)$ at position $(i, j)$ in the output feature map is defined as:</p> \[(I * K)(i, j) = \sum_m \sum_n I(i-m, j-n) K(m, n)\] <p>Here, $I$ is the input image, $K$ is the kernel, and the summations are over the dimensions of the kernel. Don’t let the notation scare you; it’s just a precise way of describing that sliding, multiplying, and summing process we just discussed!</p> <p>Different kernels will detect different features – one might look for horizontal edges, another for corners, another for specific textures. The amazing thing is that in a CNN, these kernels are not manually designed; they are <em>learned</em> during the training process! The network figures out which kernels are best for identifying the relevant features to solve the task at hand (like classifying a cat).</p> <h3 id="building-blocks-of-a-cnn">Building Blocks of a CNN</h3> <p>A typical CNN architecture consists of several specialized layers, each playing a crucial role:</p> <h4 id="1-the-convolutional-layer">1. The Convolutional Layer</h4> <p>This is where the magic happens! We’ve already discussed the convolution operation. A convolutional layer typically uses multiple kernels, each generating a different feature map. This means our network can simultaneously learn to detect a wide array of features.</p> <ul> <li> <strong>Stride:</strong> This parameter determines how many pixels the kernel shifts at each step. A stride of 1 means it moves one pixel at a time, generating a larger feature map. A stride of 2 means it skips a pixel, leading to a smaller feature map but faster computation.</li> <li> <strong>Padding:</strong> When a kernel slides over an image, pixels at the edges get convolved fewer times than those in the center. To avoid losing information from the edges or shrinking the output size too much, we can add “padding” (typically zero-value pixels) around the border of the input image. ‘Same’ padding ensures the output feature map has the same spatial dimensions as the input.</li> <li> <p><strong>Activation Function:</strong> After the convolution, the output often passes through a non-linear activation function. The most popular choice for CNNs is the <strong>Rectified Linear Unit (ReLU)</strong> function:</p> \[f(x) = \max(0, x)\] <p>ReLU simply converts any negative values to zero, while positive values remain unchanged. This introduces non-linearity, allowing the network to learn more complex patterns and vastly speeding up training compared to older activation functions like sigmoid. Without non-linearity, stacking multiple layers would simply result in a single linear transformation, no matter how deep the network.</p> </li> </ul> <h4 id="2-the-pooling-layer-or-subsampling-layer">2. The Pooling Layer (or Subsampling Layer)</h4> <p>After generating feature maps, CNNs often introduce a pooling layer. Its primary goal is to progressively reduce the spatial dimensions (width and height) of the feature maps, which helps in two ways: 1. <strong>Reduces computational load:</strong> Fewer parameters and computations in subsequent layers. 2. <strong>Encourages spatial invariance:</strong> Makes the network more robust to small shifts or distortions in the input image. If a feature (like an edge) shifts a little, the pooling layer might still capture it in the same pooled region.</p> <p>The most common type is <strong>Max Pooling</strong>. Imagine a 2x2 filter sliding over the feature map (with a stride of 2). Instead of summing, it simply takes the <em>maximum</em> value from that 2x2 region and places it into the output.</p> <p>Example of 2x2 Max Pooling:</p> <p>Input feature map:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[1, 1, 2, 4],
 [5, 6, 7, 8],
 [3, 2, 1, 0],
 [1, 2, 3, 4]]
</code></pre></div></div> <p>Output after Max Pooling (2x2 filter, stride 2):</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[6, 8],
 [3, 4]]
</code></pre></div></div> <p>(From the top-left 2x2 region <code class="language-plaintext highlighter-rouge">[[1,1],[5,6]]</code>, max is 6. From <code class="language-plaintext highlighter-rouge">[[2,4],[7,8]]</code>, max is 8, and so on.)</p> <p>Other pooling types include Average Pooling, but Max Pooling is generally preferred as it’s good at preserving the most prominent features.</p> <h4 id="3-the-fully-connected-layer">3. The Fully Connected Layer</h4> <p>After several convolutional and pooling layers, the network has learned to extract sophisticated, high-level features from the input image. At this point, the spatially organized feature maps are “flattened” into a single long vector. This vector is then fed into one or more traditional fully connected (dense) neural network layers.</p> <p>These fully connected layers act as classifiers. They take the features learned by the preceding layers and combine them to make a final prediction – for example, classifying an image as a “cat,” “dog,” or “bird.” The last fully connected layer typically uses a <strong>Softmax activation function</strong> for classification tasks, which outputs probabilities for each class, ensuring they sum up to 1.</p> \[\sigma(\mathbf{z})_j = \frac{e^{z_j}}{\sum_{k=1}^K e^{z_k}}\] <p>This formula takes the raw scores (logits) $z_j$ for each class $j$ and converts them into a probability distribution.</p> <h3 id="the-full-picture-cnn-architecture">The Full Picture: CNN Architecture</h3> <p>A typical CNN architecture often looks like this:</p> <p><code class="language-plaintext highlighter-rouge">Input Image -&gt; Conv Layer -&gt; ReLU -&gt; Pooling Layer -&gt; Conv Layer -&gt; ReLU -&gt; Pooling Layer -&gt; ... -&gt; Flatten -&gt; Fully Connected Layer -&gt; Softmax Output</code></p> <p>As the network gets “deeper” (more layers), the initial convolutional layers learn very basic features like edges and gradients. Middle layers combine these basic features to detect more complex patterns like textures, corners, or parts of objects. The deepest layers learn to identify abstract representations of entire objects or scenes. It’s a hierarchical learning process, much like how our brains build understanding from simple perceptions to complex interpretations.</p> <h3 id="the-superpowers-of-cnns">The Superpowers of CNNs</h3> <p>Why are CNNs so incredibly effective, especially for image data?</p> <ol> <li> <strong>Parameter Sharing:</strong> This is a game-changer! Unlike traditional neural networks where each neuron connects to every input pixel and learns its own weights, a CNN reuses the same kernel (set of weights) across the entire image. This drastically reduces the number of parameters the network needs to learn, making it more efficient and less prone to overfitting. It’s like having a single feature detector (e.g., for vertical edges) that you slide across the whole image, rather than needing a separate detector for every possible location a vertical edge could appear.</li> <li> <strong>Sparsity of Connections:</strong> In a convolutional layer, each output pixel in a feature map is only influenced by a small, local region of the input image (defined by the kernel size). This is a sparse connection pattern, contrasting with the dense connections in a fully connected layer. This locality is biologically plausible and computationally efficient.</li> <li> <strong>Equivariance to Translation:</strong> Because the kernels slide across the image, if a feature (like an eye) moves slightly in the input image, its corresponding activation in the feature map will also move by the same amount. This means the network can detect features regardless of their exact position, making it robust to slight shifts in object placement.</li> <li> <strong>Hierarchical Feature Learning:</strong> As mentioned, CNNs automatically learn a hierarchy of features, from simple low-level details to complex high-level concepts. This deep understanding is what allows them to perform such nuanced image analysis.</li> </ol> <h3 id="real-world-impact">Real-World Impact</h3> <p>The capabilities of CNNs have revolutionized countless fields:</p> <ul> <li> <strong>Image Classification &amp; Object Detection:</strong> Identifying objects within images (e.g., self-driving cars recognizing pedestrians, traffic signs, and other vehicles).</li> <li> <strong>Facial Recognition:</strong> Unlocking your phone, security surveillance.</li> <li> <strong>Medical Imaging:</strong> Detecting tumors, diseases, and anomalies in X-rays, MRIs, and CT scans.</li> <li> <strong>Image Segmentation:</strong> Identifying and delineating the exact boundaries of objects in an image.</li> <li> <strong>Satellite Imagery Analysis:</strong> Monitoring deforestation, urban development, and agricultural health.</li> <li> <strong>Content Moderation:</strong> Automatically flagging inappropriate content online.</li> </ul> <h3 id="wrapping-up">Wrapping Up</h3> <p>From a humble pixel to understanding the complex world around us, Convolutional Neural Networks represent a monumental leap in how computers perceive and interpret visual information. They embody a beautiful blend of biological inspiration, mathematical elegance, and computational power.</p> <p>I hope this journey into the heart of CNNs has sparked your curiosity and given you a clearer understanding of these incredible models. The field of deep learning is constantly evolving, but CNNs remain a foundational pillar, pushing the boundaries of what’s possible in computer vision.</p> <p>So, the next time your phone automatically tags your friend in a photo, or a smart security camera alerts you to a package delivery, remember the silent, powerful convolutions happening beneath the surface, teaching computers to truly “see.”</p> <p>Why not dive deeper yourself? There are many open-source datasets and frameworks (like TensorFlow and PyTorch) that allow you to experiment with building your own CNNs. The journey of discovery is just beginning!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>