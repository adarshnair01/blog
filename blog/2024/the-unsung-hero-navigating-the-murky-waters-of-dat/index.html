<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Unsung Hero: Navigating the Murky Waters of Data Cleaning Strategies | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/the-unsung-hero-navigating-the-murky-waters-of-dat/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Unsung Hero: Navigating the Murky Waters of Data Cleaning Strategies</h1> <p class="post-meta"> Created on November 10, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/data-cleaning"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Cleaning</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/preprocessing"> <i class="fa-solid fa-hashtag fa-sm"></i> Preprocessing</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/data-quality"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Quality</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Welcome, fellow data explorers, to a topic that might not always get the spotlight but is undeniably the bedrock of every successful data science endeavor: <strong>Data Cleaning</strong>.</p> <p>In my journey through the fascinating world of data science and machine learning, I’ve seen incredible algorithms, groundbreaking models, and mind-bending visualizations. But beneath every shiny, successful project, there’s always one consistent, foundational truth: <strong>clean data</strong>. It’s like building a skyscraper – you can have the most brilliant architectural design, but if your foundation is shaky, the whole thing will crumble. In data science, we have a similar adage: “Garbage In, Garbage Out” (GIGO). You can have the most sophisticated neural network, but if it’s fed dirty, inconsistent, or incomplete data, its predictions will be, well, garbage.</p> <p>When I first started, I was eager to jump straight into model building. I wanted to see those R-squared values soar and accuracy scores hit 99%! But time and again, my models would underperform, throw cryptic errors, or produce results that just didn’t make sense. It took a while, but I eventually learned the hard truth: I was skipping the most crucial step. I was trying to bake a cake with rotten eggs, stale flour, and missing sugar, then wondering why it tasted awful.</p> <p>So, let’s roll up our sleeves and confront the mess head-on. This isn’t just a technical exercise; it’s a critical mindset for anyone serious about working with data.</p> <h3 id="the-anatomy-of-a-mess-common-data-issues">The Anatomy of a Mess: Common Data Issues</h3> <p>Data can get messy in countless ways, but several culprits appear more frequently than others. Understanding these issues is the first step towards formulating an effective cleaning strategy.</p> <h4 id="1-missing-values-the-silent-holes">1. Missing Values: The Silent Holes</h4> <p>Imagine you’re trying to piece together a puzzle, but some pieces are just gone. That’s what missing values are like. They can appear as <code class="language-plaintext highlighter-rouge">NaN</code> (Not a Number), <code class="language-plaintext highlighter-rouge">None</code>, blank strings, or even specific placeholder values like <code class="language-plaintext highlighter-rouge">-999</code>.</p> <p><strong>Why they happen:</strong></p> <ul> <li>Data entry errors (someone forgot to fill in a field).</li> <li>Data collection issues (a sensor malfunctioned, a survey question was skipped).</li> <li>Data corruption during transfer.</li> <li>Intentional omission (e.g., a customer chose not to provide certain information).</li> </ul> <p><strong>Impact:</strong> Missing values can skew statistical analyses, lead to incorrect conclusions, and often cause machine learning models to crash or perform poorly. Many algorithms simply cannot handle <code class="language-plaintext highlighter-rouge">NaN</code> values.</p> <p><strong>Strategies for Handling Missing Values:</strong></p> <ul> <li> <strong>Deletion:</strong> <ul> <li> <strong>Row-wise Deletion:</strong> If a row has too many missing values, or if the dataset is very large and the number of rows with missing values is small, you might delete the entire row. This is often done using <code class="language-plaintext highlighter-rouge">df.dropna()</code>. <ul> <li> <strong>Pros:</strong> Simple, quick, ensures complete data for remaining rows.</li> <li> <strong>Cons:</strong> Can lead to significant loss of valuable data, especially in smaller datasets or if missingness isn’t random.</li> </ul> </li> <li> <strong>Column-wise Deletion:</strong> If a column has an overwhelming percentage of missing values (e.g., 70-80%), it might be best to drop the entire column, as it provides little information. <ul> <li> <strong>Pros:</strong> Simplifies the dataset, removes potentially noisy features.</li> <li> <strong>Cons:</strong> Loss of a potential feature, even if incomplete.</li> </ul> </li> </ul> </li> <li> <p><strong>Imputation (Filling in the Blanks):</strong> This involves estimating and replacing missing values with a substitute. This is generally preferred over deletion when you want to retain as much data as possible.</p> <ul> <li> <strong>Mean/Median/Mode Imputation:</strong> <ul> <li> <strong>Mean:</strong> For numerical data, replacing missing values with the column’s mean. Useful when data is normally distributed. <ul> <li> <em>Example:</em> If we have ages $[22, 25, \text{NaN}, 30, 28]$, the mean is $\frac{22+25+30+28}{4} = 26.25$.</li> <li> <em>Latex Math:</em> The mean $\bar{x}$ of $n$ observations $x_1, x_2, \ldots, x_n$ is: $\bar{x} = \frac{1}{n}\sum_{i=1}^{n} x_i$</li> </ul> </li> <li> <strong>Median:</strong> For numerical data, replacing missing values with the column’s median. More robust to outliers than the mean, especially in skewed distributions. <ul> <li> <em>Example:</em> For ages $[22, 25, \text{NaN}, 30, 100]$ (100 is an outlier), the median (after sorting: $[22, 25, 30, 100]$) is $27.5$. The mean would be $44.25$, which is heavily influenced by 100.</li> </ul> </li> <li> <strong>Mode:</strong> For categorical or discrete numerical data, replacing missing values with the most frequent value. <ul> <li> <em>Example:</em> If a ‘Color’ column has <code class="language-plaintext highlighter-rouge">['Red', 'Blue', 'Red', 'Green', NaN]</code>, the mode is ‘Red’.</li> </ul> </li> <li> <strong>Pros:</strong> Simple, quick to implement.</li> <li> <strong>Cons:</strong> Reduces variance in the data, can introduce bias if missingness is not random.</li> </ul> </li> <li> <strong>Forward-Fill/Backward-Fill:</strong> Common in time-series data, where missing values are filled with the previous or next valid observation. <code class="language-plaintext highlighter-rouge">df.fillna(method='ffill')</code> or <code class="language-plaintext highlighter-rouge">df.fillna(method='bfill')</code>. <ul> <li> <strong>Pros:</strong> Preserves trends in sequential data.</li> <li> <strong>Cons:</strong> Not suitable for non-sequential data; can propagate errors if a long sequence of missing values occurs.</li> </ul> </li> <li> <strong>Advanced Imputation (Briefly):</strong> For more complex scenarios, techniques like K-Nearest Neighbors (KNN) imputation (filling based on similar rows), or regression imputation (predicting missing values using other columns) can be used. These methods are more sophisticated but also more computationally intensive.</li> </ul> </li> </ul> <h4 id="2-outliers-the-extreme-mavericks">2. Outliers: The Extreme Mavericks</h4> <p>Outliers are data points that significantly deviate from other observations. They’re the odd ones out, the record breakers, or sometimes, just mistakes.</p> <p><strong>Why they happen:</strong></p> <ul> <li>Measurement errors (a sensor gave a faulty reading).</li> <li>Data entry errors (a typo, e.g., <code class="language-plaintext highlighter-rouge">250</code> instead of <code class="language-plaintext highlighter-rouge">25</code>).</li> <li>Natural variations (a truly exceptional event or individual).</li> </ul> <p><strong>Impact:</strong> Outliers can drastically skew statistical measures (especially the mean), distort visualizations, and negatively affect the training of sensitive machine learning models like linear regression.</p> <p><strong>Strategies for Handling Outliers:</strong></p> <ul> <li> <strong>Detection:</strong> <ul> <li> <strong>Visualization:</strong> Box plots (they clearly show points outside the “whiskers”), scatter plots.</li> <li> <strong>Statistical Methods:</strong> <ul> <li> <strong>Z-score:</strong> Measures how many standard deviations a data point is from the mean. A common threshold is a Z-score absolute value greater than 2 or 3. <ul> <li> <em>Latex Math:</em> $Z = \frac{x - \mu}{\sigma}$ where $x$ is the data point, $\mu$ is the mean, and $\sigma$ is the standard deviation.</li> </ul> </li> <li> <strong>Interquartile Range (IQR):</strong> Defines a range where most data points lie ($Q_1$ to $Q_3$, where $Q_1$ is the 25th percentile and $Q_3$ is the 75th percentile). Outliers are often defined as values below $Q_1 - 1.5 \times IQR$ or above $Q_3 + 1.5 \times IQR$. <ul> <li> <em>Latex Math:</em> $IQR = Q_3 - Q_1$. Outlier Lower Bound: $Q_1 - 1.5 \times IQR$. Outlier Upper Bound: $Q_3 + 1.5 \times IQR$.</li> </ul> </li> </ul> </li> <li> <strong>Model-based Methods:</strong> Isolation Forest or One-Class SVM.</li> </ul> </li> <li> <strong>Treatment:</strong> <ul> <li> <strong>Removal:</strong> If you’re confident an outlier is due to an error and not genuine data, you might remove it. Use with caution, as it leads to data loss.</li> <li> <strong>Transformation:</strong> Apply mathematical transformations to reduce the impact of outliers. Log transformation ($log(x)$) is common for highly skewed data, as it compresses larger values.</li> <li> <strong>Winsorization (Capping):</strong> Instead of removing outliers, you can “cap” them. This involves setting all values above a certain percentile (e.g., 99th percentile) to that percentile’s value, and all values below a certain percentile (e.g., 1st percentile) to that percentile’s value. This retains the data points but limits their extreme influence.</li> <li> <strong>Treat as Missing:</strong> If an outlier seems genuinely unrepresentative and not a simple error, you might replace it with <code class="language-plaintext highlighter-rouge">NaN</code> and then use imputation strategies.</li> </ul> </li> </ul> <h4 id="3-inconsistent-data--duplicates-the-sneaky-saboteurs">3. Inconsistent Data &amp; Duplicates: The Sneaky Saboteurs</h4> <p>These issues often manifest as variations in formatting, spelling, or outright redundant entries.</p> <p><strong>Why they happen:</strong></p> <ul> <li>Manual data entry errors.</li> <li>Merging data from different sources with varying conventions.</li> <li>Lack of data validation during collection.</li> </ul> <p><strong>Impact:</strong> Inconsistent data can lead to incorrect counts, miscategorization, and flawed analysis. Duplicates artificially inflate dataset size and can bias models towards certain observations.</p> <p><strong>Strategies:</strong></p> <ul> <li> <strong>Standardization and Correction:</strong> <ul> <li> <strong>Case Consistency:</strong> Convert all text to a uniform case (e.g., <code class="language-plaintext highlighter-rouge">df['Column'].str.lower()</code>). “USA,” “Usa,” and “usa” should all become “usa.”</li> <li> <strong>Typo Correction:</strong> For categorical data, review unique values (<code class="language-plaintext highlighter-rouge">df['Column'].unique()</code>) and manually correct obvious typos (e.g., “New Yrok” to “New York”). Fuzzy matching algorithms can help identify similar strings.</li> <li> <strong>Format Consistency:</strong> Ensure dates are in a consistent format (<code class="language-plaintext highlighter-rouge">YYYY-MM-DD</code>), numbers don’t have currency symbols or commas unless intended, etc. Regular expressions (regex) are incredibly powerful for pattern matching and extraction.</li> <li> <strong>Mapping Values:</strong> Consolidate variations of the same concept (e.g., <code class="language-plaintext highlighter-rouge">M</code>, <code class="language-plaintext highlighter-rouge">Male</code>, <code class="language-plaintext highlighter-rouge">m</code> all map to <code class="language-plaintext highlighter-rouge">Male</code>).</li> </ul> </li> <li> <strong>Duplicate Removal:</strong> <ul> <li> <strong>Exact Duplicates:</strong> Identify and remove rows that are identical across all columns. <code class="language-plaintext highlighter-rouge">df.drop_duplicates()</code> is your friend here.</li> <li> <strong>Partial Duplicates:</strong> Sometimes, only a subset of columns might make a row unique (e.g., <code class="language-plaintext highlighter-rouge">customer_id</code>). You can drop duplicates based on specific columns: <code class="language-plaintext highlighter-rouge">df.drop_duplicates(subset=['customer_id'])</code>.</li> <li> <strong>Pros:</strong> Reduces bias, improves model accuracy, saves memory.</li> <li> <strong>Cons:</strong> Ensure you’re not deleting genuinely distinct entries that happen to share some values.</li> </ul> </li> </ul> <h4 id="4-data-type-mismatches-the-hidden-roadblocks">4. Data Type Mismatches: The Hidden Roadblocks</h4> <p>This occurs when data is stored in a format that doesn’t match its true nature (e.g., numbers stored as strings, dates as generic objects).</p> <p><strong>Why they happen:</strong></p> <ul> <li>Importing data from various sources (CSV, Excel, databases) often infers types incorrectly.</li> <li>Mixed data types within a single column.</li> </ul> <p><strong>Impact:</strong> Prevents numerical calculations, incorrect sorting, and can cause errors in many data manipulation and machine learning libraries.</p> <p><strong>Strategies:</strong></p> <ul> <li> <strong>Type Conversion:</strong> <ul> <li>Convert columns to numeric: <code class="language-plaintext highlighter-rouge">pd.to_numeric(df['Column'], errors='coerce')</code> (the <code class="language-plaintext highlighter-rouge">errors='coerce'</code> argument is crucial; it turns unconvertible values into <code class="language-plaintext highlighter-rouge">NaN</code>, which you can then handle).</li> <li>Convert to datetime objects: <code class="language-plaintext highlighter-rouge">pd.to_datetime(df['Column'], errors='coerce')</code>.</li> <li>Convert to categorical: <code class="language-plaintext highlighter-rouge">df['Column'].astype('category')</code> (useful for efficiency and for models that expect categorical inputs).</li> <li> <strong>Pros:</strong> Enables correct operations, reduces memory usage for categorical data.</li> <li> <strong>Cons:</strong> Can introduce <code class="language-plaintext highlighter-rouge">NaN</code>s if conversions fail, requiring further handling.</li> </ul> </li> </ul> <h3 id="the-data-cleaning-workflow-a-strategic-approach">The Data Cleaning Workflow: A Strategic Approach</h3> <p>Data cleaning isn’t a single step; it’s an iterative process that often weaves through your entire data science project. Here’s a general workflow I’ve found incredibly useful:</p> <ol> <li> <p><strong>Understand Your Data (Exploratory Data Analysis - EDA):</strong> Before you clean, you must know what you’re cleaning. Plot histograms, box plots, scatter plots. Calculate summary statistics (<code class="language-plaintext highlighter-rouge">df.describe()</code>, <code class="language-plaintext highlighter-rouge">df.info()</code>). Look at unique values. This is where you identify potential issues.</p> </li> <li> <p><strong>Profile Your Data:</strong> Systematically identify all data quality issues. Tools like <code class="language-plaintext highlighter-rouge">df.isnull().sum()</code> for missing values, <code class="language-plaintext highlighter-rouge">df.duplicated().sum()</code> for duplicates, and <code class="language-plaintext highlighter-rouge">df.dtypes</code> for type mismatches are invaluable.</p> </li> <li> <p><strong>Strategize and Document:</strong> Based on your findings, decide on the appropriate cleaning strategy for each issue. Crucially, <strong>document your decisions!</strong> Why did you choose median imputation over mean? Why did you remove these outliers? This transparency is vital for reproducibility and collaboration.</p> </li> <li> <p><strong>Implement and Verify:</strong> Apply your chosen cleaning techniques. After each major cleaning step, verify the changes. Did the missing values disappear? Are the data types correct? Rerun your profiling steps to ensure new issues haven’t been introduced. This is often an iterative loop – clean a bit, check, clean more.</p> </li> <li> <p><strong>Automate (if possible):</strong> For production systems or recurring tasks, encapsulate your cleaning steps into a pipeline. This ensures consistency and efficiency. Scikit-learn’s <code class="language-plaintext highlighter-rouge">Pipeline</code> class is excellent for this.</p> </li> </ol> <h3 id="tools-of-the-trade-briefly">Tools of the Trade (Briefly)</h3> <ul> <li> <strong>Pandas:</strong> The workhorse for data manipulation in Python. Methods like <code class="language-plaintext highlighter-rouge">isna()</code>, <code class="language-plaintext highlighter-rouge">fillna()</code>, <code class="language-plaintext highlighter-rouge">dropna()</code>, <code class="language-plaintext highlighter-rouge">drop_duplicates()</code>, <code class="language-plaintext highlighter-rouge">astype()</code>, <code class="language-plaintext highlighter-rouge">str</code> accessor for string operations are your daily companions.</li> <li> <strong>NumPy:</strong> Often used in conjunction with Pandas for numerical operations and handling <code class="language-plaintext highlighter-rouge">NaN</code>s.</li> <li> <strong>Scikit-learn:</strong> Provides <code class="language-plaintext highlighter-rouge">SimpleImputer</code> for various imputation strategies and <code class="language-plaintext highlighter-rouge">StandardScaler</code> for outlier handling (normalization).</li> <li> <strong>Regular Expressions (re module):</strong> Invaluable for pattern matching and cleaning messy text data.</li> </ul> <h3 id="embracing-the-grime">Embracing the Grime</h3> <p>Data cleaning isn’t the most glamorous part of data science. It doesn’t involve complex algorithms or flashy visualizations. It’s often tedious, requiring patience, attention to detail, and a detective’s mindset. There’s no one-size-fits-all solution; each dataset presents its unique set of challenges.</p> <p>However, it’s precisely this foundational work that determines the success or failure of your entire project. A meticulously cleaned dataset is like a perfectly tuned engine – it runs smoothly, efficiently, and takes you exactly where you want to go. The satisfaction of working with pristine data, knowing that your insights and models are built on a solid foundation, is immense.</p> <p>So, the next time you embark on a data science adventure, remember the unsung hero: <strong>Data Cleaning</strong>. Embrace the grime, understand the mess, and strategize your way to cleaner, more reliable data. Your models (and your sanity!) will thank you for it. Happy cleaning!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>