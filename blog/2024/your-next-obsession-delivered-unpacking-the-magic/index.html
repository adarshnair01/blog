<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Your Next Obsession, Delivered: Unpacking the Magic Behind Recommender Systems | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/your-next-obsession-delivered-unpacking-the-magic/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Your Next Obsession, Delivered: Unpacking the Magic Behind Recommender Systems</h1> <p class="post-meta"> Created on September 06, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/recommender-systems"> <i class="fa-solid fa-hashtag fa-sm"></i> Recommender Systems</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/collaborative-filtering"> <i class="fa-solid fa-hashtag fa-sm"></i> Collaborative Filtering</a>   <a href="/blog/blog/tag/content-based-filtering"> <i class="fa-solid fa-hashtag fa-sm"></i> Content-Based Filtering</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>It’s a quiet evening. You’ve just finished a long day, slumped onto the couch, and instinctively reached for the TV remote. Netflix asks, “Who’s watching?” and then presents you with a meticulously curated list of shows and movies. Or maybe you’re opening Spotify, and there’s a “Discover Weekly” playlist so eerily accurate it feels like it read your mind. This isn’t magic, nor is it a coincidence. This is the sophisticated world of <strong>Recommender Systems</strong> at play, a cornerstone of modern data science and machine learning.</p> <p>My fascination with these systems began when I realized just how ubiquitous they are. From Amazon suggesting products you “might also like” to YouTube recommending your next binge-watch, recommender systems have fundamentally reshaped how we discover content, products, and even people. They bridge the gap between an overwhelming universe of choices and the unique preferences of individual users. For businesses, they’re revenue drivers; for users, they’re personal guides.</p> <p>So, how do they actually work? Let’s peel back the layers and dive into the fascinating algorithms that power these digital matchmakers.</p> <h3 id="the-core-idea-predicting-your-preferences">The Core Idea: Predicting Your Preferences</h3> <p>At its heart, a recommender system aims to predict what you, as a user, would rate or prefer for an item you haven’t yet interacted with. This prediction is based on your past behavior, the behavior of similar users, and the characteristics of the items themselves.</p> <p>We generally gather two types of data for this:</p> <ol> <li> <strong>Explicit Feedback:</strong> Direct input from users, like star ratings (1-5 stars for a movie), “likes” or “dislikes” on a video, or written reviews. This is gold-standard data, but users don’t always provide it.</li> <li> <strong>Implicit Feedback:</strong> Indirect observations of user behavior, such as purchase history, watch time, clicks, searches, or even mouse movements. This data is abundant but can be noisy (a user might click an item by accident, or watch only 5 minutes of a movie before getting interrupted, which doesn’t mean they disliked it).</li> </ol> <p>One major hurdle for any recommender system is the <strong>Cold Start Problem</strong>. What do you recommend to a brand new user with no history? Or what about a brand new item that no one has interacted with yet? Without data, the system is “cold” and struggles to make accurate predictions. We’ll see how different approaches try to tackle this.</p> <h3 id="type-1-content-based-filtering--you-like-this-so-youll-like-that">Type 1: Content-Based Filtering – You Like This, So You’ll Like <em>That</em> </h3> <p>Imagine you love sci-fi movies, particularly those starring your favorite actor, say, Tom Hanks. A content-based recommender system would analyze the characteristics (or “features”) of the movies you’ve enjoyed in the past. If you liked “Apollo 13” (sci-fi, Tom Hanks, drama) and “Cast Away” (drama, Tom Hanks, survival), it might recommend “Sully” (drama, Tom Hanks, biopic).</p> <p><strong>How it Works:</strong></p> <ol> <li> <strong>Item Representation:</strong> Each item (movie, song, article) is described by its attributes. For a movie, these could be genre (sci-fi, drama), director, actors, keywords from its synopsis, etc. We can represent these attributes as a vector. For example, a movie could be $[1, 0, 1, 0, \dots]$ where 1 means it has a particular genre (sci-fi) and 0 means it doesn’t.</li> <li> <strong>User Profile:</strong> Your preferences are built by aggregating the features of items you’ve previously liked. If you liked many sci-fi movies, your profile will have a strong “sci-fi” component.</li> <li> <p><strong>Recommendation:</strong> The system then looks for new items whose feature vectors are “similar” to your user profile vector. A common way to measure similarity between two vectors, $A$ and $B$, is using <strong>Cosine Similarity</strong>:</p> \[Similarity(A, B) = \frac{A \cdot B}{||A|| \cdot ||B||} = \frac{\sum_{i=1}^{n} A_i B_i}{\sqrt{\sum_{i=1}^{n} A_i^2} \sqrt{\sum_{i=1}^{n} B_i^2}}\] <p>This formula calculates the cosine of the angle between the two vectors. A cosine of 1 means they are perfectly similar (point in the same direction), and 0 means they are completely dissimilar (orthogonal).</p> </li> </ol> <p><strong>Pros:</strong></p> <ul> <li> <strong>Explainable:</strong> It’s easy to tell a user <em>why</em> an item was recommended (“because you liked similar sci-fi thrillers”).</li> <li> <strong>New Items:</strong> It can recommend new items, as long as they have features.</li> <li> <strong>User Independence:</strong> Recommendations for one user don’t depend on other users’ data.</li> </ul> <p><strong>Cons:</strong></p> <ul> <li> <strong>Limited Serendipity:</strong> It tends to recommend items very similar to what you already like, leading to a “filter bubble.” You might never discover something new outside your typical preferences.</li> <li> <strong>Feature Engineering:</strong> Defining and extracting meaningful features for items can be complex and labor-intensive.</li> <li> <strong>Cold Start for New Users:</strong> If a new user has no past interactions, there’s no profile to build from.</li> </ul> <h3 id="type-2-collaborative-filtering--the-power-of-the-crowd">Type 2: Collaborative Filtering – The Power of the Crowd</h3> <p>Collaborative Filtering (CF) operates on the principle that if two users shared similar tastes in the past, they will likely share similar tastes in the future. It’s like asking your friends for recommendations: “Hey, you liked that movie, and our tastes are usually similar, so what else do you recommend?”</p> <p>CF can be broadly categorized into two types:</p> <h4 id="21-memory-based-collaborative-filtering">2.1 Memory-Based Collaborative Filtering</h4> <p>These methods directly use the entire user-item interaction dataset to compute recommendations. They don’t learn a “model” in the traditional sense, but rather rely on similarities between users or items.</p> <ul> <li> <strong>User-User Collaborative Filtering:</strong> <ol> <li> <strong>Find Similar Users:</strong> Identify users who have similar ratings or interactions as the active user. We calculate similarity using metrics like <strong>Pearson Correlation</strong> or Cosine Similarity. Pearson correlation is often preferred for rating data as it accounts for users’ different rating scales (e.g., one user always rates high, another always rates low). \(Pearson(u,v) = \frac{\sum_{i \in I_{uv}} (r_{ui} - \bar{r_u})(r_{vi} - \bar{r_v})}{\sqrt{\sum_{i \in I_{uv}} (r_{ui} - \bar{r_u})^2} \sqrt{\sum_{i \in I_{uv}} (r_{vi} - \bar{r_v})^2}}\) Here, $r_{ui}$ is user $u$’s rating for item $i$, $\bar{r_u}$ is user $u$’s average rating, and $I_{uv}$ is the set of items rated by both $u$ and $v$.</li> <li> <strong>Predict Ratings:</strong> Once similar users (neighbors) are found, their ratings for unseen items are aggregated to predict the active user’s rating for those items. A common prediction formula is: \(\hat{r}_{ui} = \bar{r_u} + \frac{\sum_{v \in N} sim(u,v) \cdot (r_{vi} - \bar{r_v})}{\sum_{v \in N} |sim(u,v)|}\) Where $N$ is the set of $k$ similar users, $sim(u,v)$ is the similarity between user $u$ and user $v$, and $\hat{r}_{ui}$ is the predicted rating of user $u$ for item $i$.</li> </ol> </li> <li> <strong>Item-Item Collaborative Filtering:</strong> This approach identifies items similar to those the user has already liked. If you liked “Star Wars: A New Hope,” the system finds other items that are often liked by people who also liked “A New Hope.” This is particularly effective because item similarity tends to be more stable than user similarity over time. The similarity calculation between items is similar to user-user, typically using Cosine Similarity on item rating vectors.</li> </ul> <p><strong>Pros of Memory-Based CF:</strong></p> <ul> <li> <strong>Simple &amp; Intuitive:</strong> Easy to understand why recommendations are made.</li> <li> <strong>Serendipity:</strong> Can recommend items that are very different from a user’s past choices but are liked by similar users.</li> <li> <strong>No Feature Engineering:</strong> Doesn’t require explicit item features.</li> </ul> <p><strong>Cons of Memory-Based CF:</strong></p> <ul> <li> <strong>Scalability:</strong> For large datasets with millions of users and items, finding nearest neighbors in real-time can be computationally intensive.</li> <li> <strong>Sparsity:</strong> When the user-item interaction matrix is very sparse (most users have only interacted with a tiny fraction of items), finding enough overlapping interactions to compute reliable similarities becomes difficult.</li> <li> <strong>Cold Start:</strong> Still struggles with new users (no interaction history) and new items (no interactions to compare with).</li> </ul> <h4 id="22-model-based-collaborative-filtering-matrix-factorization">2.2 Model-Based Collaborative Filtering (Matrix Factorization)</h4> <p>Model-based approaches try to learn a predictive model from the data. The most famous example is <strong>Matrix Factorization (MF)</strong>, popularized by the Netflix Prize.</p> <p><strong>The Idea:</strong> Imagine we have a large, sparse user-item interaction matrix where rows are users, columns are items, and cells contain ratings (or implicit feedback). Most cells are empty because users only interact with a small subset of items. Matrix factorization aims to decompose this sparse matrix into two lower-rank dense matrices:</p> <ol> <li>A <strong>User-Latent Factor</strong> matrix ($P$), where each row represents a user and their “strength” or “preference” for a set of hidden, abstract characteristics (latent factors).</li> <li>An <strong>Item-Latent Factor</strong> matrix ($Q$), where each row represents an item and how much it embodies those same latent factors.</li> </ol> <p>Think of these “latent factors” as hidden dimensions that describe both users and items. For movies, a latent factor might represent “action intensity” or “art-house appeal,” even though we don’t explicitly define them. A user might have a high preference for “action intensity” and a low preference for “art-house appeal.”</p> <p>If user $u$ is represented by a vector $p_u$ (their preferences for latent factors) and item $i$ by a vector $q_i$ (its characteristics for latent factors), then the predicted rating $\hat{R}_{ui}$ is simply the dot product of these two vectors:</p> \[\hat{R}_{ui} = p_u^T q_i = \sum_{k=1}^{K} p_{uk} q_{ik}\] <p>Where $K$ is the number of latent factors (a hyperparameter we choose).</p> <p><strong>How it Works (Simplified FunkSVD):</strong> We want to find $p_u$ and $q_i$ for all users and items such that the error between our predicted rating $\hat{R}<em>{ui}$ and the actual known rating $R</em>{ui}$ is minimized. We define a loss function:</p> \[L = \sum_{(u,i) \in K_{known}} (R_{ui} - p_u^T q_i)^2 + \lambda (||p_u||^2 + ||q_i||^2)\] <table> <tbody> <tr> <td>Here, $K_{known}$ is the set of all known ratings. The second term, $\lambda (</td> <td> </td> <td>p_u</td> <td> </td> <td>^2 +</td> <td> </td> <td>q_i</td> <td> </td> <td>^2)$, is a regularization term (L2 regularization) that helps prevent overfitting by penalizing large factor values. $\lambda$ is a hyperparameter.</td> </tr> </tbody> </table> <p>We minimize this loss function using optimization techniques like Stochastic Gradient Descent. We iteratively update $p_u$ and $q_i$ by taking small steps in the direction that reduces the error for each known rating.</p> <p><strong>Pros of Matrix Factorization:</strong></p> <ul> <li> <strong>Handles Sparsity:</strong> By projecting into a lower-dimensional latent space, MF can infer preferences even from very sparse data.</li> <li> <strong>Scalability:</strong> Once the model is trained, predictions are fast ($O(K)$ per prediction).</li> <li> <strong>Accuracy:</strong> Often produces highly accurate recommendations.</li> <li> <strong>Discovers Latent Features:</strong> Uncovers hidden relationships between users and items.</li> </ul> <p><strong>Cons of Matrix Factorization:</strong></p> <ul> <li> <strong>Explainability:</strong> Harder to explain <em>why</em> a recommendation was made (“it’s because your latent factor for ‘dark, gritty sci-fi’ aligns with this movie’s ‘dark, gritty sci-fi’ factor”).</li> <li> <strong>Cold Start:</strong> Still a challenge for entirely new users or items, as they don’t have existing factor vectors ($p_u$ or $q_i$) yet.</li> <li> <strong>Computational Cost:</strong> Training the model can be computationally intensive, especially for very large datasets.</li> </ul> <h3 id="type-3-hybrid-recommender-systems--the-best-of-both-worlds">Type 3: Hybrid Recommender Systems – The Best of Both Worlds</h3> <p>Given the strengths and weaknesses of content-based and collaborative filtering, it’s natural to combine them into <strong>Hybrid Recommender Systems</strong>. These often yield the best performance in real-world scenarios.</p> <p>Common hybrid strategies include:</p> <ul> <li> <strong>Weighted Hybrid:</strong> Combining the scores from different recommenders using a linear model.</li> <li> <strong>Switching Hybrid:</strong> Using one recommender when data is sparse (e.g., content-based for new users) and another when data is rich (e.g., collaborative filtering).</li> <li> <strong>Feature Combination:</strong> Integrating content features directly into collaborative filtering models (e.g., using item features to help build item latent factors in matrix factorization). This is particularly useful for mitigating the cold start problem for new items.</li> </ul> <h3 id="challenges-and-the-road-ahead">Challenges and The Road Ahead</h3> <p>Recommender systems are a dynamic field with ongoing challenges:</p> <ul> <li> <strong>Cold Start:</strong> As discussed, it remains a critical hurdle for both new users and new items. Hybrid approaches, leveraging metadata, and even simple popularity-based recommendations can help.</li> <li> <strong>Scalability:</strong> As user bases and item catalogs grow, the computational demands for real-time recommendations become immense. Efficient algorithms and distributed computing are essential.</li> <li> <strong>Sparsity:</strong> Most users interact with only a tiny fraction of available items, leaving vast swaths of the user-item matrix empty.</li> <li> <strong>Explainability:</strong> Users often want to know <em>why</em> something was recommended. This is easier for content-based systems but harder for complex model-based and deep learning approaches.</li> <li> <strong>Fairness and Bias:</strong> Recommenders can inadvertently reinforce existing biases present in the training data, leading to “filter bubbles” (limiting exposure to diverse content) or even perpetuating stereotypes. Designing fair algorithms is a key ethical consideration.</li> <li> <strong>Real-time Recommendations:</strong> The ability to instantly adapt recommendations based on a user’s most recent interaction is crucial for many applications (e.g., news feeds, e-commerce).</li> <li> <strong>Deep Learning for Recommenders:</strong> More recently, deep learning techniques, especially those leveraging embeddings (learning dense vector representations of users and items) and sequence models (like RNNs or Transformers to model user interaction sequences), are pushing the boundaries of accuracy and sophistication.</li> </ul> <h3 id="conclusion-your-digital-concierge">Conclusion: Your Digital Concierge</h3> <p>From simple item-to-item similarity to complex matrix factorizations and hybrid models, recommender systems are a testament to the power of data science and machine learning. They tackle the fundamental human challenge of choice, helping us navigate vast digital landscapes to find what we truly love.</p> <p>Next time Netflix suggests that obscure documentary you never knew you needed, or Spotify surprises you with a track that becomes your new anthem, take a moment to appreciate the intricate dance of algorithms behind the scenes. These systems are constantly learning, adapting, and evolving, becoming ever more intelligent digital concierges, guiding us through our increasingly personalized digital lives. It’s a field brimming with fascinating challenges and endless opportunities for innovation – a truly exciting space for any aspiring data scientist or ML engineer!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>