<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Q-Learning Demystified: How AI Learns Through Trial and Error | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/q-learning-demystified-how-ai-learns-through-trial/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Q-Learning Demystified: How AI Learns Through Trial and Error</h1> <p class="post-meta"> Created on November 03, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/reinforcement-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Reinforcement Learning</a>   <a href="/blog/blog/tag/q-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Q-Learning</a>   <a href="/blog/blog/tag/artificial-intelligence"> <i class="fa-solid fa-hashtag fa-sm"></i> Artificial Intelligence</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h3 id="hey-there-fellow-explorers-of-ai">Hey there, fellow explorers of AI!</h3> <p>Have you ever taught a dog a new trick, or perhaps learned a new video game without reading the instruction manual? You probably did it through a process of trial and error, right? If you did something good, you got a reward (a treat for the dog, a high score for you). If you did something bad, you might have gotten a negative consequence (a stern “no,” or your character losing a life).</p> <p>This fundamental human (and animal) learning paradigm is precisely what Reinforcement Learning (RL) tries to mimic in artificial intelligence. And at the heart of many breakthroughs in RL, especially in its earlier days, lies a wonderfully intuitive algorithm called <strong>Q-Learning</strong>.</p> <p>Today, we’re going to pull back the curtain on Q-Learning. We’ll explore how it allows an AI agent to learn optimal actions in an environment, purely through interaction, without needing a human to label data or explicitly program every single rule. It’s like teaching a robot to navigate a complex office building just by letting it wander around, rewarding it for finding the coffee machine, and penalizing it for bumping into walls!</p> <h3 id="the-big-picture-reinforcement-learning-in-a-nutshell">The Big Picture: Reinforcement Learning in a Nutshell</h3> <p>Before we dive into the “Q,” let’s quickly recap what Reinforcement Learning is all about.</p> <p>Imagine an <strong>Agent</strong> (our AI) operating within an <strong>Environment</strong> (the world it interacts with). At any given moment, the agent is in a certain <strong>State</strong> (e.g., “robot is in the hallway facing north”). Based on this state, the agent chooses an <strong>Action</strong> (e.g., “move forward,” “turn left”).</p> <p>The environment then reacts: the state might change (e.g., “robot is now in the breakroom”), and the agent receives a <strong>Reward</strong> (or penalty). The goal of the agent is to learn a <strong>Policy</strong> – a mapping from states to actions – that maximizes its total accumulated reward over time.</p> <p>Think of it like training a pet:</p> <ul> <li> <strong>Agent:</strong> The pet.</li> <li> <strong>Environment:</strong> Your home.</li> <li> <strong>State:</strong> Pet is sitting, pet is barking, pet is near the door.</li> <li> <strong>Action:</strong> Pet sits, pet barks, pet scratches the door.</li> <li> <strong>Reward:</strong> A treat, a pat on the head, going outside.</li> <li> <strong>Policy:</strong> The learned rules that determine what the pet does in each situation to get the most treats/pats/walks.</li> </ul> <p>Q-Learning is a specific, powerful algorithm that helps agents figure out this optimal policy without ever explicitly knowing how the environment works (we call this “model-free” learning).</p> <h3 id="whats-in-a-q-understanding-q-values">What’s in a “Q”? Understanding Q-Values</h3> <p>The “Q” in Q-Learning stands for <strong>Quality</strong>. Specifically, a Q-value represents the <em>quality</em> or <em>utility</em> of taking a particular <strong>action</strong> in a particular <strong>state</strong>. It’s an estimate of the maximum discounted future reward an agent can expect to receive if it takes action $A$ in state $S$, and then acts optimally thereafter.</p> <p>In essence, the agent tries to build a mental map (or rather, a data table) of how “good” each action is in every possible situation. If our robot knows that taking “Action A” in “State S” leads to a high Q-value, it’s a good move. If it leads to a low Q-value, it’s probably a bad move.</p> <h3 id="the-q-table-ais-cheat-sheet">The Q-Table: AI’s Cheat Sheet</h3> <p>For simpler environments, the agent can store these Q-values in a data structure called a <strong>Q-table</strong>. This table has states as rows and actions as columns. Each cell $Q(S, A)$ holds the current estimated Q-value for taking action $A$ when in state $S$.</p> <table> <thead> <tr> <th style="text-align: left">State \ Action</th> <th style="text-align: center">Move Forward</th> <th style="text-align: center">Turn Left</th> <th style="text-align: center">Turn Right</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Hallway North</td> <td style="text-align: center">0.5</td> <td style="text-align: center">-0.2</td> <td style="text-align: center">0.8</td> </tr> <tr> <td style="text-align: left">Breakroom</td> <td style="text-align: center">1.2</td> <td style="text-align: center">0.1</td> <td style="text-align: center">-0.5</td> </tr> <tr> <td style="text-align: left">Office</td> <td style="text-align: center">-0.8</td> <td style="text-align: center">0.3</td> <td style="text-align: center">0.1</td> </tr> </tbody> </table> <p>(Example Q-table)</p> <p>Initially, the Q-table is typically filled with zeros or small random values because the agent has no idea what actions are good or bad. As the agent interacts with the environment, it constantly updates these Q-values, making them more accurate. The ultimate goal is for the Q-table to reflect the true optimal Q-values, guiding the agent to always pick the best action.</p> <h3 id="the-q-learning-algorithm-learning-through-experience">The Q-Learning Algorithm: Learning Through Experience</h3> <p>Let’s break down the core loop of how Q-Learning works. The agent iteratively learns by repeating the following steps:</p> <ol> <li> <strong>Initialize the Q-Table:</strong> Fill all $Q(S, A)$ values with zeros.</li> <li> <strong>Observe the Current State ($S_t$):</strong> The agent looks at its current situation.</li> <li> <strong>Choose an Action ($A_t$):</strong> This is where <strong>exploration vs. exploitation</strong> comes in. <ul> <li> <strong>Exploitation:</strong> The agent chooses the action with the highest Q-value for the current state (i.e., $\max_a Q(S_t, a)$). This is like using what it already knows is good.</li> <li> <strong>Exploration:</strong> The agent chooses a random action. This is crucial for discovering new paths, potentially even better ones, that it hasn’t tried before.</li> <li>To balance these, we use an <strong>$\epsilon$-greedy policy</strong>. With a probability $\epsilon$ (epsilon), the agent explores (takes a random action). With probability $(1 - \epsilon)$, it exploits (takes the best known action). $\epsilon$ usually starts high and slowly decays over time, so the agent explores a lot initially and then exploits more as it learns.</li> </ul> </li> <li> <strong>Perform the Action ($A_t$):</strong> The agent executes the chosen action in the environment.</li> <li> <strong>Observe New State ($S_{t+1}$) and Reward ($R_{t+1}$):</strong> The environment provides feedback.</li> <li> <p><strong>Update the Q-Value:</strong> This is the core learning step, where the Q-table is refined using the famous Q-Learning update rule (a form of the Bellman Equation):</p> <p>$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha [R_{t+1} + \gamma \max_{a} Q(S_{t+1}, a) - Q(S_t, A_t)]$</p> <p>Let’s break down this formidable-looking equation piece by piece:</p> <ul> <li>$Q(S_t, A_t)$: This is the <em>old</em> Q-value estimate for taking action $A_t$ in state $S_t$.</li> <li>$\leftarrow$: This means “is updated to.”</li> <li>$\alpha$ (alpha): The <strong>learning rate</strong>. This value (between 0 and 1) determines how much new information overrides old information. A high $\alpha$ means the agent learns quickly from new experiences but might be volatile. A low $\alpha$ means slower, more stable learning.</li> <li>$R_{t+1}$: The <strong>immediate reward</strong> received after taking action $A_t$ and landing in state $S_{t+1}$.</li> <li>$\gamma$ (gamma): The <strong>discount factor</strong>. This value (between 0 and 1) determines the importance of future rewards. A $\gamma$ close to 1 means the agent cares a lot about long-term rewards. A $\gamma$ close to 0 means it’s very short-sighted and only cares about immediate rewards.</li> <li>$\max_{a} Q(S_{t+1}, a)$: This is the <strong>maximum Q-value</strong> the agent can get from the <em>new state</em> $S_{t+1}$ by taking any possible action $a$. This represents the “best possible future” from the next state, assuming optimal play from then on.</li> <li>$[R_{t+1} + \gamma \max_{a} Q(S_{t+1}, a) - Q(S_t, A_t)]$: This entire term is the <strong>temporal difference (TD) error</strong>. It’s the difference between the agent’s <em>new estimate</em> of the Q-value (based on the immediate reward and the best possible future from the next state) and its <em>old estimate</em>. If this error is positive, the old estimate was too low; if negative, it was too high. The agent learns by reducing this error.</li> </ul> <p>In simpler terms: The agent is saying, “My old belief about how good this action was is $Q(S_t, A_t)$. But now I’ve seen the immediate reward $R_{t+1}$ and I know the best I can do from the next state $S_{t+1}$ is $\max_a Q(S_{t+1}, a)$. So, my new, more informed belief should be a weighted average of my old belief and this new experience.”</p> </li> <li> <strong>Repeat:</strong> The process continues until the agent has learned enough, usually after many thousands or millions of steps, or until it reaches a desired performance level.</li> </ol> <h3 id="the-hyperparameters-tuning-the-learning-process">The Hyperparameters: Tuning the Learning Process</h3> <p>We’ve mentioned a few important parameters. These are called <strong>hyperparameters</strong> because we set them <em>before</em> the learning process begins, and they significantly influence how well and how fast our agent learns:</p> <ul> <li> <strong>Learning Rate ($\alpha$):</strong> (e.g., 0.1) - Controls how much the Q-values are updated with each step. Too high, and learning can be unstable. Too low, and learning can be very slow.</li> <li> <strong>Discount Factor ($\gamma$):</strong> (e.g., 0.99) - Determines the importance of future rewards. Higher values emphasize long-term rewards, making the agent more strategic. Lower values make the agent more focused on immediate gains.</li> <li> <strong>Exploration Rate ($\epsilon$):</strong> (e.g., starts at 1.0 and decays to 0.01) - Balances exploring new actions versus exploiting known good actions. A common strategy is to start with a high $\epsilon$ (mostly exploration) and gradually decrease it over time (more exploitation) as the agent gains knowledge.</li> </ul> <h3 id="a-simple-analogy-learning-to-navigate-a-maze">A Simple Analogy: Learning to Navigate a Maze</h3> <p>Imagine our robot wants to find the exit of a simple maze.</p> <ul> <li> <strong>States:</strong> Each square in the maze.</li> <li> <strong>Actions:</strong> Move Up, Down, Left, Right.</li> <li> <strong>Reward:</strong> +1 for reaching the exit, -1 for hitting a wall, 0 for moving to an empty square.</li> </ul> <p>Initially, the robot’s Q-table is all zeros. It wanders around randomly ($\epsilon$ is high). When it hits a wall, it gets -1, and that Q-value for that state-action pair drops. When it eventually stumbles into the exit, it gets +1, and the Q-value for that state-action pair increases.</p> <p>Crucially, because of the $\gamma \max_a Q(S_{t+1}, a)$ term, this positive reward “propagates” backward. If being one step away from the exit leads to a high reward, then taking the action that leads to that one-step-away state also gets a boost, and so on. Over many trials, the Q-values will stabilize, effectively mapping out the “value” of each move from any square, ultimately guiding the robot to the shortest path to the exit.</p> <h3 id="limitations-of-tabular-q-learning">Limitations of Tabular Q-Learning</h3> <p>While powerful and fundamental, the basic Q-Learning we’ve discussed, which uses a Q-table, has some significant limitations:</p> <ul> <li> <strong>Curse of Dimensionality:</strong> What if our environment has millions of states (e.g., a complex video game, or a robot with continuous joint angles)? A Q-table would become impossibly large to store and update. This is where <strong>Deep Q-Networks (DQNs)</strong> come in, using neural networks to <em>approximate</em> the Q-function instead of explicitly storing it. But that’s a topic for another deep dive!</li> <li> <strong>Lack of Generalization:</strong> If the agent encounters a state it has never seen before, it doesn’t know what to do because that state isn’t in its Q-table. Tabular Q-Learning can’t generalize.</li> </ul> <h3 id="why-q-learning-still-matters">Why Q-Learning Still Matters</h3> <p>Despite these limitations, Q-Learning is an incredibly important algorithm:</p> <ul> <li> <strong>Foundation:</strong> It’s the bedrock upon which many more complex and powerful RL algorithms are built (like DQNs). Understanding Q-Learning is essential for grasping advanced RL concepts.</li> <li> <strong>Simplicity and Intuition:</strong> Its core idea of learning values through trial and error is highly intuitive and easy to grasp, making it an excellent starting point for anyone entering the field of Reinforcement Learning.</li> <li> <strong>Effectiveness in Simpler Domains:</strong> For problems with discrete states and actions and manageable state spaces (e.g., simple games, grid worlds, resource allocation tasks), Q-Learning is highly effective.</li> <li> <strong>Opens Doors:</strong> It helps us appreciate how intelligent behavior can emerge from simple learning rules, without explicit programming.</li> </ul> <h3 id="conclusion">Conclusion</h3> <p>Q-Learning is a beautiful example of how an intelligent agent can learn optimal behavior in an unknown environment, much like a child learning to navigate the world. By constantly estimating the “quality” of its actions and refining these estimates based on experience, an agent can go from clueless to competent, maximizing its cumulative rewards.</p> <p>As you continue your journey into data science and machine learning, you’ll find that Q-Learning’s principles echo throughout many other areas. It’s a testament to the power of learning from experience – a lesson just as valuable for our AI agents as it is for us. So go forth, experiment, and let your agents learn by doing!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>