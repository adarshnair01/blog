<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Unmasking Patterns: A Deep Dive into K-Means Clustering for Curious Minds | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/unmasking-patterns-a-deep-dive-into-k-means-cluste/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Unmasking Patterns: A Deep Dive into K-Means Clustering for Curious Minds</h1> <p class="post-meta"> Created on June 01, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/k-means"> <i class="fa-solid fa-hashtag fa-sm"></i> K-Means</a>   <a href="/blog/blog/tag/clustering"> <i class="fa-solid fa-hashtag fa-sm"></i> Clustering</a>   <a href="/blog/blog/tag/unsupervised-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Unsupervised Learning</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>Have you ever looked at a messy room and instinctively started grouping things – all the books together, all the clothes in one pile, all the electronics in another? That natural human tendency to find order and categorize is actually a fundamental concept in data science, and today, we’re going to explore one of its most elegant manifestations: <strong>K-Means Clustering</strong>.</p> <p>It’s one of those algorithms that, once you understand it, feels almost obvious in its brilliance. It’s a cornerstone of what we call <strong>unsupervised learning</strong>, a fascinating branch of machine learning where we let the data speak for itself, revealing its own intrinsic structure without us having to label anything beforehand. Think of it like this: instead of teaching a computer what a “cat” or “dog” is (supervised learning), we’re just giving it a huge pile of animal pictures and asking it to group similar-looking animals together.</p> <p>So, let’s pull back the curtain and see how K-Means performs its magic!</p> <h3 id="the-intuition-finding-natural-hangouts">The Intuition: Finding Natural Hangouts</h3> <p>Imagine you’re at a huge party. People are scattered everywhere, chatting, laughing, moving around. After a while, you might notice natural “clusters” forming: a group by the snacks, another by the music, a few people deep in conversation in a corner. You didn’t tell them where to go; they just gravitated towards common interests or locations.</p> <p>K-Means works much the same way with data points. Given a collection of points (say, customer ages and spending habits plotted on a graph), K-Means aims to find ‘K’ distinct centers around which these points naturally gather. ‘K’ is just a number we decide on beforehand – like saying, “I think there are roughly 3 main groups at this party.”</p> <h3 id="breaking-down-the-k-means-algorithm-step-by-step">Breaking Down the K-Means Algorithm: Step by Step</h3> <p>Let’s get practical. How does K-Means actually do this? It’s an iterative process, meaning it repeats a set of steps until it’s happy with the result.</p> <ol> <li> <p><strong>Step 1: Choose Your ‘K’ (The Number of Clusters)</strong> This is often the trickiest part, and we’ll circle back to it. For now, let’s say we <em>decide</em> we want our data to be split into, say, 3 groups (so, $K=3$).</p> </li> <li> <p><strong>Step 2: Initialize Centroids (Picking Your Starting Points)</strong> The algorithm needs a starting point. It randomly selects ‘K’ data points from your dataset and declares them as the <em>initial centroids</em> (think of these as the initial “leaders” or “centers” of your groups). These aren’t necessarily good centers yet, just placeholders.</p> <p><em>Visual Hint:</em> Imagine your data points are stars in the sky. You randomly pick 3 stars to be your initial cluster centers.</p> </li> <li> <p><strong>Step 3: Assign Data Points to the Closest Centroid (Forming the First Groups)</strong> Now, for every single data point in your dataset, K-Means calculates its distance to <em>each</em> of the ‘K’ centroids. Whichever centroid is closest, that’s the cluster the data point gets assigned to. We typically use <strong>Euclidean distance</strong> for this, which you might remember from geometry as the “straight-line distance” between two points.</p> <p><em>Visual Hint:</em> Every star in the sky now figures out which of your 3 chosen “leader” stars it’s closest to, and it joins that leader’s ‘constellation’.</p> </li> <li> <p><strong>Step 4: Update Centroids (Moving the Leaders to the Center of Their Groups)</strong> Once all data points have been assigned to a cluster, the magic happens. For each cluster, K-Means calculates the <em>mean</em> (average position) of all the data points currently assigned to it. This new average position becomes the new, updated centroid for that cluster. The old centroid disappears.</p> <p><em>Visual Hint:</em> Each “leader” star moves to the exact center of its newly formed constellation.</p> </li> <li> <p><strong>Step 5: Repeat Until Convergence (Refining the Groups)</strong> With the new centroids in place, we go back to Step 3. All data points are re-evaluated and assigned to their <em>new</em> closest centroid. Then, the centroids are updated again in Step 4. This process repeats: assign, update, assign, update…</p> <p>When does it stop?</p> <ul> <li>When the centroids no longer move significantly between iterations.</li> <li>When a maximum number of iterations has been reached.</li> <li>When the assignments of data points to clusters no longer change.</li> </ul> <p>This means the clusters have stabilized, and K-Means has found what it believes are the optimal groupings for your chosen ‘K’.</p> </li> </ol> <h3 id="the-math-behind-the-magic-the-objective-function">The Math Behind the Magic: The Objective Function</h3> <p>You might be asking, “What exactly is K-Means trying to <em>achieve</em> or <em>minimize</em> with all this moving around?” Great question! K-Means has a clear objective: it wants to minimize the <strong>Within-Cluster Sum of Squares (WCSS)</strong>, also known as <strong>inertia</strong>.</p> <p>In simpler terms, it wants to make sure that the data points within each cluster are as close as possible to their own cluster’s centroid. This results in compact, tight clusters.</p> <p>Mathematically, the objective function $J$ is defined as:</p> <table> <tbody> <tr> <td>$J = \sum_{i=1}^{K} \sum_{x \in C_i}</td> <td> </td> <td>x - \mu_i</td> <td> </td> <td>^2$</td> </tr> </tbody> </table> <p>Let’s break this down:</p> <ul> <li>$K$: The number of clusters we chose.</li> <li>$i=1$ to $K$: We sum this value for each of our $K$ clusters.</li> <li>$C_i$: Represents all the data points that belong to the $i$-th cluster.</li> <li>$x$: A specific data point within cluster $C_i$.</li> <li>$\mu_i$: The centroid (mean) of cluster $C_i$.</li> <li> <table> <tbody> <tr> <td>$</td> <td> </td> <td>x - \mu_i</td> <td> </td> <td>^2$: This is the squared Euclidean distance between data point $x$ and its cluster’s centroid $\mu_i$. Squaring the distance prevents negative values and penalizes larger distances more heavily.</td> </tr> </tbody> </table> </li> </ul> <p>So, the algorithm is constantly trying to rearrange points and centroids to make this total sum of squared distances as small as possible.</p> <h3 id="the-big-question-how-do-we-choose-k">The Big Question: How Do We Choose ‘K’?</h3> <p>Remember how I said choosing ‘K’ is tricky? Since K-Means is an unsupervised algorithm, there’s no “right” answer given by labels. We need a heuristic, and the most common one is the <strong>Elbow Method</strong>.</p> <p>Here’s how it works:</p> <ol> <li>Run the K-Means algorithm for a range of ‘K’ values (e.g., from 1 to 10).</li> <li>For each ‘K’, calculate the WCSS (the objective function $J$ we just discussed).</li> <li>Plot the WCSS values against the number of clusters ‘K’.</li> </ol> <p>What you’ll typically see is a graph where the WCSS decreases rapidly at first (as you add more clusters, you can explain the variance better), then the rate of decrease slows down significantly. This point, where the graph looks like an “elbow,” is often considered the optimal ‘K’. Why? Because adding more clusters beyond this point doesn’t give you much additional benefit in reducing the within-cluster variance; you’re just splitting existing, already compact clusters.</p> <h3 id="strengths-of-k-means">Strengths of K-Means</h3> <ul> <li> <strong>Simplicity and Interpretability:</strong> It’s easy to understand and explain how K-Means works, even without a deep math background.</li> <li> <strong>Computational Efficiency:</strong> For datasets with many features and observations, K-Means is generally quite fast, especially compared to some other clustering algorithms.</li> <li> <strong>Scalability:</strong> It can handle large datasets well, making it practical for real-world applications.</li> </ul> <h3 id="limitations-and-considerations">Limitations and Considerations</h3> <p>While powerful, K-Means isn’t perfect for every situation:</p> <ul> <li> <strong>Sensitive to Initial Centroid Placement:</strong> Because the initial centroids are random, different runs of K-Means on the same data can sometimes lead to slightly different clustering results. To mitigate this, algorithms like <strong>K-Means++</strong> exist, which intelligently select initial centroids to give better starting points.</li> <li> <strong>Requires Pre-defining ‘K’:</strong> The need to choose ‘K’ upfront can be a drawback if you have no idea how many groups are in your data. The Elbow Method helps, but it’s not always crystal clear.</li> <li> <strong>Assumes Spherical Clusters:</strong> K-Means works best when clusters are roughly spherical and of similar size and density. It struggles with irregularly shaped clusters (like crescent moons or interlocking rings) or clusters of vastly different sizes.</li> <li> <strong>Sensitive to Outliers:</strong> Because centroids are calculated as means, extreme outlier data points can disproportionately pull a centroid towards them, distorting the clusters.</li> <li> <strong>Works Only with Numerical Data:</strong> K-Means requires numerical features to calculate distances and means. Categorical data needs to be pre-processed (e.g., using one-hot encoding).</li> </ul> <h3 id="real-world-applications">Real-World Applications</h3> <p>Despite its limitations, K-Means is incredibly versatile and widely used:</p> <ul> <li> <strong>Customer Segmentation:</strong> Businesses use it to group customers based on purchasing behavior, demographics, and online activity, allowing for targeted marketing strategies.</li> <li> <strong>Document Clustering:</strong> Organizing large collections of articles, news stories, or research papers into thematic groups.</li> <li> <strong>Image Segmentation:</strong> Dividing an image into regions that share similar characteristics (e.g., color, texture) for object recognition or editing.</li> <li> <strong>Anomaly Detection:</strong> Identifying data points that don’t belong to any cluster, which could indicate fraud, network intrusion, or manufacturing defects.</li> <li> <strong>Geospatial Analysis:</strong> Grouping locations with similar characteristics (e.g., finding optimal locations for new stores based on customer density).</li> </ul> <h3 id="wrapping-up">Wrapping Up</h3> <p>K-Means clustering is a beautiful example of how simple, iterative processes can lead to powerful insights. It’s a foundational algorithm in any data scientist’s toolkit, offering a robust way to explore the inherent structure of unlabelled data.</p> <p>While it has its quirks, understanding its mechanics, its objective, and its limitations empowers you to apply it effectively and intelligently. So, the next time you see a messy dataset, remember K-Means – it might just be the key to revealing the hidden patterns within!</p> <p>Keep exploring, keep asking questions, and keep building!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>