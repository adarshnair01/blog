<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Decoding Your Model's Decisions: A Journey into ROC Curves and AUC Scores | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/decoding-your-models-decisions-a-journey-into-roc/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Decoding Your Model's Decisions: A Journey into ROC Curves and AUC Scores</h1> <p class="post-meta"> Created on July 20, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/model-evaluation"> <i class="fa-solid fa-hashtag fa-sm"></i> Model Evaluation</a>   <a href="/blog/blog/tag/roc-curve"> <i class="fa-solid fa-hashtag fa-sm"></i> ROC Curve</a>   <a href="/blog/blog/tag/auc"> <i class="fa-solid fa-hashtag fa-sm"></i> AUC</a>   <a href="/blog/blog/tag/classification"> <i class="fa-solid fa-hashtag fa-sm"></i> Classification</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hello fellow data adventurers!</p> <p>Today, I want to share a little story, a common “aha!” moment I’ve had many times in my machine learning journey, and one that I think many of you might relate to. It often starts like this:</p> <p>“My model achieved 95% accuracy! It’s fantastic!”</p> <p>…and then, the cold splash of reality when it goes into production, and suddenly, “fantastic” doesn’t quite describe the situation. It’s missing crucial positive cases, or it’s crying wolf too often. What went wrong? Why did our trusty accuracy metric betray us?</p> <p>This moment of disillusionment is exactly where we discover the limitations of simple metrics and embrace the power of more nuanced evaluation tools. Today, we’re going on a deep dive into two such indispensable tools: <strong>Receiver Operating Characteristic (ROC) curves</strong> and <strong>Area Under the Curve (AUC)</strong>. These aren’t just fancy terms; they’re diagnostic lenses that help us truly understand how well our classification models are performing.</p> <h3 id="the-foundation-binary-classification-and-probabilities">The Foundation: Binary Classification and Probabilities</h3> <p>Before we plot curves, let’s quickly re-anchor ourselves to the basics of binary classification. Imagine you’re building a model to predict if an email is spam (positive class) or not spam (negative class).</p> <p>Most classification models (like Logistic Regression, Random Forests, or neural networks) don’t just spit out “Spam” or “Not Spam” directly. Instead, they output a <em>probability</em> – a score between 0 and 1 – indicating the likelihood that an email is spam.</p> <p>For example:</p> <ul> <li>Email A: Probability of spam = 0.92</li> <li>Email B: Probability of spam = 0.15</li> <li>Email C: Probability of spam = 0.51</li> </ul> <p>To turn these probabilities into a definitive “Spam” or “Not Spam,” we need a <strong>threshold</strong>. Typically, this threshold is set at 0.5. So, if the probability is $\ge 0.5$, it’s classified as Spam; otherwise, Not Spam. But here’s the kicker: <em>this threshold is arbitrary</em>, and changing it can dramatically alter your model’s behavior. This concept of a variable threshold is central to understanding ROC.</p> <h3 id="the-confusion-matrix-where-reality-meets-prediction">The Confusion Matrix: Where Reality Meets Prediction</h3> <p>To properly evaluate our model, we first need to understand the four possible outcomes when our model makes a prediction against the actual truth. These are summarized in the <strong>Confusion Matrix</strong>:</p> <table> <thead> <tr> <th style="text-align: left"> </th> <th style="text-align: left"><strong>Actual Positive (e.g., Is Spam)</strong></th> <th style="text-align: left"><strong>Actual Negative (e.g., Not Spam)</strong></th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Predicted Positive</strong></td> <td style="text-align: left">True Positive (TP)</td> <td style="text-align: left">False Positive (FP)</td> </tr> <tr> <td style="text-align: left"><strong>Predicted Negative</strong></td> <td style="text-align: left">False Negative (FN)</td> <td style="text-align: left">True Negative (TN)</td> </tr> </tbody> </table> <p>Let’s break them down with our spam example:</p> <ul> <li> <strong>True Positive (TP)</strong>: The model correctly identified a spam email as spam. (Good!)</li> <li> <strong>True Negative (TN)</strong>: The model correctly identified a non-spam email as non-spam. (Good!)</li> <li> <strong>False Positive (FP)</strong>: The model incorrectly identified a non-spam email as spam. (Bad! A legitimate email went to spam.)</li> <li> <strong>False Negative (FN)</strong>: The model incorrectly identified a spam email as non-spam. (Bad! Spam ended up in your inbox.)</li> </ul> <p>Accuracy, in its simplest form, is just $\frac{TP + TN}{TP + TN + FP + FN}$. But what if only 1% of emails are spam? A model that labels <em>everything</em> as “Not Spam” would achieve 99% accuracy! Clearly, accuracy alone can be misleading, especially with imbalanced datasets.</p> <h3 id="the-metrics-that-matter-sensitivity-and-specificity">The Metrics That Matter: Sensitivity and Specificity</h3> <p>To get a richer picture, we need to focus on how well our model handles the positive class and the negative class separately.</p> <ol> <li> <p><strong>Sensitivity (True Positive Rate - TPR, or Recall)</strong>: This measures the proportion of actual positive cases that were correctly identified by the model. $TPR = \frac{TP}{TP + FN}$ Think of it as the model’s ability to “catch all the bad guys.” In our spam example, it’s the percentage of actual spam emails that our filter successfully blocked. A high TPR means fewer spam emails slip into your inbox.</p> </li> <li> <p><strong>Specificity (True Negative Rate - TNR)</strong>: This measures the proportion of actual negative cases that were correctly identified by the model. $TNR = \frac{TN}{TN + FP}$ This is the model’s ability to “not falsely accuse the good guys.” For spam, it’s the percentage of legitimate emails that were correctly identified as non-spam. A high TNR means fewer important emails are mistakenly sent to spam.</p> </li> </ol> <p>These two metrics often have an inverse relationship. If you want to catch <em>all</em> spam (maximize TPR), you might lower your threshold, leading to more legitimate emails being flagged as spam (decreasing TNR). Conversely, if you want to ensure <em>no</em> legitimate emails are ever flagged as spam (maximize TNR), you might raise your threshold, letting more spam slip through (decreasing TPR). It’s a classic trade-off!</p> <p>For the ROC curve, instead of Specificity, we typically use the <strong>False Positive Rate (FPR)</strong>, which is simply: $FPR = 1 - Specificity = \frac{FP}{FP + TN}$ This is the proportion of actual negative cases that were <em>incorrectly</em> identified as positive. It’s the “rate of crying wolf.”</p> <h3 id="the-roc-curve-a-visual-tale-of-trade-offs">The ROC Curve: A Visual Tale of Trade-offs</h3> <p>Now, let’s tie everything together into the <strong>Receiver Operating Characteristic (ROC) curve</strong>. This curve is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied.</p> <p><strong>How it’s built (conceptually):</strong> Imagine we have our model’s predicted probabilities for all emails in our test set.</p> <ol> <li>Start with a very high threshold (e.g., 0.99). Only emails with extremely high spam probability are classified as spam. Calculate the TPR and FPR at this threshold and plot it.</li> <li>Gradually decrease the threshold (e.g., 0.9, 0.8, 0.7… down to 0). At each threshold, calculate the new TPR and FPR.</li> <li>Plot each (FPR, TPR) pair on a 2D graph. The x-axis is FPR, and the y-axis is TPR.</li> </ol> <p>What you get is a curve that traces out the entire spectrum of trade-offs between FPR and TPR at every possible threshold.</p> <p><strong>Interpreting the ROC Curve:</strong></p> <ul> <li> <strong>The Ideal Point (0,1):</strong> The top-left corner represents a perfect classifier: 100% True Positives (caught all spam) and 0% False Positives (no legitimate emails went to spam). We rarely achieve this in reality, but it’s the target.</li> <li> <strong>The Random Classifier Line:</strong> The diagonal line from (0,0) to (1,1) represents a classifier that performs no better than random guessing. If your ROC curve hugs this line, your model is essentially flipping a coin.</li> <li> <strong>A Good Classifier:</strong> A good ROC curve will bow towards the top-left corner, staying as far away from the random line as possible. The more it bows, the better its discrimination power.</li> <li> <strong>Moving Along the Curve:</strong> Each point on the curve represents a different threshold. <ul> <li>Moving towards the top-right (higher FPR, higher TPR) means you’re lowering your threshold, becoming more lenient in classifying positives. You catch more actual positives, but also make more false positive errors.</li> <li>Moving towards the bottom-left (lower FPR, lower TPR) means you’re raising your threshold, becoming more strict. You make fewer false positive errors, but might miss more actual positives.</li> </ul> </li> </ul> <p>This curve beautifully visualizes the balance your model strikes at different operational points. Do you need to be very sensitive (high TPR) to catch all potential diseases, even if it means more false alarms (higher FPR)? Or do you need to be very specific (low FPR) to avoid inconveniencing customers, even if it means missing some positive cases (lower TPR)? The ROC curve helps you choose the right balance for your specific problem.</p> <h3 id="the-auc-score-a-single-number-to-rule-them-all">The AUC Score: A Single Number to Rule Them All</h3> <p>While the ROC curve gives us a visual story, sometimes we need a single number to compare models or quickly gauge overall performance. This is where <strong>Area Under the ROC Curve (AUC)</strong> comes in.</p> <p>The AUC is quite literally the area underneath the ROC curve. Since the curve is plotted in a square from (0,0) to (1,1), the maximum possible area is 1.</p> <p><strong>Interpreting the AUC Score:</strong></p> <ul> <li> <strong>AUC = 1.0</strong>: A perfect classifier. This means the model can perfectly distinguish between positive and negative classes.</li> <li> <strong>AUC = 0.5</strong>: A random classifier. This is equivalent to guessing. Your model’s predictions are no better than flipping a coin.</li> <li> <strong>AUC &lt; 0.5</strong>: Worse than random. This indicates that your model is systematically learning the wrong patterns. In such cases, simply flipping the predictions might give you an AUC &gt; 0.5! (e.g., if it predicts spam when it’s not, predict not-spam when it is).</li> <li> <strong>AUC between 0.5 and 1.0</strong>: The higher the AUC, the better the model is at distinguishing between positive and negative classes.</li> </ul> <p><strong>Why is AUC so powerful?</strong></p> <ol> <li> <strong>Threshold-Independence</strong>: Unlike accuracy or F1-score, AUC doesn’t depend on a specific classification threshold. It evaluates the model’s performance across <em>all possible thresholds</em>, giving you a comprehensive view of its discriminative power.</li> <li> <strong>Robust to Class Imbalance</strong>: Because it looks at the trade-off between TPR and FPR, AUC is much more robust to imbalanced datasets than accuracy. A high AUC indicates good performance even if one class is vastly underrepresented.</li> <li> <strong>Probabilistic Interpretation</strong>: AUC has a fascinating probabilistic interpretation: it represents the probability that the model ranks a randomly chosen positive instance higher than a randomly chosen negative instance. If your AUC is 0.8, there’s an 80% chance your model will assign a higher probability to a true positive than to a true negative, when picking one of each at random.</li> </ol> <h3 id="a-quick-python-pit-stop-conceptual">A Quick Python Pit Stop (Conceptual)</h3> <p>In Python, using <code class="language-plaintext highlighter-rouge">scikit-learn</code>, computing ROC and AUC is straightforward:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_curve</span><span class="p">,</span> <span class="n">auc</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Assuming you have true_labels (0s and 1s) and
# predicted_probabilities (scores between 0 and 1) from your model
# true_labels = [0, 1, 0, 1, 0, ...]
# predicted_probabilities = [0.1, 0.9, 0.3, 0.7, 0.2, ...]
</span>
<span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> <span class="nf">roc_curve</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">predicted_probabilities</span><span class="p">)</span>
<span class="n">roc_auc</span> <span class="o">=</span> <span class="nf">auc</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">darkorange</span><span class="sh">'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">'</span><span class="s">ROC curve (AUC = </span><span class="si">{</span><span class="n">roc_auc</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">navy</span><span class="sh">'</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">)</span> <span class="c1"># Random classifier line
</span><span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">False Positive Rate (FPR)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">True Positive Rate (TPR)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Receiver Operating Characteristic (ROC) Curve</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="sh">"</span><span class="s">lower right</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">The AUC score for this model is: </span><span class="si">{</span><span class="n">roc_auc</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>This snippet demonstrates how you’d get the values and plot the curve. The <code class="language-plaintext highlighter-rouge">roc_curve</code> function returns the FPR, TPR, and the thresholds used to generate them. <code class="language-plaintext highlighter-rouge">auc</code> then computes the area from these values.</p> <h3 id="important-caveats-and-when-to-look-further">Important Caveats and When to Look Further</h3> <p>While ROC and AUC are incredibly powerful, they aren’t always the <em>final</em> answer:</p> <ul> <li> <strong>Extreme Class Imbalance</strong>: In situations with extremely skewed classes (e.g., 1:1000 ratio), where the positive class is rare and critically important, the Precision-Recall (PR) curve can often be more informative than the ROC curve, especially if your primary concern is the performance of the positive class. ROC curves can sometimes be overly optimistic in such cases because a large number of true negatives can “mask” poor performance on the rare positive class.</li> <li> <strong>Cost of Errors</strong>: Remember, AUC gives you an <em>overall</em> performance metric. But in real-world scenarios, the cost of a False Positive might be drastically different from a False Negative. For example, in medical diagnosis, a False Negative (missing a disease) is usually far more critical than a False Positive (a false alarm). While ROC helps visualize the trade-off, you’ll still need to pick an optimal threshold based on domain-specific costs.</li> <li> <strong>Multi-class Classification</strong>: ROC and AUC are inherently designed for binary classification. For multi-class problems, you typically extend them using strategies like “one-vs-rest” or “one-vs-one” to create multiple binary ROC curves, or rely on other metrics like weighted F1-score.</li> </ul> <h3 id="conclusion-evaluate-with-confidence">Conclusion: Evaluate with Confidence!</h3> <p>So, the next time your machine learning model boasts a high accuracy, take a moment to peek behind the curtain. Understanding ROC curves and AUC scores will empower you to ask deeper questions, diagnose nuanced issues, and make more informed decisions about your model’s real-world applicability.</p> <p>They are not just metrics; they are lenses that reveal the intrinsic ability of your model to discriminate, allowing you to fine-tune its behavior for your specific needs, rather than blindly trusting a single number.</p> <p>Go forth, evaluate with confidence, and build models that don’t just predict, but truly understand! Happy modeling!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>