<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> From Mess to Masterpiece: My Data Cleaning Blueprint for Rock-Solid Models | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/from-mess-to-masterpiece-my-data-cleaning-blueprin/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">From Mess to Masterpiece: My Data Cleaning Blueprint for Rock-Solid Models</h1> <p class="post-meta"> Created on December 06, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/data-cleaning"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Cleaning</a>   <a href="/blog/blog/tag/data-preprocessing"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Preprocessing</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/python"> <i class="fa-solid fa-hashtag fa-sm"></i> Python</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>Welcome back to my portfolio blog. Today, I want to pull back the curtain on one of the most critical, yet often least glamorous, aspects of data science: <strong>Data Cleaning</strong>. If you’ve spent any time at all working with real-world data, you know the drill. You download a dataset, full of excitement, ready to train the next big model, only to be met with a chaotic mess of missing values, inconsistent formats, and strange outliers.</p> <p>I’ve been there countless times. I remember one early project where I spent weeks fine-tuning a complex neural network, only for its predictions to be wildly inaccurate. It was frustrating, to say the least. The breakthrough came when an experienced mentor asked me, “Show me your data, not just your code.” We went through it, cell by painstaking cell, and what we found was eye-opening: a crucial column had numbers stored as text, another had inconsistent units, and a third was riddled with missing values that I had simply ignored. My fancy algorithm was trying to learn from garbage, and naturally, it was producing garbage.</p> <p>This experience hammered home the golden rule of data science: <strong>“Garbage In, Garbage Out” (GIGO)</strong>. No matter how sophisticated your model or how powerful your computing resources, if your input data is flawed, your outputs will be too. Data cleaning isn’t just a step in the process; it’s the foundation upon which all reliable analysis and accurate predictions are built. It’s where we transform raw, noisy information into a polished, usable asset.</p> <p>In this post, I want to share my essential strategies and the mindset I’ve developed for tackling data cleaning. Think of it as my personal blueprint for turning messy data into a masterpiece ready for modeling.</p> <hr> <h3 id="1-the-first-commandment-understand-your-data-exploratory-data-analysis---eda"><strong>1. The First Commandment: Understand Your Data (Exploratory Data Analysis - EDA)</strong></h3> <p>Before you even think about cleaning, you <em>must</em> understand what you’re dealing with. This is where Exploratory Data Analysis (EDA) comes in. It’s your detective work, your chance to get intimately familiar with every nook and cranny of your dataset.</p> <p><strong>My Go-To EDA Checklist:</strong></p> <ul> <li> <strong>Initial Inspection:</strong> I always start with a quick overview. <ul> <li> <code class="language-plaintext highlighter-rouge">df.head()</code>: See the first few rows to get a feel for the data.</li> <li> <code class="language-plaintext highlighter-rouge">df.info()</code>: This is a goldmine! It tells me the column names, number of non-null values, and data types for each column. This immediately flags potential issues like numbers being stored as objects (strings) or many missing values.</li> <li> <code class="language-plaintext highlighter-rouge">df.describe()</code>: For numerical columns, this gives me statistical summaries like mean, median, standard deviation, min, max, and quartiles. It helps me spot extreme values or inconsistent ranges.</li> <li> <code class="language-plaintext highlighter-rouge">df.isnull().sum()</code>: A quick count of missing values per column. Critical for planning my next steps.</li> </ul> </li> <li> <strong>Visualizations:</strong> Pictures tell a thousand stories, especially in data. <ul> <li> <strong>Histograms/KDE plots:</strong> For numerical columns, these show the distribution of values. Are they normally distributed? Skewed? Do they have multiple peaks?</li> <li> <strong>Box Plots:</strong> Excellent for identifying potential outliers and understanding the spread of data.</li> <li> <strong>Scatter Plots:</strong> To visualize relationships between two numerical variables. Are there any strange clusters or clear correlations?</li> <li> <strong>Bar Charts:</strong> For categorical columns, to see the frequency of each category.</li> </ul> </li> </ul> <p><strong>My Personal Take:</strong> EDA isn’t just a step; it’s an ongoing conversation with your data. The more time I spend here, the fewer surprises I encounter down the line. It’s like checking the blueprint before building a house – you catch structural flaws before they become major problems.</p> <hr> <h3 id="2-the-missing-piece-of-the-puzzle-handling-missing-values"><strong>2. The Missing Piece of the Puzzle: Handling Missing Values</strong></h3> <p>Missing data is arguably the most common and frustrating problem we face. It can happen for many reasons: sensors failed, users skipped a field, data wasn’t recorded, or simply, it doesn’t apply. Leaving them as they are can lead to errors, biased results, or models that simply crash.</p> <p><strong>My Strategies for Missing Values:</strong></p> <ul> <li> <strong>a) Deletion:</strong> <ul> <li> <strong>Row Deletion (<code class="language-plaintext highlighter-rouge">df.dropna()</code>):</strong> If a row has missing values, I might delete the entire row. <ul> <li> <strong>When I use it:</strong> When the percentage of missing data in a particular row or column is very small (e.g., &lt;5% of the dataset) and the missingness is random, or if the row contains too many missing values to be useful.</li> <li> <strong>Caution:</strong> This can lead to a significant loss of valuable information, especially in smaller datasets.</li> </ul> </li> <li> <strong>Column Deletion:</strong> If a column has an overwhelmingly large number of missing values (e.g., &gt;70-80%), or if I deem it irrelevant after EDA, I might drop the entire column.</li> </ul> </li> <li> <strong>b) Imputation (Filling in the Blanks):</strong> This is often my preferred method as it preserves more data. <ul> <li> <strong>Mean/Median/Mode Imputation:</strong> <ul> <li> <strong>Concept:</strong> Replace missing numerical values with the mean or median of that column. For categorical values, I use the mode (most frequent value).</li> <li> <strong>When I use it:</strong> For numerical features, mean works well for normally distributed data, while median is more robust to outliers. Mode is perfect for categorical features.</li> <li> <strong>Example (Pythonic):</strong> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="c1"># Numerical column
</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">numerical_column</span><span class="sh">'</span><span class="p">].</span><span class="nf">fillna</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">numerical_column</span><span class="sh">'</span><span class="p">].</span><span class="nf">mean</span><span class="p">(),</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="c1"># Categorical column
</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">categorical_column</span><span class="sh">'</span><span class="p">].</span><span class="nf">fillna</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">categorical_column</span><span class="sh">'</span><span class="p">].</span><span class="nf">mode</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div> </div> </li> <li> <strong>Caution:</strong> This can reduce the variance of the data and may distort relationships between variables if not used carefully.</li> </ul> </li> <li> <strong>Forward/Backward Fill (<code class="language-plaintext highlighter-rouge">ffill</code>/<code class="language-plaintext highlighter-rouge">bfill</code>):</strong> <ul> <li> <strong>Concept:</strong> Replace a missing value with the previous (<code class="language-plaintext highlighter-rouge">ffill</code>) or next (<code class="language-plaintext highlighter-rouge">bfill</code>) valid observation.</li> <li> <strong>When I use it:</strong> This is incredibly useful for time-series data where sequential order matters.</li> <li> <strong>Example:</strong> <code class="language-plaintext highlighter-rouge">df['time_series_data'].fillna(method='ffill', inplace=True)</code> </li> </ul> </li> <li> <strong>Adding a “Missing” Category:</strong> <ul> <li> <strong>Concept:</strong> For categorical features, instead of imputing, sometimes it’s better to create a new category called “Missing” for all NaN values.</li> <li> <strong>When I use it:</strong> When the fact that a value is missing might carry valuable information itself (e.g., a user didn’t specify their age, which might be correlated with being a minor).</li> </ul> </li> <li> <strong>Advanced Imputation (Brief Mention):</strong> More sophisticated methods exist, like using K-Nearest Neighbors (KNN) to find similar data points and use their values, or regression imputation where you build a model to predict the missing values. These are powerful but also more complex.</li> </ul> </li> </ul> <p><strong>My Personal Take:</strong> There’s no one-size-fits-all solution for missing data. The choice depends heavily on the nature of the data, the percentage of missing values, and the domain context. Always evaluate the impact of your imputation strategy on the data’s distribution.</p> <hr> <h3 id="3-the-odd-ones-out-taming-outliers"><strong>3. The Odd Ones Out: Taming Outliers</strong></h3> <p>Outliers are data points that significantly deviate from other observations. They can be genuine extreme values, or they can be errors from data collection. Regardless of their origin, they can severely skew statistical analyses and model training. Imagine calculating the average income in a room, and Bill Gates walks in – your average would skyrocket!</p> <p><strong>My Approach to Outlier Detection:</strong></p> <ul> <li> <strong>a) Visual Methods (My First Stop):</strong> <ul> <li> <strong>Box Plots:</strong> As mentioned, these are fantastic for showing the spread of data and clearly marking potential outliers (points beyond the “whiskers”).</li> <li> <strong>Histograms/Scatter Plots:</strong> Help visualize unusual data points or clusters.</li> </ul> </li> <li> <strong>b) Statistical Methods:</strong> <ul> <li> <strong>Interquartile Range (IQR) Method:</strong> <ul> <li>This is a robust method. First, I find the first quartile ($Q_1$) and the third quartile ($Q_3$).</li> <li>Then, calculate the Interquartile Range: $IQR = Q_3 - Q_1$.</li> <li>Any data point below $Q_1 - 1.5 \times IQR$ or above $Q_3 + 1.5 \times IQR$ is considered an outlier.</li> <li> <strong>Why it’s good:</strong> It’s less sensitive to extreme values than methods using the mean and standard deviation.</li> </ul> </li> <li> <strong>Z-score:</strong> <ul> <li>The Z-score measures how many standard deviations a data point is from the mean.</li> <li>$Z = \frac{x - \mu}{\sigma}$ where $x$ is the data point, $\mu$ is the mean, and $\sigma$ is the standard deviation.</li> <li>Typically, data points with an absolute Z-score greater than 2 or 3 are considered outliers.</li> <li> <strong>Caution:</strong> This method assumes the data is normally distributed and is sensitive to outliers itself when calculating mean and standard deviation.</li> </ul> </li> </ul> </li> </ul> <p><strong>My Strategies for Handling Outliers:</strong></p> <ul> <li> <strong>Removal:</strong> If an outlier is clearly a data entry error (e.g., age recorded as 200), I remove it. If it’s a genuine but extremely rare event that might distort the model, I might remove it, especially if it’s not the target of my analysis.</li> <li> <strong>Transformation:</strong> Applying mathematical transformations like logarithmic or square root transformations can reduce the skewness of the data and bring outliers closer to the distribution. For example, <code class="language-plaintext highlighter-rouge">np.log(df['column'])</code>.</li> <li> <strong>Capping (Winsorization):</strong> Instead of removing, I might cap the outliers. This means replacing values below a certain lower bound (e.g., 5th percentile) with that lower bound value, and values above an upper bound (e.g., 95th percentile) with that upper bound value. This keeps the data point but reduces its extreme influence.</li> <li> <strong>Keep Them:</strong> Sometimes, outliers are the most interesting data points! In fraud detection or anomaly detection, the “outliers” are exactly what you’re trying to find. Always consider the context.</li> </ul> <p><strong>My Personal Take:</strong> Deciding how to handle outliers requires careful thought. It’s not just about statistics; it’s about understanding the domain and the potential impact on your model. Always try to investigate the <em>reason</em> for an outlier before deciding its fate.</p> <hr> <h3 id="4-the-silent-saboteur-managing-duplicate-data"><strong>4. The Silent Saboteur: Managing Duplicate Data</strong></h3> <p>Duplicate data means identical rows or entries in your dataset. This can happen from data merging, multiple submissions, or errors during data collection. Duplicates can lead to biased models that overemphasize certain observations, or inaccurate counts and aggregates.</p> <p><strong>My Approach to Duplicates:</strong></p> <ul> <li> <strong>Detection:</strong> I use <code class="language-plaintext highlighter-rouge">df.duplicated().sum()</code> to quickly count how many duplicate rows exist.</li> <li> <strong>Handling:</strong> <code class="language-plaintext highlighter-rouge">df.drop_duplicates(inplace=True)</code> is my go-to. <ul> <li> <strong><code class="language-plaintext highlighter-rouge">subset</code> parameter:</strong> Often, I don’t want to drop a row if <em>every</em> column is identical. Instead, I might specify a subset of columns (e.g., <code class="language-plaintext highlighter-rouge">['user_id', 'transaction_date']</code>) to identify duplicate transactions for the same user on the same date.</li> <li> <strong><code class="language-plaintext highlighter-rouge">keep</code> parameter:</strong> <code class="language-plaintext highlighter-rouge">keep='first'</code> (default) keeps the first occurrence, <code class="language-plaintext highlighter-rouge">keep='last'</code> keeps the last, and <code class="language-plaintext highlighter-rouge">keep=False</code> drops all duplicates.</li> </ul> </li> </ul> <p><strong>My Personal Take:</strong> Duplicates are usually straightforward to handle, but always think about <em>which</em> columns define a unique record before blindly dropping rows.</p> <hr> <h3 id="5-the-detail-oriented-standardizing-data-formats-and-types"><strong>5. The Detail-Oriented: Standardizing Data Formats and Types</strong></h3> <p>Inconsistent data formats and incorrect data types are subtle but pervasive issues. Numbers stored as strings, mixed date formats, inconsistent capitalization, or different units can cause endless headaches.</p> <p><strong>My Checklist for Formatting and Type Issues:</strong></p> <ul> <li> <strong>Correct Data Types:</strong> <ul> <li> <strong>Numbers as Strings:</strong> If <code class="language-plaintext highlighter-rouge">df.info()</code> shows a numerical column as <code class="language-plaintext highlighter-rouge">object</code>, I’ll convert it: <code class="language-plaintext highlighter-rouge">pd.to_numeric(df['column'], errors='coerce')</code>. The <code class="language-plaintext highlighter-rouge">errors='coerce'</code> is vital; it turns values that can’t be converted into NaNs, which I can then handle as missing values.</li> <li> <strong>Dates:</strong> Dates are notoriously messy. I always use <code class="language-plaintext highlighter-rouge">pd.to_datetime(df['date_column'], errors='coerce')</code> to parse them into a consistent datetime object. If there are multiple formats, I might need to specify <code class="language-plaintext highlighter-rouge">format</code> or use more advanced parsing.</li> <li> <strong>Categorical Data:</strong> For columns with a limited number of unique string values, converting them to a <code class="language-plaintext highlighter-rouge">category</code> dtype can save memory and speed up operations: <code class="language-plaintext highlighter-rouge">df['categorical_column'].astype('category')</code>.</li> </ul> </li> <li> <strong>Standardizing Text Data:</strong> <ul> <li> <strong>Case Sensitivity:</strong> Convert all text to lowercase or uppercase to treat ‘Apple’ and ‘apple’ as the same: <code class="language-plaintext highlighter-rouge">df['text_column'].str.lower()</code>.</li> <li> <strong>Whitespace:</strong> Remove leading/trailing whitespace: <code class="language-plaintext highlighter-rouge">df['text_column'].str.strip()</code>.</li> <li> <strong>Special Characters:</strong> Remove or replace unwanted characters using regular expressions. (e.g., <code class="language-plaintext highlighter-rouge">df['text_column'].str.replace('[^a-zA-Z0-9]', '')</code>).</li> <li> <strong>Inconsistent Spelling:</strong> For categorical text, I often use <code class="language-plaintext highlighter-rouge">value_counts()</code> to identify variations like ‘USA’, ‘U.S.A.’, ‘United States’. Then I map them to a single standard value.</li> </ul> </li> <li> <strong>Units and Scales:</strong> If a numerical column has mixed units (e.g., some weights in kilograms, some in pounds), I identify them and convert them all to a single, consistent unit. This usually requires domain knowledge or metadata.</li> </ul> <p><strong>My Personal Take:</strong> This stage is all about meticulous attention to detail. These small inconsistencies can lead to major errors in your analysis and models if ignored.</p> <hr> <h3 id="putting-it-all-together-the-iterative-workflow"><strong>Putting It All Together: The Iterative Workflow</strong></h3> <p>Data cleaning is rarely a linear process. It’s iterative, cyclical, and often involves going back and forth between steps.</p> <ol> <li> <strong>EDA First:</strong> Always start by exploring and understanding.</li> <li> <strong>Prioritize:</strong> Tackle the biggest, most impactful issues first (e.g., massive missing data, glaring type errors).</li> <li> <strong>Clean Incrementally:</strong> Apply one cleaning strategy at a time and re-evaluate its impact using EDA. Did fixing missing values introduce new outliers? Did converting types reveal more inconsistencies?</li> <li> <strong>Document Everything:</strong> Keep a clear record of every cleaning step you take. This is crucial for reproducibility and for understanding the evolution of your data. This is where a jupyter notebook comes in handy, logging your decisions.</li> <li> <strong>Validate:</strong> After significant cleaning, perform sanity checks. Do the distributions look reasonable? Are the summary statistics what you expect?</li> </ol> <hr> <h3 id="conclusion-the-unsung-hero"><strong>Conclusion: The Unsung Hero</strong></h3> <p>Data cleaning might not have the same flash as building a generative AI model or deploying a real-time prediction system, but I firmly believe it’s the most important skill in a data scientist’s toolkit. It’s where the real magic happens—transforming raw, imperfect data into a reliable foundation for insightful discoveries and robust machine learning models.</p> <p>The more you practice, the better you’ll become at anticipating issues, identifying patterns of messiness, and choosing the right strategy. It’s a blend of technical skill, domain knowledge, and a healthy dose of detective work.</p> <p>So, the next time you embark on a data science project, don’t rush past the cleaning phase. Embrace the mess, apply these strategies, and watch your models thank you for the squeaky-clean data.</p> <p>Happy cleaning, and may your data always be sparkling!</p> <hr> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>