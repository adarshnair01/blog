<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Your Next Obsession: Unpacking the Magic of Recommender Systems | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/2024/your-next-obsession-unpacking-the-magic-of-recomme/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Your Next Obsession: Unpacking the Magic of Recommender Systems</h1> <p class="post-meta"> Created on November 12, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/recommender-systems"> <i class="fa-solid fa-hashtag fa-sm"></i> Recommender Systems</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/artificial-intelligence"> <i class="fa-solid fa-hashtag fa-sm"></i> Artificial Intelligence</a>   <a href="/blog/blog/tag/collaborative-filtering"> <i class="fa-solid fa-hashtag fa-sm"></i> Collaborative Filtering</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>As a data science enthusiast, there are few things as universally impactful and subtly brilliant as <strong>Recommender Systems</strong>. Think about it: our daily digital lives are <em>saturated</em> with them. From Spotify curating your weekly playlist to Amazon suggesting your next impulse buy, and Netflix nudging you towards that binge-worthy series – these systems are silently shaping our choices, sometimes even before we know what we want.</p> <p>I remember my first deep dive into this topic, staring at a blank Excel sheet and wondering, “How on Earth do you predict someone’s taste?” It felt like trying to read minds, but through the lens of data. It’s an incredibly cool blend of statistics, algorithms, and human psychology.</p> <p>So, let’s pull back the curtain and explore the magic behind these digital matchmakers. Whether you’re a high school student pondering a future in AI or a fellow data scientist looking for a refresher, I promise we’ll uncover something exciting.</p> <h3 id="the-problem-they-solve-information-overload">The Problem They Solve: Information Overload</h3> <p>Imagine a world without recommender systems. You log onto Netflix and are faced with <em>thousands</em> of movies and TV shows, utterly unorganized. Or you visit an e-commerce site with millions of products. How would you ever find anything?</p> <p>This, my friends, is the <strong>paradox of choice</strong> and the core problem recommender systems tackle. We’re flooded with information and options. Recommender systems act as intelligent filters, cutting through the noise to present us with relevant, personalized suggestions. They don’t just help us find things; they help <em>businesses</em> sell things and keep us engaged. It’s a win-win!</p> <h3 id="the-big-players-types-of-recommender-systems">The Big Players: Types of Recommender Systems</h3> <p>Broadly speaking, recommender systems fall into a few main categories. Let’s break them down.</p> <h4 id="1-content-based-filtering-if-you-liked-this-youll-like-that-because-theyre-similar">1. Content-Based Filtering: “If you liked this, you’ll like that because they’re similar!”</h4> <p>This is perhaps the most intuitive approach. Imagine you just finished reading a sci-fi novel about time travel and artificial intelligence. A content-based system would look at the <em>features</em> of that book (genre: sci-fi, themes: time travel, AI) and recommend other books that share those same features.</p> <p><strong>How it works:</strong></p> <ol> <li> <strong>Item Representation:</strong> Each item (movie, book, song) is described by its attributes or “features” (e.g., genre, actors, director, keywords). This often involves techniques like Natural Language Processing (NLP) to extract features from text descriptions.</li> <li> <strong>User Profile:</strong> A user’s profile is built based on the items they have interacted with and liked. This profile is essentially a summary of the features of items the user prefers.</li> <li> <strong>Matching:</strong> The system then compares the user’s profile with the features of unseen items. Items with a high degree of similarity to the user’s preferences are recommended.</li> </ol> <p><strong>Example:</strong> If you’ve loved action-packed superhero movies, a content-based system would analyze the ‘action’ and ‘superhero’ tags associated with your past watches and suggest other movies with similar tags.</p> <p><strong>Pros:</strong></p> <ul> <li> <strong>No “cold start” for users:</strong> It can recommend items to a new user based solely on their first few interactions, without needing data from other users.</li> <li> <strong>Handles niche interests:</strong> If you like obscure indie films, it can keep recommending similar obscure indie films without needing anyone else to have liked them.</li> <li> <strong>Explainable:</strong> It’s easy to tell the user <em>why</em> an item was recommended (“because you liked X, and this item shares similar characteristics like Y and Z”).</li> </ul> <p><strong>Cons:</strong></p> <ul> <li> <strong>Limited novelty/serendipity:</strong> It tends to recommend items very similar to what you’ve already liked, potentially trapping you in a “filter bubble.” You might never discover something completely new or outside your established tastes.</li> <li> <strong>Feature engineering:</strong> Defining and extracting meaningful features for <em>all</em> items can be challenging and labor-intensive.</li> <li> <strong>Over-specialization:</strong> If you only watch superhero movies, it will only recommend superhero movies, even if you might enjoy other genres.</li> </ul> <h4 id="2-collaborative-filtering-users-like-you-also-liked-this">2. Collaborative Filtering: “Users like you also liked this!”</h4> <p>This is where things get really interesting and a bit more human-like. Instead of looking at item features, collaborative filtering (CF) focuses on <strong>user behavior</strong> and the interactions between users and items. It’s like asking your friends for recommendations – if your friends have similar tastes to you, their recommendations are probably pretty good.</p> <p>There are two main flavors of collaborative filtering:</p> <h5 id="a-user-based-collaborative-filtering-user-user-cf">a) User-Based Collaborative Filtering (User-User CF)</h5> <p><strong>How it works:</strong></p> <ol> <li> <strong>Find similar users:</strong> The system identifies users whose past preferences and behaviors are similar to yours.</li> <li> <strong>Recommend items:</strong> It then recommends items that these “similar users” have liked but you haven’t yet encountered.</li> </ol> <p><strong>Example:</strong> If User A and User B both gave high ratings to “Stranger Things” and “The Crown,” and User A also loved “Squid Game” (which User B hasn’t seen), the system might recommend “Squid Game” to User B.</p> <p><strong>Pros:</strong></p> <ul> <li> <strong>Serendipity:</strong> Can recommend items completely different from what you’ve seen before, based on others’ tastes, leading to exciting discoveries.</li> <li> <strong>No item analysis needed:</strong> Doesn’t require deep understanding or feature extraction of the items themselves.</li> </ul> <p><strong>Cons:</strong></p> <ul> <li> <strong>Scalability:</strong> Finding similar users among millions can be computationally very expensive, especially as the number of users grows.</li> <li> <strong>Sparsity:</strong> Most users interact with only a tiny fraction of available items, making it hard to find enough overlapping ratings to identify truly similar users.</li> <li> <strong>Cold start for new users:</strong> Cannot make recommendations for a brand-new user until they have enough interactions to compare with others.</li> </ul> <h5 id="b-item-based-collaborative-filtering-item-item-cf">b) Item-Based Collaborative Filtering (Item-Item CF)</h5> <p>This approach, popularized by Amazon and Netflix’s original system, flips the script. Instead of finding similar <em>users</em>, it finds similar <em>items</em>.</p> <p><strong>How it works:</strong></p> <ol> <li> <strong>Find similar items:</strong> For an item you’ve interacted with (e.g., liked, purchased), the system identifies other items that are frequently liked or purchased <em>together</em> by many users.</li> <li> <strong>Recommend items:</strong> If you liked Item X, the system recommends Item Y because many other users who liked Item X also liked Item Y.</li> </ol> <p><strong>Example:</strong> If many users who bought “Harry Potter and the Sorcerer’s Stone” also bought “Harry Potter and the Chamber of Secrets,” then if you buy the first book, the system recommends the second.</p> <p><strong>Pros:</strong></p> <ul> <li> <strong>More stable similarity:</strong> Item similarities are generally more stable over time than user similarities (user tastes change, but an item’s relationship to another item is more fixed).</li> <li> <strong>Better scalability:</strong> Pre-calculating item similarities is often more manageable than real-time user similarity calculations, especially if the number of items is less than the number of users (which is often the case).</li> <li> <strong>Handles cold start for new users better than user-based CF:</strong> A new user just needs to interact with one item, and similar items can be recommended.</li> </ul> <p><strong>Cons:</strong></p> <ul> <li> <strong>Cold start for new items:</strong> It’s hard to recommend a brand-new item until enough users have interacted with it to establish its similarity to other items.</li> </ul> <h5 id="c-matrix-factorization-unveiling-hidden-tastes">c) Matrix Factorization: Unveiling Hidden Tastes</h5> <p>Now, let’s step it up a notch. Imagine a giant table where rows are users, columns are items, and the cells contain ratings (or interaction types). This is our <strong>User-Item Interaction Matrix</strong>, often filled with many empty cells (because no user rates every item).</p> <p>Matrix factorization techniques like <strong>Singular Value Decomposition (SVD)</strong> or <strong>Alternating Least Squares (ALS)</strong> try to “break down” this large, sparse matrix into smaller, dense matrices. These smaller matrices represent <strong>latent factors</strong> or “hidden characteristics” for both users and items.</p> <p><strong>Concept:</strong> Instead of direct similarity, imagine every user has a certain “degree of liking” for various abstract concepts (e.g., “love for action,” “preference for deep plot,” “tolerance for low-budget films”). Similarly, every movie has a “degree of possessing” these same abstract concepts. We don’t know what these concepts <em>are</em>, but the algorithms try to discover them.</p> <p><strong>Math (Simplified):</strong> Let $R$ be our User-Item interaction matrix. We want to approximate $R$ by the product of two lower-dimensional matrices: $P$ (User-Latent Factor matrix) and $Q$ (Item-Latent Factor matrix).</p> \[R \approx P Q^T\] <p>Where:</p> <ul> <li>$R_{uv}$ is the rating of user $u$ for item $v$.</li> <li>$P$ is an $M \times K$ matrix, where $M$ is the number of users and $K$ is the number of latent factors. Each row $p_u$ represents user $u$’s “latent profile.”</li> <li>$Q$ is an $N \times K$ matrix, where $N$ is the number of items. Each row $q_v$ represents item $v$’s “latent profile.”</li> <li>$Q^T$ is the transpose of $Q$.</li> <li>The predicted rating for user $u$ on item $v$ would be the dot product of their respective latent factor vectors: \(\hat{R}_{uv} = p_u \cdot q_v^T = \sum_{k=1}^{K} p_{uk} q_{vk}\)</li> </ul> <p>The goal is to find $P$ and $Q$ such that the error between the actual ratings $R_{uv}$ and the predicted ratings $\hat{R}_{uv}$ is minimized, usually by solving an optimization problem:</p> \[\min_{P,Q} \sum_{(u,v) \in R_{observed}} (R_{uv} - p_u q_v^T)^2 + \lambda (\|P\|_F^2 + \|Q\|_F^2)\] <p>The $\lambda (|P|_F^2 + |Q|_F^2)$ term is a regularization factor that helps prevent overfitting by penalizing large latent factor values.</p> <p><strong>Pros:</strong></p> <ul> <li> <strong>Handles sparsity well:</strong> By learning latent factors, it can make good predictions even with very few observed ratings.</li> <li> <strong>Discover hidden patterns:</strong> Uncovers underlying dimensions of taste or item characteristics that might not be explicitly tagged.</li> <li> <strong>Excellent performance:</strong> Often yields highly accurate recommendations.</li> </ul> <p><strong>Cons:</strong></p> <ul> <li> <strong>Interpretability:</strong> The latent factors are abstract and don’t have clear human-understandable labels.</li> <li> <strong>Cold start for new users/items:</strong> Still struggles with brand-new users or items that have no interaction history.</li> </ul> <h4 id="3-hybrid-recommender-systems-the-best-of-both-worlds">3. Hybrid Recommender Systems: The Best of Both Worlds</h4> <p>In the real world, it’s rare to see a recommender system relying <em>solely</em> on one approach. Most robust systems are <strong>hybrid</strong>, combining elements of content-based and collaborative filtering to leverage their strengths and mitigate their weaknesses.</p> <p><strong>Why Hybrid?</strong></p> <ul> <li> <strong>Address cold start:</strong> Content-based methods can help new items or users.</li> <li> <strong>Improve accuracy:</strong> Combining different signals often leads to better predictions.</li> <li> <strong>Increase diversity/serendipity:</strong> Can break out of filter bubbles.</li> </ul> <p><strong>Example:</strong> Netflix famously evolved from a purely collaborative (item-item) system to a sophisticated hybrid model that incorporates content features, user behavior, contextual information, and even deep learning. When you see a new movie on Netflix, it might first be recommended to you based on its content features (e.g., similar genre to what you watch), and then later, as more people watch it, its recommendations become more refined by collaborative signals.</p> <h3 id="challenges-and-the-road-ahead">Challenges and the Road Ahead</h3> <p>Building a great recommender system isn’t without its hurdles:</p> <ul> <li> <strong>Cold Start Problem:</strong> Still a persistent issue for new users or new items. How do you recommend something when there’s no data? (Solution often involves hybrid approaches, popularity-based recommendations, or asking new users for initial preferences).</li> <li> <strong>Sparsity:</strong> Most user-item matrices are overwhelmingly empty. Algorithms need to be robust to this.</li> <li> <strong>Scalability:</strong> Processing and updating recommendations for millions of users and items in real-time is a massive engineering challenge.</li> <li> <strong>Serendipity vs. Accuracy:</strong> How do you balance recommending highly accurate items with occasionally introducing something new and unexpected that broadens a user’s taste?</li> <li> <strong>Bias:</strong> Recommender systems learn from historical data, which can reflect existing biases (e.g., gender, racial, popularity bias). Mitigating this is a crucial ethical consideration.</li> <li> <strong>Explainability:</strong> Users often want to know <em>why</em> something was recommended. This is easier for content-based systems than for complex models like deep learning or matrix factorization.</li> </ul> <h3 id="beyond-the-basics-the-future-is-bright">Beyond the Basics: The Future is Bright</h3> <p>The field of recommender systems is constantly evolving. We’re seeing exciting advancements with:</p> <ul> <li> <strong>Deep Learning:</strong> Neural networks are being used to learn complex user and item representations, often outperforming traditional matrix factorization methods.</li> <li> <strong>Reinforcement Learning:</strong> Treating recommendations as a sequence of actions, where the system learns to optimize for long-term user engagement rather than just immediate clicks.</li> <li> <strong>Context-Aware Recommendations:</strong> Incorporating real-time context like time of day, location, or even emotional state to make more relevant suggestions.</li> <li> <strong>Fairness and Ethics:</strong> Ensuring recommendations are not just accurate but also fair, diverse, and free from harmful biases.</li> </ul> <h3 id="wrapping-up">Wrapping Up</h3> <p>Recommender systems are more than just fancy algorithms; they are a cornerstone of our personalized digital world, influencing everything from what we watch to what we buy. They represent a fascinating intersection of human behavior, data science, and engineering.</p> <p>I hope this journey into the world of recommendations has sparked your curiosity! Whether you’re just starting your data science adventure or are a seasoned pro, understanding these systems is incredibly valuable. So, the next time Netflix suggests that perfect movie, take a moment to appreciate the intricate dance of data and algorithms happening behind the scenes. It’s truly amazing what we can build when we let data tell us a story about ourselves.</p> <p>Happy recommending!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>