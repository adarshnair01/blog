<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Nodes, Edges, and Neighborhood Gossip: Unpacking the Magic of Graph Neural Networks | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/2024/nodes-edges-and-neighborhood-gossip-unpacking-the/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Nodes, Edges, and Neighborhood Gossip: Unpacking the Magic of Graph Neural Networks</h1> <p class="post-meta"> Created on June 08, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/graph-neural-networks"> <i class="fa-solid fa-hashtag fa-sm"></i> Graph Neural Networks</a>   <a href="/blog/blog/tag/gnns"> <i class="fa-solid fa-hashtag fa-sm"></i> GNNs</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey there, fellow explorers of the data universe!</p> <p>Have you ever looked at a social network and wondered how it suggests friends you might know? Or seen a molecule and thought about how its atoms interact? What about predicting traffic jams, or recommending the perfect movie? All these seemingly disparate problems have one thing in common: they’re not just about individual data points; they’re about the <em>relationships</em> between them.</p> <p>For the longest time, our traditional machine learning tools, as powerful as they are, struggled with this kind of interconnected data. They loved their neat tables, their fixed-size vectors. But when the data started looking less like a spreadsheet and more like a sprawling spiderweb, they’d scratch their digital heads.</p> <p>That’s where Graph Neural Networks (GNNs) stride onto the scene, like the cool new kid in school who suddenly makes everything make sense. They’re not just a fancy algorithm; they’re a whole new paradigm for understanding structure and context. And trust me, once you grasp their core idea, you’ll start seeing graphs everywhere!</p> <h3 id="whats-a-graph-anyway-its-simpler-than-you-think">What’s a Graph, Anyway? (It’s Simpler Than You Think!)</h3> <p>Before we dive into the “neural network” part, let’s nail down what we mean by a “graph.” In mathematics and computer science, a graph $G = (V, E)$ is just a collection of:</p> <ol> <li> <strong>Nodes (or Vertices), $V$</strong>: These are your individual data points. Think of them as people in a social network, atoms in a molecule, or cities on a map.</li> <li> <strong>Edges, $E$</strong>: These represent the relationships or connections between nodes. An edge might signify a friendship, a chemical bond, or a road between cities.</li> </ol> <p>Edges can be <em>directed</em> (like following someone on Twitter, where the connection only goes one way) or <em>undirected</em> (like a Facebook friendship, where if A is friends with B, B is also friends with A). They can also have <em>weights</em> (e.g., how strong a friendship is, or the distance between cities).</p> <p>To give our GNNs something to work with, each node can also have <strong>node features</strong>, $X_v \in \mathbb{R}^d$. These are like attributes or characteristics of the node itself. For a person, it might be their age, interests, or profession. For an atom, its atomic number or electronegativity.</p> <p>A common way to represent the connections in a graph is using an <strong>Adjacency Matrix</strong>, $A$. It’s a square matrix where $A_{ij} = 1$ if there’s an edge between node $i$ and node $j$, and $A_{ij} = 0$ otherwise (for an unweighted graph). If the graph is weighted, $A_{ij}$ would be the weight of the edge.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Example Adjacency Matrix:
Let's say we have 3 nodes: A, B, C
If A is connected to B, and B is connected to C:

    A B C
A [[0,1,0],
B  [1,0,1],
C  [0,1,0]]
</code></pre></div></div> <h3 id="the-puzzling-problem-why-traditional-ml-stumbles-on-graphs">The Puzzling Problem: Why Traditional ML Stumbles on Graphs</h3> <p>So, why can’t we just take our node features and the adjacency matrix and feed them into a standard neural network or a Random Forest? Turns out, graphs present a few unique challenges:</p> <ol> <li> <strong>Arbitrary Size &amp; Complex Structure</strong>: Graphs can have any number of nodes and edges. A fixed-size input layer, like in a traditional Multi-Layer Perceptron (MLP), can’t handle this variability. How do you feed a network with 10 nodes into the same model that processes a graph with 10,000 nodes?</li> <li> <strong>Permutation Invariance</strong>: Imagine you label your nodes A, B, C. If you swap the labels to C, A, B, it’s still the <em>exact same graph</em> topologically, just described differently. A traditional neural network would see these as completely different inputs, leading to inconsistent outputs. GNNs need to be “permutation invariant” or “equivariant” – meaning their output should be consistent regardless of how we order the nodes.</li> <li> <strong>Capturing Relationships</strong>: Most importantly, traditional models focus on individual features. They don’t inherently understand the <em>structure</em> of connections or how a node’s neighbors influence it. You could add features like “number of neighbors,” but that’s a manual and limited approach.</li> </ol> <p>We need a model that inherently understands that a node isn’t an island. It’s defined by its own features <em>and</em> by the company it keeps – its neighbors.</p> <h3 id="the-aha-moment-message-passing--the-heart-of-gnns">The “Aha!” Moment: Message Passing – The Heart of GNNs</h3> <p>This is where the magic truly begins. The core idea behind almost all GNNs is <strong>message passing</strong>. It’s an elegant, powerful concept that mimics how information spreads in real-world networks.</p> <p>Imagine you’re at a party. You don’t just form opinions based on what you <em>already know</em> about someone. You also gather information from their friends, their friends’ friends, and so on. You aggregate little “messages” from your social circle to form a more complete picture.</p> <p>A GNN does something similar. For each node in the graph, it iteratively performs two steps:</p> <ol> <li> <strong>Aggregate (or Gather) Messages</strong>: Each node collects information (messages) from its direct neighbors. This information is typically a transformation of the neighbor’s current “state” or “embedding.”</li> <li> <strong>Update (or Combine) Node State</strong>: After aggregating messages from all its neighbors, the node combines this aggregated information with its own current state to compute a new, updated state (or embedding).</li> </ol> <p>This process is repeated for several “layers.” Each layer allows information to flow further out into the node’s neighborhood. If you have $L$ layers, a node’s final embedding will incorporate information from its $L$-hop neighborhood.</p> <p>Let’s look at a simplified mathematical representation for a single layer:</p> <p>For a node $v$, and its neighbors $N(v)$:</p> <p>$h_v^{(l+1)} = \text{UPDATE}^{(l)}(h_v^{(l)}, \text{AGGREGATE}^{(l)}({h_u^{(l)} \mid u \in N(v)}))$</p> <p>Here:</p> <ul> <li>$h_v^{(l)}$ is the <em>embedding</em> (a vector representation) of node $v$ at layer $l$. Initially, $h_v^{(0)}$ is just the node’s input features $X_v$.</li> <li>$\text{AGGREGATE}^{(l)}$ is a function (like sum, mean, or max) that combines the embeddings of the neighbors. This function needs to be permutation invariant (e.g., if you sum $h_u$ and $h_w$, it doesn’t matter if you sum $h_w$ and $h_u$).</li> <li>$\text{UPDATE}^{(l)}$ is a function that combines the node’s previous embedding with the aggregated neighbor information. This often involves neural network layers (like MLPs).</li> </ul> <p>The beauty of this is that the $\text{AGGREGATE}$ and $\text{UPDATE}$ functions use <em>shared learnable parameters</em> across all nodes. This allows the model to generalize to unseen nodes or even entirely new graphs, making it incredibly powerful for tasks like inductive learning.</p> <h3 id="unpacking-the-architecture-a-glimpse-into-gcns-and-gats">Unpacking the Architecture: A Glimpse into GCNs and GATs</h3> <p>While the message passing framework is general, different GNN architectures implement <code class="language-plaintext highlighter-rouge">AGGREGATE</code> and <code class="language-plaintext highlighter-rouge">UPDATE</code> in specific ways. Let’s briefly look at two popular ones:</p> <h4 id="1-graph-convolutional-networks-gcns">1. Graph Convolutional Networks (GCNs)</h4> <p>One of the foundational GNN models is the Graph Convolutional Network (GCN), introduced by Kipf and Welling in 2017. Their core idea is to adapt the concept of convolutional filters (used so effectively in image processing) to graphs.</p> <p>In a GCN, the aggregation step often involves a weighted average of neighbor features. The update rule for a node $v$ at layer $l+1$ often looks something like this:</p> <p>$h_v^{(l+1)} = \sigma \left( \sum_{u \in N(v) \cup {v}} \frac{1}{\sqrt{\tilde{D}<em>{vv}} \sqrt{\tilde{D}</em>{uu}}} W^{(l)} h_u^{(l)} \right)$</p> <p>This might look intimidating, but let’s break it down for the entire graph:</p> <p>$H^{(l+1)} = \sigma(\tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2} H^{(l)} W^{(l)})$</p> <p>Where:</p> <ul> <li>$H^{(l)}$ is the matrix of node embeddings at layer $l$ (each row is a node’s embedding).</li> <li>$W^{(l)}$ is a trainable weight matrix (the “convolutional filter”) for layer $l$. This is what the GNN learns!</li> <li>$\tilde{A} = A + I$ is the adjacency matrix with added self-loops (each node is connected to itself). This ensures that a node’s own features are included in its update.</li> <li>$\tilde{D}$ is the degree matrix of $\tilde{A}$ (a diagonal matrix where $\tilde{D}_{ii}$ is the sum of row $i$ of $\tilde{A}$).</li> <li>$\tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2}$ is a normalization term. It’s a special kind of matrix multiplication that effectively averages the features of neighbors (including the node itself). This helps prevent issues with varying node degrees.</li> <li>$\sigma$ is an activation function (like ReLU).</li> </ul> <p>In essence, a GCN layers applies a linear transformation ($W^{(l)}$) to the node features, then aggregates these transformed features from its neighbors (including itself) using a normalized sum, and finally applies an activation function. It’s like each node “sees” a weighted average of its neighbors’ (and its own) features and transforms that into a new, richer representation.</p> <h4 id="2-graph-attention-networks-gats">2. Graph Attention Networks (GATs)</h4> <p>One limitation of basic GCNs is that they treat all neighbors equally (or semi-equally, depending on the normalization). But in many real-world scenarios, some neighbors might be more important than others. Think of medical advice: you’d weigh your doctor’s opinion more heavily than a random stranger’s.</p> <p>Graph Attention Networks (GATs), introduced by Veličković et al. in 2018, address this by incorporating an <strong>attention mechanism</strong>. Instead of simply averaging neighbor features, a GAT allows each node to <em>learn</em> the importance of its neighbors.</p> <p>For a node $v$, when it aggregates information from its neighbor $u$, it computes an <strong>attention coefficient</strong>, $e_{vu}$. This coefficient indicates how much node $v$ should “pay attention” to node $u$. These coefficients are learned based on the features of $v$ and $u$.</p> <p>$e_{vu} = \text{AttentionFunction}(W h_v, W h_u)$</p> <p>Then, these raw attention scores are normalized (e.g., using a softmax function) across all neighbors of $v$ to get $\alpha_{vu}$, ensuring they sum to 1.</p> <p>$\alpha_{vu} = \frac{\exp(e_{vu})}{\sum_{k \in N(v) \cup {v}} \exp(e_{vk})}$</p> <p>Finally, the node’s new embedding is a weighted sum of its neighbors’ (and its own) transformed features, where the weights are the learned attention coefficients:</p> <p>$h_v^{(l+1)} = \sigma \left( \sum_{u \in N(v) \cup {v}} \alpha_{vu} W^{(l)} h_u^{(l)} \right)$</p> <p>GATs are powerful because they allow the model to dynamically decide which parts of the neighborhood are most relevant, leading to more expressive and robust node representations. They often employ “multi-head attention,” similar to Transformers, where multiple independent attention mechanisms compute attention scores, and their results are concatenated or averaged.</p> <h3 id="what-can-gnns-do-a-world-of-applications">What Can GNNs Do? A World of Applications!</h3> <p>The ability of GNNs to learn from structural information opens up a treasure trove of applications across various domains:</p> <ul> <li> <strong>Node Classification</strong>: Predicting the category or label of a node. <ul> <li> <em>Example:</em> Identifying fraudulent accounts in a transaction network, classifying proteins in a biological network, recommending jobs to users based on their network.</li> </ul> </li> <li> <strong>Link Prediction</strong>: Predicting the existence or strength of a connection between two nodes. <ul> <li> <em>Example:</em> Recommending friends on social media, predicting drug-target interactions in bioinformatics, auto-completing knowledge graphs.</li> </ul> </li> <li> <strong>Graph Classification / Regression</strong>: Predicting a property for an entire graph. <ul> <li> <em>Example:</em> Determining if a molecule is toxic, categorizing different types of social networks, predicting the mechanical properties of a material.</li> </ul> </li> <li> <strong>Recommendation Systems</strong>: Building sophisticated recommender systems by modeling user-item interactions as a graph. <ul> <li> <em>Example:</em> “Users who watched X also watched Y,” understanding complex preferences.</li> </ul> </li> <li> <strong>Drug Discovery &amp; Chemistry</strong>: Analyzing molecular structures (which are naturally graphs of atoms and bonds) to predict properties, identify new drug candidates, or simulate chemical reactions.</li> <li> <strong>Traffic Prediction</strong>: Modeling road networks to predict traffic flow and congestion.</li> <li> <strong>Computer Vision</strong>: Scene graph generation (describing relationships between objects in an image), point cloud processing.</li> <li> <strong>Natural Language Processing (NLP)</strong>: Representing text as graphs (e.g., dependency trees) for tasks like relation extraction or abstractive summarization.</li> </ul> <h3 id="the-why-behind-the-magic-inductive-bias-and-generalization">The “Why” Behind the Magic: Inductive Bias and Generalization</h3> <p>The real magic of GNNs lies in their <strong>inductive bias</strong>. By using shared weights across all nodes and edges and by iteratively aggregating information locally, GNNs are inherently designed to:</p> <ol> <li> <strong>Exploit Locality</strong>: They assume that a node’s immediate neighborhood is often the most informative.</li> <li> <strong>Be Permutation Invariant/Equivariant</strong>: The aggregation functions (sum, mean, max) are naturally invariant to the order of neighbors.</li> <li> <strong>Generalize to Unseen Structures</strong>: Because the “message passing” rules are learned, they can be applied to graphs with different sizes and structures, making GNNs powerful for transfer learning.</li> </ol> <p>This means GNNs learn not just about the specific features of nodes, but about the <em>patterns of relationships</em> that define different graph structures. They learn to recognize common motifs, important connections, and how information flows through a network.</p> <h3 id="looking-ahead-challenges-and-the-frontier">Looking Ahead: Challenges and the Frontier</h3> <p>While incredibly powerful, GNNs are still an active area of research. Some challenges include:</p> <ul> <li> <strong>Scalability</strong>: Applying GNNs to enormous graphs (billions of nodes/edges) can be computationally expensive.</li> <li> <strong>Deep GNNs</strong>: Stacking too many GNN layers can lead to an “over-smoothing” problem, where all node embeddings become too similar, losing their distinctiveness.</li> <li> <strong>Heterogeneous Graphs</strong>: Graphs with multiple types of nodes and edges (e.g., users, items, ratings, categories) pose additional modeling challenges.</li> <li> <strong>Dynamic Graphs</strong>: Handling graphs that change over time (nodes or edges appearing/disappearing) requires specialized architectures.</li> <li> <strong>Interpretability</strong>: Understanding <em>why</em> a GNN made a particular prediction can still be challenging.</li> </ul> <p>Despite these challenges, the field is booming with innovations, from new architectures to more efficient training methods. The future of GNNs is undoubtedly bright, promising even more profound ways to unlock insights from connected data.</p> <h3 id="final-thoughts">Final Thoughts</h3> <p>Stepping into the world of Graph Neural Networks is like upgrading from seeing individual stars to understanding the intricate constellations and galaxies they form. It’s about recognizing that context, connection, and relationships are often just as, if not more, important than individual data points.</p> <p>So, the next time you see a network diagram, a molecule, or even a friend recommendation, remember the silent, powerful algorithms of GNNs at work, teaching computers to understand the world, one connection at a time. It’s truly a fascinating frontier in machine learning, and one that holds immense potential for solving some of our most complex real-world problems. Keep exploring, keep connecting!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>