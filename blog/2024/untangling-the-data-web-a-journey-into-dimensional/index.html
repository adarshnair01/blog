<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Untangling the Data Web: A Journey into Dimensionality Reduction | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/untangling-the-data-web-a-journey-into-dimensional/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Untangling the Data Web: A Journey into Dimensionality Reduction</h1> <p class="post-meta"> Created on April 06, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/dimensionality-reduction"> <i class="fa-solid fa-hashtag fa-sm"></i> Dimensionality Reduction</a>   <a href="/blog/blog/tag/pca"> <i class="fa-solid fa-hashtag fa-sm"></i> PCA</a>   <a href="/blog/blog/tag/t-sne"> <i class="fa-solid fa-hashtag fa-sm"></i> t-SNE</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone! Have you ever tried to describe your entire life story in just a few words? Or maybe simplify a complex drawing down to its most essential lines? It’s a challenge, right? We want to capture the essence without losing the most important details. In the world of data science, we face a very similar, yet often much grander, challenge: dealing with data that has <em>too many</em> features, variables, or “dimensions.”</p> <p>Imagine trying to visualize a dataset with 100 different columns – say, 100 characteristics of a customer. We can’t even begin to plot that on a graph! This is where <strong>Dimensionality Reduction</strong> steps onto the stage, a superhero technique that helps us simplify our data without sacrificing its core meaning. It’s not just about making things pretty for visualization; it’s about making our machine learning models smarter, faster, and less prone to mistakes.</p> <p>Join me on this journey as we untangle the data web and discover the power of simplifying complexity.</p> <h2 id="the-curse-of-too-much-why-do-we-need-to-reduce-dimensions">The Curse of Too Much: Why Do We Need to Reduce Dimensions?</h2> <p>Before we dive into <em>how</em> we do it, let’s understand <em>why</em> it’s so crucial. Picture this: you’re trying to find a specific book in a library. If the library has only 10 shelves, it’s easy. But what if it has 10,000 shelves, each with a thousand books, and no clear organization? That’s what happens when our data grows too “big” in terms of its dimensions.</p> <p>This problem is affectionately known as the <strong>“Curse of Dimensionality.”</strong> As the number of features (dimensions) in our dataset increases, several problems arise:</p> <ol> <li> <strong>Sparsity of Data:</strong> Imagine points scattered in a 2D plane. They might look dense. Now imagine those same points in 100D space. They become incredibly sparse, like a few stars in a vast galaxy. Our models struggle to find patterns in such empty spaces.</li> <li> <strong>Increased Computation:</strong> More dimensions mean more calculations. Our algorithms slow down significantly, demanding more memory and processing power.</li> <li> <strong>Difficulty in Visualization:</strong> As I mentioned, we can visualize 2D or 3D data. Beyond that, it’s impossible for the human eye to grasp, making exploratory data analysis a nightmare.</li> <li> <strong>Overfitting:</strong> With many features, especially if some are noisy or irrelevant, our models might start learning the noise rather than the actual signal. They become <em>too good</em> at predicting the training data but fail miserably on new, unseen data.</li> <li> <strong>Interpretability:</strong> Understanding what a model is doing with hundreds or thousands of features is incredibly difficult.</li> </ol> <p>Dimensionality reduction helps us combat these issues by either selecting the most important features or creating entirely new, more compact features.</p> <h2 id="two-flavors-of-reduction-selection-vs-extraction">Two Flavors of Reduction: Selection vs. Extraction</h2> <p>Broadly speaking, dimensionality reduction techniques fall into two categories:</p> <ol> <li> <strong>Feature Selection:</strong> This is like choosing the best ingredients for a recipe. We identify and keep only the most relevant features from our original dataset, discarding the rest. Think of it as pruning a tree to keep only the branches that bear fruit.</li> <li> <strong>Feature Extraction:</strong> This is more like taking all the ingredients and blending them into a new, concentrated paste. We transform the data from the high-dimensional space into a lower-dimensional space, creating <em>new</em> features that are combinations of the original ones. The original features are gone, replaced by these powerful, condensed representations.</li> </ol> <p>Today, we’re going to dive deeper into two powerful <strong>feature extraction</strong> techniques: Principal Component Analysis (PCA) and t-Distributed Stochastic Neighbor Embedding (t-SNE).</p> <hr> <h2 id="principal-component-analysis-pca-finding-the-core-directions">Principal Component Analysis (PCA): Finding the Core Directions</h2> <p>Imagine you’re trying to photograph a long, thin cigar. If you take a picture from the side, you see its length. If you take it from the end, it just looks like a circle. To capture the <em>most information</em> about the cigar with just one shot, you’d want to photograph it along its longest dimension.</p> <p>That’s the core idea behind <strong>Principal Component Analysis (PCA)</strong>! It’s a linear technique that finds new dimensions (called <strong>principal components</strong>) that capture the maximum variance (spread) in our data. Think of it as rotating your coordinate system to align with the directions where your data is most spread out.</p> <h3 id="the-intuition-behind-pca">The Intuition Behind PCA</h3> <ol> <li> <strong>Variance is Key:</strong> PCA looks for the directions in your data where there’s the most “spread” or variation. The first principal component (PC1) is the direction along which the data varies the most.</li> <li> <strong>Orthogonality:</strong> The second principal component (PC2) is perpendicular (orthogonal) to PC1 and captures the next most variance. This continues for subsequent components. Each new component provides new information not captured by the previous ones.</li> <li> <strong>Projection:</strong> Once these principal components are identified, we project our original data onto these new axes. If we decide to keep, say, only the first two principal components, we’ve effectively reduced our data from its original high dimension to just two dimensions.</li> </ol> <h3 id="a-touch-of-math-conceptual">A Touch of Math (Conceptual!)</h3> <p>Let’s say we have a dataset $X$ with $N$ samples and $D$ features. PCA essentially looks for a set of orthogonal vectors, $\mathbf{w}_1, \mathbf{w}_2, …, \mathbf{w}_k$ (where $k &lt; D$), that maximize the variance of the projected data. These vectors are called the <strong>eigenvectors</strong> of the data’s covariance matrix, and the amount of variance they capture is given by their corresponding <strong>eigenvalues</strong>.</p> <p>For a single data point $\mathbf{x}<em>i$ and a principal component $\mathbf{w}_k$, the projection onto that component would be: $z</em>{ik} = \mathbf{x}_i \cdot \mathbf{w}_k = \mathbf{x}_i^T \mathbf{w}_k$</p> <p>This $z_{ik}$ is the new coordinate of $\mathbf{x}_i$ along the $k$-th principal component. We can then form a new, lower-dimensional representation of our data, $Z$.</p> <h3 id="pros-of-pca">Pros of PCA:</h3> <ul> <li> <strong>Simplicity &amp; Speed:</strong> Relatively straightforward to implement and computationally efficient, especially for large datasets.</li> <li> <strong>Unsupervised:</strong> It doesn’t need labeled data, making it versatile.</li> <li> <strong>Noise Reduction:</strong> By focusing on directions of highest variance, it can effectively filter out minor, noisy variations in the data.</li> </ul> <h3 id="cons-of-pca">Cons of PCA:</h3> <ul> <li> <strong>Linearity:</strong> PCA can only find linear relationships. If your data has complex, non-linear structures (like a Swiss roll), PCA might flatten it incorrectly, losing important information.</li> <li> <strong>Interpretability Loss:</strong> The new principal components are linear combinations of the original features, which often makes them hard to interpret in real-world terms. What does “Principal Component 1” actually mean for our customers? It’s often a mix of many things.</li> </ul> <hr> <h2 id="t-distributed-stochastic-neighbor-embedding-t-sne-preserving-local-relationships">t-Distributed Stochastic Neighbor Embedding (t-SNE): Preserving Local Relationships</h2> <p>Now, let’s talk about something a bit different: <strong>t-SNE</strong>. If PCA is about finding the major axes of global variance, t-SNE is like meticulously arranging individual pieces of a puzzle so that local relationships are preserved.</p> <p>Imagine you have a crumpled piece of paper with dots drawn on it. Some dots are close together, others are far apart. If you flatten that paper, you want the dots that were close together on the crumpled paper to <em>still be close together</em> on the flattened paper, even if the overall shape of the paper changes drastically.</p> <p>That’s precisely what <strong>t-SNE</strong> aims to do! It’s a non-linear dimensionality reduction technique primarily used for <strong>visualization</strong> of high-dimensional datasets. It tries to map high-dimensional data points to a lower-dimensional space (typically 2D or 3D) in such a way that the <em>local neighborhood structure</em> of the data is preserved as much as possible.</p> <h3 id="the-intuition-behind-t-sne">The Intuition Behind t-SNE</h3> <ol> <li> <strong>Probabilistic Similarity:</strong> t-SNE converts high-dimensional distances between data points into probabilities that represent their similarity. Points that are close in high-dimensional space have a high probability of being neighbors.</li> <li> <strong>Low-Dimensional Mapping:</strong> It then creates a corresponding set of points in a lower-dimensional space (e.g., 2D). It tries to minimize the difference between the high-dimensional similarity probabilities and the low-dimensional similarity probabilities.</li> <li> <strong>“Crowding Problem” Solution:</strong> t-SNE uses a “t-distribution” in the low-dimensional space to model similarities. This distribution has “heavier tails” than a Gaussian, which helps alleviate the “crowding problem” (where distant points in high dimensions might get crammed together in low dimensions).</li> </ol> <h3 id="pros-of-t-sne">Pros of t-SNE:</h3> <ul> <li> <strong>Excellent for Visualization:</strong> It’s incredibly good at revealing clusters and subgroups in complex datasets that linear methods like PCA might miss.</li> <li> <strong>Non-linear:</strong> It can uncover intricate, non-linear structures in data.</li> <li> <strong>Local Structure Preservation:</strong> Its strength lies in ensuring that points close to each other in high dimensions remain close in low dimensions.</li> </ul> <h3 id="cons-of-t-sne">Cons of t-SNE:</h3> <ul> <li> <strong>Computational Cost:</strong> It’s much slower and more memory-intensive than PCA, especially for very large datasets.</li> <li> <strong>Not for New Data:</strong> t-SNE is primarily for visualizing <em>existing</em> data. You can’t directly transform new data points using a pre-trained t-SNE model.</li> <li> <strong>Parameter Sensitivity:</strong> Its results can be sensitive to hyperparameter choices (like <code class="language-plaintext highlighter-rouge">perplexity</code>), which can sometimes make interpretation tricky.</li> <li> <strong>Global Structure Loss:</strong> While great at local structure, it doesn’t always preserve global distances or relationships. The size of clusters in a t-SNE plot might not correspond to their actual density in high dimensions.</li> </ul> <hr> <h2 id="other-noteworthy-mentions">Other Noteworthy Mentions</h2> <p>While PCA and t-SNE are giants, the world of dimensionality reduction is vast:</p> <ul> <li> <strong>Linear Discriminant Analysis (LDA):</strong> Similar to PCA but <em>supervised</em>. It aims to find dimensions that best separate different classes of data.</li> <li> <strong>UMAP (Uniform Manifold Approximation and Projection):</strong> A newer technique often seen as an alternative to t-SNE, offering faster computation and better preservation of global structure while still being non-linear.</li> <li> <strong>Autoencoders:</strong> A type of neural network that learns a compressed representation (encoding) of the input data in its middle layer. The network tries to reconstruct the original data from this compressed representation, forcing the middle layer to capture the most essential information.</li> </ul> <h2 id="when-to-use-what">When to Use What?</h2> <ul> <li> <strong>Need speed and linearity? PCA is your friend.</strong> Great for initial reduction, noise removal, and when interpretability of the new components isn’t paramount.</li> <li> <strong>Need to visualize complex clusters in high-dimensional data? t-SNE (or UMAP) is a fantastic choice.</strong> Especially when non-linear relationships are suspected.</li> <li><strong>Have labeled data and want to maximize class separation? LDA is worth exploring.</strong></li> </ul> <h2 id="wrapping-up-finding-clarity-in-complexity">Wrapping Up: Finding Clarity in Complexity</h2> <p>Dimensionality reduction is more than just a technique; it’s a philosophy of simplification. In our increasingly data-rich world, the ability to distill vast amounts of information into its essential components is invaluable. Whether you’re a data scientist trying to build a robust model, a researcher trying to visualize complex biological data, or even a high school student trying to make sense of a large dataset for a project, these tools empower you to find clarity in complexity.</p> <p>So, the next time you feel swamped by data, remember the power of dimensionality reduction. It’s not magic; it’s smart mathematics helping us see the forest for the trees! Keep exploring, keep questioning, and keep simplifying!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>