<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Cracking the Code: My Deep Dive into Large Language Models (LLMs) | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/cracking-the-code-my-deep-dive-into-large-language/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Cracking the Code: My Deep Dive into Large Language Models (LLMs)</h1> <p class="post-meta"> Created on April 15, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> NLP</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/transformers"> <i class="fa-solid fa-hashtag fa-sm"></i> Transformers</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   <a href="/blog/blog/tag/large-language-models"> <i class="fa-solid fa-hashtag fa-sm"></i> Large Language Models</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>My journey into the world of Large Language Models (LLMs) began not with a textbook, but with a simple conversation. I typed a query into a chatbot, something mundane, and received a response so coherent, so contextually aware, that it felt less like interacting with a program and more like chatting with a very knowledgeable, albeit digital, friend. That moment sparked an insatiable curiosity: <em>How does it do that?</em></p> <p>This blog post is a personal exploration, a sharing of my “aha!” moments and the technical insights I’ve gathered while peeling back the layers of LLMs. Whether you’re a budding data scientist, an enthusiastic high school student, or just someone fascinated by the future of AI, join me as we uncover the magic behind these modern marvels.</p> <h3 id="the-large-in-large-language-models">The “Large” in Large Language Models</h3> <p>First things first: what <em>is</em> an LLM? At its core, an LLM is a type of artificial intelligence designed to understand and generate human-like text. The “Large” isn’t just a marketing gimmick; it refers to the sheer scale:</p> <ol> <li> <strong>Parameters:</strong> These are the internal variables or “knobs” that the model learns to adjust during training. Modern LLMs can have <em>billions</em> of parameters (think 175 billion for GPT-3, even more for others!). More parameters often mean a greater capacity to learn complex patterns in language.</li> <li> <strong>Training Data:</strong> LLMs are trained on colossal amounts of text data – entire swaths of the internet, including books, articles, websites, and more. Imagine reading almost everything ever written online; that’s the kind of input these models digest.</li> </ol> <p>This immense scale isn’t just about making models bigger; it leads to <em>emergent abilities</em>. Things like reasoning, summarization, and even coding, which weren’t explicitly programmed, seem to “emerge” as the model grows large enough and is exposed to enough data. It’s like a child suddenly connecting disparate pieces of knowledge to form a new understanding.</p> <h3 id="from-words-to-numbers-tokenization-and-embeddings">From Words to Numbers: Tokenization and Embeddings</h3> <p>Computers, at their heart, understand numbers, not words. So, how does an LLM process language? It starts with two crucial steps:</p> <ol> <li> <strong>Tokenization:</strong> First, raw text is broken down into smaller units called “tokens.” These can be whole words, parts of words (like “un-“ or “-ing”), or even punctuation marks. For example, the sentence “I love machine learning!” might become <code class="language-plaintext highlighter-rouge">["I", " love", " machine", " learning", "!"]</code>. This helps manage vocabulary size and handles rare words efficiently.</li> <li> <strong>Embeddings:</strong> Each token is then converted into a numerical vector – a list of numbers. This isn’t just a random assignment. These “embedding vectors” are learned during training, and they capture the semantic meaning and relationships between words. Words with similar meanings (e.g., “king” and “queen”) will have vectors that are “close” to each other in a high-dimensional space.</li> </ol> <p>Imagine you’re trying to describe a color without using its name. You might use its position on a spectrum, its brightness, its saturation. That’s a bit like an embedding: representing a word’s meaning through its numerical “coordinates” in a multi-dimensional space.</p> <h3 id="the-secret-sauce-the-transformer-architecture">The Secret Sauce: The Transformer Architecture</h3> <p>While earlier models like Recurrent Neural Networks (RNNs) struggled with long-range dependencies in text (i.e., remembering information from the beginning of a long sentence), the <strong>Transformer</strong> architecture, introduced in 2017, revolutionized the field. It’s the backbone of virtually all modern LLMs.</p> <p>The Transformer’s magic lies in its ability to process entire sequences of text <em>in parallel</em> and, most importantly, in its <strong>Self-Attention mechanism</strong>.</p> <h4 id="self-attention-paying-attention-to-what-matters">Self-Attention: Paying Attention to What Matters</h4> <p>Think about how <em>you</em> read a sentence: “The quick brown fox jumped over the lazy dog.” If I ask you what “lazy” describes, your brain immediately connects it to “dog.” You don’t need to re-read the entire sentence word-by-word from the beginning. You “attend” to the relevant parts.</p> <p>Self-attention works similarly for the model. For each word in a sequence, it computes a “score” of how much it should “pay attention” to every other word in that same sequence. This allows the model to weigh the importance of different words when encoding a particular word’s meaning.</p> <p>It uses three key vectors derived from each word’s embedding:</p> <ul> <li> <strong>Query (Q):</strong> What am I looking for? (Like a search query)</li> <li> <strong>Key (K):</strong> What do I have? (Like an index in a database)</li> <li> <strong>Value (V):</strong> What information is associated with what I have? (The actual data)</li> </ul> <p>The attention score between a Query and a Key tells the model how relevant they are to each other. These scores are then used to create a weighted sum of the Value vectors. The technical heart of this is the <strong>Scaled Dot-Product Attention</strong>:</p> <p>$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $</p> <p>Here, $Q$, $K$, and $V$ are matrices stacked with query, key, and value vectors for all words in the sequence. $d_k$ is the dimension of the key vectors, used for scaling to prevent very large dot products that push the softmax function into regions with tiny gradients. The <code class="language-plaintext highlighter-rouge">softmax</code> function turns the scores into probabilities, ensuring they sum to 1. This weighted sum effectively tells the model, “When I’m processing <em>this</em> word, I should focus <em>this much</em> on <em>that</em> word over there.”</p> <p>To make it even more powerful, Transformers use <strong>Multi-Head Attention</strong>. Instead of just one set of Q, K, V vectors, they use multiple sets (e.g., 8 or 16 “heads”). Each head learns to attend to different aspects of the relationships between words, like focusing on grammatical dependencies in one head and semantic similarities in another. It’s like having multiple specialized “experts” looking at the same problem from different angles.</p> <h4 id="positional-encoding-understanding-order">Positional Encoding: Understanding Order</h4> <p>Even with attention, how does the Transformer know the order of words? Since it processes words in parallel, it loses the inherent sequential information. This is where <strong>Positional Encoding</strong> comes in.</p> <p>Before words enter the self-attention layers, a special vector representing its position in the sequence is added to each word’s embedding. These positional vectors are usually sine and cosine functions of different frequencies, allowing the model to distinguish between words at different positions without interfering too much with their semantic meaning.</p> <p>For example, a common positional encoding formula looks something like this: $ \text{PE}(pos, 2i) = \sin(pos / 10000^{2i/d<em>{model}}) $ $ \text{PE}(pos, 2i+1) = \cos(pos / 10000^{2i/d</em>{model}}) $ where $pos$ is the position of the token in the sequence, $i$ is the dimension within the embedding vector, and $d_{model}$ is the dimension of the embedding. This creates a unique “fingerprint” for each position.</p> <h4 id="the-rest-of-the-transformer-block">The Rest of the Transformer Block</h4> <p>After the attention mechanisms, the Transformer block typically includes a <strong>Feed-Forward Network</strong> (a simple neural network applied independently to each position), layer normalization (to stabilize training), and residual connections (to help gradients flow through deep networks).</p> <p>Modern LLMs primarily use the <strong>decoder-only</strong> architecture of the Transformer. The original Transformer had an Encoder (for understanding input) and a Decoder (for generating output). Decoder-only models are optimized for generating text sequence-by-sequence, making them perfect for tasks like chatbots and creative writing. They have a “masked” self-attention mechanism, meaning a word can only attend to previous words in the sequence, ensuring it doesn’t “cheat” by looking at future words it’s supposed to predict.</p> <h3 id="training-llms-the-grueling-marathon">Training LLMs: The Grueling Marathon</h3> <p>Building an LLM isn’t like writing a regular program; it’s more like raising a highly intelligent digital being. The process typically involves two major phases:</p> <ol> <li> <strong>Pre-training (The “Learning Everything” Phase):</strong> This is the massive, unsupervised learning phase. The model is fed vast amounts of text and asked to predict the next word in a sentence (or fill in masked words). For example, if it sees “The cat sat on the…”, it learns that “mat” or “rug” are likely continuations. By repeatedly doing this across trillions of words, the model builds an incredibly rich statistical understanding of language, grammar, facts, and even some reasoning patterns. This phase requires immense computational power (GPUs!) and time.</li> <li> <strong>Fine-tuning &amp; Alignment (The “Becoming Helpful” Phase):</strong> After pre-training, the model is a general-purpose language wizard, but it might not be very good at specific tasks or always safe/helpful. <ul> <li> <strong>Supervised Fine-tuning (SFT):</strong> The model is further trained on smaller, high-quality datasets of specific tasks (e.g., question-answering pairs, summarization examples).</li> <li> <strong>Reinforcement Learning from Human Feedback (RLHF):</strong> This is a crucial step for aligning LLMs with human values. Humans rate the quality, helpfulness, and safety of different model outputs. This feedback is then used to train a “reward model,” which in turn guides the LLM to generate responses that are preferred by humans. This is how models learn to be polite, refuse harmful requests, and provide useful information.</li> </ul> </li> </ol> <h3 id="what-can-llms-do-capabilities--limitations">What Can LLMs Do? Capabilities &amp; Limitations</h3> <p>The capabilities of LLMs are truly astonishing:</p> <ul> <li> <strong>Text Generation:</strong> Writing articles, poems, stories, emails, code.</li> <li> <strong>Summarization:</strong> Condensing long documents into key points.</li> <li> <strong>Translation:</strong> Bridging language barriers.</li> <li> <strong>Question Answering:</strong> Providing information based on vast knowledge.</li> <li> <strong>Coding Assistance:</strong> Generating code snippets, debugging, explaining code.</li> <li> <strong>Creative Tasks:</strong> Brainstorming ideas, generating dialogue for scripts.</li> </ul> <p>However, it’s crucial to understand their limitations:</p> <ul> <li> <strong>Hallucinations:</strong> LLMs can confidently generate factually incorrect information because they are pattern-matching engines, not truth-seeking ones. They predict what <em>sounds plausible</em>, not necessarily what <em>is true</em>.</li> <li> <strong>Lack of True Understanding/Reasoning:</strong> They don’t “understand” in the way humans do. They lack common sense, real-world experience, and true consciousness. Their “reasoning” is often a sophisticated form of pattern matching.</li> <li> <strong>Bias:</strong> Because they learn from human-generated data, they can inherit and even amplify biases present in that data (e.g., gender, racial, cultural biases).</li> <li> <strong>Context Window Limitations:</strong> While improving, LLMs have a finite “memory” or context window. They can only refer back to a certain number of tokens in the conversation.</li> <li> <strong>Cost &amp; Energy:</strong> Training and running large LLMs are incredibly expensive and energy-intensive.</li> </ul> <h3 id="the-future-is-bright-and-challenging">The Future is Bright (and Challenging)</h3> <p>The field of LLMs is evolving at a breathtaking pace. We’re seeing trends towards:</p> <ul> <li> <strong>Multimodality:</strong> Models that can understand and generate not just text, but also images, audio, and video.</li> <li> <strong>Efficiency:</strong> Research into making smaller, more efficient models that can run on less powerful hardware.</li> <li> <strong>Better Alignment &amp; Safety:</strong> Continued efforts to make LLMs more helpful, harmless, and honest through advanced alignment techniques.</li> <li> <strong>Personalization &amp; Integration:</strong> Seamless integration into our daily tools and personalized AI assistants.</li> </ul> <p>For me, the journey into LLMs has been a profound experience. It’s a testament to human ingenuity and the power of data and computation. While they are incredibly powerful tools, they are just that – tools. Understanding their mechanics, capabilities, and limitations is paramount for anyone navigating this rapidly changing technological landscape.</p> <p>I hope this deep dive has sparked your own curiosity and given you a clearer picture of what makes these fascinating models tick. The future with LLMs will be complex, but undoubtedly, incredibly exciting. Let’s keep exploring!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>