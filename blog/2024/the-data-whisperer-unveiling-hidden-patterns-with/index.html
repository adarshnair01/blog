<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Data Whisperer: Unveiling Hidden Patterns with Principal Component Analysis | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/2024/the-data-whisperer-unveiling-hidden-patterns-with/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Data Whisperer: Unveiling Hidden Patterns with Principal Component Analysis</h1> <p class="post-meta"> Created on July 16, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/dimensionality-reduction"> <i class="fa-solid fa-hashtag fa-sm"></i> Dimensionality Reduction</a>   <a href="/blog/blog/tag/pca"> <i class="fa-solid fa-hashtag fa-sm"></i> PCA</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/linear-algebra"> <i class="fa-solid fa-hashtag fa-sm"></i> Linear Algebra</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey there, fellow data explorer!</p> <p>Have you ever looked at a giant spreadsheet, teeming with columns – let’s say, a hundred different features for each entry – and just felt… lost? Like you’re staring at a forest but can’t see the trees, let alone the path? Welcome to the “curse of dimensionality,” a common headache in the world of data science. More dimensions often mean more complexity, harder visualization, and slower models.</p> <p>But what if I told you there’s a powerful technique, a kind of “data whisperer,” that can listen to your noisy, high-dimensional data and distill its essence, revealing its true underlying structure? That’s where <strong>Principal Component Analysis (PCA)</strong> comes in, and today, we’re going on a journey to truly understand it.</p> <h3 id="the-overwhelm-a-modern-data-problem">The Overwhelm: A Modern Data Problem</h3> <p>Imagine you’re trying to describe a person. You could list their height, weight, age, eye color, hair color, favorite food, shoe size, IQ, income, number of pets, political views… The list goes on. Each of these is a “dimension” or a “feature.” While all this information is technically relevant, some of it might be redundant, or less important for certain tasks. For instance, if you’re trying to predict someone’s overall health, perhaps height and weight (which contribute to BMI) are more correlated than eye color and favorite food.</p> <p>The more dimensions you have, the harder it is to:</p> <ol> <li> <strong>Visualize:</strong> Good luck plotting 100 dimensions! We’re stuck in 3D in the real world.</li> <li> <strong>Process:</strong> More features mean more computations, making machine learning algorithms slower and more memory-intensive.</li> <li> <strong>Avoid Noise:</strong> Not all features carry useful information; some might just be noise, confusing our models.</li> </ol> <p>PCA offers a elegant solution: <strong>dimensionality reduction</strong>, but with a twist. It doesn’t just throw away features; it creates <em>new</em> features that are combinations of the old ones, specifically designed to capture the most “information” in your data.</p> <h3 id="what-is-pca-intuitively">What is PCA, Intuitively?</h3> <p>Think of it like this: You have a scatter plot of data points in a 3D room, like a swarm of bees. If you wanted to take a 2D picture that best captures how these bees are spread out, you wouldn’t just pick one wall at random. You’d try to find an angle, a perspective, from which the swarm looks most “spread out” or “stretched.” This angle gives you the most informative 2D representation of the 3D data.</p> <p>PCA does precisely this. It finds new axes (called <strong>Principal Components</strong>) along which your data is most spread out. These new axes are orthogonal (at right angles) to each other, ensuring they capture independent directions of variance. The first principal component captures the most variance, the second captures the most remaining variance orthogonal to the first, and so on.</p> <p>The key idea is to project your data onto a lower-dimensional space (e.g., from 3D to 2D) in a way that preserves as much of the original variance (or “information”) as possible.</p> <h3 id="the-building-blocks-of-pca">The Building Blocks of PCA</h3> <p>Before we dive into the math, let’s quickly re-familiarize ourselves with a few statistical concepts that are fundamental to PCA:</p> <ol> <li> <strong>Variance:</strong> How spread out a single variable’s data points are from its mean. A high variance means the data points are widely distributed; low variance means they’re clustered closely. <ul> <li>Formula for a variable $x$: $Var(x) = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})^2$</li> </ul> </li> <li> <strong>Covariance:</strong> How two variables change together. <ul> <li>Positive covariance: If one variable increases, the other tends to increase.</li> <li>Negative covariance: If one variable increases, the other tends to decrease.</li> <li>Zero covariance: No clear linear relationship.</li> <li>Formula for variables $x$ and $y$: $Cov(x, y) = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})$</li> </ul> </li> <li> <strong>Data Centering/Standardization:</strong> Before performing PCA, it’s crucial to center your data (subtract the mean from each feature) and often standardize it (divide by the standard deviation). This ensures that features with larger scales don’t disproportionately influence the principal components.</li> </ol> <h3 id="the-math-behind-the-magic-a-walkthrough">The Math Behind the Magic: A Walkthrough</h3> <p>Alright, let’s peel back the layers and see how PCA actually works under the hood. It mostly relies on concepts from linear algebra, specifically <strong>eigenvalues</strong> and <strong>eigenvectors</strong>. Don’t let those words scare you; we’ll break them down.</p> <p>Here’s the simplified step-by-step process:</p> <h4 id="step-1-compute-the-covariance-matrix">Step 1: Compute the Covariance Matrix</h4> <p>First, we need to understand how all the variables in our dataset relate to each other. This is captured by the <strong>covariance matrix</strong>. If you have $p$ features, the covariance matrix $\Sigma$ will be a $p \times p$ symmetric matrix.</p> <ul> <li>The diagonal elements $\Sigma_{ii}$ are the variances of each individual feature.</li> <li>The off-diagonal elements $\Sigma_{ij}$ are the covariances between feature $i$ and feature $j$.</li> </ul> <p>Mathematically, if $X$ is your data matrix (where each row is a sample and each column is a feature, with means subtracted), the covariance matrix is:</p> <p>$ \Sigma = \frac{1}{n-1} X^T X $</p> <p>Where $n$ is the number of samples. This matrix tells us the “shape” and “orientation” of our data cloud in its high-dimensional space.</p> <h4 id="step-2-calculate-eigenvalues-and-eigenvectors-of-the-covariance-matrix">Step 2: Calculate Eigenvalues and Eigenvectors of the Covariance Matrix</h4> <p>This is the heart of PCA! We need to find the special directions (eigenvectors) in which our data varies most, and the magnitude of that variance (eigenvalues).</p> <ul> <li> <p><strong>Eigenvectors:</strong> Imagine a square matrix transforming a vector. Most vectors will change both their direction and magnitude. But special vectors, called eigenvectors, only get scaled (stretched or shrunk) by the transformation; their direction remains the same. In PCA, the eigenvectors of the covariance matrix are our <strong>Principal Components</strong>. They represent the new axes.</p> </li> <li> <p><strong>Eigenvalues:</strong> The scalar factor by which an eigenvector is scaled during the transformation is its eigenvalue. In PCA, the eigenvalue corresponding to a principal component quantifies the amount of variance captured along that component. A larger eigenvalue means that its corresponding eigenvector captures more variance (more “information”) from the data.</p> </li> </ul> <p>The relationship is defined by the equation:</p> <p>$ \Sigma \mathbf{v} = \lambda \mathbf{v} $</p> <p>Where:</p> <ul> <li>$\Sigma$ is the covariance matrix.</li> <li>$\mathbf{v}$ is an eigenvector (a Principal Component).</li> <li>$\lambda$ is the corresponding eigenvalue.</li> </ul> <p>By solving this equation, we get $p$ eigenvalues and $p$ corresponding eigenvectors.</p> <h4 id="step-3-select-principal-components">Step 3: Select Principal Components</h4> <p>Now we have $p$ eigenvectors (our potential principal components) and their respective eigenvalues (the variance they capture). To perform dimensionality reduction, we select only a subset of these.</p> <p>We sort the eigenvalues in <strong>descending order</strong>. The eigenvector corresponding to the largest eigenvalue is our <strong>first principal component (PC1)</strong>, capturing the most variance. The eigenvector corresponding to the second largest eigenvalue is <strong>PC2</strong>, capturing the second most variance (orthogonal to PC1), and so on.</p> <p>You decide how many principal components ($k$) to keep. A common approach is to look at the <strong>explained variance ratio</strong>, which tells you the proportion of total variance captured by each component. You might choose $k$ components that collectively explain, say, 95% of the total variance. A “scree plot” (a plot of eigenvalues in descending order) can also help visualize this.</p> <h4 id="step-4-project-data-onto-new-dimensions">Step 4: Project Data Onto New Dimensions</h4> <p>Finally, we construct a projection matrix $W$ using the $k$ selected eigenvectors (stacking them as columns). Then, we transform our original centered data matrix $X$ into a new lower-dimensional dataset $Y$:</p> <p>$ Y = X W $</p> <p>Where:</p> <ul> <li>$X$ is the original $n \times p$ centered data matrix.</li> <li>$W$ is the $p \times k$ matrix of selected eigenvectors (principal components).</li> <li>$Y$ is the new $n \times k$ data matrix, where $k &lt; p$. Each column of $Y$ represents a principal component.</li> </ul> <p>And <em>voila!</em> You now have a new dataset with fewer dimensions, where each dimension is a principal component that captures as much of the original data’s variance as possible.</p> <h3 id="why-pca-is-so-powerful">Why PCA is so Powerful</h3> <ol> <li> <strong>Dimensionality Reduction:</strong> This is the obvious one. Fewer features mean less storage, faster computation, and mitigation of the curse of dimensionality.</li> <li> <strong>Noise Reduction:</strong> Often, the components with very small eigenvalues capture mostly noise. By discarding these, you can effectively denoise your data.</li> <li> <strong>Visualization:</strong> Reducing data to 2 or 3 principal components allows for easy plotting and visual inspection, which is incredibly useful for exploratory data analysis.</li> <li> <strong>Feature Extraction:</strong> PCA doesn’t just select features; it creates <em>new</em> features that are orthogonal and uncorrelated. This can be beneficial for some machine learning algorithms that perform better with uncorrelated input features.</li> <li> <strong>Interpretability (with caution):</strong> While individual principal components might not directly correspond to a single original feature, they represent the dominant patterns in your data. Sometimes, these patterns can be interpreted (e.g., “size” vs. “shape” components).</li> </ol> <h3 id="limitations-and-considerations">Limitations and Considerations</h3> <p>No technique is a silver bullet, and PCA has its caveats:</p> <ul> <li> <strong>Linearity Assumption:</strong> PCA assumes that the principal components are linear combinations of the original features. If your data has complex non-linear structures, PCA might not capture them effectively.</li> <li> <strong>Interpretability Trade-off:</strong> While powerful for reduction, the new principal components are abstract. PC1 might be 0.7 * feature_A + 0.3 * feature_B - 0.1 * feature_C. Interpreting what “PC1” truly means in real-world terms can be challenging.</li> <li> <strong>Scaling Sensitivity:</strong> PCA is highly sensitive to the scaling of your features. If one feature has a much larger range of values than others, it will likely dominate the first principal component. Always standardize your data before applying PCA!</li> </ul> <h3 id="bringing-it-to-life-practical-use">Bringing it to Life: Practical Use</h3> <p>In Python, implementing PCA is wonderfully straightforward thanks to libraries like <code class="language-plaintext highlighter-rouge">scikit-learn</code>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="n">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="c1"># Assume you have a DataFrame called 'df'
# with your numerical features.
</span>
<span class="c1"># 1. Standardize the data
</span><span class="n">scaler</span> <span class="o">=</span> <span class="nc">StandardScaler</span><span class="p">()</span>
<span class="n">scaled_data</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>

<span class="c1"># 2. Apply PCA
# n_components can be an int (e.g., 2, 0.95)
# If 0.95, PCA will select the minimum number of components
# to explain 95% of the variance.
</span><span class="n">pca</span> <span class="o">=</span> <span class="nc">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># Let's reduce to 2 dimensions for visualization
</span><span class="n">principal_components</span> <span class="o">=</span> <span class="n">pca</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">scaled_data</span><span class="p">)</span>

<span class="c1"># Create a DataFrame for the principal components
</span><span class="n">pca_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">principal_components</span><span class="p">,</span> 
                      <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">principal component 1</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">principal component 2</span><span class="sh">'</span><span class="p">])</span>

<span class="c1"># You can also check the explained variance ratio
</span><span class="nf">print</span><span class="p">(</span><span class="n">pca</span><span class="p">.</span><span class="n">explained_variance_ratio_</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">pca</span><span class="p">.</span><span class="n">explained_variance_ratio_</span><span class="p">.</span><span class="nf">sum</span><span class="p">())</span>
</code></pre></div></div> <p>This simple code snippet can transform a high-dimensional dataset into a two-dimensional one, ready for plotting and revealing insights that were previously hidden!</p> <h3 id="my-journey-with-pca">My Journey with PCA</h3> <p>When I first learned about PCA, the concept of eigenvalues and eigenvectors felt daunting. It sounded like something out of advanced physics, far removed from “simplifying data.” But as I delved deeper, seeing how these elegant mathematical constructs directly translated into finding the most important directions in a dataset, it clicked. It’s like finding the perfect lens to bring a blurry image into sharp focus.</p> <p>PCA is more than just a technique; it’s a philosophy of finding efficiency and clarity in complexity. It teaches us that not all information is created equal, and by strategically prioritizing variance, we can often see the bigger picture more clearly.</p> <p>So, the next time you face a forest of features, remember PCA. Let it be your guide, your data whisperer, helping you to unveil the hidden patterns and tell a clearer, more concise story with your data.</p> <p>Happy exploring!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>