<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Navigating the Unknown: My Journey into Q-Learning and Reinforcement Learning | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/navigating-the-unknown-my-journey-into-q-learning/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Navigating the Unknown: My Journey into Q-Learning and Reinforcement Learning</h1> <p class="post-meta"> Created on June 18, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/reinforcement-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Reinforcement Learning</a>   <a href="/blog/blog/tag/q-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Q-Learning</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone! Today, I want to pull back the curtain on one of the most fascinating areas of Artificial Intelligence: Reinforcement Learning (RL). Specifically, we’re going to dive deep into a foundational algorithm called <strong>Q-Learning</strong>. If you’ve ever been curious about how intelligent agents learn to make decisions in dynamic environments, or how AlphaGo beat the world champion in Go, you’re in for a treat.</p> <p>My personal journey into RL felt a bit like stumbling into a new world. It wasn’t about predicting a number or classifying an image; it was about <em>teaching</em> an agent to <em>think</em> and <em>act</em>. And at the heart of many of these early explorations, I found Q-Learning – a simple, yet incredibly powerful idea.</p> <h3 id="the-world-of-reinforcement-learning-a-grand-adventure">The World of Reinforcement Learning: A Grand Adventure</h3> <p>Before we pinpoint Q-Learning, let’s set the stage. Imagine you’re trying to teach a dog a new trick. You don’t program every single muscle movement. Instead, you give it treats (rewards!) when it does something right, and maybe a stern “no” (negative reward or penalty) when it does something wrong. Over time, through trial and error, the dog learns to associate certain actions with positive outcomes.</p> <p>This, in a nutshell, is Reinforcement Learning. We have:</p> <ol> <li> <strong>An Agent:</strong> This is our “dog,” the entity that learns and makes decisions.</li> <li> <strong>An Environment:</strong> This is the “world” the agent interacts with (e.g., a maze, a game board, a physical room).</li> <li> <strong>States ($s$):</strong> A snapshot of the environment at a given time (e.g., the dog’s position, the game board’s configuration).</li> <li> <strong>Actions ($a$):</strong> The moves the agent can make in a given state (e.g., move forward, jump, bark).</li> <li> <strong>Rewards ($r$):</strong> Feedback from the environment, indicating how “good” or “bad” an action was in a particular state. Positive rewards encourage behavior, negative rewards discourage it.</li> </ol> <p>The ultimate goal of our agent is to learn a <strong>policy</strong> – essentially, a strategy or a set of rules that tells it what action to take in every possible state, maximizing the <em>cumulative reward</em> over time. It’s not just about getting the immediate treat, but about getting the most treats in the long run.</p> <h3 id="enter-q-learning-the-quality-of-an-action">Enter Q-Learning: The “Quality” of an Action</h3> <p>Now, let’s talk about the “Q.” What does it stand for? Many believe it means “Quality.” And that’s a perfect way to think about it!</p> <p>Q-Learning is a <strong>model-free</strong> reinforcement learning algorithm. What does “model-free” mean? It means our agent doesn’t need to know how the environment works upfront. It doesn’t need a map of the maze or a rulebook for the game. Instead, it learns <em>solely through experience</em>. It tries things, observes the outcomes, and updates its internal “knowledge base.”</p> <p>The core idea behind Q-Learning is to learn <strong>Q-values</strong>. A Q-value, denoted as $Q(s, a)$, represents the <em>expected total future reward</em> an agent can receive by taking a specific action $a$ in a specific state $s$, and then following an optimal policy thereafter.</p> <p>Think of it like this: “How <em>good</em> is it to perform action ‘A’ when I am in state ‘S’?” The higher the Q-value, the better that action is considered.</p> <h3 id="the-q-table-our-agents-secret-weapon">The Q-Table: Our Agent’s Secret Weapon</h3> <p>To store these Q-values, our agent uses something called a <strong>Q-Table</strong>. This is essentially a giant lookup table where rows represent states and columns represent actions. Each cell $Q(s,a)$ holds the numerical Q-value for taking action $a$ in state $s$.</p> <table> <thead> <tr> <th style="text-align: left">State / Action</th> <th style="text-align: left">Action 1</th> <th style="text-align: left">Action 2</th> <th style="text-align: left">Action 3</th> <th style="text-align: left">…</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">State 1</td> <td style="text-align: left">$Q(s_1, a_1)$</td> <td style="text-align: left">$Q(s_1, a_2)$</td> <td style="text-align: left">$Q(s_1, a_3)$</td> <td style="text-align: left">…</td> </tr> <tr> <td style="text-align: left">State 2</td> <td style="text-align: left">$Q(s_2, a_1)$</td> <td style="text-align: left">$Q(s_2, a_2)$</td> <td style="text-align: left">$Q(s_2, a_3)$</td> <td style="text-align: left">…</td> </tr> <tr> <td style="text-align: left">…</td> <td style="text-align: left">…</td> <td style="text-align: left">…</td> <td style="text-align: left">…</td> <td style="text-align: left">…</td> </tr> </tbody> </table> <p>Initially, this table might be filled with zeros or small random numbers. Our agent’s job is to explore the environment, take actions, observe rewards, and update these Q-values until they accurately reflect the true “quality” of each action-state pair.</p> <h3 id="the-q-learning-algorithm-how-we-update-our-knowledge">The Q-Learning Algorithm: How We Update Our Knowledge</h3> <p>The magic of Q-Learning lies in its update rule. Each time our agent takes an action, moves to a new state, and receives a reward, it uses this experience to refine its Q-Table.</p> <p>Here’s the general flow:</p> <ol> <li> <strong>Initialize the Q-Table:</strong> Fill it with zeros.</li> <li> <strong>For each episode (or ‘round’ of learning):</strong> <ul> <li><strong>Start in an initial state ($s$).</strong></li> <li> <strong>Choose an action ($a$).</strong> How does the agent choose? This is where <strong>exploration vs. exploitation</strong> comes in. <ul> <li> <strong>Exploitation:</strong> The agent chooses the action $a$ that has the highest Q-value for the current state $s$, based on its current Q-Table: $\arg\max_a Q(s, a)$. This is doing what it <em>thinks</em> is best.</li> <li> <strong>Exploration:</strong> The agent sometimes chooses a random action, even if it doesn’t seem optimal. Why? To discover new, potentially better paths or avoid getting stuck in local optima. A common strategy is <strong>$\epsilon$-greedy</strong> (epsilon-greedy), where with a small probability $\epsilon$ (e.g., 0.1), the agent explores, and with probability $1-\epsilon$, it exploits. Over time, $\epsilon$ often decays, meaning the agent explores less as it learns more.</li> </ul> </li> <li><strong>Execute action ($a$).</strong></li> <li><strong>Observe the new state ($s’$) and immediate reward ($r$).</strong></li> <li> <strong>Update the Q-value for $Q(s, a)$</strong> using the Q-Learning update rule! This is the core equation:</li> </ul> \[Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s, a)]\] <p>Let’s break down this powerful equation:</p> <ul> <li> <strong>$Q(s, a)$ (left side):</strong> This is the Q-value we are updating. It’s our current estimate of how good it is to take action $a$ in state $s$.</li> <li> <strong>$Q(s, a)$ (right side):</strong> This is our <em>old</em> Q-value before the update.</li> <li> <strong>$\alpha$ (alpha): The Learning Rate.</strong> This parameter (between 0 and 1) determines how much we value new information over old information. A high $\alpha$ means the agent quickly adapts to new experiences, potentially forgetting old ones. A low $\alpha$ means the agent learns slowly, relying more on its accumulated knowledge.</li> <li> <strong>$r$: The Immediate Reward.</strong> This is the reward we just received for taking action $a$ in state $s$.</li> <li> <strong>$\gamma$ (gamma): The Discount Factor.</strong> This parameter (between 0 and 1) determines the importance of future rewards. A $\gamma$ close to 1 means the agent considers future rewards almost as important as immediate rewards (long-sighted). A $\gamma$ close to 0 means the agent is short-sighted and primarily cares about immediate rewards.</li> <li> <strong>$\max_{a’} Q(s’, a’)$:</strong> This is the “future optimal Q-value.” It represents the <em>maximum</em> Q-value for the <em>next</em> state $s’$, across all possible actions $a’$. We’re essentially asking: “If I arrive in the next state $s’$, what’s the best possible move I <em>could</em> make from there, according to my current Q-table?”</li> <li> <strong>$[r + \gamma \max_{a’} Q(s’, a’) - Q(s, a)]$:</strong> This entire term is known as the <strong>Temporal Difference (TD) Error</strong>. It’s the difference between what we <em>expected</em> ($Q(s,a)$) and what we <em>actually experienced</em> ($r + \gamma \max_{a’} Q(s’, a’)$). If this error is positive, our previous estimate was too low; if it’s negative, it was too high. We use this error to adjust our $Q(s,a)$.</li> </ul> </li> <li> <strong>Repeat:</strong> Continue this process for many, many episodes. Over time, the Q-values in the table will converge to their optimal values, allowing the agent to infer the best action for any given state.</li> </ol> <h3 id="a-walkthrough-robot-in-a-maze">A Walkthrough: Robot in a Maze</h3> <p>Let’s imagine a tiny robot in a 3x3 grid maze.</p> <table> <thead> <tr> <th>S</th> <th>.</th> <th>.</th> </tr> </thead> <tbody> <tr> <td>.</td> <td>X</td> <td>.</td> </tr> <tr> <td>.</td> <td>.</td> <td>G</td> </tr> </tbody> </table> <ul> <li> <strong>S:</strong> Start state.</li> <li> <strong>G:</strong> Goal state (reward of +10).</li> <li> <strong>X:</strong> Obstacle (negative reward of -5 if entered, robot stays in place).</li> <li> <strong>.</strong>: Empty cell (small negative reward of -1 for each step, to encourage shorter paths).</li> <li> <strong>Actions:</strong> Up, Down, Left, Right.</li> </ul> <p>Initially, our Q-table is all zeros.</p> <p><strong>Episode 1:</strong></p> <ul> <li>Robot starts at S. $\epsilon$ is high, so it explores.</li> <li>It randomly chooses “Right”.</li> <li>New state: middle cell. Reward: -1.</li> <li>Update $Q(S, \text{Right})$ using the formula. Since all future Q-values are 0, it mainly uses the -1 reward.</li> <li>It might then randomly choose “Down”.</li> <li>New state: bottom-middle cell. Reward: -1.</li> <li>Update $Q(\text{middle}, \text{Down})$.</li> <li>…and so on. It might bump into X, get a -5 reward, and learn to avoid that path. Or it might randomly stumble into G.</li> </ul> <p><strong>After many episodes:</strong></p> <ul> <li>The robot learns that moving “Right” from S is okay, but moving “Down” from the middle cell might lead to the obstacle X, so $Q(\text{middle}, \text{Down})$ becomes very low.</li> <li>It discovers paths to G. The Q-values along the shortest, safest path to G will become significantly higher than other paths.</li> <li>For example, $Q(\text{cell before G}, \text{Right})$ will be very high because it leads directly to the large +10 reward. This high Q-value will then “backpropagate” through the update rule to the previous cells, making actions that lead towards the goal more attractive.</li> </ul> <p>Eventually, our robot, driven by its Q-Table, will learn to navigate the maze efficiently, avoiding the obstacle and taking the shortest path to the goal, even though nobody explicitly programmed the path for it. It just learned through consistent trial, error, and feedback.</p> <h3 id="key-parameters-and-their-significance">Key Parameters and Their Significance</h3> <ul> <li> <strong>Learning Rate ($\alpha$):</strong> Imagine you’re correcting a drawing. A high $\alpha$ means you’re sketching boldly, quickly changing lines. A low $\alpha$ means you’re making tiny, precise adjustments.</li> <li> <strong>Discount Factor ($\gamma$):</strong> How much do you care about tomorrow versus today? A high $\gamma$ means you’re thinking long-term (saving for retirement). A low $\gamma$ means you’re focused on immediate gratification (spending your paycheck now).</li> <li> <strong>Exploration Rate ($\epsilon$):</strong> How adventurous are you? A high $\epsilon$ means you’re trying new restaurants every week. A low $\epsilon$ means you stick to your favorite few.</li> </ul> <p>Tuning these parameters is crucial for optimal learning performance.</p> <h3 id="limitations-and-the-path-forward">Limitations and the Path Forward</h3> <p>While Q-Learning is brilliant for understanding the fundamentals of RL, it does have some practical limitations:</p> <ol> <li> <strong>The Curse of Dimensionality:</strong> For environments with a huge number of states or actions (e.g., a complex video game, a robot navigating a real city), the Q-Table becomes astronomically large and impossible to store or update efficiently. This is the biggest hurdle for basic Q-Learning.</li> <li> <strong>Continuous State/Action Spaces:</strong> If states or actions are continuous (e.g., steering angle of a car, exact position in a room), we can’t represent them discretely in a table.</li> </ol> <p>This is where the field of Reinforcement Learning takes another exciting leap! To overcome these limitations, researchers developed techniques like <strong>Deep Q-Networks (DQN)</strong>, which use neural networks to <em>approximate</em> the Q-values instead of storing them in a table. This allows agents to generalize across similar states and handle vast or continuous environments – but that’s a story for another time!</p> <h3 id="wrapping-up">Wrapping Up</h3> <p>Q-Learning is truly a cornerstone of Reinforcement Learning. It’s an elegant demonstration of how an agent can learn optimal behavior in an unknown environment purely through interaction, rewards, and a smart update rule. My first successful Q-Learning agent, stumbling its way through a simple grid, felt like watching a child take its first steps. It’s a powerful reminder that complex intelligence can emerge from simple, iterative learning processes.</p> <p>If you’re eager to build intelligent agents, understanding Q-Learning is an indispensable first step. It’s a foundational concept that paves the way for understanding more advanced algorithms that are now solving incredibly complex problems in AI.</p> <p>Keep exploring, keep learning, and who knows what amazing agents you’ll empower next!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>