<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Decoding the Giants: My Journey into the World of Large Language Models | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/decoding-the-giants-my-journey-into-the-world-of-l/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Decoding the Giants: My Journey into the World of Large Language Models</h1> <p class="post-meta"> Created on December 25, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/natural-language-processing"> <i class="fa-solid fa-hashtag fa-sm"></i> Natural Language Processing</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   <a href="/blog/blog/tag/transformers"> <i class="fa-solid fa-hashtag fa-sm"></i> Transformers</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>My desk is often a battlefield of scattered papers, half-empty coffee cups, and a laptop glowing with lines of Python code. But lately, amid the usual chaos, there’s been a new, almost magical presence: a chatbot, fluent and articulate, helping me brainstorm blog post ideas, debug code, or even just offering a philosophical musing at 2 AM. This isn’t just any chatbot; this is a <strong>Large Language Model (LLM)</strong>, and honestly, encountering these systems feels like glimpsing the future.</p> <p>For me, someone deeply embedded in the world of Data Science and Machine Learning Engineering, the rise of LLMs has been nothing short of a paradigm shift. It’s a field that’s moving at breakneck speed, and staying on top of it means constantly learning, questioning, and sometimes, feeling a delightful sense of awe. In this post, I want to take you on a journey through the core concepts of LLMs, from their foundational ideas to the cutting-edge innovations that make them so powerful, yet also so complex. My aim is to make it accessible for anyone curious, whether you’re just starting your tech journey or are a seasoned practitioner looking for a refreshed perspective.</p> <h3 id="the-large-in-llm-its-all-about-scale-and-data">The “Large” in LLM: It’s All About Scale (and Data!)</h3> <p>Before we dive into the “language model” part, let’s tackle the elephant in the room: the “Large.” When we talk about large language models, we’re really talking about two things: <strong>scale of parameters</strong> and <strong>scale of training data</strong>.</p> <p>Imagine building a brain. The more neurons and connections it has, the more complex thoughts it can potentially process. In an artificial neural network, these “connections” are parameters – the adjustable weights and biases that the model learns during training. Early language models had millions of parameters. Today’s LLMs? We’re talking billions, even trillions. For instance, models like GPT-3 boast 175 billion parameters, while others are pushing even higher.</p> <p>This isn’t just a number game. This immense scale, coupled with training on truly colossal datasets (often comprising a significant chunk of the internet’s text and code), unlocks something remarkable: <strong>emergent abilities</strong>. These are capabilities that weren’t explicitly programmed or even easily predicted. It’s like pouring enough water into a container until, suddenly, it starts to flow in a completely new way. LLMs, when scaled sufficiently, spontaneously exhibit abilities like complex reasoning, code generation, summarization, and even a rudimentary form of “understanding” context, which they simply didn’t possess at smaller scales.</p> <h3 id="at-its-core-the-language-models-simple-goal">At its Core: The Language Model’s Simple Goal</h3> <p>Strip away the hype, the billions of parameters, and the emergent superpowers, and at its heart, a language model has one surprisingly simple, yet incredibly powerful, job: <strong>predicting the next word (or more accurately, the next <em>token</em>) in a sequence.</strong></p> <p>Think about it this way: if I start a sentence, “The cat sat on the…”, what’s the most probable next word? “Mat,” “couch,” or “roof” probably come to mind. It’s highly unlikely to be “bicycle” or “quantum physics.” A language model does this millions of times over, learning these probabilities from the vast amount of text it reads.</p> <p>Mathematically, a language model tries to estimate the probability of a word given all the preceding words in a sequence. If we denote a sequence of words as $w_1, w_2, …, w_t$, the model learns to compute:</p> <table> <tbody> <tr> <td>$P(w_t</td> <td>w_1, w_2, …, w_{t-1})$</td> </tr> </tbody> </table> <p>This equation essentially asks: “What is the probability of the <em>t</em>-th word, given all the words that came before it?” By repeatedly predicting the next token and appending it to the sequence, these models can generate coherent, contextually relevant, and even creative text, one token at a time. This is the foundation of almost everything LLMs do, from writing essays to answering complex questions.</p> <h3 id="the-brain-behind-the-magic-the-transformer-architecture">The Brain Behind the Magic: The Transformer Architecture</h3> <p>For decades, Recurrent Neural Networks (RNNs) and their variants like LSTMs (Long Short-Term Memory networks) were the go-to for sequence processing. They were good, but struggled with long-range dependencies – remembering information from the beginning of a very long sentence or document. This is where the <strong>Transformer architecture</strong>, introduced in the 2017 paper “Attention Is All You Need,” truly revolutionized the field.</p> <p>The Transformer’s core innovation is <strong>self-attention</strong>. Instead of processing words sequentially, like RNNs, self-attention allows the model to weigh the importance of all other words in the input sequence when processing each word.</p> <p>Imagine you’re reading the sentence: “The animal didn’t cross the street because it was too tired.” When trying to understand what “it” refers to, your brain immediately connects “it” to “the animal.” Self-attention mimics this. For each word in the sentence, it calculates an “attention score” with every other word. These scores determine how much focus (or “attention”) the model should pay to each word when encoding a particular word.</p> <p>Here’s a simplified way to think about it:</p> <ol> <li> <strong>Query, Key, Value:</strong> For each word, the model generates three vectors: a <em>Query</em> (what am I looking for?), a <em>Key</em> (what information do I have?), and a <em>Value</em> (what information should I provide?).</li> <li> <strong>Scoring:</strong> For each word, its Query vector is compared against all other words’ Key vectors. This comparison results in an “attention score” – a number indicating how relevant another word is to the current word.</li> <li> <strong>Weighting:</strong> These scores are then normalized (e.g., using a softmax function) to get weights. Words with higher scores get higher weights.</li> <li> <strong>Combining:</strong> Finally, the Value vectors of all words are multiplied by their respective attention weights and summed up. This weighted sum becomes the new representation of the current word, enriched by the relevant information from the entire sequence.</li> </ol> <p>This parallel processing, enabled by self-attention, is incredibly efficient and allows Transformers to capture long-range dependencies far more effectively than their predecessors. Most modern LLMs, including the famous GPT series, are built upon this Transformer architecture, typically focusing on the <strong>decoder-only</strong> version for generating text.</p> <h3 id="beyond-prediction-from-pre-training-to-alignment">Beyond Prediction: From Pre-training to Alignment</h3> <p>If a language model simply predicts the next word, how does it become so good at following instructions, writing code, or summarizing complex documents? This is where the multi-stage training process comes in:</p> <ol> <li> <p><strong>Pre-training:</strong> This is the initial, massive unsupervised learning phase. The model is fed trillions of words from the internet (books, articles, websites, code, etc.) and simply tasked with predicting the next word. Through this vast exposure, it learns grammar, facts about the world, common sense, different writing styles, and even basic reasoning patterns. It’s like giving a child every book in the world and asking them to guess the next word in every sentence – they’ll eventually learn a lot about how language and the world work.</p> </li> <li> <p><strong>Instruction Tuning (Supervised Fine-tuning - SFT):</strong> After pre-training, the model is intelligent but doesn’t necessarily know how to <em>follow instructions</em>. It might just continue a story instead of answering a question directly. To fix this, it’s fine-tuned on a smaller, high-quality dataset of human-written “instructions” and “responses.” For example, an instruction might be “Summarize this article:” followed by an article, and the response would be a human-written summary. This teaches the model to understand and respond to user prompts in a helpful way.</p> </li> <li> <p><strong>Reinforcement Learning from Human Feedback (RLHF):</strong> This is the secret sauce that makes models like ChatGPT feel so incredibly aligned with human intent. RLHF takes the instruction-tuned model a step further:</p> <ul> <li> <strong>Human Preference Data:</strong> Humans are shown several responses generated by the model for a given prompt and asked to rank them from best to worst.</li> <li> <strong>Reward Model Training:</strong> This human preference data is used to train a separate “reward model” that learns to predict which response a human would prefer.</li> <li> <strong>Reinforcement Learning:</strong> Finally, the language model is fine-tuned again, but this time using reinforcement learning. The reward model acts as a “critic,” guiding the LLM to generate responses that maximize the predicted human preference score. This iterative process is crucial for aligning the model’s outputs with human values, safety guidelines, and helpfulness.</li> </ul> </li> </ol> <p>This multi-stage process transforms a powerful next-word predictor into an intelligent, responsive assistant.</p> <h3 id="the-power-and-the-pitfalls">The Power and the Pitfalls</h3> <p>The capabilities unlocked by LLMs are genuinely astonishing:</p> <ul> <li> <strong>Creative Content Generation:</strong> Writing poems, stories, marketing copy.</li> <li> <strong>Information Retrieval and Summarization:</strong> Quickly extracting key information from long texts.</li> <li> <strong>Code Generation and Debugging:</strong> Writing code snippets, explaining errors, translating between languages.</li> <li> <strong>Language Translation:</strong> More nuanced and context-aware than ever before.</li> <li> <strong>Interactive Assistants:</strong> Revolutionizing customer service, education, and personal productivity.</li> </ul> <p>However, it’s crucial to understand their limitations:</p> <ul> <li> <strong>Hallucinations:</strong> LLMs can confidently generate factually incorrect information. They are pattern matchers, not truth-seekers.</li> <li> <strong>Bias:</strong> Trained on human data, they can inherit and amplify societal biases present in that data.</li> <li> <strong>Lack of True Understanding/Common Sense:</strong> While they can mimic understanding, they don’t possess consciousness or genuine common sense in the human sense. They operate on statistical relationships.</li> <li> <strong>Computational Cost:</strong> Training and running these models requires immense computational resources.</li> <li> <strong>Ethical Concerns:</strong> Misinformation, job displacement, and potential misuse are significant societal challenges.</li> </ul> <h3 id="my-personal-takeaway">My Personal Takeaway</h3> <p>Working with LLMs feels like being at the frontier of something monumental. As a data scientist and MLE, I see them as incredibly powerful tools that, when understood and applied responsibly, can augment human capabilities in ways we’re only just beginning to grasp. The journey from predicting the next word to crafting coherent narratives and solving complex problems is a testament to the power of scaled data, clever architecture, and iterative human alignment.</p> <p>The field is still rapidly evolving. New architectures, training techniques, and applications are emerging constantly. My advice to anyone interested is simple: dive in. Experiment. Ask questions. Understand the underlying mechanics, but also critically evaluate their outputs and acknowledge their limitations.</p> <p>We are not just training models; we are shaping a new interface for human knowledge and creativity. And that, to me, is incredibly exciting. The next chapter in this story is being written right now, and I, for one, am thrilled to be a part of it.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>