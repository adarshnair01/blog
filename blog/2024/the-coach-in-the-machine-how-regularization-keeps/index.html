<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Coach in the Machine: How Regularization Keeps Our Models Honest | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/the-coach-in-the-machine-how-regularization-keeps/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Coach in the Machine: How Regularization Keeps Our Models Honest</h1> <p class="post-meta"> Created on May 19, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/regularization"> <i class="fa-solid fa-hashtag fa-sm"></i> Regularization</a>   <a href="/blog/blog/tag/overfitting"> <i class="fa-solid fa-hashtag fa-sm"></i> Overfitting</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/model-training"> <i class="fa-solid fa-hashtag fa-sm"></i> Model Training</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>As a budding data scientist, there’s a certain thrill in watching your machine learning model train. The loss function drops, the accuracy soars on your training set, and you feel like a wizard. “Eureka!” you might exclaim, imagining your model conquering the world.</p> <p>But then, you unleash it on new, unseen data – the real test. And… <em>crash</em>. Your model, once a shining beacon of predictive power, now performs barely better than random chance. What happened? You, my friend, have just met the infamous beast called <strong>overfitting</strong>.</p> <p>It’s a rite of passage for anyone diving into machine learning, and understanding how to combat it is absolutely crucial. This brings us to a technique that’s as fundamental as it is elegant: <strong>Regularization</strong>. Think of it as the wise coach who tells your model, “Don’t just memorize the playbook; <em>understand</em> the game.”</p> <h3 id="the-peril-of-overfitting-when-memorizing-trumps-understanding">The Peril of Overfitting: When Memorizing Trumps Understanding</h3> <p>Let’s imagine you’re studying for a big exam. You could:</p> <ol> <li> <strong>Memorize every single answer from past exams.</strong> You’d ace those specific questions, but if the teacher changes even a word, you’re lost.</li> <li> <strong>Understand the core concepts.</strong> You might not get every past question perfectly, but you can tackle <em>any</em> new question related to the topic.</li> </ol> <p>In machine learning, your model faces a similar choice.</p> <ul> <li> <strong>Overfitting</strong> is like option 1. Your model becomes incredibly good at predicting the outcomes for the <em>training data</em> it has seen. It learns not just the underlying patterns but also the noise, the quirks, and even specific data points. It essentially “memorizes” the training set. When presented with new data, which inevitably has different noise and nuances, its performance tanks.</li> <li> <strong>Underfitting</strong> (the opposite problem) is like not studying at all. The model is too simple, can’t capture the underlying patterns, and performs poorly even on training data.</li> </ul> <p>Graphically, imagine trying to draw a line through a set of data points.</p> <ul> <li>An <strong>underfit</strong> model might be a straight line trying to fit a curve – it misses the trend.</li> <li>A <strong>just-right</strong> model finds a nice curve that captures the general trend.</li> <li>An <strong>overfit</strong> model would be a wildly squiggly line that passes through <em>every single data point</em>, including the noisy outliers. It looks perfect for the training points, but it’s utterly useless for predicting future points.</li> </ul> <p>Mathematically, overfitting often manifests as a model with very large coefficients (weights). These large weights mean that tiny changes in input features can lead to dramatic changes in predictions. The model is too sensitive, too eager to explain every single wiggle in the training data, leading to high variance.</p> <h3 id="enter-regularization-the-models-moral-compass">Enter Regularization: The Model’s Moral Compass</h3> <p>So, how do we prevent our model from becoming an over-eager memorizer? We introduce <strong>regularization</strong>.</p> <p>At its heart, regularization modifies the model’s <strong>loss function</strong>. The loss function is what the model tries to minimize during training – it measures how “wrong” the model’s predictions are.</p> <p>A standard loss function (like Mean Squared Error for regression) looks something like this: $ J(\mathbf{w}) = \frac{1}{2N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2 $ Where:</p> <ul> <li>$ N $ is the number of training examples.</li> <li>$ y_i $ is the actual output for the $i$-th example.</li> <li>$ \hat{y}_i $ is the model’s predicted output for the $i$-th example.</li> <li>$ \mathbf{w} $ represents the model’s parameters (the weights/coefficients).</li> </ul> <p>This function tells the model: “Try to make your predictions as close to the actual values as possible.”</p> <p>Regularization adds a <strong>penalty term</strong> to this loss function. This penalty discourages the model from having overly large weights. Essentially, it tells the model: “Yes, minimize the error, but also try to keep your weights small and simple. Don’t be too confident in any single feature’s importance unless absolutely necessary.”</p> <p>The new, regularized loss function looks like this: $ J<em>{regularized}(\mathbf{w}) = \underbrace{\frac{1}{2N} \sum</em>{i=1}^{N} (y<em>i - \hat{y}_i)^2}</em>{\text{Original Loss (Minimize Error)}} + \underbrace{\lambda \cdot \text{Penalty Term}(\mathbf{w})}_{\text{Regularization Term (Minimize Complexity)}} $</p> <p>Here, $ \lambda $ (lambda) is the <strong>regularization parameter</strong>. It controls the strength of the penalty.</p> <ul> <li>If $ \lambda $ is small, the penalty is weak, and the model behaves more like an unregularized model (risk of overfitting).</li> <li>If $ \lambda $ is large, the penalty is strong, forcing weights to be very small, potentially leading to underfitting (model too simple).</li> </ul> <p>Choosing the right $ \lambda $ is a crucial part of model tuning, often done through techniques like cross-validation.</p> <p>Let’s dive into the two most common types of regularization: L1 and L2.</p> <h3 id="1-l2-regularization-ridge-regression">1. L2 Regularization (Ridge Regression)</h3> <p>Also known as <strong>Ridge Regression</strong>, L2 regularization adds a penalty based on the <em>sum of the squared values</em> of the weights.</p> <p>The L2 regularized loss function: $ J<em>{Ridge}(\mathbf{w}) = \frac{1}{2N} \sum</em>{i=1}^{N} (y<em>i - \hat{y}_i)^2 + \lambda \sum</em>{j=1}^{P} w_j^2 $ Where:</p> <ul> <li>$ P $ is the number of features (and thus weights).</li> <li>$ w_j $ is the $j$-th weight.</li> </ul> <p><strong>Intuition:</strong></p> <ul> <li>The $ \sum w_j^2 $ term penalizes large weights more severely than small weights. Squaring a large number makes it even larger, so this term strongly discourages any single weight from becoming extremely large.</li> <li>L2 regularization drives weights towards zero, but it rarely makes them <em>exactly</em> zero. It shrinks them, distributing the importance across all features rather than letting one feature dominate.</li> <li>Think of it like a tax on the <em>magnitude</em> of your beliefs. If you’re too confident (large weight), you pay a higher tax. This encourages you to spread your confidence more evenly.</li> </ul> <p><strong>Key Benefit:</strong> Prevents weights from becoming too large, which helps reduce the model’s sensitivity to specific training data points, thus lowering variance and improving generalization.</p> <h3 id="2-l1-regularization-lasso-regression">2. L1 Regularization (Lasso Regression)</h3> <p>Also known as <strong>Lasso Regression</strong> (Least Absolute Shrinkage and Selection Operator), L1 regularization adds a penalty based on the <em>sum of the absolute values</em> of the weights.</p> <p>The L1 regularized loss function: $ J<em>{Lasso}(\mathbf{w}) = \frac{1}{2N} \sum</em>{i=1}^{N} (y<em>i - \hat{y}_i)^2 + \lambda \sum</em>{j=1}^{P} |w_j| $</p> <p><strong>Intuition:</strong></p> <ul> <li> <table> <tbody> <tr> <td>The $ \sum</td> <td>w_j</td> <td>$ term also penalizes large weights, but its effect is slightly different from L2.</td> </tr> </tbody> </table> </li> <li>Crucially, L1 regularization has a property of <strong>sparsity</strong>. It tends to drive some weights <em>exactly</em> to zero.</li> <li>Think of it like being forced to choose your most important features. If a feature isn’t contributing much, L1 will just zero out its weight and discard it.</li> </ul> <p><strong>Key Benefit:</strong></p> <ul> <li> <strong>Feature Selection:</strong> Because it can drive weights to zero, L1 regularization is excellent for automatic feature selection. If you have many features, but only a few are truly important, Lasso will help you identify those by effectively ignoring the less important ones. This results in a simpler, more interpretable model.</li> <li>Reduces model complexity and aids in interpretability.</li> </ul> <h3 id="l1-vs-l2-a-quick-comparison">L1 vs. L2: A Quick Comparison</h3> <table> <thead> <tr> <th style="text-align: left">Feature</th> <th style="text-align: left">L1 Regularization (Lasso)</th> <th style="text-align: left">L2 Regularization (Ridge)</th> <th> </th> <th> </th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Penalty Term</td> <td style="text-align: left">$ \lambda \sum</td> <td style="text-align: left">w_j</td> <td>$</td> <td>$ \lambda \sum w_j^2 $</td> </tr> <tr> <td style="text-align: left">Effect on Weights</td> <td style="text-align: left">Drives some weights exactly to zero</td> <td style="text-align: left">Shrinks weights towards zero</td> <td> </td> <td> </td> </tr> <tr> <td style="text-align: left">Feature Selection</td> <td style="text-align: left">Yes, performs automatic feature selection</td> <td style="text-align: left">No, but reduces impact of less important features</td> <td> </td> <td> </td> </tr> <tr> <td style="text-align: left">Model Interpretability</td> <td style="text-align: left">Good (simpler model with fewer features)</td> <td style="text-align: left">Moderate (all features retained)</td> <td> </td> <td> </td> </tr> <tr> <td style="text-align: left">Geometric Analogy</td> <td style="text-align: left">Diamond-shaped constraint</td> <td style="text-align: left">Circular constraint</td> <td> </td> <td> </td> </tr> </tbody> </table> <h3 id="elastic-net-regularization-the-best-of-both-worlds">Elastic Net Regularization: The Best of Both Worlds</h3> <p>What if you want the feature selection power of L1 but also the group shrinkage and stability of L2? That’s where <strong>Elastic Net regularization</strong> comes in. It’s a hybrid approach that combines both L1 and L2 penalties:</p> <table> <tbody> <tr> <td>$ J<em>{ElasticNet}(\mathbf{w}) = \frac{1}{2N} \sum</em>{i=1}^{N} (y<em>i - \hat{y}_i)^2 + \lambda_1 \sum</em>{j=1}^{P}</td> <td>w*j</td> <td>+ \lambda_2 \sum*{j=1}^{P} w_j^2 $</td> </tr> </tbody> </table> <p>Here, you have two regularization parameters, $ \lambda_1 $ and $ \lambda_2 $, controlling the strength of the L1 and L2 components, respectively. Often, this is reparameterized with a single $ \lambda $ and an $ \alpha $ parameter to blend the L1/L2 ratio. Elastic Net is particularly useful when you have many highly correlated features.</p> <h3 id="beyond-l1-and-l2-other-regularization-techniques">Beyond L1 and L2: Other Regularization Techniques</h3> <p>While L1 and L2 are dominant for linear models, regularization is a broad concept, and other forms exist, especially for more complex models like neural networks:</p> <ul> <li> <strong>Dropout:</strong> In neural networks, randomly “dropping out” (setting to zero) a percentage of neurons during training. This forces the network to learn more robust features and prevents over-reliance on any single neuron.</li> <li> <strong>Early Stopping:</strong> Monitoring the model’s performance on a separate validation set during training. When the validation error stops decreasing (or starts increasing), you stop training, even if the training error is still going down. This prevents the model from memorizing the training data’s noise.</li> <li> <strong>Data Augmentation:</strong> Creating more training data by applying transformations (e.g., rotating, flipping, cropping images; synonym replacement in text). This exposes the model to more variations and makes it more robust.</li> </ul> <h3 id="the-art-of-tuning-lambda-lambda">The Art of Tuning Lambda ($\lambda$)</h3> <p>Remember $ \lambda $? The regularization parameter is crucial.</p> <ul> <li>A $ \lambda $ too small and your model still overfits.</li> <li>A $ \lambda $ too large and your model underfits (too simple, can’t capture the signal).</li> </ul> <p>Finding the optimal $ \lambda $ is typically done through <strong>hyperparameter tuning</strong> using techniques like:</p> <ul> <li> <strong>Cross-validation:</strong> Splitting your data into multiple folds, training on some and validating on others, and averaging the results for different $ \lambda $ values.</li> <li> <strong>Grid Search:</strong> Trying out a predefined grid of $ \lambda $ values.</li> <li> <strong>Random Search:</strong> Randomly sampling $ \lambda $ values from a distribution.</li> </ul> <p>The goal is to find the $ \lambda $ that gives the best performance on <em>unseen data</em>, not just the training set.</p> <h3 id="my-personal-takeaway">My Personal Takeaway</h3> <p>When I first encountered regularization, it felt a bit like a cheat code. “Wait, you’re telling me I <em>don’t</em> want my model to be perfect on the training data?” It goes against the intuitive desire to achieve 100% accuracy.</p> <p>But that’s the profound lesson: <strong>Perfection on training data is often a mirage.</strong> The true goal of machine learning is <em>generalization</em> – building models that can make accurate predictions on data they’ve never seen before. Regularization is our steadfast ally in achieving that goal.</p> <p>From simple linear regression to complex neural networks, regularization techniques are ubiquitous. They are a fundamental tool in the machine learning engineer’s toolkit, ensuring our models are not just smart, but also wise, adaptable, and robust. So, the next time your model trains, don’t just celebrate dropping loss; celebrate the subtle influence of regularization, guiding your model towards true understanding. It’s the coach that keeps your model honest, and ultimately, more powerful.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>