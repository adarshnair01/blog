<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Time Travel for Neural Networks: My Journey with Recurrent Neural Networks | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/time-travel-for-neural-networks-my-journey-with-re/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Time Travel for Neural Networks: My Journey with Recurrent Neural Networks</h1> <p class="post-meta"> Created on May 08, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/recurrent-neural-networks"> <i class="fa-solid fa-hashtag fa-sm"></i> Recurrent Neural Networks</a>   <a href="/blog/blog/tag/rnn"> <i class="fa-solid fa-hashtag fa-sm"></i> RNN</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> NLP</a>   <a href="/blog/blog/tag/time-series"> <i class="fa-solid fa-hashtag fa-sm"></i> Time Series</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey there, fellow explorers of the AI universe! Have you ever looked at a sequence of data – like words in a sentence, notes in a melody, or stock prices over time – and wondered how a machine could possibly make sense of it? For a long time, standard neural networks struggled with this. They were like brilliant statisticians with amnesia, excellent at analyzing individual data points but completely forgetting what came before.</p> <p>Today, I want to take you on a journey through one of the most elegant and powerful solutions to this problem: <strong>Recurrent Neural Networks (RNNs)</strong>. Think of this as my personal journal entry, chronicling my “aha!” moments and struggles in understanding how we gave neural networks a memory.</p> <h3 id="the-problem-when-order-matters">The Problem: When Order Matters</h3> <p>Imagine you’re trying to build an AI that can understand sentences. If you give a standard Feedforward Neural Network (FNN) the words “I am happy” versus “Am I happy?”, it might struggle. Why? Because an FNN processes each input independently. It sees “I,” then “am,” then “happy,” but it doesn’t inherently remember “I” when it’s processing “am.” It treats each word as a fresh start, losing the crucial context that makes human language, well, <em>human</em>.</p> <p>This isn’t just about language. What about predicting the next word in a text message? Or forecasting stock prices based on historical trends? Or even generating music where each note depends on the ones before it? In all these scenarios, the order of information isn’t just important; it’s everything.</p> <p>This was the fundamental challenge I faced early in my deep learning adventures. How do you design a network that doesn’t just process data, but <em>remembers</em> it, carrying context from one step to the next?</p> <h3 id="the-aha-moment-introducing-recurrence">The “Aha!” Moment: Introducing Recurrence</h3> <p>The ingenious solution behind RNNs is surprisingly simple yet profoundly effective: <strong>a loop</strong>.</p> <p>Instead of just flowing data in one direction (input -&gt; hidden layers -&gt; output), RNNs introduce a feedback loop. This loop allows information from a previous step in the sequence to be fed back into the network as an additional input for the current step.</p> <p>Think of it like this: A standard neural network is like a photographer taking individual snapshots. An RNN, on the other hand, is like a video camera. It not only captures the current frame but also remembers what happened in the previous frames, allowing it to understand motion, context, and sequences.</p> <p>To truly understand this “loop,” it helps to “unroll” the RNN over time.</p> <p>Imagine we have a sequence of inputs, $x_1, x_2, x_3, \ldots, x_T$. At each time step $t$:</p> <ol> <li>The network receives the current input $x_t$.</li> <li>It also receives information from the previous time step, specifically, its <em>hidden state</em> from the previous step, $h_{t-1}$.</li> <li>It then computes a new hidden state, $h_t$, which encapsulates information from both $x_t$ and $h_{t-1}$. This $h_t$ is essentially the network’s “memory” at time $t$.</li> <li>Finally, it can produce an output $y_t$ based on $h_t$.</li> </ol> <p>The magic here is that the <em>same set of weights</em> is used at each time step. This means the network learns a <em>process</em> for transforming sequential data, not just static patterns.</p> <h3 id="the-math-behind-the-memory">The Math Behind the Memory</h3> <p>Let’s get a little technical and look at the core equations that govern a simple RNN. Don’t worry, we’ll break them down.</p> <p>For each time step $t$, the hidden state $h_t$ is calculated as: $h_t = \tanh(W_{hh}h_{t-1} + W_{xh}x_t + b_h)$</p> <p>And the output $y_t$ is calculated as: $y_t = W_{hy}h_t + b_y$ (often followed by a softmax activation for classification tasks)</p> <p>Let’s unpack these symbols:</p> <ul> <li>$x_t$: This is our input at the current time step (e.g., a word in a sentence, a stock price at a specific day).</li> <li>$h_t$: This is the <strong>hidden state</strong> at the current time step. It’s the network’s memory, encapsulating information from $x_t$ and all previous inputs.</li> <li>$h_{t-1}$: This is the hidden state from the previous time step. This is where the “recurrent” part comes in – feeding past information back into the current calculation.</li> <li>$W_{hh}$: These are the weights connecting the previous hidden state ($h_{t-1}$) to the current hidden state ($h_t$). This is how the network learns to process its own memory.</li> <li>$W_{xh}$: These are the weights connecting the current input ($x_t$) to the current hidden state ($h_t$).</li> <li>$W_{hy}$: These are the weights connecting the current hidden state ($h_t$) to the output ($y_t$).</li> <li>$b_h$, $b_y$: These are bias terms, just like in any neural network, allowing the network to shift the activation function.</li> <li>$\tanh$: This is an activation function (hyperbolic tangent), typically used in the hidden layer to introduce non-linearity. Other activations like ReLU can also be used.</li> <li>The second equation for $y_t$ might use a different activation function depending on the task (e.g., sigmoid for binary classification, softmax for multi-class classification, or no activation for regression).</li> </ul> <p>The key takeaway is that the same $W_{hh}$, $W_{xh}$, $W_{hy}$, $b_h$, and $b_y$ are used <em>across all time steps</em>. This parameter sharing is what makes RNNs powerful and efficient for variable-length sequences.</p> <h3 id="training-rnns-backpropagation-through-time-bptt">Training RNNs: Backpropagation Through Time (BPTT)</h3> <p>So, how do we train these memory-endowed networks? Just like other neural networks, we use backpropagation. But because of the recurrent connections, we need a special version called <strong>Backpropagation Through Time (BPTT)</strong>.</p> <p>BPTT essentially involves unrolling the network for the entire sequence, calculating the loss at each time step, and then summing up the gradients from all time steps to update the weights. Imagine calculating the derivative of a very long chain rule expression. It gets complicated, fast!</p> <p>And herein lies the biggest challenge I encountered with vanilla RNNs: <strong>The Vanishing and Exploding Gradient Problem.</strong></p> <h3 id="the-gradient-predicament-vanishing-and-exploding">The Gradient Predicament: Vanishing and Exploding</h3> <p>During BPTT, gradients are repeatedly multiplied by the weight matrices at each time step.</p> <ul> <li> <strong>Vanishing Gradients:</strong> If the weights are small (or the activation function’s derivative is small), these multiplications can cause the gradients to shrink exponentially as they propagate backward through many time steps. This means that information from earlier time steps ($h_1$, $h_2$) has a negligible impact on the loss calculation at later time steps, effectively making the network “forget” long-term dependencies. It’s like trying to hear a whisper across a noisy football stadium – the signal gets lost.</li> <li> <strong>Exploding Gradients:</strong> Conversely, if the weights are large, the gradients can grow exponentially, leading to extremely large weight updates that can destabilize the network and cause training to diverge (weights become <code class="language-plaintext highlighter-rouge">NaN</code> or <code class="language-plaintext highlighter-rouge">inf</code>). This is like an echo chamber where a small sound becomes a deafening roar.</li> </ul> <p>This was a significant roadblock for simple RNNs, making them struggle with sequences longer than a few dozen steps. My initial excitement about RNNs was tempered by the realization that they were often short-sighted.</p> <h3 id="the-evolution-smarter-memory-with-lstms-and-grus">The Evolution: Smarter Memory with LSTMs and GRUs</h3> <p>Fortunately, brilliant minds in the field didn’t stop there. To address the vanishing gradient problem, more sophisticated recurrent architectures were developed. The most famous ones are <strong>Long Short-Term Memory (LSTM) networks</strong> and <strong>Gated Recurrent Units (GRUs)</strong>.</p> <p>I won’t dive deep into their complex internal mechanisms here (that’s a whole other blog post!), but the core idea is that they introduce “gates.” Think of these gates as intelligent librarians or bouncers for the network’s memory:</p> <ul> <li> <strong>Forget Gate:</strong> Decides what information from the previous hidden state should be discarded.</li> <li> <strong>Input Gate:</strong> Decides what new information from the current input should be stored in the memory cell.</li> <li> <strong>Output Gate:</strong> Decides what part of the memory cell’s content should be exposed as the current hidden state.</li> </ul> <p>By selectively remembering, forgetting, and updating information, LSTMs and GRUs can effectively carry relevant information over much longer sequences, largely mitigating the vanishing gradient problem. They allowed me to build models that could truly understand long-term dependencies, opening up a whole new world of possibilities.</p> <h3 id="real-world-applications-where-rnns-shine">Real-World Applications: Where RNNs Shine</h3> <p>Despite their challenges (or rather, thanks to the evolution into LSTMs/GRUs), RNNs have revolutionized many areas of AI:</p> <ol> <li> <strong>Natural Language Processing (NLP):</strong> <ul> <li> <strong>Machine Translation:</strong> Google Translate, for instance, uses sophisticated RNN variants (often called Encoder-Decoder architectures) to understand a sentence in one language and generate it in another.</li> <li> <strong>Text Generation:</strong> RNNs can learn the style and patterns of text (like Shakespeare’s plays or scientific papers) and generate new, coherent sentences.</li> <li> <strong>Sentiment Analysis:</strong> Understanding if a review is positive or negative, by considering the sequence of words.</li> <li> <strong>Speech Recognition:</strong> Converting spoken words into text.</li> </ul> </li> <li> <strong>Time Series Prediction:</strong> <ul> <li> <strong>Stock Market Prediction:</strong> While notoriously difficult, RNNs can identify patterns in historical stock prices to attempt future forecasts.</li> <li> <strong>Weather Forecasting:</strong> Predicting temperature, rainfall, etc., based on past weather patterns.</li> </ul> </li> <li> <strong>Music Generation:</strong> <ul> <li>Learning musical styles and composing new pieces note by note.</li> </ul> </li> <li> <strong>Video Analysis:</strong> <ul> <li>Analyzing sequences of frames to understand actions or events in videos.</li> </ul> </li> </ol> <h3 id="my-takeaway-a-foundational-step">My Takeaway: A Foundational Step</h3> <p>My journey with Recurrent Neural Networks was a pivotal one. They represent a fundamental shift in how neural networks perceive and process data, moving beyond static snapshots to dynamic sequences. While vanilla RNNs have their limitations, they laid the groundwork for the more advanced architectures like LSTMs and GRUs, which continue to be incredibly relevant today (even with the rise of Transformers).</p> <p>Understanding RNNs isn’t just about memorizing equations; it’s about grasping the beautiful concept of giving a machine memory, allowing it to learn from context and the flow of time. It’s about empowering AI to understand the narratives embedded in data, whether they’re written in words, numbers, or notes.</p> <p>So, the next time you marvel at an AI generating coherent text or translating a phrase instantly, remember the humble recurrent loop – the elegant mechanism that taught neural networks to remember. It’s a testament to human ingenuity in solving complex problems, and it continues to inspire me to explore the frontiers of artificial intelligence.</p> <p>What sequence will you teach an RNN to understand next? The possibilities are truly endless!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>