<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Taming the Overzealous Model: A Guide to Regularization | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/taming-the-overzealous-model-a-guide-to-regulariza/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Taming the Overzealous Model: A Guide to Regularization</h1> <p class="post-meta"> Created on August 05, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/regularization"> <i class="fa-solid fa-hashtag fa-sm"></i> Regularization</a>   <a href="/blog/blog/tag/overfitting"> <i class="fa-solid fa-hashtag fa-sm"></i> Overfitting</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/model-optimization"> <i class="fa-solid fa-hashtag fa-sm"></i> Model Optimization</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>My journey into machine learning has been a rollercoaster of “aha!” moments and head-scratching frustrations. One of the earliest and most profound lessons I learned was the difference between a model that <em>memorizes</em> and a model that <em>understands</em>. It’s the difference between a student who aced an exam by cramming specific answers and one who truly grasped the underlying concepts.</p> <p>In machine learning, this distinction is called <strong>generalization</strong>. We don’t just want our models to perform well on the data they were trained on (their “homework”). We want them to make accurate predictions on <em>new, unseen data</em> (the “final exam”). The pitfall? A phenomenon called <strong>overfitting</strong>.</p> <h3 id="the-problem-when-your-model-knows-too-much-about-the-wrong-things">The Problem: When Your Model Knows Too Much (About the Wrong Things)</h3> <p>Imagine you’re trying to predict house prices based on features like size, number of bedrooms, location, and maybe even the color of the front door. You gather a dataset, train a model, and it’s fantastic! Your model predicts the prices of houses in your training set almost perfectly. You’re ecstatic.</p> <p>But then, you feed it data for a brand new house, and the prediction is wildly off. What happened?</p> <p>Your model, in its eagerness to be perfect on the training data, started to pick up on noise and tiny, irrelevant fluctuations unique to that specific dataset. It became <em>too complex</em>, forming intricate, convoluted rules that worked only for the examples it had seen. It’s like a student who memorizes every minor detail of every past exam, including the typo in question 3 and the specific shade of ink used by the examiner, rather than understanding the core subject matter.</p> <p>This overly complex model has <strong>high variance</strong> – it’s very sensitive to the specific training data. If you gave it a slightly different training set, it would learn a completely different, equally complex set of rules. This is overfitting.</p> <p>Visually, if you plot your data points and your model’s prediction line, an overfit model might weave and bend wildly to hit every single training point, even outliers. It’s too specific; it lacks the broad understanding needed to generalize.</p> <h3 id="the-solution-regularization--guiding-your-model-towards-wisdom">The Solution: Regularization – Guiding Your Model Towards Wisdom</h3> <p>So, how do we encourage our models to learn the <em>essential</em> patterns without getting bogged down in the noise? We introduce <strong>regularization</strong>.</p> <p>At its heart, regularization is a technique that modifies the learning algorithm to prevent overfitting. It does this by adding a “penalty” to the model’s complexity. Think of it as a mentor gently nudging your model, saying, “Hey, let’s keep things simple. Don’t get too carried away with those elaborate theories when a simpler explanation works just as well.”</p> <p>How does it penalize complexity? Primarily, by discouraging large coefficient values (or “weights”) in our model. In many machine learning models (like linear regression, logistic regression, or neural networks), these coefficients determine the influence of each feature. A large coefficient means that a small change in that feature leads to a large change in the prediction, indicating that the model is heavily relying on that specific feature or combination of features. Regularization says, “Let’s reduce that reliance a bit.”</p> <p>Let’s dive into the mathematics, but don’t worry, we’ll keep it intuitive.</p> <h4 id="the-core-idea-modifying-the-cost-function">The Core Idea: Modifying the Cost Function</h4> <p>When we train a machine learning model, our goal is usually to minimize a <strong>cost function</strong> (or loss function). This function measures how “wrong” our model’s predictions are compared to the actual values. For instance, in linear regression, we often use the Mean Squared Error (MSE):</p> <p>$ J(\theta) = \frac{1}{2m} \sum<em>{i=1}^{m} (h</em>\theta(x^{(i)}) - y^{(i)})^2 $</p> <p>Here:</p> <ul> <li>$J(\theta)$ is the cost function we want to minimize.</li> <li>$m$ is the number of training examples.</li> <li>$h_\theta(x^{(i)})$ is our model’s prediction for the $i$-th example.</li> <li>$y^{(i)}$ is the actual value for the $i$-th example.</li> <li>$\theta$ represents the model’s parameters (coefficients/weights).</li> </ul> <p>Regularization simply <em>adds another term</em> to this cost function. This new term is the “penalty” for complexity.</p> <p>Let’s look at the two most common types: L1 and L2 regularization.</p> <h4 id="1-l2-regularization-ridge-regression">1. L2 Regularization (Ridge Regression)</h4> <p>L2 regularization adds a penalty term proportional to the square of the magnitude of the coefficients to the cost function.</p> <p>The new cost function looks like this:</p> <p>$ J(\theta) = \underbrace{\frac{1}{2m} \sum<em>{i=1}^{m} (h</em>\theta(x^{(i)}) - y^{(i)})^2}<em>{\text{Original Loss}} + \underbrace{\lambda \sum</em>{j=1}^{n} \theta<em>j^2}</em>{\text{L2 Regularization Term}} $</p> <p>Let’s break down the new term:</p> <ul> <li>$\lambda$ (lambda) is a crucial <strong>hyperparameter</strong> – a value we set <em>before</em> training. It controls the strength of the regularization. A larger $\lambda$ means a stronger penalty on the coefficients.</li> <li>$\sum_{j=1}^{n} \theta_j^2$ is the sum of the squares of all the model’s coefficients (excluding the bias term, $\theta_0$, which is usually not regularized).</li> </ul> <p><strong>Intuition:</strong> By adding $\lambda \sum \theta_j^2$ to the cost, the model is now incentivized not only to fit the data well but also to keep its coefficients small. If a coefficient tries to become very large to fit a specific noisy data point, the squared term will make the overall cost significantly higher, and the optimization algorithm will try to shrink it back down.</p> <p><strong>Effect:</strong> L2 regularization tends to shrink coefficients towards zero, but it rarely makes them <em>exactly</em> zero. It creates a model where all features are still considered, but their influence is toned down. It’s like telling an enthusiastic chef to use a little less salt in every dish, rather than omitting it entirely. It reduces the impact of less important features and spreads the importance more evenly among all features.</p> <h4 id="2-l1-regularization-lasso-regression">2. L1 Regularization (Lasso Regression)</h4> <p>L1 regularization, on the other hand, adds a penalty term proportional to the <em>absolute value</em> of the magnitude of the coefficients.</p> <p>The cost function becomes:</p> <table> <tbody> <tr> <td>$ J(\theta) = \underbrace{\frac{1}{2m} \sum<em>{i=1}^{m} (h</em>\theta(x^{(i)}) - y^{(i)})^2}<em>{\text{Original Loss}} + \underbrace{\lambda \sum</em>{j=1}^{n}</td> <td>\theta*j</td> <td>}*{\text{L1 Regularization Term}} $</td> </tr> </tbody> </table> <p>Key differences:</p> <ul> <li> <table> <tbody> <tr> <td>The regularization term is $\lambda \sum_{j=1}^{n}</td> <td>\theta_j</td> <td>$, the sum of the absolute values of the coefficients.</td> </tr> </tbody> </table> </li> </ul> <p><strong>Intuition:</strong> While L2 shrinks coefficients, L1 has a unique property: it can drive some coefficients to <em>exactly zero</em>.</p> <p><strong>Effect:</strong> This means L1 regularization performs <strong>feature selection</strong>. It effectively identifies the least important features and completely removes their influence from the model by setting their coefficients to zero. It’s like a minimalist decluttering their home, deciding what’s truly essential and discarding everything else. This can lead to simpler, more interpretable models, especially when you have a very large number of features, some of which might be redundant or irrelevant.</p> <h4 id="l1-vs-l2-a-quick-summary">L1 vs. L2: A Quick Summary</h4> <table> <thead> <tr> <th style="text-align: left">Feature</th> <th style="text-align: left">L1 Regularization (Lasso)</th> <th style="text-align: left">L2 Regularization (Ridge)</th> <th> </th> <th> </th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Penalty Term</strong></td> <td style="text-align: left">Sum of absolute values ($</td> <td style="text-align: left">\theta_j</td> <td>$)</td> <td>Sum of squares ($\theta_j^2$)</td> </tr> <tr> <td style="text-align: left"><strong>Effect on Coeffs</strong></td> <td style="text-align: left">Shrinks coefficients; can drive some to <em>exactly zero</em>.</td> <td style="text-align: left">Shrinks coefficients towards zero, but rarely to zero.</td> <td> </td> <td> </td> </tr> <tr> <td style="text-align: left"><strong>Feature Selection</strong></td> <td style="text-align: left">Yes, inherently performs feature selection.</td> <td style="text-align: left">No, keeps all features but reduces their impact.</td> <td> </td> <td> </td> </tr> <tr> <td style="text-align: left"><strong>Model Complexity</strong></td> <td style="text-align: left">Produces sparser models (fewer non-zero coefficients).</td> <td style="text-align: left">Produces models with all features, but less emphasis.</td> <td> </td> <td> </td> </tr> <tr> <td style="text-align: left"><strong>Robustness</strong></td> <td style="text-align: left">More robust to outliers in features.</td> <td style="text-align: left">More sensitive to highly correlated features (spreads impact).</td> <td> </td> <td> </td> </tr> </tbody> </table> <h4 id="the-hyperparameter-lambda-tuning-for-goldilocks">The Hyperparameter $\lambda$: Tuning for Goldilocks</h4> <p>Remember $\lambda$? This little Greek letter holds the key to how much regularization your model applies.</p> <ul> <li>If $\lambda$ is too small (approaching zero), the regularization term has little effect, and your model might still overfit.</li> <li>If $\lambda$ is too large, the regularization term dominates, forcing coefficients to be tiny (or zero). This can lead to <strong>underfitting</strong>, where the model is too simple to capture the underlying patterns in the data – like a student who simplifies concepts too much and misses crucial details.</li> </ul> <p>The goal is to find the “just right” $\lambda$ that balances fitting the training data well with keeping the model simple enough to generalize. This is typically done through techniques like <strong>cross-validation</strong>, where you test different $\lambda$ values on a separate validation set and pick the one that yields the best generalization performance.</p> <h4 id="beyond-l1-and-l2-a-glimpse-at-other-regularization-techniques">Beyond L1 and L2: A Glimpse at Other Regularization Techniques</h4> <p>While L1 and L2 are fundamental, the world of regularization is vast! Here are a few other common ones:</p> <ol> <li> <p><strong>Elastic Net Regularization</strong>: This is a hybrid approach that combines both L1 and L2 penalties. It gets the best of both worlds: the feature selection ability of L1 and the coefficient shrinking and stability of L2. $ J(\theta) = \frac{1}{2m} \sum<em>{i=1}^{m} (h</em>\theta(x^{(i)}) - y^{(i)})^2 + \lambda<em>1 \sum</em>{j=1}^{n} |\theta<em>j| + \lambda_2 \sum</em>{j=1}^{n} \theta_j^2 $</p> </li> <li> <p><strong>Dropout (for Neural Networks)</strong>: In neural networks, dropout randomly “turns off” a fraction of neurons during each training iteration. This forces the network to learn more robust features because it can’t rely on any single neuron or specific combination of neurons. It’s like forming many different “mini-networks” that learn slightly different things, then combining their wisdom.</p> </li> <li> <p><strong>Early Stopping</strong>: This is a simple yet effective technique. You monitor your model’s performance on a separate validation set during training. When the performance on the <em>validation set</em> starts to degrade (even if the training set performance is still improving), you stop training. This prevents the model from continuing to learn noise from the training data.</p> </li> <li> <p><strong>Data Augmentation</strong>: For tasks like image recognition, you can create more training data by applying minor, realistic transformations (rotations, flips, zooms, color shifts) to your existing images. This exposes the model to a wider variety of inputs, making it more robust and less likely to overfit to specific orientations or lighting conditions.</p> </li> </ol> <h3 id="why-regularization-is-non-negotiable-in-your-data-science-toolkit">Why Regularization is Non-Negotiable in Your Data Science Toolkit</h3> <p>In my experience, regularization isn’t just an optional add-on; it’s a fundamental pillar of building robust, deployable machine learning models. Without it, you risk creating models that are academic curiosities – brilliant on paper (or on training data) but useless in the real world.</p> <p>By understanding and applying regularization, you empower your models to:</p> <ul> <li> <strong>Generalize Better</strong>: Make more reliable predictions on unseen data.</li> <li> <strong>Be More Robust</strong>: Less sensitive to noise and outliers in your training data.</li> <li> <strong>Be More Interpretable</strong>: Especially with L1 regularization, you can gain insights into which features truly matter.</li> <li> <strong>Be More Stable</strong>: Less prone to large swings in performance due to minor changes in the data.</li> </ul> <h3 id="final-thoughts-the-art-of-balance">Final Thoughts: The Art of Balance</h3> <p>The journey of a machine learning practitioner is often about finding the right balance: between bias and variance, between underfitting and overfitting, and between simplicity and complexity. Regularization is one of our most powerful tools in striking that balance.</p> <p>It teaches us that sometimes, less is more. By gently constraining our models, we don’t limit their potential; we guide them towards true understanding, ensuring they perform not just brilliantly on homework, but spectacularly on life’s ever-changing final exams.</p> <p>So, next time you’re training a model and see its performance on the validation set starting to plateau or even worsen, remember regularization. It might just be the guiding hand your model needs to become truly wise. Keep experimenting, keep learning, and keep regularizing!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>