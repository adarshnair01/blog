<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Art of Updating Your Beliefs: An Introduction to Bayesian Statistics | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/the-art-of-updating-your-beliefs-an-introduction-t/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Art of Updating Your Beliefs: An Introduction to Bayesian Statistics</h1> <p class="post-meta"> Created on July 24, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/bayesian-statistics"> <i class="fa-solid fa-hashtag fa-sm"></i> Bayesian Statistics</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/probability"> <i class="fa-solid fa-hashtag fa-sm"></i> Probability</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/statistical-inference"> <i class="fa-solid fa-hashtag fa-sm"></i> Statistical Inference</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h3 id="the-art-of-updating-your-beliefs-an-introduction-to-bayesian-statistics">The Art of Updating Your Beliefs: An Introduction to Bayesian Statistics</h3> <p>Hey there, fellow data explorer!</p> <p>Have you ever wondered how we truly learn and adapt our understanding of the world? It’s not just about collecting new information; it’s about <em>integrating</em> that information with what we <em>already believe</em>. If you think about it, that’s how we navigate daily life – from deciding if it’s going to rain, to figuring out if a friend will be late. This intuitive process of updating our beliefs in the face of new evidence is precisely what Bayesian Statistics is all about.</p> <p>For a long time, statistics felt a bit rigid to me. Like a strict referee, telling me what’s “significant” based on hypothetical repetitions. But then I met Bayesian statistics, and it felt like a breath of fresh air. It’s a framework that not only embraces our existing knowledge (our “beliefs”) but also provides a rigorous, mathematical way to update them as we observe new data. It feels more human, more aligned with how we naturally think.</p> <p>So, let’s dive into this captivating world where statistics meets common sense, and learn how to become better “belief-updaters”!</p> <h3 id="two-ways-of-looking-at-probability-a-quick-detour">Two Ways of Looking at Probability: A Quick Detour</h3> <p>Before we get to the star of the show, it’s helpful to understand the foundational difference between two major schools of thought in statistics:</p> <ol> <li> <strong>Frequentist Statistics</strong>: This is what you might encounter more often in introductory courses. It defines probability as the <em>long-run frequency</em> of an event if we were to repeat an experiment infinitely many times. For example, if you say a coin has a 50% chance of landing heads, a Frequentist interprets this as: if you flip the coin an infinite number of times, it will land heads half the time. Crucially, in Frequentist thinking, the “true” probability of an event (like a coin being fair) is a fixed, unknown constant.</li> <li> <strong>Bayesian Statistics</strong>: Ah, here’s where things get interesting! Bayesians view probability as a <em>degree of belief</em> or a measure of plausibility. It’s subjective and can be updated as new evidence comes in. So, for our coin, a Bayesian might say, “Based on what I know, I believe there’s an 80% chance this coin is fair.” This belief isn’t just a hunch; it’s a quantifiable measure that can be updated scientifically.</li> </ol> <p>The key takeaway? Frequentists see probability as objective and physical, while Bayesians see it as a quantifiable measure of our knowledge or uncertainty about a proposition.</p> <h3 id="the-heart-of-bayesianism-bayes-theorem">The Heart of Bayesianism: Bayes’ Theorem</h3> <p>At the core of all Bayesian magic lies a remarkably elegant formula, first articulated by Reverend Thomas Bayes in the 18th century. It’s known simply as <strong>Bayes’ Theorem</strong>:</p> \[P(H|E) = \frac{P(E|H) P(H)}{P(E)}\] <p>Don’t let the symbols intimidate you! Let’s break down what each part means intuitively, and you’ll see just how powerful it is.</p> <ul> <li> <table> <tbody> <tr> <td>$P(H</td> <td>E)$ <strong>(The Posterior)</strong>: This is what we <em>really</em> want to know. It’s the probability of our <strong>Hypothesis (H)</strong> being true <em>given the new Evidence (E)</em> we’ve observed. This is our <em>updated belief</em> after seeing the data.</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>$P(E</td> <td>H)$ <strong>(The Likelihood)</strong>: This tells us how likely it is to observe the <strong>Evidence (E)</strong> <em>if our Hypothesis (H) were true</em>. It’s how well our hypothesis predicts the data we just saw. Stronger predictions lead to a higher likelihood.</td> </tr> </tbody> </table> </li> <li>$P(H)$ <strong>(The Prior)</strong>: This is our <strong>Prior Belief</strong> in the Hypothesis (H) <em>before</em> we’ve seen any new evidence. It’s what we already knew, or believed, or reasonably assumed. This is where your existing knowledge, expert opinion, or even previous experimental results come into play.</li> <li>$P(E)$ <strong>(The Evidence / Marginal Likelihood)</strong>: This is the overall probability of observing the <strong>Evidence (E)</strong>, regardless of whether our specific hypothesis (H) is true. It acts as a normalization constant, ensuring that our posterior probabilities sum to 1. Often, for comparing hypotheses, we don’t need to calculate this term directly because it’s constant for all hypotheses being evaluated against the same evidence.</li> </ul> <p>Think of it like being a detective:</p> <ul> <li> <strong>Prior ($P(H)$)</strong>: You have an initial hunch about who the culprit is.</li> <li> <table> <tbody> <tr> <td>**Likelihood ($P(E</td> <td>H)$)**: You collect a new piece of evidence (e.g., a footprint). How likely would you be to find <em>this specific footprint</em> if your prime suspect (your hypothesis) really was the culprit?</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>**Posterior ($P(H</td> <td>E)$)**: Given that footprint, how much stronger is your belief that your suspect is guilty? This is your updated hunch.</td> </tr> </tbody> </table> </li> </ul> <h3 id="the-bayesian-learning-loop-how-we-get-smarter">The Bayesian Learning Loop: How We Get Smarter</h3> <p>One of the most beautiful aspects of Bayesian statistics is its iterative nature. Once you calculate a <strong>posterior probability</strong> based on some evidence, that posterior can then become your <strong>prior</strong> for the next batch of new evidence!</p> <p>Imagine you’re trying to figure out if a coin is fair.</p> <ol> <li> <strong>Initial Prior</strong>: You might start with a general belief that most coins are fair, so your initial “prior” might lean towards a 50/50 chance for heads/tails.</li> <li> <strong>First Evidence</strong>: You flip the coin 10 times and get 7 heads.</li> <li> <strong>First Posterior</strong>: You use Bayes’ Theorem to update your belief based on these 10 flips. Your belief might now shift slightly away from 50/50 towards it being a bit biased for heads.</li> <li> <strong>New Prior</strong>: This <em>new belief</em> (your first posterior) now becomes your <em>prior</em> for the next set of observations.</li> <li> <strong>Second Evidence</strong>: You flip the coin another 10 times and get 4 heads.</li> <li> <strong>Second Posterior</strong>: You update your belief again, using your previous posterior as the new prior, incorporating this latest data.</li> </ol> <p>With each new piece of evidence, your belief (captured by the posterior distribution) becomes more refined, more accurate, and your uncertainty about the true nature of the coin decreases. This is a powerful, elegant way to <em>learn from data</em>.</p> <h3 id="a-practical-example-is-this-coin-fair">A Practical Example: Is This Coin Fair?</h3> <p>Let’s put this into action with our coin flip example. We want to estimate $\theta$, the true probability of getting heads.</p> <p><strong>Step 1: Formulate Your Prior Belief ($P(\theta)$)</strong></p> <p>Before we flip the coin even once, what do we believe about $\theta$?</p> <ul> <li>Maybe we have no strong opinion, so we assume $\theta$ could be anywhere from 0 to 1 with equal likelihood. This is a <strong>uniform prior</strong>.</li> <li>In Bayesian statistics, we often express priors for probabilities (like $\theta$) using the <strong>Beta distribution</strong>. A Beta distribution is defined by two parameters, $\alpha$ and $\beta$.</li> <li>A uniform prior can be represented as a Beta(1,1) distribution. Its probability density function (PDF) is given by: $ P(\theta) = \frac{\theta^{\alpha-1}(1-\theta)^{\beta-1}}{B(\alpha, \beta)} $ For Beta(1,1): $P(\theta) \propto \theta^{1-1}(1-\theta)^{1-1} = \theta^0 (1-\theta)^0 = 1$. This means all values of $\theta$ are equally likely, just as we wanted.</li> </ul> <p><strong>Step 2: Collect Evidence (Data!) ($E$)</strong></p> <p>Let’s flip the coin 10 times and observe the results. Suppose we get <strong>7 Heads</strong> and <strong>3 Tails</strong>. So, $n = 10$ flips, $k = 7$ heads.</p> <table> <tbody> <tr> <td>**Step 3: Determine the Likelihood ($P(E</td> <td>\theta)$)**</td> </tr> </tbody> </table> <p>How likely is it to observe 7 heads in 10 flips <em>if</em> the true probability of heads is $\theta$? This is a classic <strong>Binomial distribution</strong> problem. The likelihood function is: $ P(E|\theta) = \binom{n}{k} \theta^k (1-\theta)^{n-k} $ For our data: $P(\text{7 Heads in 10 flips}|\theta) = \binom{10}{7} \theta^7 (1-\theta)^{10-7} = \binom{10}{7} \theta^7 (1-\theta)^3 $</p> <table> <tbody> <tr> <td>**Step 4: Calculate the Posterior Belief ($P(\theta</td> <td>E)$)**</td> </tr> </tbody> </table> <p>Now we combine our prior and likelihood using Bayes’ Theorem. $ P(\theta|E) \propto P(E|\theta) P(\theta) $ (We often drop $P(E)$ because it’s a normalizing constant).</p> <p>Let’s plug in our specific distributions: $ P(\theta|E) \propto \left[ \binom{10}{7} \theta^7 (1-\theta)^3 \right] \cdot \left[ \frac{\theta^{1-1}(1-\theta)^{1-1}}{B(1,1)} \right] $ $ P(\theta|E) \propto \theta^7 (1-\theta)^3 \cdot \theta^0 (1-\theta)^0 $ $ P(\theta|E) \propto \theta^{7+0} (1-\theta)^{3+0} $ $ P(\theta|E) \propto \theta^{8-1} (1-\theta)^{4-1} $</p> <p>This looks exactly like another Beta distribution! This is a beautiful property called <strong>conjugacy</strong>. When your prior and likelihood combine to form a posterior of the same distributional family, they are called conjugate priors. For a Beta prior and a Binomial likelihood, the posterior is also a Beta distribution.</p> <p>Our posterior distribution is <strong>Beta(8, 4)</strong>.</p> <p>What does this mean?</p> <ul> <li>Our prior was Beta(1,1), representing complete uncertainty (uniform).</li> <li>After seeing 7 heads and 3 tails, our belief about $\theta$ has shifted. The mean of a Beta($\alpha, \beta$) distribution is $\frac{\alpha}{\alpha+\beta}$. <ul> <li>Prior mean: $\frac{1}{1+1} = 0.5$</li> <li>Posterior mean: $\frac{8}{8+4} = \frac{8}{12} \approx 0.67$</li> </ul> </li> </ul> <p>So, after 10 flips, our “best guess” for the probability of heads has moved from 0.5 (fair) to about 0.67, with our uncertainty also having decreased. If we were to get more data, say another 10 flips resulting in 6 heads and 4 tails, our <em>new prior</em> would be Beta(8,4), and our new posterior would be Beta(8+6, 4+4) = Beta(14,8). Our belief keeps refining!</p> <h3 id="why-bayesian-statistics-shines-in-data-science">Why Bayesian Statistics Shines in Data Science</h3> <p>Now that you’ve grasped the core ideas, let’s talk about why Bayesian statistics is such a powerful tool in a data scientist’s arsenal:</p> <ol> <li> <strong>Incorporating Prior Knowledge</strong>: This is huge! Often, we’re not starting from scratch. We might have domain expertise, results from previous experiments, or expert opinions. Bayesian methods provide a formal way to include this information, leading to more robust models, especially with limited data.</li> <li> <strong>Quantifying Uncertainty</strong>: Instead of just a single “best estimate” (like a point estimate in Frequentist statistics), Bayesian methods give you an entire <em>probability distribution</em> for your parameters. This posterior distribution shows you not only the most probable values but also the range of plausible values and how likely each is. This full picture of uncertainty is incredibly valuable for decision-making.</li> <li> <strong>Small Data Problems</strong>: When you have very little data, Frequentist methods can struggle to provide reliable inferences. Bayesian methods, by allowing you to incorporate prior beliefs, can still yield sensible results even with sparse data, making them invaluable in fields like rare disease studies or early-stage A/B testing.</li> <li> <strong>Intuitive Interpretation</strong>: Statements like “There is a 95% probability that the conversion rate is between 2% and 4%” are natural and directly interpretable for Bayesians. Frequentist confidence intervals, while widely used, are often misinterpreted as such, when their true definition is more nuanced (e.g., “if we repeated the experiment many times, 95% of the calculated intervals would contain the true parameter”).</li> <li> <strong>A Natural Fit for Machine Learning</strong>: Many advanced machine learning techniques, particularly in areas like reinforcement learning and deep learning, are leveraging Bayesian principles for things like uncertainty estimation in predictions, active learning, and more robust model training. Bayesian Optimization and Bayesian Neural Networks are growing areas.</li> </ol> <h3 id="challenges-and-considerations">Challenges and Considerations</h3> <p>While incredibly powerful, Bayesian statistics isn’t without its challenges:</p> <ul> <li> <strong>Choosing Priors</strong>: While priors are a strength, choosing “good” priors can sometimes be tricky. If your prior is very strong and inaccurate, it might take a lot of data to override it.</li> <li> <strong>Computational Complexity</strong>: For complex models, calculating the posterior distribution exactly can be mathematically intractable. This is where advanced computational methods like Markov Chain Monte Carlo (MCMC) come into play, which approximate the posterior by drawing samples. These methods can be computationally intensive.</li> </ul> <h3 id="embracing-the-bayesian-mindset">Embracing the Bayesian Mindset</h3> <p>Bayesian statistics isn’t just a set of equations; it’s a way of thinking. It’s about accepting that uncertainty is inherent, and that our knowledge is constantly evolving. It encourages us to be transparent about our initial assumptions and to rigorously update them as the world provides us with new information.</p> <p>As you continue your journey in data science and machine learning, you’ll find that a Bayesian mindset equips you with a profound and flexible framework for understanding data, making predictions, and quantifying the confidence in your conclusions. It’s a journey of continuous learning, just like life itself.</p> <p>So, go forth, embrace the beautiful dance between your beliefs and the data, and start updating your understanding of the world, one posterior at a time!</p> <p>Happy Bayes-ing!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>