<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> PyTorch vs. TensorFlow: Unveiling the Deep Learning Titans (A Data Scientist's Dilemma) | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/pytorch-vs-tensorflow-unveiling-the-deep-learning/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">PyTorch vs. TensorFlow: Unveiling the Deep Learning Titans (A Data Scientist's Dilemma)</h1> <p class="post-meta"> Created on October 06, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/pytorch"> <i class="fa-solid fa-hashtag fa-sm"></i> PyTorch</a>   <a href="/blog/blog/tag/tensorflow"> <i class="fa-solid fa-hashtag fa-sm"></i> TensorFlow</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey there, fellow explorers of the digital frontier!</p> <p>If you’re anything like me, you’ve probably stood at the precipice of a new deep learning project, staring down the formidable choice: <strong>PyTorch or TensorFlow?</strong> It’s a question that sparks lively debates in forums, fuels late-night coding sessions, and for many of us, defines our initial foray into the fascinating world of neural networks.</p> <p>As someone who’s navigated the exhilarating (and sometimes frustrating!) landscape of data science and machine learning engineering, I’ve had my hands dirty with both. And let me tell you, there’s no single “better” framework. Instead, it’s about understanding their souls, their strengths, and ultimately, choosing the right tool for <em>your</em> specific quest. So, pull up a chair, grab a metaphorical (or real) coffee, and let’s embark on this journey to demystify PyTorch and TensorFlow.</p> <h3 id="my-first-foray-the-deep-learning-awakening">My First Foray: The Deep Learning Awakening</h3> <p>I remember my early days, grappling with convolutional neural networks and the elusive concept of backpropagation. Everything felt like magic until I started digging into how these frameworks actually worked. That’s when the names PyTorch and TensorFlow kept popping up, like two colossal statues guarding the gates of AI innovation. Both promised to make building complex neural networks easier, but they spoke slightly different languages.</p> <p>At their core, both frameworks provide a robust ecosystem for building, training, and deploying deep learning models. They offer specialized data structures (like <code class="language-plaintext highlighter-rouge">tensors</code> or <code class="language-plaintext highlighter-rouge">tf.Tensor</code>s) that are highly optimized for numerical computation on CPUs and especially GPUs, allowing us to perform operations like matrix multiplications at lightning speed.</p> <p>For instance, a simple linear operation, which is the backbone of many neural networks, can be expressed as: $ \mathbf{y} = \mathbf{W}\mathbf{x} + \mathbf{b} $ where $\mathbf{x}$ is the input vector, $\mathbf{W}$ is the weight matrix, $\mathbf{b}$ is the bias vector, and $\mathbf{y}$ is the output. Both PyTorch and TensorFlow excel at executing this (and far more complex operations) efficiently across potentially millions of parameters.</p> <p>But the real divergence, the philosophical split if you will, often boils down to how they handle something called <strong>computation graphs</strong>.</p> <h3 id="computation-graphs-the-blueprint-of-your-model">Computation Graphs: The Blueprint of Your Model</h3> <p>Imagine your neural network as a recipe. Each step—an input, a multiplication, an addition, an activation function—is an ingredient or an instruction. A computation graph is essentially this entire recipe, mapped out. It shows how data flows through your network and how operations depend on each other. This graph is crucial for several reasons:</p> <ol> <li> <strong>Optimization:</strong> The framework can analyze the graph to find efficiencies, like parallelizing operations.</li> <li> <strong>Automatic Differentiation (Autograd):</strong> The graph makes it possible to automatically calculate gradients, which are essential for training neural networks using algorithms like gradient descent. The chain rule of calculus is applied across the graph to determine how much each parameter contributes to the final error.</li> </ol> <p>Here’s where PyTorch and TensorFlow historically took different paths.</p> <h4 id="pytorchs-dynamic-graph-the-eager-chef">PyTorch’s Dynamic Graph: The “Eager Chef”</h4> <p>PyTorch, famously developed by Facebook’s AI Research lab (FAIR), embraced <strong>dynamic computation graphs</strong>, often referred to as “eager execution.” Think of it like a chef who decides the next step in a recipe <em>as they go along</em>. They’re constantly evaluating, adjusting, and cooking, one ingredient at a time.</p> <p>When you write code in PyTorch, the operations are executed immediately. The graph is built on the fly, step by step, as your data flows through the network.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># PyTorch example (pseudo-code)
</span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mf">0.5</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">w</span> <span class="o">+</span> <span class="n">b</span>  <span class="c1"># Operation executes immediately
</span><span class="n">z</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>    <span class="c1"># Another operation executes immediately
</span>
<span class="nf">print</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> <span class="c1"># Output: tensor(11.5000, grad_fn=&lt;SumBackward0&gt;)
</span><span class="n">z</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span> <span class="c1"># Gradients computed on the fly based on the execution path
</span><span class="nf">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span> <span class="c1"># Output: tensor([3., 4.])
</span></code></pre></div></div> <p><strong>What this means for you:</strong></p> <ul> <li> <strong>Pythonic Debugging:</strong> Because operations are run immediately, you can use standard Python debugging tools (like <code class="language-plaintext highlighter-rouge">pdb</code>) to step through your code, inspect tensors, and see exactly what’s happening at each stage. It feels just like writing regular Python code.</li> <li> <strong>Flexibility:</strong> This dynamic nature is fantastic for research and experimentation. You can easily change your model architecture on the fly, handle variable-length inputs (common in NLP), or implement complex control flow (if-else statements, loops) that depend on the data.</li> </ul> <h4 id="tensorflows-static-graph-tf-1x-the-master-plan-chef">TensorFlow’s Static Graph (TF 1.x): The “Master Plan Chef”</h4> <p>TensorFlow, developed by Google, initially championed <strong>static computation graphs</strong>. Imagine our chef again, but this time they meticulously plan out <em>every single step</em> of the recipe, write it down, optimize it, and <em>then</em> execute the entire plan without deviation.</p> <p>In TensorFlow 1.x, you first defined the entire computation graph symbolically. No actual computations happened until you ran a <code class="language-plaintext highlighter-rouge">tf.Session</code> and “fed” data into this pre-defined graph.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># TensorFlow 1.x example (conceptual pseudo-code, not runnable directly without tf.compat.v1)
# Define graph nodes
</span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nc">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">random_normal</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)))</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nc">Variable</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>

<span class="c1"># Define operations that form the graph
</span><span class="n">y</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>

<span class="c1"># Nothing computes yet!
</span>
<span class="c1"># To run:
# with tf.Session() as sess:
#     sess.run(tf.global_variables_initializer())
#     output = sess.run(y, feed_dict={x: [[1.0, 2.0]]})
#     print(output)
</span></code></pre></div></div> <p><strong>What this meant for TF 1.x:</strong></p> <ul> <li> <strong>Optimization &amp; Deployment:</strong> Once defined, the static graph could be optimized extensively (e.g., pruned, merged operations) and easily deployed to different platforms (CPUs, GPUs, TPUs, mobile, web) without requiring the Python interpreter.</li> <li> <strong>Scalability:</strong> Made it easier to distribute computations across multiple devices.</li> <li> <strong>Debugging Challenges:</strong> Debugging could be notoriously difficult. If an error occurred deep within the graph, it was often hard to pinpoint exactly where, as you couldn’t inspect intermediate tensor values as easily.</li> </ul> <h3 id="tensorflow-2x-the-convergence--best-of-both-worlds">TensorFlow 2.x: The Convergence – Best of Both Worlds?</h3> <p>Recognizing the immense popularity and benefits of PyTorch’s eager execution, TensorFlow made a monumental shift with <strong>TensorFlow 2.0</strong>. It now defaults to eager execution! This means you can write and debug TensorFlow code much like PyTorch.</p> <p>However, for production and performance, TF 2.x introduces <code class="language-plaintext highlighter-rouge">tf.function</code>. You can decorate a Python function with <code class="language-plaintext highlighter-rouge">@tf.function</code>, and TensorFlow will trace its execution once, converting it into an efficient, callable TensorFlow graph. This gives you the flexibility of eager execution during development and the performance/deployability benefits of static graphs when you’re ready.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># TensorFlow 2.x example (closer to PyTorch now)
</span><span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">constant</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">])</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">constant</span><span class="p">([</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">])</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">constant</span><span class="p">([</span><span class="mf">0.5</span><span class="p">])</span>

<span class="c1"># Eager execution by default
</span><span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">w</span> <span class="o">+</span> <span class="n">b</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_sum</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> <span class="c1"># Output: tf.Tensor(11.5, shape=(), dtype=float32)
</span>
<span class="nd">@tf.function</span> <span class="c1"># This converts the function into a callable TensorFlow graph
</span><span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">bias</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="nc">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">input_data</span> <span class="o">*</span> <span class="n">weights</span> <span class="o">+</span> <span class="n">bias</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_sum</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
    <span class="n">gradients</span> <span class="o">=</span> <span class="n">tape</span><span class="p">.</span><span class="nf">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="p">[</span><span class="n">weights</span><span class="p">,</span> <span class="n">bias</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">gradients</span>

<span class="c1"># Call the @tf.function wrapped function
</span><span class="n">current_loss</span><span class="p">,</span> <span class="n">current_grads</span> <span class="o">=</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Loss: </span><span class="si">{</span><span class="n">current_loss</span><span class="si">}</span><span class="s">, Grads: </span><span class="si">{</span><span class="n">current_grads</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>This evolution has significantly narrowed the gap between the two frameworks in terms of core development experience.</p> <h3 id="beyond-graphs-ecosystem-and-philosophy">Beyond Graphs: Ecosystem and Philosophy</h3> <p>While computation graphs are a fundamental differentiator, the choice between PyTorch and TensorFlow also boils down to their broader ecosystems, community, and philosophical leanings.</p> <h4 id="pytorchs-strengths-the-researchers-darling">PyTorch’s Strengths: The Researcher’s Darling</h4> <ol> <li> <strong>Pythonic &amp; Intuitive:</strong> If you’re comfortable with Python, PyTorch feels incredibly natural. Its API is designed to be object-oriented and directly mimics Python’s structure, making it easy to learn and use.</li> <li> <strong>Excellent for Research &amp; Prototyping:</strong> The ease of debugging and dynamic graph nature make PyTorch ideal for quickly experimenting with new ideas, iterating on models, and tackling novel research problems. This is why it’s incredibly popular in academia and research labs.</li> <li> <strong>Strong Academic Community:</strong> Many cutting-edge research papers and open-source implementations are initially released in PyTorch.</li> <li> <strong><code class="language-plaintext highlighter-rouge">torchvision</code>, <code class="language-plaintext highlighter-rouge">torchaudio</code>, <code class="language-plaintext highlighter-rouge">torchtext</code>:</strong> These powerful libraries provide pre-trained models, datasets, and utilities for computer vision, audio processing, and natural language processing, accelerating development.</li> </ol> <h4 id="tensorflows-strengths-the-industry-workhorse">TensorFlow’s Strengths: The Industry Workhorse</h4> <ol> <li> <strong>Comprehensive Ecosystem (TF Extended - TFX):</strong> TensorFlow boasts an unparalleled end-to-end platform for the entire ML lifecycle. This includes: <ul> <li> <strong>TensorBoard:</strong> For powerful model visualization and debugging.</li> <li> <strong>TensorFlow Serving:</strong> For high-performance, flexible serving of ML models in production.</li> <li> <strong>TensorFlow Lite:</strong> For deploying models on mobile and embedded devices.</li> <li> <strong>TensorFlow.js:</strong> For running models directly in the browser or Node.js.</li> <li> <strong>TPU Support:</strong> Excellent integration with Google’s custom-built Tensor Processing Units for massive-scale training.</li> </ul> </li> <li> <strong>Production Readiness &amp; Scalability:</strong> TensorFlow has been built from the ground up with large-scale production deployment in mind. Its robust capabilities for distributed training and model serving make it a go-to for enterprise solutions.</li> <li> <strong>Keras Integration:</strong> Keras, a high-level API for building neural networks, is now the official high-level API for TensorFlow. It simplifies model construction, making it very beginner-friendly and abstracting away much of the complexity. You can build powerful models with just a few lines of code.</li> <li> <strong>Mature and Robust:</strong> Being around longer, TensorFlow has a very mature and battle-tested codebase, with extensive documentation and a vast community.</li> </ol> <h3 id="when-to-choose-which-my-two-cents">When to Choose Which? My Two Cents</h3> <p>The choice often boils down to your primary goal:</p> <ul> <li> <strong>Choose PyTorch if:</strong> <ul> <li>You’re a <strong>researcher or student</strong> focused on rapid prototyping, experimentation, and implementing novel architectures.</li> <li>You value <strong>deep understanding</strong> and fine-grained control over your model’s operations, making debugging a breeze.</li> <li>You prefer a highly <strong>Pythonic</strong> and object-oriented approach.</li> <li>Your project involves <strong>dynamic graph structures</strong> (like some RNNs or graph neural networks where the computation path might change based on input).</li> </ul> </li> <li> <strong>Choose TensorFlow if:</strong> <ul> <li>You’re building <strong>large-scale production applications</strong> that require robust deployment, serving, and monitoring tools.</li> <li>You need to deploy models to <strong>mobile devices, web browsers, or edge devices</strong>.</li> <li>You benefit from a <strong>comprehensive ecosystem</strong> that covers the entire ML lifecycle, from data ingestion to deployment.</li> <li>You’re working on a <strong>team where Keras is the standard</strong>, or you want to leverage Google’s TPUs for training.</li> <li>Scalability and efficient distributed training are paramount.</li> </ul> </li> </ul> <h3 id="the-bottom-line-its-not-a-zero-sum-game">The Bottom Line: It’s Not a Zero-Sum Game</h3> <p>In reality, many data scientists and MLEs are becoming proficient in <em>both</em>. The fundamental concepts of deep learning (tensors, backpropagation, neural network architectures) are transferable. Learning one makes learning the other significantly easier, especially with TF 2.x’s convergence towards an eager execution paradigm.</p> <p>For my portfolio, I make it a point to showcase projects in both frameworks. It demonstrates versatility and an understanding that different problems call for different tools.</p> <p>So, don’t let the choice intimidate you. Pick one, dive in, build something amazing, and then explore the other. The deep learning world is vast and exciting, and mastering these frameworks will empower you to create truly transformative AI solutions. Happy coding!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>