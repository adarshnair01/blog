<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Decoding the Giants: A Deep Dive into Large Language Models | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/decoding-the-giants-a-deep-dive-into-large-languag/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Decoding the Giants: A Deep Dive into Large Language Models</h1> <p class="post-meta"> Created on October 31, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/natural-language-processing"> <i class="fa-solid fa-hashtag fa-sm"></i> Natural Language Processing</a>   <a href="/blog/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Deep Learning</a>   <a href="/blog/blog/tag/transformers"> <i class="fa-solid fa-hashtag fa-sm"></i> Transformers</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>As a budding data scientist and machine learning enthusiast, few things have captivated my imagination quite like Large Language Models (LLMs). There’s something almost magical about typing a prompt into a chatbot and receiving a coherent, often insightful, response, or watching an AI generate a poem, summarize an article, or even write code. It feels like a leap into a new era of human-computer interaction, and honestly, it’s exhilarating to be part of it.</p> <p>But what <em>are</em> these digital wizards, really? How do they work? This isn’t magic; it’s a testament to incredible advancements in deep learning, massive datasets, and sheer computational power. Let’s embark on a journey to understand the architecture, training, and potential of LLMs, from their foundational concepts to the cutting-edge techniques that give them their voice.</p> <h3 id="whats-so-large-about-large-language-models">What’s So “Large” About Large Language Models?</h3> <p>Before we dive into the “how,” let’s grasp the “large.” When we talk about LLMs, we’re talking about models with an astronomical number of parameters – the numerical values that the model learns during training. While early neural networks might have had thousands or millions of parameters, LLMs like GPT-3 boast 175 billion, and newer models are pushing into the trillions.</p> <p>This isn’t just about size for size’s sake. More parameters generally mean a greater capacity for the model to learn complex patterns and relationships within data.</p> <p>Then there’s the training data. LLMs are trained on truly colossal datasets, often petabytes of text and code scraped from the internet – books, articles, websites, conversations, scientific papers, you name it. Imagine feeding a model a significant chunk of all human-generated text publicly available! This vast diet of information allows them to learn grammar, facts, reasoning, and even subtle nuances of human communication.</p> <p>Finally, the “large” extends to the computational resources required. Training these models demands supercomputing clusters packed with thousands of high-end GPUs, crunching data for weeks or even months. It’s an engineering marvel in itself.</p> <h3 id="from-simple-neurons-to-deep-understanding-a-quick-recap">From Simple Neurons to Deep Understanding: A Quick Recap</h3> <p>To understand LLMs, we first need to appreciate their roots in neural networks. Remember how a single neuron in a neural network takes inputs, applies weights, sums them up, and passes them through an activation function? And how multiple layers of these neurons can learn incredibly complex patterns? That’s the core idea.</p> <p>For processing sequences like text, early deep learning models used Recurrent Neural Networks (RNNs) and their more advanced cousins, Long Short-Term Memory (LSTMs) networks. These models had a “memory” that allowed them to process words one by one, carrying information from previous words to influence the current prediction.</p> <p>However, RNNs and LSTMs struggled with very long sequences. Information from the beginning of a long sentence would often “fade” by the time the model reached the end. This is known as the “long-term dependency problem.” Imagine trying to hold an entire paragraph in your short-term memory to understand the meaning of the last word – it’s tough!</p> <h3 id="the-breakthrough-the-transformer-architecture">The Breakthrough: The Transformer Architecture</h3> <p>The real game-changer for LLMs came with the introduction of the <strong>Transformer architecture</strong> in 2017, detailed in the seminal paper “Attention Is All You Need.” This architecture completely revolutionized how sequence data is processed, largely by doing away with sequential processing and embracing parallelization.</p> <p>The core idea? Instead of trying to process words in order, let the model look at <em>all</em> words in a sentence simultaneously and figure out how important each word is to every other word. This “looking at all words” is achieved through a mechanism called <strong>Self-Attention</strong>.</p> <h4 id="self-attention-paying-attention-to-what-matters">Self-Attention: Paying Attention to What Matters</h4> <p>Imagine you’re reading the sentence: “The animal didn’t cross the street because it was too tired.” What does “it” refer to? As a human, you immediately know it refers to “the animal.” How? Because you “pay attention” to “animal” when you see “it.”</p> <p>Self-attention works similarly. For each word in an input sequence, it calculates a score of how much it should “pay attention” to every other word in the sequence, including itself, to understand its context.</p> <p>Let’s break it down a little with some math, but don’t worry, we’ll keep it intuitive. For each word (or more accurately, each “token” – a word or sub-word unit) in the input, we create three different vectors:</p> <ol> <li> <strong>Query (Q):</strong> What I’m looking for.</li> <li> <strong>Key (K):</strong> What I have.</li> <li> <strong>Value (V):</strong> The actual information I’m passing along.</li> </ol> <p>These vectors are learned during training. To determine how much attention word $i$ should pay to word $j$, we essentially perform a dot product between the Query vector of word $i$ and the Key vector of word $j$. A higher dot product means more relevance.</p> <p>The full self-attention calculation for a sequence is often expressed as:</p> <p>$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$</p> <p>Let’s unpack this:</p> <ul> <li>$Q$, $K$, $V$ are matrices where each row corresponds to the Query, Key, or Value vector for a token in the sequence.</li> <li>$QK^T$: This computes the dot product similarity scores between all pairs of queries and keys. It tells us how relevant each word is to every other word.</li> <li>$\sqrt{d_k}$: This is a scaling factor (where $d_k$ is the dimension of the key vectors) that helps stabilize gradients during training.</li> <li>$softmax(…)$: This function turns the raw scores into a probability distribution, ensuring all attention weights sum to 1 for each query. This gives us the “attention weights” – how much each word should pay attention to others.</li> <li>The result is then multiplied by $V$: We essentially take a weighted sum of the Value vectors, where the weights are the attention scores. If word ‘A’ pays a lot of attention to word ‘B’, then ‘B’s Value vector will contribute more to ‘A’s new representation.</li> </ul> <h4 id="multi-head-attention-diverse-perspectives">Multi-Head Attention: Diverse Perspectives</h4> <p>Instead of just one set of Q, K, V matrices, Transformers use “Multi-Head Attention.” This means they run the self-attention mechanism multiple times in parallel, each with different learned Q, K, V matrices. Think of it like having several “lenses” through which the model views the relationships between words. Each head might learn to focus on different aspects: one head might learn grammatical dependencies, another might focus on semantic relationships. The outputs from all heads are then concatenated and linearly transformed.</p> <h4 id="positional-encoding-preserving-order">Positional Encoding: Preserving Order</h4> <p>Self-attention, by its nature, processes all words simultaneously, losing their original order. But word order is crucial for meaning! “Dog bites man” is very different from “Man bites dog.” To solve this, Transformers inject <strong>Positional Encodings</strong> – numerical vectors added to the input word embeddings – that contain information about each word’s position in the sequence. These can be fixed patterns (like sinusoidal functions) or learned embeddings.</p> <h3 id="the-decoder-only-transformer-the-heart-of-generative-llms">The Decoder-Only Transformer: The Heart of Generative LLMs</h3> <p>The original Transformer had an Encoder-Decoder structure, great for tasks like machine translation. However, most modern generative LLMs (like the GPT series) use a <strong>Decoder-Only</strong> architecture.</p> <p>A Decoder-Only Transformer predicts the <em>next</em> word based on all the <em>previous</em> words. This is achieved using a slight modification to self-attention called <strong>masked self-attention</strong>. When the model is predicting the next word, it’s “masked” or prevented from “seeing” any future words in the sequence. This ensures it only attends to what has already been generated or input.</p> <h3 id="training-an-llm-the-dance-of-prediction-and-alignment">Training an LLM: The Dance of Prediction and Alignment</h3> <p>How do these colossal models actually learn? The training process generally involves two main stages:</p> <ol> <li> <strong>Pre-training (The Heavy Lifting):</strong> <ul> <li> <strong>Objective:</strong> The primary goal is <strong>causal language modeling</strong>. The model is given a massive amount of raw text and trained to predict the next word in a sequence, given the preceding words. For example, if it sees “The cat sat on the…”, it should predict “mat” (or a probability distribution over possible next words, where “mat” has a high probability).</li> <li> <strong>Loss Function:</strong> This prediction task uses a loss function (like cross-entropy) that measures how far off the model’s prediction is from the actual next word. The model then uses techniques like <strong>gradient descent</strong> and <strong>backpropagation</strong> to adjust its billions of parameters, slowly getting better at predicting the next word.</li> <li> <strong>Emergent Abilities:</strong> This seemingly simple task, performed on trillions of tokens across vast datasets, somehow imbues the model with incredible general-purpose linguistic abilities – understanding context, generating coherent text, even exhibiting some forms of reasoning. These are often called “emergent properties” because they aren’t explicitly programmed but arise from the scale of the model and data.</li> </ul> </li> <li> <strong>Fine-tuning (Refining for Specific Tasks and Human Alignment):</strong> <ul> <li> <strong>Supervised Fine-Tuning (SFT):</strong> After pre-training, an LLM is often further fine-tuned on smaller, high-quality, task-specific datasets. For instance, to make it good at summarization, you might fine-tune it on many examples of “document, summary” pairs.</li> <li> <strong>Reinforcement Learning from Human Feedback (RLHF):</strong> This is a crucial step for making LLMs helpful, harmless, and honest – the “chat” experience we often interact with. <ul> <li>Humans rank or score different model responses to a prompt.</li> <li>This human feedback is used to train a separate <strong>reward model</strong>, which learns to predict human preferences.</li> <li>Finally, the LLM itself is fine-tuned using reinforcement learning (e.g., Proximal Policy Optimization or PPO), using the reward model to guide its learning. The LLM tries to generate responses that maximize the reward predicted by the reward model, effectively aligning itself with human values and instructions. This is why models like ChatGPT feel so conversational and helpful.</li> </ul> </li> </ul> </li> </ol> <h3 id="the-amazing-capabilities-and-lingering-challenges">The Amazing Capabilities and Lingering Challenges</h3> <p>LLMs, particularly those with RLHF, can perform an astonishing array of tasks:</p> <ul> <li> <strong>Text Generation:</strong> Writing stories, poems, emails, articles.</li> <li> <strong>Summarization:</strong> Condensing long texts into key points.</li> <li> <strong>Translation:</strong> Translating between languages.</li> <li> <strong>Question Answering:</strong> Providing factual or conceptual answers.</li> <li> <strong>Code Generation:</strong> Writing code snippets, debugging.</li> <li> <strong>Creative Brainstorming:</strong> Generating ideas, outlines.</li> </ul> <p>However, they are not without limitations:</p> <ul> <li> <strong>Hallucinations:</strong> LLMs can confidently generate false information, making things up that sound plausible but are factually incorrect. They are pattern-matching machines, not truth-tellers.</li> <li> <strong>Bias:</strong> As they are trained on internet data, they can inherit and perpetuate biases present in that data (e.g., gender, racial, cultural stereotypes).</li> <li> <strong>Lack of Real-World Understanding:</strong> They don’t “understand” the world in a human sense; they understand statistical relationships between words. They don’t experience gravity or emotions.</li> <li> <strong>Computational Cost:</strong> Training and even running large LLMs is very expensive in terms of energy and hardware.</li> <li> <strong>Ethical Concerns:</strong> Misinformation, misuse for malicious purposes, and potential job displacement are significant societal challenges that require careful consideration.</li> </ul> <h3 id="looking-ahead-the-future-is-bright-and-complex">Looking Ahead: The Future is Bright and Complex</h3> <p>The field of LLMs is evolving at a breakneck pace. We’re seeing advancements in:</p> <ul> <li> <strong>Multimodality:</strong> Models that can process and generate not just text, but also images, audio, and video.</li> <li> <strong>Efficiency:</strong> Developing smaller, more efficient models that can run on less powerful hardware.</li> <li> <strong>Reliability:</strong> Techniques to improve factuality and reduce hallucinations.</li> <li> <strong>Control:</strong> Better methods for users to guide and constrain model behavior.</li> </ul> <p>For me, the journey into LLMs has been a rollercoaster of wonder and intellectual challenge. From decoding the elegance of the Transformer to marveling at the emergent intelligence from sheer scale, it’s a field brimming with possibilities. As data scientists and machine learning engineers, we have a unique opportunity to shape this future, harnessing these powerful tools responsibly and innovatively.</p> <p>So, the next time you interact with an LLM, remember the billions of parameters, the petabytes of data, and the ingenious attention mechanisms working tirelessly beneath the surface. It’s not magic, it’s a testament to human ingenuity and the relentless pursuit of making machines understand and communicate like us. And we’re just getting started.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>