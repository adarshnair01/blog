<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Navigating the Forest of Decisions: Unraveling Random Forests | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/2024/navigating-the-forest-of-decisions-unraveling-rand/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Navigating the Forest of Decisions: Unraveling Random Forests</h1> <p class="post-meta"> Created on November 23, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/random-forests"> <i class="fa-solid fa-hashtag fa-sm"></i> Random Forests</a>   <a href="/blog/blog/tag/ensemble-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Ensemble Learning</a>   <a href="/blog/blog/tag/decision-trees"> <i class="fa-solid fa-hashtag fa-sm"></i> Decision Trees</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>Welcome back to my corner of the data science universe. Today, I’m thrilled to share insights into an algorithm that consistently blows me away with its elegance and power: <strong>Random Forests</strong>. If you’ve ever felt overwhelmed by data, or wondered how machines seem to “know” so much, you’re in for a treat. This isn’t just about crunching numbers; it’s about building a robust decision-making system inspired by the wisdom of crowds.</p> <p>Imagine you’re trying to decide what movie to watch tonight. You could ask <em>one</em> friend who’s a self-proclaimed movie critic. Their opinion might be strong, but what if their taste is super niche, or they only watch horror films when you’re in the mood for comedy? You might end up with a terrible recommendation.</p> <p>Now, what if you asked <em>a hundred</em> friends? Some love action, some adore romance, some only watch documentaries. Each friend gives you a recommendation, perhaps based on their unique experiences and preferences. If you then go with the movie that the <em>majority</em> of your friends recommend, chances are, you’ll pick something much more broadly appealing and enjoyable.</p> <p>This, in a nutshell, is the core philosophy behind Random Forests. Instead of relying on one “expert” decision-maker, it leverages the collective wisdom of many simpler “experts” to make incredibly robust and accurate predictions.</p> <h3 id="the-lone-tree-our-first-expert--decision-trees">The Lone Tree: Our First Expert – Decision Trees</h3> <p>Before we get lost in the forest, let’s understand the individual trees. The building block of a Random Forest is the <strong>Decision Tree</strong>. Think of a Decision Tree like a flowchart. You start at the ‘root’ of the tree, ask a question, and based on the answer, you move down a specific path to another question, and so on, until you reach a ‘leaf’ node that gives you a final decision or prediction.</p> <p>Let’s use a super simple example: deciding if you should bring an umbrella today.</p> <ul> <li> <strong>Is it raining?</strong> <ul> <li> <strong>Yes:</strong> Bring an umbrella. (Leaf node: YES)</li> <li> <strong>No:</strong> <ul> <li> <strong>Is the sky cloudy?</strong> <ul> <li> <strong>Yes:</strong> Bring an umbrella (just in case!). (Leaf node: YES)</li> <li> <strong>No:</strong> Leave the umbrella at home. (Leaf node: NO)</li> </ul> </li> </ul> </li> </ul> </li> </ul> <p>Each question is a ‘split’ based on a feature (e.g., ‘raining’, ‘cloudy’). The goal is to make splits that best separate your data into pure groups. For example, if ‘raining’ is a perfect predictor of needing an umbrella, that’s a great split!</p> <p>In technical terms, decision trees recursively partition the input space. At each node, they select the feature and split point that best divides the data, often minimizing a metric like <strong>Gini Impurity</strong> or maximizing <strong>Information Gain (Entropy)</strong>.</p> <ul> <li> <strong>Gini Impurity</strong> measures how often a randomly chosen element from the set would be incorrectly labeled if it were randomly labeled according to the distribution of labels in the subset. A Gini impurity of 0 means all elements belong to a single class (perfectly pure).</li> <li> <strong>Entropy</strong> measures the randomness or uncertainty in a set of observations. Higher entropy means more uncertainty. Information Gain is the reduction in entropy achieved by making a split.</li> </ul> <p>Decision trees are intuitive and easy to understand. They can even handle different types of data (numerical, categorical) seamlessly. However, they have a major Achilles’ heel: <strong>overfitting</strong>. A single deep decision tree can become overly complex, learning the training data <em>too</em> well, including its noise and idiosyncrasies. It’s like our movie critic friend who knows every obscure detail about their niche genre but can’t recommend a general crowd-pleaser. When presented with new, unseen data, its performance often tanks.</p> <h3 id="the-wisdom-of-the-crowd-ensemble-learning">The Wisdom of the Crowd: Ensemble Learning</h3> <p>This is where the magic of “ensemble learning” comes in. Instead of relying on a single, potentially overfit, decision tree, we combine the predictions from many different trees. This strategy is called <strong>Bagging</strong>, short for <strong>Bootstrap Aggregating</strong>.</p> <h4 id="step-1-bootstrap--creating-diverse-training-sets-d_k">Step 1: Bootstrap – Creating Diverse Training Sets ($D_k^*$)</h4> <p>Imagine you have a dataset of 100 movie ratings. Instead of training one tree on all 100 ratings, we create multiple <em>new</em> datasets. For each new dataset, we randomly sample 100 ratings <em>with replacement</em> from the original 100. This means some ratings might appear multiple times in a new dataset, while others might not appear at all.</p> <p>This process is called <strong>bootstrapping</strong>. If our original dataset is $D$ with $N$ samples, we create $N_{trees}$ new datasets, $D_k^<em>$, each by sampling $N$ times with replacement from $D$. Each $D_k^</em>$ will be slightly different from the original $D$ and from each other. This diversity is crucial because it ensures that each tree trained on these subsets will also be slightly different.</p> <h4 id="step-2-aggregating--combining-predictions">Step 2: Aggregating – Combining Predictions</h4> <p>Once we have $N_{trees}$ individual decision trees, each trained on its unique bootstrapped dataset, how do we combine their predictions?</p> <ul> <li>For <strong>classification</strong> problems (like predicting if a customer will churn or not), we use a <strong>majority vote</strong>. If 70 out of 100 trees predict “churn,” then the Random Forest predicts “churn.”</li> <li>For <strong>regression</strong> problems (like predicting house prices), we typically take the <strong>average</strong> of all tree predictions.</li> </ul> <p>This aggregation step significantly reduces the variance of our model, making it more robust and less prone to overfitting than any single tree.</p> <h3 id="the-random-twist-feature-randomness">The “Random” Twist: Feature Randomness</h3> <p>Bagging alone is powerful, but Random Forests add another layer of randomness that makes them truly exceptional: <strong>feature randomness</strong> (also known as random subspace method).</p> <p>When a decision tree is being built, at <em>each split</em> (each question in our flowchart), instead of considering all available features to find the best split, the Random Forest algorithm only considers a random <em>subset</em> of features.</p> <p>Let’s say you have 10 features (e.g., ‘genre’, ‘director’, ‘actor’, ‘IMDB_score’, ‘budget’, etc.). When a tree needs to decide its next split, it might randomly pick only 3 of those 10 features and choose the best split from <em>only those 3</em>.</p> <p>Why do this? It further decorrelates the trees. If there’s one overwhelmingly strong feature (like ‘IMDB_score’), every single decision tree in a bagged ensemble might choose that feature as its first split. This would make all the trees very similar, limiting the benefit of combining them. By forcing each tree to consider only a random subset of features at each split, we ensure that the trees are much more diverse and unique, even if some features are dominant. It’s like having different experts who specialize in different aspects of the problem.</p> <h3 id="the-random-forest-algorithm-putting-it-all-together">The Random Forest Algorithm: Putting It All Together</h3> <p>So, here’s the complete recipe for building a Random Forest:</p> <ol> <li> <strong>Decide on the number of trees</strong> you want to grow (let’s call it $N_{trees}$, typically 100-500).</li> <li>For each of the $N_{trees}$: <ul> <li> <strong>Bootstrap a sample</strong> of your training data. This means creating a new dataset $D_k^<em>$ by sampling $N$ data points *with replacement</em> from your original training data $D$.</li> <li> <strong>Grow a decision tree</strong> ($h_k$) on this bootstrapped dataset $D_k^*$.</li> <li> <strong>At each node</strong> of the tree, instead of considering all $M$ features, randomly select a small subset of $m$ features (where $m \ll M$, usually $m = \sqrt{M}$ for classification or $m = M/3$ for regression). Find the best split <em>only among these $m$ features</em>.</li> <li> <strong>Grow the tree to its maximum depth</strong> without pruning. This is counter-intuitive for single trees but essential here: because we’re averaging many trees, individual overfit trees average out their errors.</li> </ul> </li> <li>To make a <strong>prediction</strong> for a new input $\mathbf{x}$: <ul> <li>Each of the $N_{trees}$ trees predicts an output, $h_k(\mathbf{x})$.</li> <li>For <strong>classification</strong>, the final prediction $\hat{y}$ is the <strong>majority vote</strong> of all tree predictions: $\hat{y} = \text{mode} {h_k(\mathbf{x})}<em>{k=1}^{N</em>{trees}}$</li> <li>For <strong>regression</strong>, the final prediction $\hat{y}$ is the <strong>average</strong> of all tree predictions: $\hat{y} = \frac{1}{N_{trees}} \sum_{k=1}^{N_{trees}} h_k(\mathbf{x})$</li> </ul> </li> </ol> <h3 id="why-random-forests-are-so-powerful">Why Random Forests Are So Powerful</h3> <p>Random Forests have become a go-to algorithm in many data science challenges, and for good reason:</p> <ul> <li> <strong>Reduced Overfitting:</strong> The combination of bagging and feature randomness effectively tackles the overfitting problem that plagues individual decision trees. By averaging many diverse, high-variance trees, the overall model’s variance is greatly reduced, leading to better generalization on unseen data.</li> <li> <strong>High Accuracy:</strong> They often achieve very high accuracy compared to many other algorithms.</li> <li> <strong>Handles High Dimensionality:</strong> They can manage datasets with a large number of features without much struggle.</li> <li> <strong>Robust to Noise and Missing Data:</strong> The ensemble nature makes them less sensitive to noisy data or missing values in the dataset.</li> <li> <strong>Feature Importance:</strong> Random Forests provide a way to estimate the importance of each feature. By observing how much each feature reduces impurity (e.g., Gini impurity) across all trees, we can rank features by their predictive power. This is incredibly useful for understanding your data!</li> <li> <strong>Parallelizable:</strong> Each tree is built independently, meaning the process of training can be easily parallelized, making it efficient on modern computing architectures.</li> </ul> <h3 id="when-to-use-and-not-use-random-forests">When to Use (and Not Use) Random Forests</h3> <p><strong>Use them when:</strong></p> <ul> <li>You need a highly accurate model.</li> <li>Your data has a mix of numerical and categorical features.</li> <li>You want an algorithm that is relatively robust to outliers and noise.</li> <li>You want to understand which features are most important.</li> <li>You’re looking for a good default model to start with, especially when unsure which algorithm to pick.</li> </ul> <p><strong>Consider alternatives when:</strong></p> <ul> <li> <strong>Interpretability is paramount:</strong> While feature importance helps, understanding the exact reasoning behind a specific prediction from a Random Forest (a ‘black box’ for individual predictions) is much harder than with a single decision tree or a linear model.</li> <li> <strong>Speed of prediction is critical:</strong> For very large numbers of trees, predicting with a Random Forest can be slower than simpler models.</li> <li> <strong>Your dataset is extremely large:</strong> Training many trees can be computationally intensive, though parallelization helps.</li> </ul> <h3 id="key-parameters-to-tweak-conceptual">Key Parameters to Tweak (Conceptual)</h3> <p>When you implement a Random Forest, you’ll encounter a few important parameters:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">n_estimators</code>: The number of trees in the forest. More trees generally lead to better performance but increase computation time.</li> <li> <code class="language-plaintext highlighter-rouge">max_features</code>: The number of features to consider at each split. This is the ‘m’ we talked about. Common choices are <code class="language-plaintext highlighter-rouge">sqrt(M)</code> for classification and <code class="language-plaintext highlighter-rouge">M/3</code> for regression.</li> <li> <code class="language-plaintext highlighter-rouge">max_depth</code>: The maximum depth of each tree. Often, you let trees grow fully (or set a very high value) because the ensemble handles overfitting.</li> <li> <code class="language-plaintext highlighter-rouge">min_samples_leaf</code>: The minimum number of samples required to be at a leaf node. This can help prevent individual trees from becoming too specific.</li> </ul> <p>Here’s a conceptual peek at how you might use it in a popular library like scikit-learn in Python:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Just a conceptual idea, not runnable code without data
</span><span class="kn">from</span> <span class="n">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Assume X and y are your features and target variable
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
</span>
<span class="c1"># Initialize the Random Forest model
# n_estimators=100 means we'll build 100 decision trees
# random_state ensures reproducibility of the random sampling
</span><span class="n">model</span> <span class="o">=</span> <span class="nc">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="sh">'</span><span class="s">sqrt</span><span class="sh">'</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Train the model on your training data
# model.fit(X_train, y_train)
</span>
<span class="c1"># Make predictions on new data
# predictions = model.predict(X_test)
</span></code></pre></div></div> <h3 id="wrapping-up">Wrapping Up</h3> <p>Random Forests are a beautiful demonstration of how combining simple, diverse models can create a remarkably powerful and accurate prediction system. They harness the power of randomness and collective intelligence to build a robust model that stands strong against the complexities of real-world data.</p> <p>From identifying diseases to recommending products or predicting stock prices, Random Forests are silently powering many intelligent systems around us. My journey with this algorithm has been incredibly rewarding, and I hope this deep dive encourages you to explore its fascinating capabilities further.</p> <p>Now it’s your turn to wander into the forest and see what insights you can uncover! Happy modeling!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>