<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Wisdom of Crowds in AI: Unveiling Ensemble Learning | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/the-wisdom-of-crowds-in-ai-unveiling-ensemble-lear/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Wisdom of Crowds in AI: Unveiling Ensemble Learning</h1> <p class="post-meta"> Created on April 17, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/ensemble-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Ensemble Learning</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   <a href="/blog/blog/tag/algorithms"> <i class="fa-solid fa-hashtag fa-sm"></i> Algorithms</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey everyone!</p> <p>Welcome back to the portfolio journal where we demystify the magic behind modern AI. Today, I want to share one of my absolute favorite concepts in machine learning – one that, to me, embodies the true spirit of collective intelligence: <strong>Ensemble Learning</strong>.</p> <p>Imagine you’re trying to predict the outcome of a complex situation, say, whether a new movie will be a box-office hit. Would you trust the opinion of just one movie critic, no matter how renowned? Or would you feel more confident if you heard predictions from a panel of ten diverse critics, each with their own unique taste and analytical style?</p> <p>Most likely, you’d prefer the panel. Some critics might focus on acting, others on direction, special effects, or the screenplay. Their individual opinions might sometimes be off, but when you combine their varied perspectives, their collective judgment often turns out to be remarkably accurate and robust.</p> <p>This isn’t just a human phenomenon; it’s a profound principle that machine learning engineers harness daily to build incredibly powerful and reliable models. Welcome to the world of Ensemble Learning.</p> <h3 id="what-is-ensemble-learning-anyway">What is Ensemble Learning, Anyway?</h3> <p>At its core, Ensemble Learning is the process of combining predictions from multiple individual machine learning models (often called “base learners” or “weak learners”) to achieve a single, more robust, and more accurate prediction than any individual model could achieve alone.</p> <p>Think of it as creating a “supermodel” by bringing together a team of diverse, individual models. Each individual model might have its strengths and weaknesses, but when they work together, their combined intelligence far surpasses their individual capabilities.</p> <p><strong>Why do we bother with this complexity?</strong> Because, in practice, it dramatically improves model performance, reduces the risk of overfitting (where a model learns the training data too well and fails on new data), and makes predictions more stable.</p> <h3 id="the-power-duo-diversity--accuracy">The Power Duo: Diversity + Accuracy</h3> <p>The success of any ensemble hinges on two critical factors, much like our panel of movie critics:</p> <ol> <li> <strong>Diversity:</strong> The individual models in your ensemble should be diverse. They should ideally make different types of errors on different parts of the data. If all your critics always agree, you don’t gain much from having a panel! This diversity can come from using different algorithms, training them on different subsets of the data, or even feeding them different features.</li> <li> <strong>Accuracy:</strong> While we want diversity, we don’t want completely clueless models. Each individual model should be at least “weakly accurate,” meaning it performs better than random guessing. If your critics are consistently wrong, their combined opinion won’t help!</li> </ol> <p>When you have a collection of diverse, reasonably accurate models, their errors tend to “cancel out” when their predictions are aggregated. If one model makes a mistake, another might correct it, leading to a much better overall decision.</p> <h3 id="the-grand-strategies-of-ensemble-learning">The Grand Strategies of Ensemble Learning</h3> <p>There are several fascinating ways to build these powerful model teams. Let’s dive into the three most common and influential types:</p> <h4 id="1-bagging-bootstrap-aggregating">1. Bagging (Bootstrap Aggregating)</h4> <p>Imagine you’re teaching a class of students a complex subject. Instead of giving them all the same textbook and tests, you create many slightly different versions of the textbook by sampling chapters and topics with replacement, and then you give each student one of these personalized “textbooks” to study. When it’s time for the final exam, you average their scores or take a majority vote on answers. That’s the essence of Bagging!</p> <p><strong>How it works:</strong> Bagging creates multiple versions of your training dataset by sampling with replacement (this is called “bootstrapping”). Each base model is then trained independently on one of these bootstrapped datasets. Because each dataset is slightly different, each model learns a slightly different “view” of the problem.</p> <p>For <strong>regression problems</strong>, the predictions from all base models are simply averaged. For <strong>classification problems</strong>, a majority vote determines the final prediction.</p> <p>One of the most famous and powerful algorithms built on bagging is the <strong>Random Forest</strong>.</p> <h5 id="deep-dive-random-forest"><strong>Deep Dive: Random Forest</strong></h5> <p>A Random Forest takes bagging to the next level, specifically for decision trees. Besides training each tree on a bootstrapped subset of data, it also introduces randomness in feature selection. When a tree is deciding how to split a node, it only considers a random subset of all available features.</p> <p>This dual source of randomness (data sampling + feature sampling) makes the individual decision trees highly diverse and decorrelated.</p> <p><strong>Why is this so effective?</strong> Decision trees, on their own, are prone to overfitting. They can become too specialized to the training data. By averaging many deep, diverse trees in a Random Forest, we drastically reduce the <em>variance</em> of the model without significantly increasing its <em>bias</em>.</p> <p><strong>A bit of Math Intuition for Bagging:</strong> If you have $M$ independent models, and each model has a variance of $\sigma^2$ and their errors are uncorrelated, then the variance of their average prediction is given by:</p> <p>$Var(\frac{1}{M}\sum_{i=1}^M X_i) = \frac{1}{M^2}\sum_{i=1}^M Var(X_i) = \frac{1}{M^2} \cdot M \sigma^2 = \frac{\sigma^2}{M}$</p> <p>This simplified formula beautifully illustrates why bagging reduces variance: by averaging $M$ independent predictions, the overall variance is reduced by a factor of $M$. While real-world models aren’t perfectly independent, the principle holds, and the reduction in variance is significant.</p> <h4 id="2-boosting">2. Boosting</h4> <p>Now, let’s switch gears. Imagine a student who keeps making mistakes on a particular type of math problem. Instead of giving them a new, full textbook each time, a dedicated tutor identifies exactly where they went wrong, explains the concept, and provides <em>more focused practice</em> on those specific challenging problems. Each time, the student gets a bit better, specifically targeting their weaknesses. This iterative, corrective process is the heart of Boosting.</p> <p><strong>How it works:</strong> Boosting builds an ensemble sequentially. Each new base model is trained to <strong>correct the errors</strong> made by the previous models in the sequence. Models aren’t independent; they learn from the collective mistakes of their predecessors.</p> <p>The core idea is to give more “weight” or importance to the data points that were misclassified or poorly predicted by the previous models. This forces the subsequent models to focus their learning on these challenging instances.</p> <h5 id="deep-dive-adaboost-adaptive-boosting"><strong>Deep Dive: AdaBoost (Adaptive Boosting)</strong></h5> <p>AdaBoost was one of the first successful boosting algorithms. It works by:</p> <ol> <li>Training an initial weak learner on the original dataset.</li> <li>After the first learner makes predictions, it increases the “weight” of the misclassified data points.</li> <li>A second weak learner is trained, specifically focusing on these newly emphasized, difficult data points.</li> <li>This process continues for many iterations, with each new learner trying to fix the mistakes of the combined ensemble that came before it.</li> <li>Finally, all the weak learners’ predictions are combined using a weighted majority vote, where learners that performed better get more say.</li> </ol> <p><strong>Math Intuition for AdaBoost:</strong> In AdaBoost, after each learner $h_m(x)$ is trained, its error rate $\epsilon_m$ is calculated. Based on this error, a weight $\alpha_m$ is assigned to the learner: $\alpha_m = \frac{1}{2} \ln \left( \frac{1 - \epsilon_m}{\epsilon_m} \right)$ Then, the weights of the misclassified samples ($w_i$) are updated to give them more importance in the next training iteration: $w_i \leftarrow w_i \cdot e^{\alpha_m}$ (if sample $i$ was misclassified) The final prediction is a weighted sum of all base learners: $H(x) = \text{sign}\left(\sum_{m=1}^M \alpha_m h_m(x)\right)$.</p> <h5 id="deep-dive-gradient-boosting-machines-gbm"><strong>Deep Dive: Gradient Boosting Machines (GBM)</strong></h5> <p>Gradient Boosting is a more generalized and widely used boosting framework. Instead of focusing on sample weights like AdaBoost, GBM focuses on <strong>residuals</strong>.</p> <p>Here’s the simplified idea:</p> <ol> <li>Train an initial model $F_0(x)$ (often just the mean of the target variable).</li> <li>Calculate the “residuals” – the errors made by $F_0(x)$. For example, if $y$ is the true value and $F_0(x)$ is the prediction, the residual is $y - F_0(x)$.</li> <li>Train a new weak learner ($h_1(x)$) specifically to predict these residuals. Essentially, it learns to fix the mistakes of $F_0(x)$.</li> <li>Update the main model: $F_1(x) = F_0(x) + \nu \cdot h_1(x)$, where $\nu$ is a “learning rate” (a small number like 0.1) to prevent overfitting and make the learning process gradual.</li> <li>Repeat steps 2-4: calculate new residuals based on $F_1(x)$, train $h_2(x)$ to predict <em>these</em> residuals, and update the model to $F_2(x) = F_1(x) + \nu \cdot h_2(x)$, and so on.</li> </ol> <p>The name “Gradient Boosting” comes from the fact that instead of strictly predicting residuals, each new learner is trained to predict the <strong>negative gradient of the loss function</strong> with respect to the current ensemble’s predictions. This is a more general and powerful approach to minimizing any differentiable loss function.</p> <p><strong>Modern Powerhouses:</strong> XGBoost, LightGBM, and CatBoost are highly optimized and incredibly popular implementations of Gradient Boosting, often dominating Kaggle competitions and industry benchmarks due to their speed and performance.</p> <h4 id="3-stacking-stacked-generalization">3. Stacking (Stacked Generalization)</h4> <p>Now for the ultimate team-up! Imagine our movie critic panel again. Instead of just averaging their scores, what if we hired a “super-critic” whose <em>job</em> it was to listen to all the individual critics, understand their biases and strengths, and then make a final, refined prediction? This super-critic learns <em>how to combine</em> the individual predictions. That’s Stacking.</p> <p><strong>How it works:</strong> Stacking involves two levels of models:</p> <ol> <li> <strong>Level 0 (Base Models):</strong> You train several diverse base models (e.g., a Decision Tree, a Support Vector Machine, a Neural Network) on the training data.</li> <li> <strong>Level 1 (Meta-Learner):</strong> The predictions generated by these Level 0 models become the <em>input features</em> for a new, “meta-learner” model. The meta-learner is trained to learn the optimal way to combine these base predictions to make the final output.</li> </ol> <p><strong>Benefits:</strong> Stacking can often achieve superior performance because the meta-learner can learn complex relationships between the base models’ predictions, potentially correcting their systematic errors in ways that simple averaging or voting cannot.</p> <p><strong>Challenge:</strong> It’s more complex to implement and has a higher risk of overfitting, especially if the meta-learner is too complex or if cross-validation isn’t carefully used to generate the meta-features.</p> <h3 id="when-to-embrace-ensemble-learning">When to Embrace Ensemble Learning</h3> <p>You should consider ensemble methods when:</p> <ul> <li> <strong>Accuracy is paramount:</strong> When you need the absolute best predictive performance.</li> <li> <strong>Robustness is key:</strong> When you want a model that performs consistently well across different data variations and is less sensitive to noise or outliers.</li> <li> <strong>Single models struggle:</strong> If a single model (even after hyperparameter tuning) isn’t meeting your performance goals.</li> <li> <strong>Reducing overfitting/variance (Bagging) or bias (Boosting):</strong> Depending on your specific problem.</li> </ul> <h3 id="the-trade-offs-a-balanced-view">The Trade-offs: A Balanced View</h3> <p>While incredibly powerful, ensemble methods aren’t without their considerations:</p> <ul> <li> <strong>Computational Cost:</strong> Training multiple models can be much slower and require more computational resources than training a single model.</li> <li> <strong>Memory Usage:</strong> Storing multiple models can consume more memory.</li> <li> <strong>Interpretability:</strong> Ensembles are often “black boxes.” It’s harder to understand <em>why</em> an ensemble made a particular prediction compared to a single, simpler model.</li> <li> <strong>Complexity:</strong> More components mean more moving parts to manage and potentially more hyperparameters to tune.</li> </ul> <h3 id="conclusion-the-collective-powerhouse">Conclusion: The Collective Powerhouse</h3> <p>Ensemble Learning is a testament to the idea that diversity and collaboration often lead to superior outcomes. Whether it’s the parallel power of Bagging, the sequential error-correction of Boosting, or the sophisticated layering of Stacking, these techniques transform individual models into a collective powerhouse.</p> <p>As you continue your journey in data science, I encourage you to experiment with these methods. They are some of the most reliable tools in a machine learning engineer’s toolkit for achieving top-tier performance on real-world problems.</p> <p>So next time you’re building a model, don’t just rely on a lone genius; assemble a dream team!</p> <p>Happy modeling!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>