<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> My Journey into Decision Trees: Unpacking the Logic Behind Our Choices | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/my-journey-into-decision-trees-unpacking-the-logic/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">My Journey into Decision Trees: Unpacking the Logic Behind Our Choices</h1> <p class="post-meta"> Created on May 07, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/decision-trees"> <i class="fa-solid fa-hashtag fa-sm"></i> Decision Trees</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/classification"> <i class="fa-solid fa-hashtag fa-sm"></i> Classification</a>   <a href="/blog/blog/tag/regression"> <i class="fa-solid fa-hashtag fa-sm"></i> Regression</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hello there, fellow data explorer!</p> <p>Today, I want to share a piece of my personal learning journey that truly demystified a core concept in Machine Learning: <strong>Decision Trees</strong>. Remember those flowcharts we used in school to decide if we should bring an umbrella or if a plant needed watering? Decision Trees are essentially the algorithmic, super-powered version of that, and they’re incredibly intuitive once you peek under the hood.</p> <p>For me, the allure of machine learning always came from the idea of making computers “think.” But how do they <em>think</em>? How do they make decisions? Decision Trees, with their crisp, tree-like structure, offer one of the most transparent answers to this question. They’re like an open book, showing you exactly why a prediction was made. And that, my friends, is powerful.</p> <p>Let’s embark on this journey together.</p> <h3 id="what-exactly-is-a-decision-tree">What Exactly Is a Decision Tree?</h3> <p>Imagine you’re trying to decide if you should go out for a hike today. You’d probably ask yourself a series of questions:</p> <ol> <li>Is it raining? (If yes, probably no hike)</li> <li>Is it too cold? (If yes, no hike)</li> <li>Do I have time? (If no, no hike)</li> <li>…and so on.</li> </ol> <p>A Decision Tree works in much the same way. It’s a flowchart-like structure where:</p> <ul> <li> <strong>Internal nodes</strong> (the circles in our hike example) represent a “test” on an attribute (e.g., “Is it raining?”).</li> <li> <strong>Branches</strong> (the arrows) represent the outcome of that test (e.g., “yes” or “no”).</li> <li> <strong>Leaf nodes</strong> (the final boxes) represent a class label (for classification, e.g., “Hike” or “No Hike”) or a numerical value (for regression).</li> </ul> <p>The goal? To break down a dataset into smaller, more homogeneous (pure) subsets based on feature values, until you reach a decision.</p> <h3 id="the-art-of-splitting-how-decisions-are-made">The Art of Splitting: How Decisions Are Made</h3> <p>This is where the magic (and the math!) happens. Building a Decision Tree isn’t about randomly asking questions. It’s about asking the <em>best</em> question at each step to get us closer to a clear decision.</p> <p>Think about our hike example again. What’s the <em>most important</em> question? For me, it’s usually “Is it raining?” because if the answer is “yes,” the other questions almost don’t matter – I’m probably not going. This is the essence of “splitting” in Decision Trees. We want to find the feature and the split point that best divides our data into the purest possible groups.</p> <h4 id="purity-and-impurity-the-guiding-lights">Purity and Impurity: The Guiding Lights</h4> <p>In the world of Decision Trees, “purity” refers to how uniform the outcomes are within a node. If a node only contains “Hike” outcomes, it’s 100% pure. If it’s a mix of “Hike” and “No Hike,” it’s impure. Our algorithm’s mission is to maximize purity at each split.</p> <p>To quantify this, we use metrics of <em>impurity</em>:</p> <ol> <li> <p><strong>Gini Impurity (for Classification Trees):</strong> This is often the default choice and quite intuitive. Gini impurity measures the probability of incorrectly classifying a randomly chosen element from the dataset if it were randomly labeled according to the class distribution in the node. A Gini impurity of 0 means the node is perfectly pure (all elements belong to the same class).</p> <p>The formula for Gini Impurity for a node with $C$ classes is: \(G = 1 - \sum_{i=1}^{C} (p_i)^2\) Where $p_i$ is the proportion of observations belonging to class $i$ in that node.</p> <p>Let’s say a node has 10 samples: 7 “Hike” and 3 “No Hike”. $p_{Hike} = 7/10 = 0.7$ $p_{NoHike} = 3/10 = 0.3$ $G = 1 - (0.7)^2 - (0.3)^2 = 1 - 0.49 - 0.09 = 1 - 0.58 = 0.42$</p> <p>Now, if we split this node and get two new nodes:</p> <ul> <li>Node A: 5 “Hike”, 0 “No Hike” ($G_A = 1 - (1)^2 = 0$)</li> <li>Node B: 2 “Hike”, 3 “No Hike” ($G_B = 1 - (2/5)^2 - (3/5)^2 = 1 - 0.16 - 0.36 = 0.48$)</li> </ul> <p>We then calculate the weighted average Gini of the child nodes and compare it to the parent. The goal is to minimize this weighted average Gini or, equivalently, maximize the “Gini Gain.”</p> </li> <li> <p><strong>Entropy and Information Gain (for Classification Trees):</strong> Entropy, derived from information theory, measures the disorder or uncertainty in a node. Higher entropy means more uncertainty (more mixed classes). Like Gini, an entropy of 0 means perfect purity.</p> <p>The formula for Entropy is: \(E = - \sum_{i=1}^{C} p_i \log_2 (p_i)\) (We typically use $\log_2$ because we’re thinking about bits of information, but other bases can be used).</p> <p>Using our previous node (7 “Hike”, 3 “No Hike”): $E = - (0.7 \log_2(0.7) + 0.3 \log_2(0.3))$ $E \approx - (0.7 \times -0.51 + 0.3 \times -1.74)$ $E \approx - (-0.357 - 0.522) \approx 0.879$</p> <p>When we split a node, we want the split that results in the largest <strong>Information Gain (IG)</strong>. Information Gain is simply the reduction in entropy (or Gini impurity) after a dataset is split on an attribute. \(IG(T, A) = E(T) - \sum_{v \in Values(A)} \frac{|T_v|}{|T|} E(T_v)\) Where $T$ is the parent node, $A$ is the attribute being split, $T_v$ is the subset of $T$ where attribute $A$ has value $v$, and $|T_v|/|T|$ is the proportion of samples in $T_v$.</p> <p>The algorithm iterates through all possible features and all possible split points (for continuous features) to find the one that yields the highest Information Gain (or Gini Gain). This process is then repeated recursively for each child node until a stopping condition is met.</p> </li> <li> <p><strong>For Regression Trees:</strong> Decision Trees aren’t just for classification! They can predict continuous values too. For regression problems, instead of aiming for class purity, we aim to minimize the variance or Mean Squared Error (MSE) within each node. The split that results in the largest reduction in MSE (or variance) is chosen.</p> <p>For a node with $N$ samples and target values $y_i$, the MSE is: \(MSE = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y})^2\) Where $\hat{y}$ is the mean of the target values in that node. The prediction for a new sample falling into a leaf node is simply the average of the target values of the training samples in that leaf.</p> </li> </ol> <h3 id="a-simple-example-deciding-to-watch-a-movie">A Simple Example: Deciding to Watch a Movie</h3> <p>Let’s imagine we’re building a tiny Decision Tree to predict if a friend will like a movie based on a few attributes:</p> <table> <thead> <tr> <th style="text-align: left">Genre</th> <th style="text-align: left">Actor Popularity</th> <th style="text-align: left">Rating (1-5)</th> <th style="text-align: left">Friend Likes?</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Comedy</td> <td style="text-align: left">High</td> <td style="text-align: left">4</td> <td style="text-align: left">Yes</td> </tr> <tr> <td style="text-align: left">Action</td> <td style="text-align: left">High</td> <td style="text-align: left">3</td> <td style="text-align: left">Yes</td> </tr> <tr> <td style="text-align: left">Drama</td> <td style="text-align: left">Low</td> <td style="text-align: left">2</td> <td style="text-align: left">No</td> </tr> <tr> <td style="text-align: left">Comedy</td> <td style="text-align: left">Medium</td> <td style="text-align: left">5</td> <td style="text-align: left">Yes</td> </tr> <tr> <td style="text-align: left">Action</td> <td style="text-align: left">Low</td> <td style="text-align: left">1</td> <td style="text-align: left">No</td> </tr> <tr> <td style="text-align: left">Drama</td> <td style="text-align: left">High</td> <td style="text-align: left">3</td> <td style="text-align: left">Yes</td> </tr> </tbody> </table> <p><strong>Initial State:</strong> 4 “Yes”, 2 “No”. Let’s calculate initial Gini Impurity: $G_{root} = 1 - (4/6)^2 - (2/6)^2 = 1 - (0.667)^2 - (0.333)^2 = 1 - 0.444 - 0.111 = 0.445$</p> <p><strong>Split 1: By “Genre”</strong></p> <ul> <li> <strong>Comedy:</strong> (2 Yes, 0 No) -&gt; $G_{Comedy} = 0$ (Pure!)</li> <li> <strong>Action:</strong> (2 Yes, 0 No) -&gt; $G_{Action} = 0$ (Pure!)</li> <li> <strong>Drama:</strong> (0 Yes, 2 No) -&gt; $G_{Drama} = 0$ (Pure!)</li> </ul> <p>Weighted average Gini after splitting by Genre: $(2/6) \times 0 + (2/6) \times 0 + (2/6) \times 0 = 0$ Gini Gain = $0.445 - 0 = 0.445$ (Maximum possible gain!)</p> <p>In this simplified example, splitting by “Genre” gives us perfectly pure nodes. Our Decision Tree would stop here, with the root node splitting into three branches for Comedy, Action, and Drama, each leading to a leaf node with a definite “Yes” or “No” prediction.</p> <p>In a real-world scenario, you’d compare this gain with gains from splitting on “Actor Popularity” or “Rating” (e.g., Rating &lt; 3 vs. Rating &gt;= 3) and choose the best one.</p> <h3 id="the-perks-of-being-a-tree">The Perks of Being a Tree</h3> <p>Why do we love Decision Trees?</p> <ol> <li> <strong>Interpretability &amp; Explainability (White Box):</strong> This is huge! You can literally visualize the tree and understand <em>why</em> a particular decision was made. No black boxes here.</li> <li> <strong>Handles Various Data Types:</strong> They can naturally handle both numerical and categorical features.</li> <li> <strong>Minimal Data Preprocessing:</strong> Unlike many algorithms, Decision Trees don’t require feature scaling (like normalization or standardization).</li> <li> <strong>No Linearity Assumption:</strong> They can capture non-linear relationships between features and the target variable.</li> </ol> <h3 id="the-thorny-side-when-trees-get-overgrown">The Thorny Side: When Trees Get Overgrown</h3> <p>However, like any powerful tool, Decision Trees have their weaknesses:</p> <ol> <li> <strong>Overfitting:</strong> This is their biggest Achilles’ heel. A tree can become too complex, learning the noise in the training data rather than the underlying patterns. Imagine a tree that asks “Is the pixel at (X,Y) exactly this shade of blue?” for an image classification task – it’s too specific and won’t generalize to new images. This results in excellent performance on training data but poor performance on unseen data.</li> <li> <strong>Instability:</strong> Small changes in the training data can lead to a completely different tree structure. This makes them less robust.</li> <li> <strong>Bias towards Dominant Classes:</strong> If there’s a class imbalance, the tree might be biased towards the majority class.</li> <li> <strong>Local Optima:</strong> The greedy approach of choosing the best split at each step doesn’t guarantee a globally optimal tree.</li> </ol> <h3 id="taming-the-overgrown-tree-pruning-and-hyperparameters">Taming the Overgrown Tree: Pruning and Hyperparameters</h3> <p>To combat overfitting and improve generalization, we employ strategies like <strong>pruning</strong> and careful selection of <strong>hyperparameters</strong>:</p> <ol> <li> <strong>Pre-pruning (Early Stopping):</strong> We can stop the tree from growing too deep or complex in the first place. Common hyperparameters include: <ul> <li> <code class="language-plaintext highlighter-rouge">max_depth</code>: The maximum depth of the tree. A smaller depth prevents the tree from asking too many questions.</li> <li> <code class="language-plaintext highlighter-rouge">min_samples_split</code>: The minimum number of samples required to split an internal node. If a node has fewer samples than this, it won’t split further.</li> <li> <code class="language-plaintext highlighter-rouge">min_samples_leaf</code>: The minimum number of samples required to be at a leaf node. This ensures that leaves aren’t based on too few observations.</li> <li> <code class="language-plaintext highlighter-rouge">max_features</code>: The number of features to consider when looking for the best split.</li> </ul> </li> <li> <strong>Post-pruning (Cost-Complexity Pruning):</strong> This involves growing a full tree first and then removing (pruning) branches that don’t add significant value. It’s often more effective but computationally more expensive. The idea is to find a subtree that minimizes both the error rate and the number of nodes (cost-complexity).</li> </ol> <h3 id="beyond-a-single-tree-the-power-of-the-forest">Beyond a Single Tree: The Power of the Forest</h3> <p>While a single Decision Tree is fascinating, its limitations, particularly overfitting and instability, led to the development of more robust ensemble methods. Algorithms like <strong>Random Forests</strong> and <strong>Gradient Boosting</strong> leverage the power of <em>many</em> Decision Trees, trained strategically, to achieve much higher predictive accuracy and stability. We’ll save these exciting topics for another day, but it’s important to know that Decision Trees are the fundamental building blocks of some of the most powerful and widely used machine learning algorithms today.</p> <h3 id="wrapping-up-my-thoughts">Wrapping Up My Thoughts</h3> <p>Learning about Decision Trees was a pivotal moment in my understanding of machine learning. It provided a tangible, visual way to see how an algorithm “thinks” and makes decisions. They might not always be the most accurate models out of the box, especially when compared to complex neural networks, but their interpretability makes them invaluable for tasks where understanding <em>why</em> a prediction was made is as important as the prediction itself.</p> <p>So, the next time you’re faced with a tough decision, perhaps you’ll mentally draw a Decision Tree. And when you’re working with data, remember the elegant simplicity and profound power of this unassuming, yet incredibly effective, algorithm.</p> <p>Keep learning, keep exploring, and keep building! I hope this peek into the world of Decision Trees was as enlightening for you as it was for me. What’s your favorite aspect of Decision Trees? Let me know!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>