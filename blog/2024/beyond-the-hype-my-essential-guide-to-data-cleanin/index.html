<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Beyond the Hype: My Essential Guide to Data Cleaning Strategies | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/beyond-the-hype-my-essential-guide-to-data-cleanin/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Beyond the Hype: My Essential Guide to Data Cleaning Strategies</h1> <p class="post-meta"> Created on August 08, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/data-cleaning"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Cleaning</a>   <a href="/blog/blog/tag/data-preprocessing"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Preprocessing</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/data-quality"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Quality</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="beyond-the-hype-my-essential-guide-to-data-cleaning-strategies">Beyond the Hype: My Essential Guide to Data Cleaning Strategies</h2> <p>Hey everyone! You know, when you first get into data science or machine learning, the headlines are always about the latest groundbreaking AI model, deep learning architectures, or mind-bending algorithms. And don’t get me wrong, that stuff is incredibly exciting! It’s what often draws us into this field. But after spending some time building models and seeing them fail or succeed, I’ve come to realize something profound: the most glamorous part of the job isn’t always the most impactful. Often, the unsung hero, the quiet workhorse behind every robust model, is <strong>data cleaning</strong>.</p> <p>It sounds a bit mundane, doesn’t it? “Data cleaning.” It’s not as flashy as training a neural network or deploying a new AI. Yet, I’ve learned firsthand that <em>garbage in, garbage out</em> isn’t just a cliché; it’s a fundamental truth in our domain. You can have the most sophisticated algorithm in the world, but if your input data is flawed, inconsistent, or riddled with errors, your model will be, at best, mediocre, and at worst, completely misleading.</p> <p>In this post, I want to take you through my personal journey and strategies for tackling the often-messy reality of raw data. Think of this as a practical, behind-the-scenes look at how I approach turning chaotic datasets into clean, reliable foundations for powerful machine learning applications.</p> <h3 id="what-even-is-data-cleaning">What Even <em>Is</em> Data Cleaning?</h3> <p>Before we dive into the strategies, let’s briefly define what we’re talking about. Data cleaning, also known as data scrubbing or data wrangling, is the process of detecting and correcting (or removing) corrupt or inaccurate records from a dataset. It involves identifying incomplete, incorrect, inaccurate, or irrelevant parts of the data and then replacing, modifying, or deleting them. My goal is always to improve data quality, thereby increasing the accuracy, reliability, and effectiveness of any analysis or model built upon it.</p> <h3 id="the-rogues-gallery-common-data-quality-issues">The Rogues’ Gallery: Common Data Quality Issues</h3> <p>Over my projects, I’ve encountered a consistent set of villains in the data quality story. Recognizing them is the first step towards vanquishing them.</p> <ol> <li> <strong>Missing Data:</strong> The silent killer. Values that aren’t there when they should be.</li> <li> <strong>Inconsistent Data &amp; Duplicates:</strong> Different representations for the same entity (e.g., “NY” vs. “New York”) or identical records appearing multiple times.</li> <li> <strong>Outliers:</strong> Data points that significantly deviate from other observations. They can be genuine but extreme, or simply errors.</li> <li> <strong>Incorrect Data Types &amp; Formatting:</strong> Numbers stored as strings, dates in weird formats, or categorical data treated as numerical.</li> <li> <strong>Structural Errors:</strong> Typos, inconsistent naming conventions (e.g., <code class="language-plaintext highlighter-rouge">user_id</code> vs. <code class="language-plaintext highlighter-rouge">UserID</code>), or malformed records.</li> </ol> <p>Let’s roll up our sleeves and explore how I tackle each of these challenges.</p> <h3 id="my-toolkit-for-data-transformation-core-strategies">My Toolkit for Data Transformation: Core Strategies</h3> <h4 id="1-the-missing-piece-handling-missing-data">1. The Missing Piece: Handling Missing Data</h4> <p>Missing data is perhaps the most common issue I encounter. It can arise for many reasons: data entry errors, system failures, privacy concerns, or simply values not being applicable. Dealing with it effectively is crucial because most machine learning algorithms can’t handle <code class="language-plaintext highlighter-rouge">NaN</code> (Not a Number) values directly.</p> <p><strong>My Strategies:</strong></p> <ul> <li> <strong>Deletion (When to Consider):</strong> <ul> <li> <strong>Row-wise Deletion:</strong> If a row has too many missing values, or if I have a very large dataset and only a tiny fraction of rows have missing data, I might delete the entire row. This is usually my last resort for fear of losing valuable information.</li> <li> <strong>Column-wise Deletion:</strong> If a feature (column) has an overwhelming percentage of missing values (say, &gt;70-80%), it might not be useful. I’ll consider dropping the entire column.</li> <li> <strong>Caveat:</strong> Deleting data can lead to information loss and introduce bias if the missingness isn’t random.</li> </ul> </li> <li> <strong>Imputation (Filling the Gaps):</strong> This is where I spend most of my effort. Imputation means estimating and filling in the missing values. <ul> <li> <strong>Simple Imputation:</strong> <ul> <li> <strong>Mean/Median/Mode:</strong> For numerical data, I often replace missing values with the mean or median of the existing data in that column. The median is more robust to outliers. For categorical data, the mode (most frequent value) is my go-to. <ul> <li> <em>Example for Mean:</em> If a feature $X$ has missing values, I might replace them with its mean: $\bar{x} = \frac{1}{n}\sum_{i=1}^{n} x_i$.</li> </ul> </li> <li> <strong>Constant Value:</strong> Sometimes, replacing with a specific constant like 0, -1, or “Unknown” makes sense, especially for categorical data where the absence of a value might carry meaning.</li> </ul> </li> <li> <strong>Advanced Imputation:</strong> <ul> <li> <strong>Forward/Backward Fill (for Time Series):</strong> In time-series data, I often carry the last observed value forward (<code class="language-plaintext highlighter-rouge">ffill</code>) or the next observed value backward (<code class="language-plaintext highlighter-rouge">bfill</code>). This assumes that the value doesn’t change drastically over short periods.</li> <li> <strong>Regression Imputation:</strong> If a feature’s missingness is correlated with other features, I can build a predictive model (e.g., linear regression) using the existing features to predict the missing values. It’s more complex but can yield better estimates.</li> <li> <strong>K-Nearest Neighbors (K-NN) Imputation:</strong> This method finds the <code class="language-plaintext highlighter-rouge">k</code> most similar complete rows to the row with missing data and uses their values to impute. It’s powerful as it considers the structure of the data.</li> </ul> </li> </ul> </li> </ul> <p>My choice of imputation strategy depends heavily on the data type, the percentage of missing values, and the context of the problem. Always remember to impute <em>after</em> splitting your data into training and testing sets to avoid data leakage!</p> <h4 id="2-the-identity-crisis-tackling-inconsistent-data--duplicates">2. The Identity Crisis: Tackling Inconsistent Data &amp; Duplicates</h4> <p>This category is all about ensuring uniformity and uniqueness in my dataset. Inconsistent data can arise from human error, different data sources, or poor data entry systems.</p> <p><strong>My Strategies:</strong></p> <ul> <li> <strong>Standardization &amp; Normalization:</strong> <ul> <li> <strong>Text Data:</strong> I often convert all text to lowercase, remove extra whitespace, and correct common misspellings (e.g., “usa”, “USA”, “U.S.A.” all become “usa”). Regular expressions (<code class="language-plaintext highlighter-rouge">re</code> module in Python) are invaluable here.</li> <li> <strong>Numerical Data:</strong> For machine learning, scaling numerical features (e.g., Min-Max scaling or Z-score normalization) is common. This isn’t strictly “cleaning” but ensures consistency in feature ranges.</li> </ul> </li> <li> <strong>Handling Categorical Inconsistencies:</strong> <ul> <li> <strong>Mapping:</strong> If I have categories like “Male”, “M”, “male”, I’ll map them all to a single consistent form like “Male”. Python dictionaries are perfect for this.</li> <li> <strong>Fuzzy Matching:</strong> For more complex text inconsistencies (e.g., “Microsoft Corp.” vs. “Microsoft Corporation”), libraries like <code class="language-plaintext highlighter-rouge">fuzzywuzzy</code> can help identify and group similar strings.</li> </ul> </li> <li> <strong>Deduplication:</strong> <ul> <li>Identifying and removing duplicate rows is straightforward with Pandas’ <code class="language-plaintext highlighter-rouge">df.drop_duplicates()</code>. But first, I need to define what constitutes a duplicate. Is it identical values across all columns, or just a subset of key identifiers? I always check for exact duplicates and then consider partial duplicates based on unique identifiers if any.</li> </ul> </li> </ul> <h4 id="3-the-maverick-dealing-with-outliers">3. The Maverick: Dealing with Outliers</h4> <p>Outliers are data points that lie an abnormal distance from other values. They can significantly skew statistical analyses and impact model performance. Sometimes they are genuine, extreme observations; other times, they are simply errors.</p> <p><strong>My Strategies:</strong></p> <ul> <li> <strong>Detection:</strong> <ul> <li> <strong>Visual Inspection:</strong> Histograms, box plots, and scatter plots are my first tools. They quickly highlight unusual data points.</li> <li> <strong>Statistical Methods:</strong> <ul> <li> <table> <tbody> <tr> <td> <strong>Z-score:</strong> For normally distributed data, a Z-score measures how many standard deviations an observation is from the mean. Values with $</td> <td>Z</td> <td>&gt; 3$ (or sometimes 2) are often considered outliers.</td> </tr> </tbody> </table> <ul> <li>$Z = \frac{x - \mu}{\sigma}$ (where $\mu$ is the mean and $\sigma$ is the standard deviation).</li> </ul> </li> <li> <strong>Interquartile Range (IQR):</strong> This is more robust to skewed data. I calculate $IQR = Q_3 - Q_1$ (where $Q_3$ is the 75th percentile and $Q_1$ is the 25th percentile). Outliers are typically identified as values below $Q_1 - 1.5 \times IQR$ or above $Q_3 + 1.5 \times IQR$.</li> </ul> </li> <li> <strong>Model-Based Methods:</strong> More advanced techniques like Isolation Forests or One-Class SVMs can detect multivariate outliers, which is useful when outliers aren’t obvious in single features.</li> </ul> </li> <li> <strong>Handling:</strong> <ul> <li> <strong>Removal:</strong> If I’m confident an outlier is due to data entry error or measurement error, I might remove it. This is a cautious step, as removing genuine extreme values can lead to a loss of information and potentially bias the model.</li> <li> <strong>Transformation:</strong> Log transformation or square root transformation can reduce the impact of outliers by compressing the range of values.</li> <li> <strong>Capping (Winsorization):</strong> I might replace extreme outlier values with a value at a certain percentile (e.g., 99th or 1st percentile). This keeps the data point but limits its extreme influence.</li> <li> <strong>Treating as Missing:</strong> Sometimes, I’ll convert outliers to <code class="language-plaintext highlighter-rouge">NaN</code> and then apply one of the imputation strategies.</li> </ul> </li> </ul> <p>My decision here is heavily influenced by the domain knowledge and the potential impact on the business problem.</p> <h4 id="4-the-mismatch-correcting-incorrect-data-types--formatting">4. The Mismatch: Correcting Incorrect Data Types &amp; Formatting</h4> <p>This seems basic, but it’s a constant battle. Data often comes in with incorrect data types or inconsistent formatting, preventing proper analysis or model training.</p> <p><strong>My Strategies:</strong></p> <ul> <li> <strong>Type Conversion:</strong> <ul> <li> <strong>Numeric:</strong> Ensuring numerical columns are actually numbers (integers or floats) is crucial. I use <code class="language-plaintext highlighter-rouge">pd.to_numeric()</code> in Pandas, often with <code class="language-plaintext highlighter-rouge">errors='coerce'</code> to turn unconvertible values into <code class="language-plaintext highlighter-rouge">NaN</code> for later imputation.</li> <li> <strong>Dates:</strong> Dates are notorious! They can be <code class="language-plaintext highlighter-rouge">YYYY-MM-DD</code>, <code class="language-plaintext highlighter-rouge">MM/DD/YY</code>, <code class="language-plaintext highlighter-rouge">DD-Mon-YYYY</code>, etc. I use <code class="language-plaintext highlighter-rouge">pd.to_datetime()</code> to standardize them into a single format, making it easier to extract features like year, month, or day of the week.</li> <li> <strong>Categorical:</strong> If a column has a limited number of unique string values, I’ll convert it to a <code class="language-plaintext highlighter-rouge">category</code> data type in Pandas. This saves memory and can speed up operations.</li> </ul> </li> <li> <strong>String Manipulation:</strong> <ul> <li>Removing unwanted characters (e.g., currency symbols, ‘%’ signs) from numerical strings before conversion.</li> <li>Splitting or combining text fields (e.g., separating “First Name Last Name” into two columns).</li> </ul> </li> </ul> <h4 id="5-the-architecture-flaw-resolving-structural-errors">5. The Architecture Flaw: Resolving Structural Errors</h4> <p>Structural errors are often about how the dataset itself is organized or presented.</p> <p><strong>My Strategies:</strong></p> <ul> <li> <strong>Column Renaming:</strong> Ensuring consistent, clear, and descriptive column names (e.g., converting “Sales Amt” to “sales_amount”).</li> <li> <strong>Merging &amp; Joining:</strong> If data is spread across multiple tables, correctly merging or joining them based on common keys is a critical step to create a unified dataset.</li> <li> <strong>Reshaping Data:</strong> Sometimes, data might be in a “wide” format when a “long” format is needed (or vice-versa), especially for time-series or panel data. Pandas’ <code class="language-plaintext highlighter-rouge">pivot</code>, <code class="language-plaintext highlighter-rouge">melt</code>, and <code class="language-plaintext highlighter-rouge">stack</code>/<code class="language-plaintext highlighter-rouge">unstack</code> functions are indispensable here.</li> <li> <strong>Labeling Consistency:</strong> Ensuring that all categories within a categorical variable are correctly spelled and grouped.</li> </ul> <h3 id="my-data-cleaning-workflow-best-practices">My Data Cleaning Workflow: Best Practices</h3> <ol> <li> <strong>Exploratory Data Analysis (EDA) First:</strong> I never jump straight into cleaning. I always start with extensive EDA to understand the data’s structure, identify potential issues visually, and gain domain insights. This includes looking at distributions, unique values, correlations, and summary statistics.</li> <li> <strong>Document Everything:</strong> As I clean, I keep a detailed log of all transformations, deletions, and imputations. This makes my work reproducible and transparent.</li> <li> <strong>Iterative Process:</strong> Data cleaning isn’t a one-and-done task. It’s an iterative process. I clean a bit, re-evaluate, perform more EDA, and clean more. New issues often surface after initial fixes.</li> <li> <strong>Version Control:</strong> I treat my cleaning scripts like any other code. Git is essential to track changes and revert if something goes wrong.</li> <li> <strong>Small Batches &amp; Testing:</strong> When applying a new cleaning rule, I often test it on a small subset of the data first, then apply it broadly, always checking the results.</li> </ol> <h3 id="the-tools-of-my-trade-python-focus">The Tools of My Trade (Python Focus)</h3> <ul> <li> <strong>Pandas:</strong> The absolute bedrock. For almost everything: data loading, inspection, manipulation, type conversion, missing value handling, filtering, and aggregation.</li> <li> <strong>NumPy:</strong> Often works hand-in-hand with Pandas, especially for numerical operations and handling <code class="language-plaintext highlighter-rouge">NaN</code> values.</li> <li> <strong>Scikit-learn (Impute module):</strong> Offers excellent tools for imputation, like <code class="language-plaintext highlighter-rouge">SimpleImputer</code> and <code class="language-plaintext highlighter-rouge">KNNImputer</code>.</li> <li> <strong>Matplotlib &amp; Seaborn:</strong> For visual EDA to spot anomalies and understand distributions.</li> <li> <strong>Regular Expressions (<code class="language-plaintext highlighter-rouge">re</code> module):</strong> Indispensable for complex string pattern matching and cleaning.</li> </ul> <h3 id="conclusion-embrace-the-mess-create-the-magic">Conclusion: Embrace the Mess, Create the Magic</h3> <p>Data cleaning might not get the same fanfare as building the next groundbreaking AI model, but I’ve found it to be one of the most critical and rewarding aspects of any data science project. It’s where you truly get to know your data, understand its quirks, and transform it from a raw, unruly mess into a refined, reliable foundation.</p> <p>By meticulously cleaning your data, you’re not just fixing errors; you’re actively enhancing the quality of your insights, increasing the robustness of your models, and ultimately, building more trustworthy and impactful solutions. So, next time you dive into a new dataset, embrace the mess, put on your cleaning gloves, and get ready to create some real magic! Your future self (and your models) will thank you.</p> <p>What are your favorite data cleaning tricks or toughest challenges? Share them in the comments!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>