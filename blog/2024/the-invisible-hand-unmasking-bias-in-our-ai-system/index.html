<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Invisible Hand: Unmasking Bias in Our AI Systems | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/the-invisible-hand-unmasking-bias-in-our-ai-system/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Invisible Hand: Unmasking Bias in Our AI Systems</h1> <p class="post-meta"> Created on April 20, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/ai-ethics"> <i class="fa-solid fa-hashtag fa-sm"></i> AI Ethics</a>   <a href="/blog/blog/tag/bias"> <i class="fa-solid fa-hashtag fa-sm"></i> Bias</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/responsible-ai"> <i class="fa-solid fa-hashtag fa-sm"></i> Responsible AI</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey there, future innovators and curious minds!</p> <p>I remember the first time I truly “got” machine learning. It felt like magic – algorithms learning from data, making predictions, seeing patterns no human could. The promise was immense: impartial decisions, unbiased insights, a future where AI could transcend human error and prejudice. For a long time, I envisioned AI as a purely objective force, a digital judge weighing facts without emotion or preconception.</p> <p>But then, as I delved deeper into the field, I started noticing cracks in this pristine vision. Stories emerged: facial recognition systems failing to identify people of color, hiring algorithms favoring men, loan applications being unfairly rejected for certain demographics. It hit me like a ton of bricks: <strong>our AI, despite its mathematical purity, can be incredibly biased.</strong></p> <p>This isn’t about malicious programmers intentionally building discriminatory systems (though that can happen). This is about something far more insidious and subtle, an “invisible hand” guiding our algorithms towards unfair outcomes. Understanding this bias, identifying its sources, and developing strategies to mitigate it is not just a technical challenge; it’s an ethical imperative. It’s about building a future where AI genuinely serves <em>everyone</em>.</p> <h3 id="what-exactly-is-bias-in-machine-learning">What Exactly Is Bias in Machine Learning?</h3> <p>When we talk about “bias” in everyday language, we often mean prejudice or a preconceived notion against someone or something. In statistics and machine learning, “bias” has a slightly broader meaning. Statistically, bias refers to the tendency of an estimator to consistently deviate from the true value. For example, if we consistently overestimate a value, our estimator is biased.</p> <p>But in the context of AI ethics, when we talk about <strong>bias in machine learning</strong>, we’re primarily referring to systematic and unfair discrimination against certain individuals or groups in the outcomes of an AI system. This discrimination often stems from underlying societal inequalities reflected in the data the AI learns from, or from decisions made during the AI’s development.</p> <p>It’s crucial to understand that AI doesn’t <em>invent</em> bias. Instead, it often <em>learns, amplifies, and perpetuates</em> existing biases present in our world and our data. Think of an AI as a mirror: if the world it reflects is distorted, the AI’s reflection will also be distorted.</p> <h3 id="where-does-the-invisible-hand-creep-in-the-lifecycle-of-a-model">Where Does the Invisible Hand Creep In? (The Lifecycle of a Model)</h3> <p>Bias isn’t just one thing; it’s a multi-headed hydra that can sneak into every stage of an AI’s lifecycle. Let’s trace its path:</p> <h4 id="1-data-collection-the-foundation-of-all-bias">1. Data Collection: The Foundation of All Bias</h4> <p>This is, by far, the most significant source of bias. Our data is a snapshot of the world, and if that snapshot is incomplete, unrepresentative, or reflects historical inequalities, our AI will absorb those flaws.</p> <ul> <li> <strong>Historical Bias (Societal Bias):</strong> Our world isn’t perfectly fair. Historical and societal prejudices (e.g., gender roles, racial discrimination) mean that past data often reflects discriminatory practices. If an AI learns from old hiring data where men were disproportionately hired for leadership roles, it might conclude that “male” is a strong predictor for “leader,” perpetuating the bias.</li> <li> <strong>Selection Bias/Sampling Bias:</strong> This occurs when the data used to train the model does not accurately represent the population it will be used on. <ul> <li> <strong>Example:</strong> Imagine training a facial recognition system primarily on images of lighter-skinned men because that’s what’s readily available in public datasets. When deployed, this system will naturally perform poorly on women or people of color, leading to higher error rates and potential misidentification.</li> <li> <strong>Another Example:</strong> A medical diagnostic AI trained mostly on data from a specific demographic (e.g., men over 50) might miss crucial signs of disease in other groups (e.g., younger women), leading to misdiagnosis.</li> </ul> </li> <li> <strong>Underrepresentation Bias:</strong> A specific type of selection bias where certain groups are simply not present enough in the dataset for the model to learn effectively about them. This leads to the model “ignoring” or performing poorly on these groups.</li> </ul> <h4 id="2-data-preprocessing-cleaning-up-or-messing-up">2. Data Preprocessing: Cleaning Up, or Messing Up?</h4> <p>Even after collecting data, how we prepare it can introduce new biases or exacerbate existing ones.</p> <ul> <li> <strong>Labeling Bias:</strong> This happens when the human annotators who label data introduce their own biases. <ul> <li> <strong>Example:</strong> In sentiment analysis, if annotators from a particular cultural background consistently label sarcasm as negative, an AI trained on this data might struggle with sarcasm from other cultural contexts.</li> <li> <strong>Example:</strong> When labeling images, if annotators associate certain activities with specific genders (e.g., “cooking” with women), the AI will learn these stereotypical associations.</li> </ul> </li> <li> <strong>Measurement Bias:</strong> Inconsistent or inaccurate ways of measuring features across different groups. <ul> <li> <strong>Example:</strong> Using income as a proxy for “creditworthiness” might disproportionately disadvantage groups historically subjected to economic oppression, even if their actual repayment risk is similar.</li> </ul> </li> </ul> <h4 id="3-algorithm-design-the-blueprint-matters">3. Algorithm Design: The Blueprint Matters</h4> <p>While algorithms themselves are mathematical, the choices we make in designing them can influence bias.</p> <ul> <li> <strong>Algorithmic Bias (Inductive Bias):</strong> Every algorithm makes assumptions about the data it will encounter – this is its “inductive bias.” Sometimes, these assumptions can unintentionally lead to unfair outcomes. <ul> <li> <strong>Example:</strong> A model might prioritize overall accuracy across the entire dataset, potentially sacrificing accuracy for minority groups if their misclassifications have less impact on the overall score.</li> </ul> </li> <li> <strong>Feature Selection/Engineering Bias:</strong> Deciding which features to include or exclude, and how to create new features, can introduce bias. If we remove a feature that is truly predictive for a minority group, or if we create a feature that indirectly encodes a protected attribute, we might be building in bias.</li> </ul> <h4 id="4-model-evaluation--deployment-the-last-mile">4. Model Evaluation &amp; Deployment: The Last Mile</h4> <p>Even after building what seems like a fair model, the way we evaluate and deploy it can perpetuate bias.</p> <ul> <li> <strong>Evaluation Bias:</strong> If we evaluate our models using metrics that don’t account for fairness across different groups, we might miss significant disparities. A model with 90% accuracy might be 95% accurate for the majority group but only 70% accurate for a minority group.</li> <li> <strong>Deployment Bias (Systemic Bias in Application):</strong> How the model is actually used in the real world can also introduce bias. If a predictive policing model, even if “fair” in its predictions, is deployed in a way that disproportionately targets specific neighborhoods for increased surveillance, it can perpetuate systemic injustices.</li> </ul> <h3 id="why-should-we-care-the-real-world-impact">Why Should We Care? The Real-World Impact</h3> <p>Understanding bias isn’t just an academic exercise. Biased AI systems have real, tangible, and often devastating consequences:</p> <ul> <li> <strong>Healthcare:</strong> AI diagnostics showing lower accuracy for certain racial or gender groups, leading to misdiagnosis or delayed treatment.</li> <li> <strong>Criminal Justice:</strong> Predictive policing models disproportionately targeting minority neighborhoods, or risk assessment tools leading to harsher sentences for certain demographics.</li> <li> <strong>Hiring &amp; Employment:</strong> Algorithms screening resumes that implicitly penalize female names or certain educational backgrounds, perpetuating gender or racial imbalances in industries.</li> <li> <strong>Financial Services:</strong> Loan or credit approval systems that unfairly deny services based on zip codes or other proxies for protected characteristics.</li> <li> <strong>Social Media &amp; News:</strong> Recommendation systems that reinforce harmful stereotypes or create “filter bubbles” of biased information.</li> </ul> <p>These aren’t hypothetical scenarios; they are happening right now, shaping our society in subtle yet powerful ways.</p> <h3 id="how-do-we-fight-back-strategies-for-mitigation">How Do We Fight Back? Strategies for Mitigation</h3> <p>Combating bias in ML is a complex, ongoing challenge that requires a multi-faceted approach. There’s no single magic bullet, but rather a combination of technical rigor, ethical awareness, and diverse perspectives.</p> <h4 id="1-data-centric-approaches-fixing-the-source">1. Data-Centric Approaches (Fixing the Source)</h4> <p>Since data is often the biggest culprit, focusing on it is crucial.</p> <ul> <li> <strong>Diverse and Representative Data Collection:</strong> Actively seek out and collect data that accurately represents all groups the model will interact with. This might mean oversampling minority groups or intentionally diversifying data sources.</li> <li> <strong>Bias Detection and Auditing:</strong> Use statistical tools and domain expertise to identify biases in your raw data and labels <em>before</em> training. Look for imbalances, missing values correlated with specific groups, or strong associations that reflect stereotypes.</li> <li> <strong>Data Augmentation and Re-sampling:</strong> Techniques like synthetic data generation or re-weighting data points can help balance datasets and reduce the impact of underrepresented groups.</li> <li> <strong>Fairness Metrics:</strong> Beyond overall accuracy, we need to evaluate models on specific fairness metrics across different groups. For example: <ul> <li> <table> <tbody> <tr> <td> <strong>Disparate Impact:</strong> This measures whether the proportion of favorable outcomes is similar across groups. Mathematically, for two groups $G_1$ and $G_2$, we might check if the ratio of positive outcomes $P(\hat{Y}=1</td> <td>G=g_1) / P(\hat{Y}=1</td> <td>G=g_2)$ is within a certain range (e.g., 0.8 to 1.25, known as the “four-fifths rule”).</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td> <strong>Equalized Odds:</strong> This focuses on equal true positive rates and true negative rates across groups. For example, $P(\hat{Y}=1</td> <td>Y=1, G=g_1) = P(\hat{Y}=1</td> <td>Y=1, G=g_2)$ (equal opportunity for true positives).</td> </tr> </tbody> </table> </li> <li>There are many others, each with different implications for fairness. The choice of metric depends on the specific context and ethical considerations.</li> </ul> </li> </ul> <h4 id="2-algorithmic-approaches-fairness-by-design">2. Algorithmic Approaches (Fairness by Design)</h4> <p>Researchers are developing new algorithms and techniques to make models inherently fairer.</p> <ul> <li> <strong>Fairness-aware Algorithms:</strong> These are algorithms specifically designed to minimize bias during training. This might involve adding a “fairness constraint” to the loss function during optimization, effectively telling the model: “Be accurate, but also be fair across these groups.”</li> <li> <strong>Regularization for Fairness:</strong> Similar to how L1/L2 regularization prevents overfitting, we can add fairness-related regularization terms to discourage the model from learning discriminatory patterns.</li> <li> <strong>Adversarial Debiasing:</strong> Using an adversarial network to try and “fool” the main model into not learning sensitive attributes, thereby reducing bias.</li> <li> <strong>Causal Inference:</strong> Moving beyond mere correlation to understand causal relationships can help us identify and mitigate sources of bias more effectively.</li> </ul> <h4 id="3-human-in-the-loop--ethical-oversight-the-big-picture">3. Human-in-the-Loop &amp; Ethical Oversight (The Big Picture)</h4> <p>Technology alone isn’t enough. Human judgment and ethical frameworks are indispensable.</p> <ul> <li> <strong>Continuous Monitoring and Auditing:</strong> Bias isn’t a one-time fix. Models can drift, and new biases can emerge. Regular monitoring of model performance across different groups is essential.</li> <li> <strong>Interdisciplinary Teams:</strong> Data scientists shouldn’t work in a vacuum. Collaborating with ethicists, sociologists, legal experts, and community representatives can provide crucial insights and perspectives to identify and address bias.</li> <li> <strong>Transparency and Explainability (XAI):</strong> Understanding <em>why</em> an AI makes a particular decision (e.g., using LIME or SHAP values) can help us uncover hidden biases. If a loan algorithm consistently prioritizes a certain background, XAI can help surface that.</li> <li> <strong>Ethical Guidelines and Regulations:</strong> Developing clear ethical principles and potentially regulatory frameworks for AI development and deployment can guide responsible innovation.</li> <li> <strong>Bias Bounties/Red Teaming:</strong> Actively seeking out biases in models, perhaps by hiring external teams to try and “break” the fairness of a system, similar to bug bounties.</li> </ul> <h3 id="a-glimpse-into-the-math-disparate-impact-ratio">A Glimpse into the Math: Disparate Impact Ratio</h3> <p>Let’s quickly touch on a simple concept often used to detect bias: the <strong>Disparate Impact Ratio</strong>.</p> <p>Imagine we have a loan approval model ($\hat{Y}=1$ for approval, $\hat{Y}=0$ for rejection) and two demographic groups, $G_A$ and $G_B$. We want to see if the approval rate for one group is significantly lower than for the other.</p> <p>We can calculate the approval probability for each group:</p> <ul> <li> <table> <tbody> <tr> <td>$P(\hat{Y}=1</td> <td>G=G_A)$ = Probability of approval for group A</td> </tr> </tbody> </table> </li> <li> <table> <tbody> <tr> <td>$P(\hat{Y}=1</td> <td>G=G_B)$ = Probability of approval for group B</td> </tr> </tbody> </table> </li> </ul> <p>The Disparate Impact Ratio (DIR) is then: $DIR = \frac{P(\hat{Y}=1 | G=G_A)}{P(\hat{Y}=1 | G=G_B)}$</p> <p>If $DIR$ is significantly less than 1 (e.g., less than 0.8, the “four-fifths rule”), it suggests that group A is being disparately impacted compared to group B. This is just one of many mathematical ways to quantify and detect unfairness. The challenge, of course, is that there isn’t one universal definition of “fairness,” and different metrics capture different aspects.</p> <h3 id="conclusion-the-ongoing-journey">Conclusion: The Ongoing Journey</h3> <p>The journey to build truly fair and unbiased AI systems is challenging, but incredibly rewarding. It requires vigilance, critical thinking, and a commitment to ethical principles. As future data scientists and machine learning engineers, you are not just building models; you are shaping the future. You have the power to create systems that amplify human potential, or systems that reinforce existing inequalities.</p> <p>Let’s choose to be the architects of a more equitable future. Let’s learn to recognize the invisible hand of bias, understand its origins, and equip ourselves with the tools and mindset to counteract it. The magic of machine learning should be accessible and beneficial to <em>everyone</em>. It’s a responsibility we all share, and it starts with understanding.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>