<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Alchemist's Secret: Unlocking Superpowers in Your Data with Feature Engineering | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/blog/2024/the-alchemists-secret-unlocking-superpowers-in-you/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://adarshnair.online" rel="external nofollow noopener" target="_blank"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/about/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="https://adarshnair.online/books/" rel="external nofollow noopener" target="_blank">books </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Alchemist's Secret: Unlocking Superpowers in Your Data with Feature Engineering</h1> <p class="post-meta"> Created on April 04, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/blog/tag/feature-engineering"> <i class="fa-solid fa-hashtag fa-sm"></i> Feature Engineering</a>   <a href="/blog/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   <a href="/blog/blog/tag/python"> <i class="fa-solid fa-hashtag fa-sm"></i> Python</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Hey fellow data explorers and aspiring ML alchemists!</p> <p>Have you ever looked at a raw dataset and felt a mix of excitement and overwhelm? It’s like staring at a pile of Lego bricks – you know there’s potential for an amazing creation, but it’s not immediately obvious how to build it. This feeling, my friends, is where one of the most crucial, yet often underestimated, superpowers in data science comes into play: <strong>Feature Engineering</strong>.</p> <p>Think of it this way: Machine Learning models are like very eager, very smart students. You give them information, and they learn patterns. But what if the information you give them isn’t in the best format? What if it’s incomplete, convoluted, or just plain <em>raw</em>? That’s where Feature Engineering steps in. It’s the art and science of transforming raw data into meaningful and useful features that models can learn from more effectively.</p> <h3 id="why-bother-the-garbage-in-garbage-out-principle">Why Bother? The “Garbage In, Garbage Out” Principle</h3> <p>You’ve probably heard the saying “Garbage in, garbage out” (GIGO). In machine learning, this isn’t just a catchy phrase; it’s a fundamental truth. A fancy, complex deep learning model fed with poorly chosen or poorly structured features will often perform worse than a simple linear model given well-engineered features.</p> <p>My early days in data science were a testament to this. I’d spend hours tweaking hyper-parameters, trying different algorithms, and wondering why my model wasn’t improving. Then, I’d read about someone who added a simple <em>interaction term</em> or <em>extracted a weekday from a timestamp</em>, and suddenly their model’s performance would skyrocket. That’s when the lightbulb truly went off for me: <strong>Feature Engineering is often the biggest lever you can pull for model improvement.</strong></p> <p>It’s about making your data speak the model’s language more clearly. It’s about injecting domain knowledge, creativity, and a touch of intuition into the numbers.</p> <h3 id="the-alchemists-toolkit-common-feature-engineering-techniques">The Alchemist’s Toolkit: Common Feature Engineering Techniques</h3> <p>Let’s get our hands dirty and explore some of the common transformations and creations we can perform.</p> <h4 id="1-crafting-insights-from-numerical-features">1. Crafting Insights from Numerical Features</h4> <p>Numerical data seems straightforward, right? Just numbers. But oh, the secrets they can hide!</p> <ul> <li> <strong>Binning (or Discretization):</strong> Sometimes, the exact value of a continuous variable isn’t as important as the <em>range</em> it falls into. <ul> <li> <strong>Example:</strong> Instead of <code class="language-plaintext highlighter-rouge">age</code> (e.g., 25, 31, 48), we might bin it into categories like <code class="language-plaintext highlighter-rouge">young</code>, <code class="language-plaintext highlighter-rouge">adult</code>, <code class="language-plaintext highlighter-rouge">senior</code>. This can help models capture non-linear relationships or make them more robust to outliers. If a model expects a linear relationship, binning can linearize it.</li> <li> <strong>Code Idea:</strong> <code class="language-plaintext highlighter-rouge">pd.cut(df['age'], bins=[0, 18, 65, 100], labels=['child', 'adult', 'senior'])</code> </li> </ul> </li> <li> <strong>Transformations:</strong> Data often doesn’t follow a neat normal distribution. Skewed data can sometimes mislead models. <ul> <li> <strong>Log Transform:</strong> A common technique for highly skewed data, like income or sales figures. <code class="language-plaintext highlighter-rouge">$ log(x) $</code> can compress a wide range of values and make the distribution more symmetrical, which is beneficial for models that assume normality (like linear regression).</li> <li> <strong>Power Transforms (Square Root, Cube Root, etc.):</strong> Similar to log transforms, these can also help stabilize variance and normalize distributions.</li> <li> <strong>Polynomial Features:</strong> If you suspect a non-linear relationship between a feature and your target, you can create polynomial terms. For a feature <code class="language-plaintext highlighter-rouge">$ x $</code>, you might add <code class="language-plaintext highlighter-rouge">$ x^2 $</code>, <code class="language-plaintext highlighter-rouge">$ x^3 $</code>, etc. This allows linear models to capture curved relationships.</li> </ul> </li> <li> <strong>Interaction Features:</strong> Sometimes, the combination of two features tells a different story than each feature alone. <ul> <li> <strong>Example:</strong> Predicting house prices. <code class="language-plaintext highlighter-rouge">Number of Bedrooms</code> and <code class="language-plaintext highlighter-rouge">Square Footage</code> are important, but <code class="language-plaintext highlighter-rouge">Square Footage per Bedroom</code> (<code class="language-plaintext highlighter-rouge">$ \frac{Square Footage}{Number of Bedrooms} $</code>) might be even more indicative of spaciousness or luxury.</li> <li> <strong>Code Idea:</strong> <code class="language-plaintext highlighter-rouge">df['sqft_per_bedroom'] = df['Square Footage'] / df['Number of Bedrooms']</code> </li> </ul> </li> </ul> <h4 id="2-decoding-categorical-secrets">2. Decoding Categorical Secrets</h4> <p>Categorical data (like <code class="language-plaintext highlighter-rouge">color</code>, <code class="language-plaintext highlighter-rouge">city</code>, <code class="language-plaintext highlighter-rouge">product_type</code>) needs special handling because models primarily understand numbers.</p> <ul> <li> <strong>One-Hot Encoding:</strong> This is your go-to for nominal (unordered) categorical features. It creates a new binary (0 or 1) column for each unique category. <ul> <li> <strong>Example:</strong> If you have <code class="language-plaintext highlighter-rouge">color = ['red', 'blue', 'green']</code>, One-Hot Encoding creates <code class="language-plaintext highlighter-rouge">is_red</code>, <code class="language-plaintext highlighter-rouge">is_blue</code>, <code class="language-plaintext highlighter-rouge">is_green</code> columns. If an observation is ‘red’, <code class="language-plaintext highlighter-rouge">is_red</code> will be 1, others 0.</li> <li> <strong>Why?</strong> Prevents the model from assuming an artificial order or magnitude between categories. <code class="language-plaintext highlighter-rouge">$ 1 &lt; 2 &lt; 3 $</code>, but <code class="language-plaintext highlighter-rouge">red &lt; blue &lt; green</code> doesn’t make sense.</li> <li> <strong>Caution:</strong> Can lead to a high number of features (the “curse of dimensionality”) if a category has many unique values.</li> <li> <strong>Code Idea:</strong> <code class="language-plaintext highlighter-rouge">pd.get_dummies(df['color'])</code> </li> </ul> </li> <li> <strong>Label Encoding (or Ordinal Encoding):</strong> Used when there <em>is</em> an inherent order (ordinality) in your categories. <ul> <li> <strong>Example:</strong> <code class="language-plaintext highlighter-rouge">t-shirt_size = ['small', 'medium', 'large']</code> could be encoded as <code class="language-plaintext highlighter-rouge">0, 1, 2</code>.</li> <li> <strong>Why?</strong> Preserves the order, reducing the number of features compared to one-hot encoding.</li> <li> <strong>Caution:</strong> Don’t use if there’s no logical order, as the model will misinterpret the numerical relationship.</li> <li> <strong>Code Idea:</strong> <code class="language-plaintext highlighter-rouge">from sklearn.preprocessing import OrdinalEncoder</code> </li> </ul> </li> <li> <strong>Target Encoding (or Mean Encoding):</strong> This advanced technique replaces a category with the mean of the target variable for that category. <ul> <li> <strong>Example:</strong> If predicting house prices, replace <code class="language-plaintext highlighter-rouge">city</code> with the <code class="language-plaintext highlighter-rouge">average_house_price_in_that_city</code>.</li> <li> <strong>Why?</strong> Can be incredibly powerful as it directly injects information about the target into the feature.</li> <li> <strong>Caution:</strong> Highly susceptible to data leakage if not done carefully (e.g., using the target mean calculated from the <em>entire</em> dataset during training). Always use proper cross-validation or holdout sets for target encoding.</li> </ul> </li> </ul> <h4 id="3-time-traveling-with-date-and-time-features">3. Time Traveling with Date and Time Features</h4> <p>Dates and times are a goldmine of information, often overlooked as just a single column.</p> <ul> <li> <strong>Extracting Components:</strong> Break down a timestamp into its constituent parts: <ul> <li> <code class="language-plaintext highlighter-rouge">year</code>, <code class="language-plaintext highlighter-rouge">month</code>, <code class="language-plaintext highlighter-rouge">day_of_week</code>, <code class="language-plaintext highlighter-rouge">day_of_year</code>, <code class="language-plaintext highlighter-rouge">hour</code>, <code class="language-plaintext highlighter-rouge">minute</code>, <code class="language-plaintext highlighter-rouge">second</code>.</li> <li> <code class="language-plaintext highlighter-rouge">is_weekend</code>, <code class="language-plaintext highlighter-rouge">is_holiday</code>.</li> <li> <strong>Why?</strong> Sales often peak on weekends, traffic at certain hours, energy consumption varies by season.</li> <li> <strong>Code Idea:</strong> <code class="language-plaintext highlighter-rouge">df['timestamp'].dt.month</code>, <code class="language-plaintext highlighter-rouge">df['timestamp'].dt.day_of_week</code> </li> </ul> </li> <li> <strong>Cyclical Features:</strong> For things like <code class="language-plaintext highlighter-rouge">month</code> or <code class="language-plaintext highlighter-rouge">day_of_week</code>, simply encoding them as <code class="language-plaintext highlighter-rouge">1, 2, ..., 12</code> (for month) implies a linear relationship. But January (1) is closer to December (12) than to July (7). We can use sine and cosine transformations to capture this cyclical nature. <ul> <li><code class="language-plaintext highlighter-rouge">$ sin(\frac{2\pi \times month}{12}) $</code></li> <li><code class="language-plaintext highlighter-rouge">$ cos(\frac{2\pi \times month}{12}) $</code></li> <li>This creates two features that represent the position on a circle, preserving the continuity between the beginning and end of a cycle.</li> </ul> </li> <li> <strong>Time Differences:</strong> The elapsed time between two events can be a powerful predictor. <ul> <li> <strong>Example:</strong> <code class="language-plaintext highlighter-rouge">time_since_last_purchase</code>, <code class="language-plaintext highlighter-rouge">duration_of_call</code>.</li> </ul> </li> </ul> <h4 id="4-unearthing-insights-from-text-data-a-glimpse">4. Unearthing Insights from Text Data (A Glimpse)</h4> <p>Text is complex, but even simple features can be effective.</p> <ul> <li> <strong>Length &amp; Count Metrics:</strong> <code class="language-plaintext highlighter-rouge">word_count</code>, <code class="language-plaintext highlighter-rouge">char_count</code>, <code class="language-plaintext highlighter-rouge">average_word_length</code>, <code class="language-plaintext highlighter-rouge">number_of_punctuations</code>.</li> <li> <strong>Presence Flags:</strong> <code class="language-plaintext highlighter-rouge">has_link</code>, <code class="language-plaintext highlighter-rouge">has_question_mark</code>, <code class="language-plaintext highlighter-rouge">is_all_caps</code>.</li> <li> <strong>TF-IDF (Term Frequency-Inverse Document Frequency):</strong> This technique quantifies how important a word is to a document in a collection of documents. A word appearing frequently in one document but rarely across the whole corpus gets a high TF-IDF score, indicating its importance.</li> <li> <strong>Word Embeddings:</strong> (Advanced) Representing words as dense numerical vectors where semantically similar words are close to each other in the vector space.</li> </ul> <h3 id="the-feature-engineering-workflow-my-personal-journal">The Feature Engineering Workflow: My Personal Journal</h3> <p>Feature Engineering isn’t a one-and-done step; it’s an iterative process, much like a detective piecing together clues.</p> <ol> <li> <strong>Exploratory Data Analysis (EDA):</strong> This is where I spend a <em>lot</em> of time. Visualizing distributions, scatter plots, correlation matrices, and cross-tabulations. This phase helps me understand the raw data’s characteristics, identify outliers, missing values, and potential relationships with the target variable. It’s like staring at the raw ingredients, trying to imagine what meal they could become.</li> <li> <strong>Hypothesis Generation:</strong> Based on EDA and my domain knowledge (or research), I start brainstorming. “What if I combine these two columns?” “Could the square of this variable explain the target better?” “Does the day of the week matter here?”</li> <li> <strong>Implementation:</strong> I use tools like <code class="language-plaintext highlighter-rouge">Pandas</code> for data manipulation and <code class="language-plaintext highlighter-rouge">Scikit-learn</code>’s transformers to create these new features. This is where the code meets the concept.</li> <li> <strong>Model Training &amp; Evaluation:</strong> I train a baseline model with the new features and evaluate its performance. Did it improve? Did it get worse?</li> <li> <strong>Iteration &amp; Refinement:</strong> This is the most crucial part. If the model improved, great! Can I do more? If not, why? Maybe the feature wasn’t useful, or I need to try a different transformation. It’s a loop of trying, testing, and refining.</li> </ol> <h3 id="the-pitfalls-dont-fall-into-the-traps">The Pitfalls: Don’t Fall into the Traps!</h3> <p>Even the most seasoned alchemists face challenges.</p> <ul> <li> <strong>Data Leakage:</strong> This is the silent killer. It happens when you inadvertently use information from the target variable that wouldn’t be available at prediction time to create a feature. For example, calculating the mean of the target <em>across the entire dataset</em> and using it to encode a categorical feature during training without proper cross-validation. Your model will perform <em>amazingly</em> on your training data, but spectacularly fail in the real world.</li> <li> <strong>Overfitting:</strong> Creating too many complex, highly specific features can lead your model to memorize the training data, rather than learn generalizable patterns. It’s like teaching a student to only solve the exact problems from the textbook, not the underlying concepts.</li> <li> <strong>Computational Cost:</strong> More features generally mean longer training times and more memory usage. There’s a balance to strike between complexity and efficiency.</li> </ul> <h3 id="conclusion-the-art-science-and-craft-of-data-alchemy">Conclusion: The Art, Science, and Craft of Data Alchemy</h3> <p>Feature Engineering is truly where the magic happens in machine learning. It’s not just about applying formulas; it’s about understanding your data, understanding your problem, and creatively transforming information into powerful signals. It’s a blend of statistical thinking, domain expertise, programming skills, and a healthy dose of curiosity.</p> <p>It requires patience, experimentation, and a willingness to get things wrong before you get them right. But when you hit upon that perfect feature that unlocks significant model improvement, it feels like discovering a hidden treasure.</p> <p>So, next time you face a raw dataset, don’t just jump to model building. Put on your alchemist’s hat, grab your tools, and start exploring how you can engineer some truly super-powered features. Your models (and your portfolio) will thank you!</p> <p>Keep exploring, keep learning, and keep engineering!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>