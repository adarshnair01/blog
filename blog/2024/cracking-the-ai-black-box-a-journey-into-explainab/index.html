<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Cracking the AI Black Box: A Journey into Explainable AI (XAI) | Adarsh Nair </title> <meta name="author" content="Adarsh Nair"> <meta name="description" content="A deep dive into machine learning, AI, and data science. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/blog/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/blog/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/blog/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/blog/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adarshnair.online/blog/blog/2024/cracking-the-ai-black-box-a-journey-into-explainab/"> <script src="/blog/assets/js/theme.js?v=5fea5159b787642c1bbc1f334d60f883"></script> <link defer rel="stylesheet" href="/blog/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/blog/cv/"> Adarsh Nair </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/blog/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/cv/">CV </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Cracking the AI Black Box: A Journey into Explainable AI (XAI)</h1> <p class="post-meta"> Created on December 12, 2024 by Adarsh Nair </p> <p class="post-tags"> <a href="/blog/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/blog/tag/explainable-ai"> <i class="fa-solid fa-hashtag fa-sm"></i> Explainable AI</a>   <a href="/blog/blog/tag/xai"> <i class="fa-solid fa-hashtag fa-sm"></i> XAI</a>   <a href="/blog/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Machine Learning</a>   <a href="/blog/blog/tag/interpretability"> <i class="fa-solid fa-hashtag fa-sm"></i> Interpretability</a>   <a href="/blog/blog/tag/ai-ethics"> <i class="fa-solid fa-hashtag fa-sm"></i> AI Ethics</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="cracking-the-ai-black-box-a-journey-into-explainable-ai-xai">Cracking the AI Black Box: A Journey into Explainable AI (XAI)</h2> <p>Hey there! If you’re anything like me, you’ve probably been captivated by the sheer power of Artificial Intelligence. From recommending your next favorite song to diagnosing diseases, AI is everywhere. It feels like magic sometimes, doesn’t it? But then, there’s that nagging question: <em>How</em> did it do that? Why did it recommend <em>this</em> song, or predict <em>that</em> outcome? For a long time, the answer was often a shrug and “the model just knows.” This, my friends, is the infamous “AI black box” problem, and it’s what led me down the exciting path of Explainable AI (XAI).</p> <p>I remember the first time I built a really complex deep learning model. It achieved amazing accuracy on a task I thought was impossible. I was ecstatic! But when a stakeholder asked, “Can you show me <em>why</em> it made that specific prediction for this one instance?” I fumbled. My model was a brilliant, opaque oracle. It just…worked. And that’s when it hit me: power without understanding can be dangerous, frustrating, and ultimately, limiting.</p> <h3 id="the-great-ai-mystery-why-do-we-need-explanations">The Great AI Mystery: Why Do We Need Explanations?</h3> <p>Imagine going to a doctor who tells you, “You have X disease, and I’m prescribing Y drug.” You’d probably ask, “Why? What are the symptoms leading to this diagnosis? What are the side effects of Y?” A good doctor explains. Now imagine an AI loan officer denies your application without any reason. Or an AI in a self-driving car makes an unexpected turn that causes an accident, and no one can figure out <em>why</em>. These aren’t hypothetical scenarios; they’re real challenges we face as AI becomes more integrated into our lives.</p> <p>So, why is understanding crucial?</p> <ol> <li> <strong>Building Trust and Adoption:</strong> If we don’t understand how an AI works, how can we trust it, especially in high-stakes fields like medicine, finance, or criminal justice? Transparency fosters confidence.</li> <li> <strong>Ensuring Fairness and Mitigating Bias:</strong> AI models can, inadvertently, learn biases present in historical data. An AI might deny loans to certain demographics not because of credit risk, but because the training data reflected past discriminatory practices. XAI helps us identify and correct these biases.</li> <li> <strong>Debugging and Improving Models:</strong> When an AI makes a mistake, how do we fix it if we don’t know the root cause? Explanations help us pinpoint erroneous feature interactions or data issues.</li> <li> <strong>Regulatory Compliance:</strong> New regulations, like GDPR in Europe, grant individuals the “right to explanation” for algorithmic decisions that significantly affect them.</li> <li> <strong>Scientific Discovery and Learning:</strong> Sometimes, AI can uncover patterns or relationships in data that human experts missed. By explaining <em>what</em> it learned, AI can augment human intelligence and lead to new scientific insights.</li> </ol> <p>This is where Explainable AI (XAI) steps in. At its core, XAI is a set of techniques and methodologies designed to make AI models more understandable to humans. It’s about pulling back the curtain on the black box.</p> <h3 id="interpretability-vs-explainability-a-subtle-but-important-distinction">Interpretability vs. Explainability: A Subtle but Important Distinction</h3> <p>Before we dive into the cool tools, let’s quickly clarify two terms often used interchangeably:</p> <ul> <li> <strong>Interpretability:</strong> This refers to the <em>degree to which a human can understand the cause and effect</em> of a model’s decisions. For example, a simple linear regression model is highly interpretable because you can directly see how each feature contributes to the output.</li> <li> <strong>Explainability:</strong> This is about providing <em>human-understandable reasons</em> for a specific prediction made by a model. It’s about answering “why did <em>this specific decision</em> happen?”</li> </ul> <p>While related, the distinction is important. Some models are inherently interpretable (like decision trees), while for others (like deep neural networks), we need post-hoc explanation techniques to generate explanations.</p> <h3 id="peeking-inside-how-xai-techniques-work">Peeking Inside: How XAI Techniques Work</h3> <p>So, how do we actually get these “explanations”? XAI methods generally fall into a few categories:</p> <h4 id="1-local-explanations-understanding-individual-predictions">1. Local Explanations: Understanding Individual Predictions</h4> <p>This is often the most requested type of explanation: “Why did the model make <em>this particular decision</em> for <em>this specific input</em>?”</p> <h5 id="a-lime-local-interpretable-model-agnostic-explanations">a) LIME: Local Interpretable Model-agnostic Explanations</h5> <p>Imagine trying to understand why your friend recommended a specific movie. You wouldn’t ask them to explain their entire movie-watching history, right? You’d ask about <em>this movie</em>: “What parts of this movie made you like it?”</p> <p>LIME (Local Interpretable Model-agnostic Explanations) works similarly. For a specific prediction from a black-box model, LIME:</p> <ol> <li> <strong>Perturbs the input data</strong> slightly around that specific data point.</li> <li> <strong>Gets predictions</strong> from the black-box model for these perturbed samples.</li> <li> <strong>Trains a simple, interpretable model</strong> (like a linear regression or a decision tree) on these perturbed samples, weighted by their proximity to the original data point. This local, simpler model then acts as an explanation for the complex model’s behavior <em>in that specific vicinity</em>.</li> </ol> <p>Think of it like this: even if you can’t understand the entire global landscape of a mountain range (your complex model), you can understand the immediate terrain around your current position using a simple map (your local, interpretable model).</p> <p>Mathematically, LIME tries to find an explanation model $g$ that approximates the black-box model $f$ locally around a specific instance $x$:</p> <p>$ \xi(x) = \text{argmin}_{g \in G} \mathcal{L}(f, g, \pi_x) + \Omega(g) $</p> <p>Where:</p> <ul> <li>$\mathcal{L}$ is a loss function measuring how well $g$ approximates $f$ on the perturbed samples.</li> <li>$\pi_x$ is a measure of proximity to the original instance $x$.</li> <li>$\Omega(g)$ is a measure of the complexity of the explanation model $g$ (we want it simple!).</li> </ul> <p>LIME is <em>model-agnostic</em>, meaning it can be applied to <em>any</em> black-box model, which is a huge strength.</p> <h5 id="b-shap-shapley-additive-explanations">b) SHAP: SHapley Additive exPlanations</h5> <p>If LIME is like asking “what parts of this movie made you like it?”, SHAP is like asking “how much <em>credit</em> does each actor deserve for the movie’s success?” SHAP values come from cooperative game theory, specifically the concept of Shapley values.</p> <p>In simple terms, SHAP assigns each feature an “importance value” (a SHAP value) for a particular prediction. This value represents how much that feature <em>contributed</em> to pushing the prediction from the average prediction to the actual prediction for that instance. It does this by considering all possible combinations (coalitions) of features and calculating the marginal contribution of a feature when it’s added to any coalition.</p> <p>The core idea behind SHAP is an additive feature attribution model:</p> <p>$ g(z’) = \phi_0 + \sum_{i=1}^M \phi_i z’_i $</p> <p>Where:</p> <ul> <li>$g$ is the explanation model.</li> <li>$z’$ is a simplified input (e.g., a binary vector indicating presence or absence of a feature).</li> <li>$\phi_0$ is the base value (the expected output of the model when no features are present).</li> <li>$\phi_i$ are the SHAP values for each feature $i$, representing its contribution to the prediction.</li> </ul> <p>SHAP offers a unified framework for interpreting any model, providing both local explanations (for individual predictions) and global insights (by aggregating SHAP values across many predictions). It’s widely considered a gold standard in XAI due to its strong theoretical foundations.</p> <h4 id="2-global-explanations-understanding-the-overall-model-behavior">2. Global Explanations: Understanding the Overall Model Behavior</h4> <p>While local explanations are great for individual cases, sometimes we want to understand the <em>general trends</em> or overall behavior of our model.</p> <h5 id="a-feature-importance">a) Feature Importance</h5> <p>This is perhaps the simplest and most common global explanation. Many tree-based models (like Random Forests or Gradient Boosting Machines) naturally provide feature importance scores, indicating which features were most influential <em>across all predictions</em>. While useful, it doesn’t tell us the <em>direction</em> of the influence (e.g., does a higher value of this feature increase or decrease the prediction?).</p> <h5 id="b-partial-dependence-plots-pdps">b) Partial Dependence Plots (PDPs)</h5> <p>PDPs show the marginal effect of one or two features on the predicted outcome of a model. They illustrate how the predicted outcome changes on average when we vary the value of a specific feature, while holding all other features constant (by averaging them out).</p> <p>Imagine you’re predicting house prices. A PDP for “square footage” would show you how the predicted price changes as square footage increases, assuming typical values for other features like number of bedrooms or location.</p> <p>$ \hat{f}<em>S(x_S) = E</em>{x_C} [\hat{f}(x_S, x_C)] = \int \hat{f}(x_S, x_C) dP_{x_C}(x_C) $</p> <p>This formula represents the expected prediction value when we fix a subset of features $S$ to $x_S$ and average over the remaining features $C$. PDPs are fantastic for understanding global relationships.</p> <h5 id="c-individual-conditional-expectation-ice-plots">c) Individual Conditional Expectation (ICE) Plots</h5> <p>While PDPs show the <em>average</em> effect, ICE plots show how the prediction for <em>each individual instance</em> changes as you vary a feature. This is useful because the average effect shown by a PDP might mask heterogeneous effects (e.g., increasing square footage increases house price for some neighborhoods but has little effect in others). ICE plots can reveal these nuanced interactions.</p> <h3 id="challenges-and-the-road-ahead">Challenges and The Road Ahead</h3> <p>XAI is a rapidly evolving field, and it’s not without its challenges:</p> <ul> <li> <strong>Fidelity vs. Interpretability</strong>: There’s often a trade-off. The most accurate models (like deep neural networks) tend to be the least interpretable. XAI methods try to bridge this gap, but the explanation itself is an approximation. Is the explanation faithful to the model’s true logic?</li> <li> <strong>Human-Centered Explanations</strong>: An explanation that’s mathematically sound might still be incomprehensible to a human user. XAI needs to consider cognitive science and user experience.</li> <li> <strong>Computational Cost</strong>: Generating explanations for complex models, especially with methods like SHAP that involve many perturbations, can be computationally expensive.</li> <li> <strong>Misinformation and Manipulation</strong>: Can XAI explanations be manipulated to hide bias or mislead users? This raises important ethical considerations.</li> </ul> <p>Despite these hurdles, XAI is undeniably crucial. It’s pushing the boundaries of what AI can achieve, making it not just powerful, but also responsible, trustworthy, and understandable.</p> <h3 id="my-take-ais-future-is-transparent">My Take: AI’s Future is Transparent</h3> <p>For anyone building or interacting with AI, understanding XAI isn’t just a technical skill; it’s a critical mindset. It empowers us to debug, to improve, to ensure fairness, and ultimately, to trust the intelligent systems we create. From my own portfolio projects, integrating XAI tools like LIME and SHAP has transformed how I approach model deployment. It’s no longer just about optimizing a metric; it’s about answering “why?”</p> <p>As future data scientists, machine learning engineers, or even just technically-minded citizens, embracing XAI means you’re not just building smart machines, you’re building <em>wise</em> machines – ones that can articulate their reasoning and stand up to scrutiny. The black box is slowly but surely being demystified, and that, to me, is incredibly exciting!</p> <p>What are your thoughts on AI explanations? Have you encountered situations where knowing the “why” was critical? Let me know in the comments below!</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/cracking-the-ai-black-box-why-explainable-ai-xai-i/">Cracking the AI Black Box: Why Explainable AI (XAI) is Our Superpower</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/the-secret-life-of-states-how-markov-chains-predic/">The Secret Life of States: How Markov Chains Predict Our Next Move (Without Remembering the Past!)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/blog/2026/gradient-descent-unpacking-the-engine-of-machine-l/">Gradient Descent: Unpacking the Engine of Machine Learning</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adarsh Nair. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/blog/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script src="/blog/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/blog/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/blog/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/blog/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/blog/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/blog/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/blog/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/blog/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/blog/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/blog/assets/js/search-data.js"></script> <script src="/blog/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>